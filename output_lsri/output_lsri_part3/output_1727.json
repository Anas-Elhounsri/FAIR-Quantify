{
  "application_domain": [
    {
      "confidence": 45.89,
      "result": {
        "type": "String",
        "value": "Natural Language Processing"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citation <a name=\"citation\"></a>",
        "parent_header": [
          "CpG Transformer"
        ],
        "type": "Text_excerpt",
        "value": "If you find this repository useful in your research, please cite our [paper](https://www.biorxiv.org/content/10.1101/2021.06.08.447547v2).\n```bibtex\n@article{dewaele2022cpg,\n  title={CpG Transformer for imputation of single-cell methylomes},\n  author={De Waele, Gaetan and Clauwaert, Jim and Menschaert, Gerben and Waegeman, Willem},\n  journal={Bioinformatics},\n  volume={38},\n  number={3},\n  pages={597--603},\n  year={2022},\n  publisher={Oxford University Press}\n}\n```\n\n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "De Waele, Gaetan and Clauwaert, Jim and Menschaert, Gerben and Waegeman, Willem",
        "format": "bibtex",
        "title": "CpG Transformer for imputation of single-cell methylomes",
        "type": "Text_excerpt",
        "value": "@article{dewaele2022cpg,\n    publisher = {Oxford University Press},\n    year = {2022},\n    pages = {597--603},\n    number = {3},\n    volume = {38},\n    journal = {Bioinformatics},\n    author = {De Waele, Gaetan and Clauwaert, Jim and Menschaert, Gerben and Waegeman, Willem},\n    title = {CpG Transformer for imputation of single-cell methylomes},\n}"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/gdewael/cpg-transformer"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2021-05-07T13:46:20Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-28T12:13:27Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "CpG Transformer for imputation of single-cell methylomes"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9496135055313925,
      "result": {
        "original_header": "CpG Transformer",
        "type": "Text_excerpt",
        "value": "This repository contains code, pre-trained models and instructions on how to use CpG Transformer ([published paper link](https://doi.org/10.1093/bioinformatics/btab746))\nfor imputation of single-cell methylomes. \n**New from 24-11-2021 onwards:** CpG Transformer now supports continuous methylation calls as input, for which it will train a regression model in the same fashion as described in our publication, but then with the mean-squared error as loss function. Performances for regression have not been benchmarked as of yet. \n<details><summary>Table of contents</summary>\n  \n- [Comparison of single-cell methylome imputation performance](#perf-comp)\n- [Installation](#install)\n- [Usage](#usage)\n  - [Quick Start](#quickstart)\n  - [Input formatting](#input)\n  - [Training](#train)\n  - [Imputation and denoising](#impute)\n  - [Benchmarking](#benchmark)\n  - [Interpretation](#interpret)\n- [Pre-trained models](#pretrained)\n- [Citation](#citation)\n- [License](#license)\n</details> \n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9425783979051128,
      "result": {
        "original_header": "Comparison of single-cell methylome imputation performance <a name=\"perf-comp\"></a>",
        "type": "Text_excerpt",
        "value": "\n\\* Results obtained with reproduced, optimized code, also found in this repository. \n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.96322341068398,
      "result": {
        "original_header": "Pre-trained models <a name=\"pretrained\"></a>",
        "type": "Text_excerpt",
        "value": "Pre-trained CpG Transformer models for all tested [datasets](#perf-comp) are available as PyTorch model state dicts in `data/model_checkpoints/`.\n \n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/gdewael/cpg-transformer/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/notebooks/train_cpg_transformer.ipynb"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/notebooks/train_cpg_transformer.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/notebooks/impute_cpg_transformer.ipynb"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/notebooks/impute_cpg_transformer.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/data/genomic-contexts/genomic_contexts_data.ipynb"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/data/genomic-contexts/genomic_contexts_data.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 10
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/gdewael/cpg-transformer/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "gdewael/cpg-transformer"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "CpG Transformer"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://colab.research.google.com/assets/colab-badge.svg"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://colab.research.google.com/assets/colab-badge.svg"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation <a name=\"install\"></a>",
        "parent_header": [
          "CpG Transformer"
        ],
        "type": "Text_excerpt",
        "value": "CpG Transformer is implemented in [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning).\n\nIf you have one or more GPUs on your machine, we recommend running CpG Transformer locally using a conda environment.\nMake sure to install the correct version of [PyTorch](https://pytorch.org/get-started/locally/) (using the cuda version that is installed on your system).\nThe following shows an example installation process for a system running CUDA 11.1:\n\n```bash\ngit clone https://github.com/gdewael/cpg-transformer.git\ncd cpg-transformer\nconda create --name cpgtransformer python=3.9\nsource activate cpgtransformer\npip install -r requirements.txt\n```\n\nIn case CpG Transformer loses backwards compatibility with more-recent versions of PyTorch and PyTorch Lightning: this repo has been tested with up to Python 3.9, PyTorch 1.10, PyTorch Lightning 1.5\n\nFor CaMelia training and imputation, additionally do:\n```bash\npip install catboost\n```\n\nIf your machine does not have a GPU, we provide Google Colab transfer learning and imputation notebooks that run on Google cloud resources. (see [Quick Start](#quickstart)).\n\n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/gdewael/cpg-transformer/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "MIT License\n\nCopyright (c) 2021 gdewael\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/LICENSE",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License <a name=\"license\"></a>",
        "parent_header": [
          "CpG Transformer"
        ],
        "type": "Text_excerpt",
        "value": "This source code is licensed under the MIT license found in the `LICENSE` file in the root directory of this source tree.\n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "cpg-transformer"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "gdewael"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 140351,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 41648,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Awk",
        "size": 577,
        "type": "Programming_language",
        "value": "Awk"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_documentation": [
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "https://pytorch-lightning.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-06 06:02:49",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 36
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Quick Start  <a name=\"quickstart\"></a>",
        "parent_header": [
          "CpG Transformer",
          "Usage <a name=\"usage\"></a>"
        ],
        "type": "Text_excerpt",
        "value": "\nTo quickly test out CpG Transformer, we provide Google Drive access to the preprocessed files for the Ser dataset, which can be downloaded [here](https://drive.google.com/drive/folders/1zNvyOX0F0ztDFEsgwaeTdsxJYo0_fQgg).\n\n```bash\n# Train a CpG Transformer model\npython train_cpg_transformer.py X_ser.npz y_ser.npz pos_ser.npz --gpus 1 # train from scratch with one gpu\npython train_cpg_transformer.py X_ser.npz y_ser.npz pos_ser.npz --gpus 2 --accelerator ddp # train with multiple gpus\npython train_cpg_transformer.py X_ser.npz y_ser.npz pos_ser.npz --gpus 1 --transfer_checkpoint data/model_checkpoints/Ser_model.pt # transfer learning\n\n# Impute a dataset with a trained model\npython impute_genome.py cpg_transformer X_ser.npz y_ser.npz pos_ser.npz output_ser.npz --model_checkpoint path/to/saved/model.ckpt\n```\n\nWe additionally provide Google Colab notebooks for those with no local GPU resources:\n- Training: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gdewael/cpg-transformer/blob/main/notebooks/train_cpg_transformer.ipynb)\n- Imputation: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gdewael/cpg-transformer/blob/main/notebooks/impute_cpg_transformer.ipynb)\n\n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Input formatting  <a name=\"input\"></a>",
        "parent_header": [
          "CpG Transformer",
          "Usage <a name=\"usage\"></a>"
        ],
        "type": "Text_excerpt",
        "value": "CpG Transformer uses NumPy `.npz` zipped archive files as inputs. More specifically, 3 input files are necessary:\n\n**(1)** `X.npz`. An encoded genome as a dictionary of NumPy arrays. Every key-value pair corresponds to a chromosome, with the key the name of the chromosome (e.g.: `'chr1'`) and the value a 1D NumPy array of encoded sequence (e.g.: `np.array([0,2,2,3,...,1,1,2])`). Sequences are encoded according to:\n```\n{'A': 0, 'T': 1, 'C': 2, 'G': 3, 'N': 4,\n'M': 5, 'R': 6, 'W': 7, 'S': 8, 'Y': 9,\n'K': 10, 'V': 11, 'H': 12, 'D': 13,\n'B': 14, 'X': 15}\n```\n\n**(2)** `y.npz`. A partially observed methylation matrix as a dictionary of NumPy arrays. Every key-value pair corresponds to a chromosome, with the key the name of the chromosome (e.g.: `'chr1'`) and the value a 2D NumPy array corresponding to the methylation matrix for that chromosome. Every methylation matrix is a `# sites * # cells` matrix with every element at row `i` and column `j` denoting the methylation state of CpG site `i` of cell `j`. Methylation states are encoded by `-1 = unknown`, `0 = unmethylated`, `1 = methylated`. For training, we recommend only including CpG sites where at least one cell has an observed state, as columns without observation confer no useful information when training. Note that CpG Transformer only accepts forwards strand methylation states. If your methylation calls are recorded for both strands separately, you should combine them to the forward strand. **New:** for continuous methylation calls, methylation states should be encoded as values `-1 = unknown`, or `[0...1] = methylation frequency`.\n\n**(3)** `pos.npz`. Positions (0-indexed) of all input CpG sites as a dictionary of NumPy arrays. Every key-value pair corresponds to a chromosome, with the key the name of the chromosome (e.g.: `'chr1'`) and the value a 1D NumPy array corresponding to the locations of all profiled CpG sites in that chromosome (columns in the second input).\n\nExample:\n\n```python\n>>> X['chr1'].shape\n(197195432,)\n>>> X['chr1']\narray([4, 4, 4, ..., 1, 1, 2], dtype=int8)\n\n>>> y['chr1'].shape\n(1190072, 20)\n>>> y['chr1']\narray([[-1,  1, -1, ..., -1,  1, -1],\n       [-1,  1, -1, ...,  1, -1, -1],\n       [-1,  0, -1, ...,  0, -1, -1],\n       ...,\n       [-1, -1, -1, ..., -1, -1, -1],\n       [-1, -1, -1, ..., -1, -1, -1],\n       [-1, -1, -1, ..., -1, -1, -1]], dtype=int8)\n\n>>> pos['chr1'].shape\n(1190072,)\n>>> pos['chr1']\narray([  3000573,   3000725,   3000900, ..., 197194914, 197194986,\n       197195054], dtype=int32)\n```\n\nFor the datasets used in our paper, we provide template preprocessing scripts in the `data` folder, along with [instructions](https://github.com/gdewael/cpg-transformer/tree/main/data#readme) on where to download all relevant data. In addition, we provide a simple script for .tsv file inputs. (see the [README](https://github.com/gdewael/cpg-transformer/tree/main/data#readme)).\n\n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Training <a name=\"train\"></a>",
        "parent_header": [
          "CpG Transformer",
          "Usage <a name=\"usage\"></a>"
        ],
        "type": "Text_excerpt",
        "value": "Separate training scripts are provided for training CpG Transformer models (`train_cpg_transformer.py`), DeepCpG (`train_deepcpg.py`) and CaMelia (`train_camelia.py`). In the following, only the arguments to CpG Transformer will be shown. For all scripts, arguments and their explanations can be accessed with the `-h` help flag.\n\nArguments to CpG Transformer are split into 4 groups: (1) DataModule arguments concern how the data will be preprocessed and loaded for the model to use, (2) Model arguments concern model architecture and training, (3) Logging arguments determine how the training process can be followed and where model weights will be saved and (4) pl.Trainer arguments list all arguments to the PyTorch Lightning trainer object. Most of these arguments are not applicable to standard use of CpG Transformer but are kept in for full flexibility. For more information on pl.Trainer we refer its [documentation](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#).\n\n\n<details><summary>All train_cpg_transformer.py flags</summary>\n\n```\nusage: train_cpg_transformer.py [-h] [--segment_size int] [--fracs float [float ...]] [--mask_p float]\n                                [--mask_random_p float] [--resample_cells int] [--resample_cells_val int]\n                                [--val_keys str [str ...]] [--test_keys str [str ...]] [--batch_size int]\n                                [--n_workers int] [--transfer_checkpoint str] [--RF int]\n                                [--n_conv_layers int] [--DNA_embed_size int] [--cell_embed_size int]\n                                [--CpG_embed_size int] [--n_transformers int] [--act str]\n                                [--mode {2D,axial,intercell,intracell,none}] [--transf_hsz int]\n                                [--n_heads int] [--head_dim int] [--window int] [--layernorm boolean]\n                                [--CNN_do float] [--transf_do float] [--lr float] [--lr_decay_factor float]\n                                [--warmup_steps int] [--tensorboard boolean] [--log_folder str]\n                                [--experiment_name str] [--earlystop boolean] [--patience int]\n                                [--logger [str_to_bool]] [--checkpoint_callback [str_to_bool]]\n                                [--default_root_dir str] [--gradient_clip_val float]\n                                [--gradient_clip_algorithm str] [--process_position int] [--num_nodes int]\n                                [--num_processes int] [--gpus _gpus_allowed_type]\n                                [--auto_select_gpus [str_to_bool]] [--tpu_cores _gpus_allowed_type]\n                                [--log_gpu_memory str] [--progress_bar_refresh_rate int]\n                                [--overfit_batches _int_or_float_type] [--track_grad_norm float]\n                                [--check_val_every_n_epoch int] [--fast_dev_run [str_to_bool_or_int]]\n                                [--accumulate_grad_batches int] [--max_epochs int] [--min_epochs int]\n                                [--max_steps int] [--min_steps int] [--max_time str]\n                                [--limit_train_batches _int_or_float_type]\n                                [--limit_val_batches _int_or_float_type]\n                                [--limit_test_batches _int_or_float_type]\n                                [--limit_predict_batches _int_or_float_type]\n                                [--val_check_interval _int_or_float_type] [--flush_logs_every_n_steps int]\n                                [--log_every_n_steps int] [--accelerator str]\n                                [--sync_batchnorm [str_to_bool]] [--precision int] [--weights_summary str]\n                                [--weights_save_path str] [--num_sanity_val_steps int]\n                                [--truncated_bptt_steps int] [--resume_from_checkpoint str] [--profiler str]\n                                [--benchmark [str_to_bool]] [--deterministic [str_to_bool]]\n                                [--reload_dataloaders_every_epoch [str_to_bool]]\n                                [--auto_lr_find [str_to_bool_or_str]] [--replace_sampler_ddp [str_to_bool]]\n                                [--terminate_on_nan [str_to_bool]]\n                                [--auto_scale_batch_size [str_to_bool_or_str]]\n                                [--prepare_data_per_node [str_to_bool]] [--plugins str] [--amp_backend str]\n                                [--amp_level str] [--distributed_backend str]\n                                [--move_metrics_to_cpu [str_to_bool]] [--multiple_trainloader_mode str]\n                                [--stochastic_weight_avg [str_to_bool]]\n                                X y pos\n\nTraining script for CpG Transformer.\n\npositional arguments:\n  X                     NumPy file containing encoded genome.\n  y                     NumPy file containing methylation matrix.\n  pos                   NumPy file containing positions of CpG sites.\n\noptional arguments:\n  -h, --help            show this help message and exit\n```\n\n</details>\n\n<details><summary>DataModule arguments</summary>\n    \n```\nDataModule:\n  Data Module arguments\n\n  --segment_size int    Bin size in number of CpG sites (columns) that every batch will contain.\n                        If GPU memory is exceeded, this option can be lowered. (default: 1024)\n  --fracs float [float ...]\n                        Fraction of every chromosome that will go to train, val, test\n                        respectively. Is ignored for chromosomes that occur in --val_keys or\n                        --test_keys. (default: [1, 0, 0])\n  --mask_p float        How many sites to mask each batch as a percentage of the number of\n                        columns in the batch. (default: 0.25)\n  --mask_random_p float\n                        The percentage of masked sites to instead randomize. (default: 0.2)\n  --resample_cells int  Whether to resample cells every training batch. Reduces complexity. If\n                        GPU memory is exceeded, this option can be used. (default: None)\n  --resample_cells_val int\n                        Whether to resample cells every validation batch. If GPU memory is\n                        exceeded, this option can be used. (default: None)\n  --val_keys str [str ...]\n                        Names/keys of validation chromosomes. (default: ['chr5'])\n  --test_keys str [str ...]\n                        Names/keys of test chromosomes. (default: ['chr10'])\n  --batch_size int      Batch size. (default: 1)\n  --n_workers int       Number of worker threads to use in data loading. Increase if you\n                        experience a CPU bottleneck. (default: 4)\n```\n    \n</details>\n\n<details><summary>Model arguments</summary>\n    \n```\nModel:\n  CpG Transformer Hyperparameters\n\n  --transfer_checkpoint str\n                        .ckpt file to transfer model weights from. Has to be either a `.ckpt`\n                        pytorch lightning checkpoint or a `.pt` pytorch state_dict. If a `.ckpt`\n                        file is provided, then all following model arguments will not be used\n                        (apart from `--lr`). If a `.pt` file is provided, then all following\n                        model arguments HAVE to correspond to the arguments of the saved model.\n                        When doing transfer learning, a lower-than-default learning rate (`--lr`)\n                        is advised. (default: None)\n  --RF int              Receptive field of the underlying CNN. (default: 1001)\n  --n_conv_layers int   Number of convolutional layers, only 2 or 3 are possible. (default: 2)\n  --DNA_embed_size int  Output embedding hidden size of the CNN. (default: 32)\n  --cell_embed_size int\n                        Cell embedding hidden size. (default: 32)\n  --CpG_embed_size int  CpG embedding hidden size. (default: 32)\n  --n_transformers int  Number of transformer modules to use. (default: 4)\n  --act str             Activation function in transformer feed-forward, either relu or gelu.\n                        (default: relu)\n  --mode {2D,axial,intercell,intracell,none}\n                        Attention mode. (default: axial)\n  --transf_hsz int      Hidden dimension size in the transformer. (default: 64)\n  --n_heads int         Number of self-attention heads. (default: 8)\n  --head_dim int        Hidden dimensionality of each head. (default: 8)\n  --window int          Window size of row-wise sliding window attention, should be odd. (default: 21)\n  --layernorm boolean   Whether to apply layernorm in transformer modules. (default: True)\n  --CNN_do float        Dropout rate in the CNN to embed DNA context. (default: 0.0)\n  --transf_do float     Dropout rate on the self-attention matrix. (default: 0.2)\n  --lr float            Learning rate. (default: 0.0005)\n  --lr_decay_factor float\n                        Learning rate multiplicative decay applied after every epoch. (default:\n                        0.9)\n  --warmup_steps int    Number of steps over which the learning rate will linearly warm up.\n                        (default: 1000)\n```\n    \n</details>\n\n<details><summary>Logging arguments</summary>\n    \n```\nLogging:\n  Logging arguments\n\n  --tensorboard boolean\n                        Whether to use tensorboard. If True, then training progress can be\n                        followed by using (1) `tensorboard --logdir logfolder/` in a separate\n                        terminal and (2) accessing at localhost:6006. (default: True)\n  --log_folder str      Folder where the tensorboard logs will be saved. Will additinally contain\n                        saved model checkpoints. (default: logfolder)\n  --experiment_name str\n                        Name of the run within the log folder. (default: experiment)\n  --earlystop boolean   Whether to use early stopping after the validation loss has not decreased\n                        for `patience` epochs. (default: True)\n  --patience int        Number of epochs to wait for a possible decrease in validation loss\n                        before early stopping. (default: 10)\n```\n    \n</details>\n\n<details><summary>PyTorch Lightning Trainer arguments</summary>\n    \n```\npl.Trainer:\n  --logger [str_to_bool]\n                        Logger (or iterable collection of loggers) for experiment tracking. A\n                        ``True`` value uses the default ``TensorBoardLogger``. ``False`` will\n                        disable logging. (default: True)\n  --checkpoint_callback [str_to_bool]\n                        If ``True``, enable checkpointing. It will configure a default\n                        ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n                        :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`.\n                        (default: True)\n  --default_root_dir str\n                        Default path for logs and weights when no logger/ckpt_callback passed.\n                        Default: ``os.getcwd()``. Can be remote file paths such as\n                        `s3://mybucket/path` or 'hdfs://path/' (default: None)\n  --gradient_clip_val float\n                        0 means don't clip. (default: 0.0)\n  --gradient_clip_algorithm str\n                        'value' means clip_by_value, 'norm' means clip_by_norm. Default: 'norm'\n                        (default: norm)\n  --process_position int\n                        orders the progress bar when running multiple models on same machine.\n                        (default: 0)\n  --num_nodes int       number of GPU nodes for distributed training. (default: 1)\n  --num_processes int   number of processes for distributed training with\n                        distributed_backend=\"ddp_cpu\" (default: 1)\n  --gpus _gpus_allowed_type\n                        number of gpus to train on (int) or which GPUs to train on (list or str)\n                        applied per node (default: None)\n  --auto_select_gpus [str_to_bool]\n                        If enabled and `gpus` is an integer, pick available gpus automatically.\n                        This is especially useful when GPUs are configured to be in \"exclusive\n                        mode\", such that only one process at a time can access them. (default:\n                        False)\n  --tpu_cores _gpus_allowed_type\n                        How many TPU cores to train on (1 or 8) / Single TPU to train on [1]\n                        (default: None)\n  --log_gpu_memory str  None, 'min_max', 'all'. Might slow performance (default: None)\n  --progress_bar_refresh_rate int\n                        How often to refresh progress bar (in steps). Value ``0`` disables\n                        progress bar. Ignored when a custom progress bar is passed to\n                        :paramref:`~Trainer.callbacks`. Default: None, means a suitable value\n                        will be chosen based on the environment (terminal, Google COLAB, etc.).\n                        (default: None)\n  --overfit_batches _int_or_float_type\n                        Overfit a fraction of training data (float) or a set number of batches\n                        (int). (default: 0.0)\n  --track_grad_norm float\n                        -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf'\n                        infinity-norm. (default: -1)\n  --check_val_every_n_epoch int\n                        Check val every n train epochs. (default: 1)\n  --fast_dev_run [str_to_bool_or_int]\n                        runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es) of\n                        train, val and test to find any bugs (ie: a sort of unit test). (default:\n                        False)\n  --accumulate_grad_batches int\n                        Accumulates grads every k batches or as set up in the dict. (default: 1)\n  --max_epochs int      Stop training once this number of epochs is reached. Disabled by default\n                        (None). If both max_epochs and max_steps are not specified, defaults to\n                        ``max_epochs`` = 1000. (default: None)\n  --min_epochs int      Force training for at least these many epochs. Disabled by default\n                        (None). If both min_epochs and min_steps are not specified, defaults to\n                        ``min_epochs`` = 1. (default: None)\n  --max_steps int       Stop training after this number of steps. Disabled by default (None).\n                        (default: None)\n  --min_steps int       Force training for at least these number of steps. Disabled by default\n                        (None). (default: None)\n  --max_time str        Stop training after this amount of time has passed. Disabled by default\n                        (None). The time duration can be specified in the format DD:HH:MM:SS\n                        (days, hours, minutes seconds), as a :class:`datetime.timedelta`, or a\n                        dictionary with keys that will be passed to :class:`datetime.timedelta`.\n                        (default: None)\n  --limit_train_batches _int_or_float_type\n                        How much of training dataset to check (float = fraction, int =\n                        num_batches) (default: 1.0)\n  --limit_val_batches _int_or_float_type\n                        How much of validation dataset to check (float = fraction, int =\n                        num_batches) (default: 1.0)\n  --limit_test_batches _int_or_float_type\n                        How much of test dataset to check (float = fraction, int = num_batches)\n                        (default: 1.0)\n  --limit_predict_batches _int_or_float_type\n                        How much of prediction dataset to check (float = fraction, int =\n                        num_batches) (default: 1.0)\n  --val_check_interval _int_or_float_type\n                        How often to check the validation set. Use float to check within a\n                        training epoch, use int to check every n steps (batches). (default: 1.0)\n  --flush_logs_every_n_steps int\n                        How often to flush logs to disk (defaults to every 100 steps). (default:\n                        100)\n  --log_every_n_steps int\n                        How often to log within steps (defaults to every 50 steps). (default: 50)\n  --accelerator str     Previously known as distributed_backend (dp, ddp, ddp2, etc...). Can also\n                        take in an accelerator object for custom hardware. (default: None)\n  --sync_batchnorm [str_to_bool]\n                        Synchronize batch norm layers between process groups/whole world.\n                        (default: False)\n  --precision int       Double precision (64), full precision (32) or half precision (16). Can be\n                        used on CPU, GPU or TPUs. (default: 32)\n  --weights_summary str\n                        Prints a summary of the weights when training begins. (default: top)\n  --weights_save_path str\n                        Where to save weights if specified. Will override default_root_dir for\n                        checkpoints only. Use this if for whatever reason you need the\n                        checkpoints stored in a different place than the logs written in\n                        `default_root_dir`. Can be remote file paths such as `s3://mybucket/path`\n                        or 'hdfs://path/' Defaults to `default_root_dir`. (default: None)\n  --num_sanity_val_steps int\n                        Sanity check runs n validation batches before starting the training\n                        routine. Set it to `-1` to run all batches in all validation dataloaders.\n                        (default: 2)\n  --truncated_bptt_steps int\n                        Deprecated in v1.3 to be removed in 1.5. Please use :paramref:`~pytorch_l\n                        ightning.core.lightning.LightningModule.truncated_bptt_steps` instead.\n                        (default: None)\n  --resume_from_checkpoint str\n                        Path/URL of the checkpoint from which training is resumed. If there is no\n                        checkpoint file at the path, start from scratch. If resuming from mid-\n                        epoch checkpoint, training will start from the beginning of the next\n                        epoch. (default: None)\n  --profiler str        To profile individual steps during training and assist in identifying\n                        bottlenecks. (default: None)\n  --benchmark [str_to_bool]\n                        If true enables cudnn.benchmark. (default: False)\n  --deterministic [str_to_bool]\n                        If true enables cudnn.deterministic. (default: False)\n  --reload_dataloaders_every_epoch [str_to_bool]\n                        Set to True to reload dataloaders every epoch. (default: False)\n  --auto_lr_find [str_to_bool_or_str]\n                        If set to True, will make trainer.tune() run a learning rate finder,\n                        trying to optimize initial learning for faster convergence.\n                        trainer.tune() method will set the suggested learning rate in self.lr or\n                        self.learning_rate in the LightningModule. To use a different key set a\n                        string instead of True with the key name. (default: False)\n  --replace_sampler_ddp [str_to_bool]\n                        Explicitly enables or disables sampler replacement. If not specified this\n                        will toggled automatically when DDP is used. By default it will add\n                        ``shuffle=True`` for train sampler and ``shuffle=False`` for val/test\n                        sampler. If you want to customize it, you can set\n                        ``replace_sampler_ddp=False`` and add your own distributed sampler.\n                        (default: True)\n  --terminate_on_nan [str_to_bool]\n                        If set to True, will terminate training (by raising a `ValueError`) at\n                        the end of each training batch, if any of the parameters or the loss are\n                        NaN or +/-inf. (default: False)\n  --auto_scale_batch_size [str_to_bool_or_str]\n                        If set to True, will `initially` run a batch size finder trying to find\n                        the largest batch size that fits into memory. The result will be stored\n                        in self.batch_size in the LightningModule. Additionally, can be set to\n                        either `power` that estimates the batch size through a power search or\n                        `binsearch` that estimates the batch size through a binary search.\n                        (default: False)\n  --prepare_data_per_node [str_to_bool]\n                        If True, each LOCAL_RANK=0 will call prepare data. Otherwise only\n                        NODE_RANK=0, LOCAL_RANK=0 will prepare data (default: True)\n  --plugins str         Plugins allow modification of core behavior like ddp and amp, and enable\n                        custom lightning plugins. (default: None)\n  --amp_backend str     The mixed precision backend to use (\"native\" or \"apex\") (default: native)\n  --amp_level str       The optimization level to use (O1, O2, etc...). (default: O2)\n  --distributed_backend str\n                        deprecated. Please use 'accelerator' (default: None)\n  --move_metrics_to_cpu [str_to_bool]\n                        Whether to force internal logged metrics to be moved to cpu. This can\n                        save some gpu memory, but can make training slower. Use with attention.\n                        (default: False)\n  --multiple_trainloader_mode str\n                        How to loop over the datasets when there are multiple train loaders. In\n                        'max_size_cycle' mode, the trainer ends one epoch when the largest\n                        dataset is traversed, and smaller datasets reload when running out of\n                        their data. In 'min_size' mode, all the datasets reload when reaching the\n                        minimum length of datasets. (default: max_size_cycle)\n  --stochastic_weight_avg [str_to_bool]\n                        Whether to use `Stochastic Weight Averaging (SWA)\n                        <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-\n                        averaging/>_` (default: False)\n```\n    \n</details>\n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Imputation and denoising <a name=\"impute\"></a>",
        "parent_header": [
          "CpG Transformer",
          "Usage <a name=\"usage\"></a>"
        ],
        "type": "Text_excerpt",
        "value": "Once a model is trained on already-observed sites, it can be used to impute unobserved sites. We provide `impute_genome.py` for genome-wide imputation using either CpG Transformer, DeepCpG or CaMelia models. For all methods, the same three inputs are expected as with the training scripts: `X.npz`, `y.npz` and `pos.npz`. In addition, an output location e.g. `output.npz` should be specified. By default, `impute_genome.py` will return model predictions for every chromosome. To change this behavior, chromosomes can also be selected based on the `--keys` flag. The script will also predict every site irregardless of whether they are observer or not. To modulate this behavior, the `--denoise` flag can be set to `False`. Doing this will make it so that the output file will only impute unobserved methylation states and not impute/denoise observed methylation states. It has to be noted that CpG Transformer is the only model to explicitly model denoising in its design (objective function). For more detailed information about additional flags, use `python impute_genome.py -h`.\n\nIn terms of speed, CpG Transformer and DeepCpG will be fastest (when using a GPU) and CaMelia slowest due to its heavy preprocessing. (I have tried my best to vectorize and parallellize as much as possible, any contributors interested in improving are welcome to open a pull request.)\n\n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Benchmarking <a name=\"benchmark\"></a>",
        "parent_header": [
          "CpG Transformer",
          "Usage <a name=\"usage\"></a>"
        ],
        "type": "Text_excerpt",
        "value": "Because CpG Transformer masks and randomizes multiple CpG sites per batch, performances reported during training are negatively biased. In practical use (imputation), this forms no problem, as no masking takes place at this point. For benchmarking however, CaMelia and DeepCpG may have an unfair advantage because they do not train using masking strategies. To perform a fair comparison between all models, a separate benchmarking script `benchmark.py` is provided for CpG Transformer. To benchmark DeepCpG and CaMelia, `impute_genome.py` script can be used with one or more test chromosomes specified. It has to be noted that CpG Transformer was not designed to separately mask and predict sites like this. Consequently, the benchmark script may take a lot of time to run.\n\n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Interpretation <a name=\"interpret\"></a>",
        "parent_header": [
          "CpG Transformer",
          "Usage <a name=\"usage\"></a>"
        ],
        "type": "Text_excerpt",
        "value": "See the `./interpretation` folder.\n"
      },
      "source": "https://raw.githubusercontent.com/gdewael/cpg-transformer/main/README.md",
      "technique": "header_analysis"
    }
  ]
}