{
  "application_domain": [
    {
      "confidence": 18.05,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citation",
        "parent_header": [
          "Horovod"
        ],
        "type": "Text_excerpt",
        "value": "Please cite Horovod in your publications if it helps your research:\n\n::\n\n    @article{sergeev2018horovod,\n      Author = {Alexander Sergeev and Mike Del Balso},\n      Journal = {arXiv preprint arXiv:1802.05799},\n      Title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},\n      Year = {2018}\n    }\n\n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "References",
        "parent_header": [
          "Horovod"
        ],
        "type": "Text_excerpt",
        "value": "The Horovod source code was based off the Baidu `tensorflow-allreduce <https://github.com/baidu-research/tensorflow-allreduce>`_\nrepository written by Andrew Gibiansky and Joel Hestness. Their original work is described in the article\n`Bringing HPC Techniques to Deep Learning <http://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/>`_.\n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Alexander Sergeev and Mike Del Balso",
        "format": "bibtex",
        "title": "Horovod: fast and easy distributed deep learning in {TensorFlow}",
        "type": "Text_excerpt",
        "value": "@article{sergeev2018horovod,\n    year = {2018},\n    title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},\n    journal = {arXiv preprint arXiv:1802.05799},\n    author = {Alexander Sergeev and Mike Del Balso},\n}"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "regular_expression"
    }
  ],
  "code_of_conduct": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# Code of Conduct\n\nHorovod is a project hosted under the LF AI Foundation. As such, we would like to urge you to please be mindful of and adhere to the Linux Foundation\u2019s [Code of Conduct](https://lfprojects.org/policies/code-of-conduct/) when contributing to the Horovod.\n\nIf you have any questions or concerns, please email info@lfai.foundation.\n\nThank you. \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/CODE_OF_CONDUCT.md",
      "technique": "file_exploration"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/horovod/horovod"
      },
      "technique": "GitHub_API"
    }
  ],
  "contributing_guidelines": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "## Contributing to Horovod\n\n**Thanks for taking the time to contribute!**\n\nRefer to the following guidelines to contribute new functionality or bug fixes to Horovod:\n1. Use [autopep8](https://github.com/hhatto/autopep8) to format the Python code.\n2. Use [clang-format](https://clang.llvm.org/docs/ClangFormat.html) to format C++ code.\n3. Add unit tests for any new code you write.\n4. Run unit tests in both CPU and GPU environments.\n\n### Code of Conduct\n\nPlease be mindful of and adhere to the Linux Foundation's\n[Code of Conduct](https://lfprojects.org/policies/code-of-conduct) when contributing to Horovod.\n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/CONTRIBUTING.md",
      "technique": "file_exploration"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2017-08-09T19:39:59Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-10-05T22:22:07Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9966127180580753,
      "result": {
        "original_header": "Horovod",
        "type": "Text_excerpt",
        "value": "Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.\nThe goal of Horovod is to make distributed deep learning fast and easy to use. \n\nHorovod is hosted by the `LF AI & Data Foundation <https://lfdl.io>`_ (LF AI & Data). If you are a company that is deeply\ncommitted to using open source technologies in artificial intelligence, machine, and deep learning, and want to support\nthe communities of open source projects in these domains, consider joining the LF AI & Data Foundation. For details\nabout who's involved and how Horovod plays a role, read the Linux Foundation `announcement <https://lfdl.io/press/2018/12/13/lf-deep-learning-welcomes-horovod-distributed-training-framework-as-newest-project/>`_. \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9471255889704171,
      "result": {
        "original_header": "Why Horovod?",
        "type": "Text_excerpt",
        "value": "The primary motivation for this project is to make it easy to take a single-GPU training script and successfully scale\nit to train across many GPUs in parallel. This has two aspects: \nInternally at Uber we found the MPI model to be much more straightforward and require far less code changes than previous\nsolutions such as Distributed TensorFlow with parameter servers. Once a training script has been written for scale with\nHorovod, it can run on a single-GPU, multiple-GPUs, or even multiple hosts without any further code changes.\nSee the `Usage <#usage>`__ section for more details. \nIn addition to being easy to use, Horovod is fast. Below is a chart representing the benchmark that was done on 128\nservers with 4 Pascal GPUs each connected by RoCE-capable 25 Gbit/s network: \nHorovod achieves 90% scaling efficiency for both Inception V3 and ResNet-101, and 68% scaling efficiency for VGG-16.\nSee `Benchmarks <docs/benchmarks.rst>`_ to find out how to reproduce these numbers. \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9737223621167752,
      "result": {
        "original_header": "Concepts",
        "type": "Text_excerpt",
        "value": "Horovod core principles are based on `MPI <http://mpi-forum.org/>`_ concepts such as *size*, *rank*,\n*local rank*, **allreduce**, **allgather**, **broadcast**, and **alltoall**. See `this page <docs/concepts.rst>`_\nfor more details.\n \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8966525058124105,
      "result": {
        "original_header": "Supported frameworks",
        "type": "Text_excerpt",
        "value": "See these pages for Horovod examples and best practices: \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9045833201148594,
      "result": {
        "original_header": "Gloo",
        "type": "Text_excerpt",
        "value": "For environments that have support both MPI and Gloo, you can choose to use Gloo at runtime by passing the ``--gloo`` argument to ``horovodrun``: \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9113770032814117,
      "result": {
        "original_header": "mpi4py",
        "type": "Text_excerpt",
        "value": "Horovod supports mixing and matching Horovod collectives with other MPI libraries, such as `mpi4py <https://mpi4py.scipy.org>`_,\nprovided that the MPI was built with multi-threading support. \n    # Verify that MPI multi-threading is supported.\n    assert hvd.mpi_threads_supported() \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9349688310655683,
      "result": {
        "original_header": "Inference",
        "type": "Text_excerpt",
        "value": "Learn how to optimize your model for inference and remove Horovod operations from the graph `here <docs/inference.rst>`_. \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9908860798410145,
      "result": {
        "original_header": "Tensor Fusion",
        "type": "Text_excerpt",
        "value": "One of the unique things about Horovod is its ability to interleave communication and computation coupled with the ability\nto batch small **allreduce** operations, which results in improved performance. We call this batching feature Tensor Fusion. \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8269695882759894,
      "result": {
        "original_header": "Horovod Timeline",
        "type": "Text_excerpt",
        "value": "Use Horovod timeline to analyze Horovod performance.\nSee `here <docs/timeline.rst>`__ for full details and usage instructions. \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9886591264030525,
      "result": {
        "original_header": "Automated Performance Tuning",
        "type": "Text_excerpt",
        "value": "Selecting the right values to efficiently make use of Tensor Fusion and other advanced Horovod features can involve\na good amount of trial and error. We provide a system to automate this performance optimization process called\n**autotuning**, which you can enable with a single command line argument to ``horovodrun``. \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Documentation",
        "parent_header": [
          "Horovod"
        ],
        "type": "Text_excerpt",
        "value": "- `Latest Release <https://horovod.readthedocs.io/en/stable>`_\n- `master <https://horovod.readthedocs.io/en/latest>`_\n\n|\n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "https://horovod.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "format": "wiki",
        "type": "Url",
        "value": "https://github.com/horovod/horovod/wiki"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "regular_expression"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/horovod/horovod/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/horovod/horovod/master/examples/adasum/adasum_bench.ipynb"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/examples/adasum/adasum_bench.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 2225
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/horovod/horovod/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "horovod/horovod"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Horovod"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "regular_expression"
    }
  ],
  "has_build_file": [
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/horovod/horovod/master/docker/horovod-ray/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/docker/horovod-ray/Dockerfile",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/horovod/horovod/master/docker/horovod-nvtabular/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/docker/horovod-nvtabular/Dockerfile",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/horovod/horovod/master/docker/horovod/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/docker/horovod/Dockerfile",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/horovod/horovod/master/docker/horovod-cpu/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/docker/horovod-cpu/Dockerfile",
      "technique": "file_exploration"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/horovod/horovod/master/assert-package-versions.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/horovod/horovod/master/.github/timeout-and-retry.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/horovod/horovod/master/examples/spark/keras/get_gpu_resources.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/horovod/horovod/master/.buildkite/gen-pipeline.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/lfai/artwork/master/lfaidata-assets/lfaidata-project-badge/graduate/color/lfaidata-project-badge-graduate-color.png"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Install",
        "parent_header": [
          "Horovod"
        ],
        "type": "Text_excerpt",
        "value": "To install Horovod on Linux or macOS:\n\n1. Install `CMake <https://cmake.org/install/>`__\n\n.. raw:: html\n\n    <p/>\n\n2. If you've installed TensorFlow from `PyPI <https://pypi.org/project/tensorflow>`__, make sure that ``g++-5`` or above is installed.\n   Starting with TensorFlow 2.10 a C++17-compliant compiler like ``g++8`` or above will be required.\n\n   If you've installed PyTorch from `PyPI <https://pypi.org/project/torch>`__, make sure that ``g++-5`` or above is installed.\n\n   If you've installed either package from `Conda <https://conda.io>`_, make sure that the ``gxx_linux-64`` Conda package is installed.\n\n.. raw:: html\n\n    <p/>\n\n3. Install the ``horovod`` pip package.\n\n   To run on CPUs:\n\n   .. code-block:: bash\n\n      $ pip install horovod\n\n   To run on GPUs with NCCL:\n\n   .. code-block:: bash\n\n      $ HOROVOD_GPU_OPERATIONS=NCCL pip install horovod\n\nFor more details on installing Horovod with GPU support, read `Horovod on GPU <docs/gpus.rst>`_.\n\nFor the full list of Horovod installation options, read the `Installation Guide <docs/install.rst>`_.\n\nIf you want to use MPI, read `Horovod with MPI <docs/mpi.rst>`_.\n\nIf you want to use Conda, read `Building a Conda environment with GPU support for Horovod <docs/conda.rst>`_.\n\nIf you want to use Docker, read `Horovod in Docker <docs/docker.rst>`_.\n\nTo compile Horovod from source, follow the instructions in the `Contributor Guide <docs/contributors.rst>`_.\n\n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Horovod Process Sets",
        "parent_header": [
          "Horovod"
        ],
        "type": "Text_excerpt",
        "value": "Horovod allows you to concurrently run distinct collective operations in different groups of processes taking part in\none distributed training. Set up ``hvd.process_set`` objects to make use of this capability.\n\nSee `Process Sets <docs/process_set.rst>`__ for detailed instructions.\n\n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Guides",
        "parent_header": [
          "Horovod"
        ],
        "type": "Text_excerpt",
        "value": "1. Run distributed training in Microsoft Azure using `Batch AI and Horovod <https://github.com/Azure/BatchAI/tree/master/recipes/Horovod>`_.\n2. `Distributed model training using Horovod <https://spell.ml/blog/distributed-model-training-using-horovod-XvqEGRUAACgAa5th>`_.\n\nSend us links to any user guides you want to publish on this site\n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9808597449274632,
      "result": {
        "original_header": "Horovod",
        "type": "Text_excerpt",
        "value": ".. raw:: html \n.. image:: https://badge.fury.io/py/horovod.svg\n   :target: https://badge.fury.io/py/horovod\n   :alt: PyPI Version \n.. image:: https://badge.buildkite.com/6f976bc161c69d9960fc00de01b69deb6199b25680a09e5e26.svg?branch=master\n   :target: https://buildkite.com/horovod/horovod\n   :alt: Build Status \n.. image:: https://readthedocs.org/projects/horovod/badge/?version=latest\n   :target: https://horovod.readthedocs.io/en/latest/\n   :alt: Documentation Status \n.. image:: https://img.shields.io/badge/slack-chat-green.svg?logo=slack\n   :target: https://forms.gle/cPGvty5hp31tGfg79\n   :alt: Slack \n.. image:: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :target: https://img.shields.io/badge/License-Apache%202.0-blue.svg\n   :alt: License \n.. image:: https://app.fossa.com/api/projects/git%2Bgithub.com%2Fhorovod%2Fhorovod.svg?type=shield\n   :target: https://app.fossa.com/projects/git%2Bgithub.com%2Fhorovod%2Fhorovod?ref=badge_shield\n   :alt: FOSSA Status \n.. image:: https://bestpractices.coreinfrastructure.org/projects/2373/badge\n   :target: https://bestpractices.coreinfrastructure.org/projects/2373\n   :alt: CII Best Practices \n.. image:: https://pepy.tech/badge/horovod\n   :target: https://pepy.tech/project/horovod\n   :alt: Downloads \n   <p><img src=\"https://raw.githubusercontent.com/lfai/artwork/master/lfaidata-assets/lfaidata-project-badge/graduate/color/lfaidata-project-badge-graduate-color.png\" alt=\"LF AI & Data\" width=\"200\"/></p> \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9721509699664749,
      "result": {
        "original_header": "Why Horovod?",
        "type": "Text_excerpt",
        "value": ".. image:: https://user-images.githubusercontent.com/16640218/38965607-bf5c46ca-4332-11e8-895a-b9c137e86013.png\n   :alt: 512-GPU Benchmark \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9999914540119287,
      "result": {
        "original_header": "Supported frameworks",
        "type": "Text_excerpt",
        "value": "- `Horovod with TensorFlow <docs/tensorflow.rst>`_\n- `Horovod with XLA in Tensorflow <xla.rst>`_\n- `Horovod with Keras <docs/keras.rst>`_\n- `Horovod with PyTorch <docs/pytorch.rst>`_\n- `Horovod with MXNet <docs/mxnet.rst>`_\n \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9548763539687836,
      "result": {
        "original_header": "Gloo",
        "type": "Text_excerpt",
        "value": "`Gloo <https://github.com/facebookincubator/gloo>`_ is an open source collective communications library developed by Facebook. \nGloo comes included with Horovod, and allows users to run Horovod without requiring MPI to be installed. \nFor environments that have support both MPI and Gloo, you can choose to use Gloo at runtime by passing the ``--gloo`` argument to ``horovodrun``: \n     $ horovodrun --gloo -np 2 python train.py\n \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9233572046793266,
      "result": {
        "original_header": "mpi4py",
        "type": "Text_excerpt",
        "value": "Horovod supports mixing and matching Horovod collectives with other MPI libraries, such as `mpi4py <https://mpi4py.scipy.org>`_,\nprovided that the MPI was built with multi-threading support. \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9218030499154235,
      "result": {
        "original_header": "Horovod Timeline",
        "type": "Text_excerpt",
        "value": ".. image:: https://user-images.githubusercontent.com/16640218/29735271-9e148da0-89ac-11e7-9ae0-11d7a099ac89.png\n   :alt: Horovod Timeline \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9989704288037482,
      "result": {
        "original_header": "Troubleshooting",
        "type": "Text_excerpt",
        "value": "See `Troubleshooting <docs/troubleshooting.rst>`_ and submit a `ticket <https://github.com/horovod/horovod/issues/new>`_\nif you can't find an answer. \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9925809836134613,
      "result": {
        "original_header": "Publications",
        "type": "Text_excerpt",
        "value": "1. Sergeev, A., Del Balso, M. (2017) *Meet Horovod: Uber\u2019s Open Source Distributed Deep Learning Framework for TensorFlow*.\nRetrieved from `https://eng.uber.com/horovod/ <https://eng.uber.com/horovod/>`_ \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.957085026679056,
      "result": {
        "original_header": "Gloo",
        "type": "Text_excerpt",
        "value": "     $ horovodrun --gloo -np 2 python train.py\n \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8529504846241724,
      "result": {
        "original_header": "mpi4py",
        "type": "Text_excerpt",
        "value": "    import horovod.tensorflow as hvd \n    from mpi4py import MPI\n    assert hvd.size() == MPI.COMM_WORLD.Get_size() \n    from mpi4py import MPI\n    import horovod.tensorflow as hvd \n    print('COMM_WORLD rank: %d, Horovod rank: %d' % (MPI.COMM_WORLD.rank, hvd.rank())) \n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/horovod/horovod/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "baidu, deep-learning, deeplearning, keras, machine-learning, machinelearning, mpi, mxnet, pytorch, ray, spark, tensorflow, uber"
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "   Horovod\n   Copyright 2018 Uber Technologies, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/LICENSE",
      "technique": "file_exploration"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "horovod"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "horovod"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 2427426,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "C++",
        "size": 1023283,
        "type": "Programming_language",
        "value": "C++"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "CMake",
        "size": 81422,
        "type": "Programming_language",
        "value": "CMake"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 27789,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Cuda",
        "size": 27469,
        "type": "Programming_language",
        "value": "Cuda"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Dockerfile",
        "size": 20907,
        "type": "Programming_language",
        "value": "Dockerfile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "C",
        "size": 1420,
        "type": "Programming_language",
        "value": "C"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Mustache",
        "size": 1045,
        "type": "Programming_language",
        "value": "Mustache"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1802.05799>`_\n\n\nReferences\n----------\nThe Horovod source code was based off the Baidu `tensorflow-allreduce <https://github.com/baidu-research/tensorflow-allreduce>`_\nrepository written by Andrew Gibiansky and Joel Hestness. Their original work is described in the article\n`Bringing HPC Techniques to Deep Learning <http://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/>`_.\n\nGetting Involved\n----------------\n- `Community Slack <https://forms.gle/cPGvty5hp31tGfg79>`_ for collaboration and discussion\n- `Horovod Announce <https://lists.lfai.foundation/g/horovod-announce>`_ for updates on the project\n- `Horovod Technical-Discuss <https://lists.lfai.foundation/g/horovod-technical-discuss>`_ for public discussion\n- `Horovod Security <https://lists.lfai.foundation/g/horovod-security>`_ to report security vulnerabilities\n\n\n.. inclusion-marker-end-do-not-remove\n   Place contents above here if they should also appear in read-the-docs.\n   Contents below are already part of the read-the-docs table of contents."
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1802.05799"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1802.05799 <https://arxiv.org/abs/1802.05799>`_\n\n\nReferences\n----------\nThe Horovod source code was based off the Baidu `tensorflow-allreduce <https://github.com/baidu-research/tensorflow-allreduce>`_\nrepository written by Andrew Gibiansky and Joel Hestness. Their original work is described in the article\n`Bringing HPC Techniques to Deep Learning <http://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/>`_.\n\nGetting Involved\n----------------\n- `Community Slack <https://forms.gle/cPGvty5hp31tGfg79>`_ for collaboration and discussion\n- `Horovod Announce <https://lists.lfai.foundation/g/horovod-announce>`_ for updates on the project\n- `Horovod Technical-Discuss <https://lists.lfai.foundation/g/horovod-technical-discuss>`_ for public discussion\n- `Horovod Security <https://lists.lfai.foundation/g/horovod-security>`_ to report security vulnerabilities\n\n\n.. inclusion-marker-end-do-not-remove\n   Place contents above here if they should also appear in read-the-docs.\n   Contents below are already part of the read-the-docs table of contents."
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "regular_expression"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maxhgerlach",
          "type": "User"
        },
        "date_created": "2023-06-12T09:21:57Z",
        "date_published": "2023-06-12T09:26:22Z",
        "description": "### Fixed\r\n\r\n- Fixed build with gcc 12. ([#3925](https://github.com/horovod/horovod/pull/3925))\r\n- PyTorch: Fixed build on ROCm. ([#3928](https://github.com/horovod/horovod/pull/3928))\r\n- TensorFlow: Fixed local_rank_op. ([#3940](https://github.com/horovod/horovod/pull/3940))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.28.1",
        "name": "v0.28.1: Build fixes (ROCm, GCC 12) ",
        "release_id": 108191753,
        "tag": "v0.28.1",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.28.1",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/108191753",
        "value": "https://api.github.com/repos/horovod/horovod/releases/108191753",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.28.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maxhgerlach",
          "type": "User"
        },
        "date_created": "2023-05-09T08:31:33Z",
        "date_published": "2023-05-10T09:13:23Z",
        "description": "### Added\r\n\r\n- TensorFlow: Added new `get_local_and_global_gradients` to PartialDistributedGradientTape to retrieve local and non-local gradients separately. ([#3859](https://github.com/horovod/horovod/pull/3859))\r\n\r\n### Changed\r\n\r\n- Improved reducescatter performance by allocating output tensors before enqueuing the operation. ([#3824](https://github.com/horovod/horovod/pull/3824))\r\n- TensorFlow: Ensured that `tf.logical_and` within allreduce `tf.cond` runs on CPU. ([#3885](https://github.com/horovod/horovod/pull/3885))\r\n- TensorFlow: Added support for Keras 2.11+ optimizers. ([#3860](https://github.com/horovod/horovod/pull/3860))\r\n- `CUDA_VISIBLE_DEVICES` environment variable is no longer passed to remote nodes. ([#3865](https://github.com/horovod/horovod/pull/3865))\r\n\r\n### Fixed\r\n\r\n- Fixed build with ROCm. ([#3839](https://github.com/horovod/horovod/pull/3839), [#3848](https://github.com/horovod/horovod/pull/3848))\r\n- Fixed build of Docker image horovod-nvtabular. ([#3851](https://github.com/horovod/horovod/pull/3851))\r\n- Fixed linking recent NCCL by defaulting CUDA runtime library linkage to static and ensuring that weak symbols are overridden. ([#3867](https://github.com/horovod/horovod/pull/3867), [#3846](https://github.com/horovod/horovod/pull/3846))\r\n- Fixed compatibility with TensorFlow 2.12 and recent nightly versions. ([#3864](https://github.com/horovod/horovod/pull/3864), [#3894](https://github.com/horovod/horovod/pull/3894), [#3906](https://github.com/horovod/horovod/pull/3906), [#3907](https://github.com/horovod/horovod/pull/3907))\r\n- Fixed missing arguments of Keras allreduce function. ([#3905](https://github.com/horovod/horovod/pull/3905))\r\n- Updated with_device functions in MXNet and PyTorch to skip unnecessary cudaSetDevice calls. ([#3912](https://github.com/horovod/horovod/pull/3912))\r\n\r\n",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.28.0",
        "name": "v0.28.0: Keras 2.11+ optimizers, faster reducescatter, fixes for latest TensorFlow, CUDA, NCCL",
        "release_id": 102461038,
        "tag": "v0.28.0",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.28.0",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/102461038",
        "value": "https://api.github.com/repos/horovod/horovod/releases/102461038",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.28.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maxhgerlach",
          "type": "User"
        },
        "date_created": "2023-02-01T14:43:42Z",
        "date_published": "2023-02-01T17:51:19Z",
        "description": "### Added\r\n\r\n- Keras: Added `PartialDistributedOptimizer` API. ([#3738](https://github.com/horovod/horovod/pull/3738))\r\n- Added `HOROVOD_SPARK_USE_LOCAL_RANK_GPU_INDEX` environment variable to ignore GPU device indices assigned by Spark and always use local rank GPU device in Spark estimators. ([#3737](https://github.com/horovod/horovod/pull/3737))\r\n- Added support for reducescatter arguments `prescale_factor` and `postscale_factor` and moved averaging into Horovod backend. ([#3815](https://github.com/horovod/horovod/pull/3815))\r\n- Spark Estimator: Added support for custom data loaders in TorchEstimator. ([#3787](https://github.com/horovod/horovod/pull/3787))\r\n- Spark Estimator: Added NVTabular data loader for TorchEstimator. ([#3787](https://github.com/horovod/horovod/pull/3787))\r\n\r\n### Changed\r\n\r\n- Improved NCCL performance for fused allgather operations through padding for better memory alignment. ([#3727](https://github.com/horovod/horovod/pull/3727))\r\n- Improved look-ahead tensor fusion buffer size estimates when allgather and other operations are mixed. ([#3727](https://github.com/horovod/horovod/pull/3727))\r\n\r\n### Fixed\r\n\r\n- ROCm: Fixed GPU MPI operations support in build. ([#3746](https://github.com/horovod/horovod/pull/3746))\r\n- PyTorch: Fixed linking order to avoid using Gloo from PyTorch dynamic libraries. ([#3750](https://github.com/horovod/horovod/pull/3750))\r\n- Fixed memory leak in `MPI_GPUAllgather`. ([#3727](https://github.com/horovod/horovod/pull/3727))\r\n- TensorFlow: Fixed deprecation warnings when building with TensorFlow 2.11. ([#3767](https://github.com/horovod/horovod/pull/3767)) \r\n- Keras: Added support for additional arguments to `SyncBatchNormalization._moments()`. ([#3775](https://github.com/horovod/horovod/pull/3775))\r\n- Fixed version number parsing with pypa/packaging 22.0. ([#3794](https://github.com/horovod/horovod/pull/3794))\r\n- TensorFlow: Fixed linking with nightly versions leading up to TensorFlow 2.12. ([#3755](https://github.com/horovod/horovod/pull/3755))\r\n- TensorFlow: Fixed handling of `tf.IndexedSlices` types when scaling local gradients. ([#3786](https://github.com/horovod/horovod/pull/3786))\r\n- Added missing `MEMCPY_IN_FUSION_BUFFER` timeline event for reducescatter. ([#3808](https://github.com/horovod/horovod/pull/3808))\r\n- Fixed build of Docker image horovod-nvtabular. ([#3817](https://github.com/horovod/horovod/pull/3817))\r\n- TensorFlow: Several fixes for allreduce and grouped allreduce handling of `tf.IndexedSlices`. ([#3813](https://github.com/horovod/horovod/pull/3813))\r\n- Spark: Restricted PyArrow to versions < 11.0. ([#3830](https://github.com/horovod/horovod/pull/3830))\r\n- TensorFlow: Resolved conflicts between multiple optimizer wrappers reusing the same gradient accumulation counter. ([#3783](https://github.com/horovod/horovod/pull/3783))\r\n- TensorFlow/Keras: Fixed `DistributedOptimizer` with Keras 2.11+. ([#3822](https://github.com/horovod/horovod/pull/3822))\r\n- PyTorch, ROCm: Fixed allreduce average on process sets. ([#3815](https://github.com/horovod/horovod/pull/3815))\r\n",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.27.0",
        "name": "Custom data loaders in Spark TorchEstimator, more model parallelism in Keras, improved allgather performance, fixes for latest PyTorch and TensorFlow versions",
        "release_id": 90992413,
        "tag": "v0.27.0",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.27.0",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/90992413",
        "value": "https://api.github.com/repos/horovod/horovod/releases/90992413",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.27.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "EnricoMi",
          "type": "User"
        },
        "date_created": "2022-10-14T08:18:27Z",
        "date_published": "2022-10-14T08:20:56Z",
        "description": "### Fixed\r\n\r\n- Fixed packaging import during install to occur after install_requires. ([#3741](https://github.com/horovod/horovod/pull/3741))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.26.1",
        "name": "Hotfix: Fixing packaging import during install",
        "release_id": 79891614,
        "tag": "v0.26.1",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.26.1",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/79891614",
        "value": "https://api.github.com/repos/horovod/horovod/releases/79891614",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.26.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "EnricoMi",
          "type": "User"
        },
        "date_created": "2022-10-13T06:46:02Z",
        "date_published": "2022-10-13T12:29:50Z",
        "description": "### Added\r\n\r\n- Spark Estimator: Added support for custom data loaders in KerasEstimator. ([#3603](https://github.com/horovod/horovod/pull/3603))\r\n- Spark Estimator: Added NVTabular data loader for KerasEstimator. ([#3603](https://github.com/horovod/horovod/pull/3603))\r\n- Spark Estimator: Added gradient accumulation support to Spark torch estimator. ([#3681](https://github.com/horovod/horovod/pull/3681))\r\n- TensorFlow: Added `register_local_var` functionality to distributed optimizers and local gradient aggregators. ([#3695](https://github.com/horovod/horovod/pull/3695))\r\n- TensorFlow: Added support for local variables for `BroadcastGlobalVariablesCallback`. ([#3703](https://github.com/horovod/horovod/pull/3703))\r\n- Enabled use of native `ncclAvg` op for NCCL allreduces. ([#3646](https://github.com/horovod/horovod/pull/3646))\r\n- Added support for additional reduction operations for `allreduce` (min, max, product). ([#3660](https://github.com/horovod/horovod/pull/3660))\r\n- Added 2D torus `allreduce` using NCCL. ([#3608](https://github.com/horovod/horovod/pull/3608))\r\n- Added support for Petastorm reader level parallel shuffling. ([#3665](https://github.com/horovod/horovod/pull/3665))\r\n- Added random seed support for Lightning datamodule to generate reproducible data loading outputs. ([#3665](https://github.com/horovod/horovod/pull/3665))\r\n- Added support for `int8` and `uint8` `allreduce` and `grouped_allreduce` in TensorFlow. ([#3649](https://github.com/horovod/horovod/pull/3649))\r\n- Added support for batched memory copies in `GPUAllgather`. ([#3590](https://github.com/horovod/horovod/pull/3590))\r\n- Added support for batched memory copies in `GPUReducescatter`. ([#3621](https://github.com/horovod/horovod/pull/3621))\r\n- Added `hvd.grouped_allgather()` and `hvd.grouped_reducescatter()` operations. ([#3594](https://github.com/horovod/horovod/pull/3594))\r\n- Added warning messages if output tensor memory allocations fail. ([#3594](https://github.com/horovod/horovod/pull/3594))\r\n- Added `register_local_source` and `use_generic_names` funtionality to `DistributedGradientTape`. ([#3628](https://github.com/horovod/horovod/pull/3628))\r\n- Added `PartialDistributedGradientTape()` API for model parallel use cases. ([#3643](https://github.com/horovod/horovod/pull/3643))\r\n- Spark/Lightning: Added `reader_worker_count` and `reader_pool_type`. ([#3612](https://github.com/horovod/horovod/pull/3612))\r\n- Spark/Lightning: Added `transformation_edit_fields` and `transformation_removed_fields` param for `EstimatorParams`. ([#3651](https://github.com/horovod/horovod/pull/3651))\r\n- TensorFlow: Added doc string for `hvd.grouped_allreduce()`. ([#3594](https://github.com/horovod/horovod/pull/3594))\r\n- ROCm: Enabled `alltoall`. ([#3654](https://github.com/horovod/horovod/pull/3654))\r\n\r\n### Changed\r\n\r\n- Default Petastorm reader pool is changed from `process` to `thread` for lower memory usage. ([#3665](https://github.com/horovod/horovod/pull/3665))\r\n- Keras: Support only legacy optimizers in Keras 2.11+. ([#3725](https://github.com/horovod/horovod/pull/3725))\r\n- Gloo: When negotiating, use `gather` rather than `allgather`. ([#3633](https://github.com/horovod/horovod/pull/3633))\r\n- Use `packaging.version` instead of `distutils` version classes. ([#3700](https://github.com/horovod/horovod/pull/3700))\r\n\r\n### Deprecated\r\n\r\n- Deprecated field `shuffle_buffer_size` from `EstimatorParams`. Use `shuffle` to enable shuffle or not. ([#3665](https://github.com/horovod/horovod/pull/3665))\r\n\r\n### Removed\r\n\r\n- Build: Removed std::regex use for better cxxabi11 compatibility. ([#3584](https://github.com/horovod/horovod/pull/3584))\r\n\r\n### Fixed\r\n\r\n- TensorFlow: Fixed the optimizer iteration increments when `backward_passes_per_step > 1`. ([#3631](https://github.com/horovod/horovod/pull/3631))\r\n- Fixed `FuseResponses()` on `BATCHED_D2D_PADDING` edge cases for Reducescatter and/or ROCm. ([#3621](https://github.com/horovod/horovod/pull/3621))\r\n- PyTorch: Fixed Reducescatter functions to raise `HorovodInternalError` rather than `RuntimeError`. ([#3594](https://github.com/horovod/horovod/pull/3594))\r\n- PyTorch on GPUs without GPU operations: Fixed grouped allreduce to set CPU device in tensor table. ([#3594](https://github.com/horovod/horovod/pull/3594))\r\n- Fixed race condition in PyTorch allocation handling. ([#3639](https://github.com/horovod/horovod/pull/3639))\r\n- Build: Fixed finding `nvcc` (if not in `$PATH`) with older versions of CMake. ([#3682](https://github.com/horovod/horovod/pull/3682))\r\n- Fixed `reducescatter()` and `grouped_reducescatter()` to raise clean exceptions for scalar inputs. ([#3699](https://github.com/horovod/horovod/pull/3699))\r\n- Updated Eigen submodule to fix build on macOS with aarch64. ([#3619](https://github.com/horovod/horovod/pull/3619))\r\n- Build: Correctly select files in `torch/` directory to be hipified. ([#3588](https://github.com/horovod/horovod/pull/3588))\r\n- Build: Modify regex match for CUDA|ROCm in `FindPytorch.cmake`. ([#3593](https://github.com/horovod/horovod/pull/3593))\r\n- Build: Fixed ROCm-specific build failure. ([#3630](https://github.com/horovod/horovod/pull/3630))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.26.0",
        "name": "Better support for model parallel, more reduction operations for allreduce (min, max, product), grouped allgather and reducedscatter, Petastorm reader level parallel shuffling, NVTabular data loader",
        "release_id": 79772012,
        "tag": "v0.26.0",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.26.0",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/79772012",
        "value": "https://api.github.com/repos/horovod/horovod/releases/79772012",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.26.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "EnricoMi",
          "type": "User"
        },
        "date_created": "2022-06-21T09:09:57Z",
        "date_published": "2022-06-21T09:19:16Z",
        "description": "### Added\r\n\r\n- Added `hvd.reducescatter()` operation with implementations in NCCL, MPI, and Gloo. ([#3299](https://github.com/horovod/horovod/pull/3299), [#3574](https://github.com/horovod/horovod/pull/3574))\r\n- Added AMD GPU XLA Op Implementation. ([#3486](https://github.com/horovod/horovod/pull/3486))\r\n- Added Horovod job to spin up distributed TensorFlow Data Service. ([#3525](https://github.com/horovod/horovod/pull/3525))\r\n- Spark: Expose random seed as an optional parameter. ([#3517](https://github.com/horovod/horovod/pull/3517))\r\n- Add Helm Chart. ([#3546](https://github.com/horovod/horovod/pull/3546))\r\n- Elastic: Add elastic run API. ([#3503](https://github.com/horovod/horovod/pull/3503))\r\n- Spark Estimator: Expose random seed for model training reproducibility. ([#3517](https://github.com/horovod/horovod/pull/3517))\r\n- Spark Estimator: Add option whether to use GPUs at all. ([#3526](https://github.com/horovod/horovod/pull/3526))\r\n- Spark Estimator: Expose parameter to set start method for `multiprocessing`. ([#3580](https://github.com/horovod/horovod/pull/3580))\r\n\r\n### Changed\r\n\r\n- MXNet: Updated allreduce functions to newer `op` API. ([#3299](https://github.com/horovod/horovod/pull/3299))\r\n- TensorFlow: Make TensorFlow output allocations asynchronous when using NCCL backend. ([#3464](https://github.com/horovod/horovod/pull/3464))\r\n- TensorFlow: Clear locally accumulated gradient by assigning with `zeros_like` to avoid infinite gradient not correctly cleared up. ([#3505](https://github.com/horovod/horovod/pull/3505))\r\n- Make `HorovodVersionMismatchError` subclass `ImportError` instead of just a standard `Exception`. ([#3549](https://github.com/horovod/horovod/pull/3549))\r\n- Elastic: Catch any exception to prevent the discovery thread from silently dying. ([#3436](https://github.com/horovod/horovod/pull/3436))\r\n- Horovodrun: Exit check_build (`--check-build`) via `sys.exit` to flush stdout. ([#3272](https://github.com/horovod/horovod/pull/3272))\r\n- Spark: Use `env` to set environment vars in remote shell. ([#3489](https://github.com/horovod/horovod/pull/3489))\r\n- Build: Avoid redundant ptx generation for maximum specified compute capability. ([#3509](https://github.com/horovod/horovod/pull/3509))\r\n\r\n### Deprecated\r\n\r\n- MXNet: Deprecated `average` argument of allreduce functions. ([#3299](https://github.com/horovod/horovod/pull/3299))\r\n- Public and internal APIs: deprecate use of np, min_np, max_np,\r\n  use num_proc, min_num_proc, and max_num_proc, respectively, instead. ([#3409](https://github.com/horovod/horovod/pull/3409))\r\n- Horovodrun: Providing multiple NICS as comma-separated string via `--network-interface` is deprecated,\r\n  use `--network-interface` multiple times or `--network-interfaces` instead. ([#3506](https://github.com/horovod/horovod/pull/3506))\r\n- horovod.run: Argument `network_interface` with comma-separated string is deprecated,\r\n  use `network_interfaces` with `Iterable[str]` instead. ([#3506](https://github.com/horovod/horovod/pull/3506))\r\n\r\n### Fixed\r\n\r\n- Fallback to NCCL shared lib if static one is not found. ([#3500]((https://github.com/horovod/horovod/pull/3500))\r\n- Spark/Lightning: Added missing `tranform_spec` for Petastorm datamodule. ([#3543](https://github.com/horovod/horovod/pull/3543))\r\n- Spark/Lightning: Fixed PTL Spark example with checkpoint usage by calling `save_hyperparameters()`. ([#3527](https://github.com/horovod/horovod/pull/3527))\r\n- Elastic: Fixed empty hostname returned from `HostDiscoveryScript`. ([#3490](https://github.com/horovod/horovod/pull/3490))\r\n- TensorFlow 2.9: Fixed build for API change related to `tensorflow_accelerator_device_info`. ([#3513](https://github.com/horovod/horovod/pull/3513))\r\n- TensorFlow 2.10: Bumped build partially to C++17. ([#3558](https://github.com/horovod/horovod/pull/3558))\r\n- TensorFlow: Fixed gradient update timing in TF `AggregationHelperEager`. ([#3496](https://github.com/horovod/horovod/pull/3496))\r\n- TensorFlow: Fixed resource `NotFoundError` in TF `AggregationHelper`. ([#3499](https://github.com/horovod/horovod/pull/3499))\r\n",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.25.0",
        "name": "Reducescatter for NCCL, MPI and Gloo, AMD GPU XLA Op implementation, Spark Estimator improvements, TensorFlow Data Service Horovod job, Elastic run API",
        "release_id": 69911233,
        "tag": "v0.25.0",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.25.0",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/69911233",
        "value": "https://api.github.com/repos/horovod/horovod/releases/69911233",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.25.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "EnricoMi",
          "type": "User"
        },
        "date_created": "2022-04-22T08:04:48Z",
        "date_published": "2022-04-21T08:28:34Z",
        "description": "### Fixed\r\n\r\n- Make DBFSLocalStore support \"file:/dbfs/...\", implement get_localized_path. ([#3510](https://github.com/horovod/horovod/pull/3510))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.24.3",
        "name": "Hotfix: DBFSLocalStore get_localized_path implementation",
        "release_id": 64933310,
        "tag": "v0.24.3",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.24.3",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/64933310",
        "value": "https://api.github.com/repos/horovod/horovod/releases/64933310",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.24.3"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2022-03-10T18:37:13Z",
        "date_published": "2022-03-10T18:38:34Z",
        "description": "### Fixed\r\n\r\n- [Setup] Require fsspec >= 2010.07.0 ([#3451](https://github.com/horovod/horovod/pull/3451))\r\n- Fix ignored cuda arch flags ([#3462]((https://github.com/horovod/horovod/pull/3462))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.24.2",
        "name": "Hotfix: Fix ignored cuda arch flags",
        "release_id": 61528227,
        "tag": "v0.24.2",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.24.2",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/61528227",
        "value": "https://api.github.com/repos/horovod/horovod/releases/61528227",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.24.2"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "EnricoMi",
          "type": "User"
        },
        "date_created": "2022-03-03T20:39:04Z",
        "date_published": "2022-03-03T20:39:53Z",
        "description": "### Fixed\r\n\r\n- Extended CMake build script to often find CUDA even if `nvcc` is not in `$PATH`. ([#3444](https://github.com/horovod/horovod/pull/3444))\r\n",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.24.1",
        "name": "Hotfix: CMake better finding CUDA",
        "release_id": 60936167,
        "tag": "v0.24.1",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.24.1",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/60936167",
        "value": "https://api.github.com/repos/horovod/horovod/releases/60936167",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.24.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2022-03-02T15:52:13Z",
        "date_published": "2022-03-02T15:57:12Z",
        "description": "### Added\r\n\r\n- Ray: Added elastic keyword parameters to RayExecutor API: This API supports both static (non-elastic) and elastic Horovod jobs. ([#3190](https://github.com/horovod/horovod/issues/3190))\r\n- TensorFlow: Added in-place broadcasting of variables. ([#3128](https://github.com/horovod/horovod/pull/3128))\r\n- Elastic: Added support for resurrecting blacklisted hosts. ([#3319](https://github.com/horovod/horovod/pull/3319))\r\n- MXNet: Added support for MXNet async dependency engine. ([#3242](https://github.com/horovod/horovod/pull/3242), [#2963](https://github.com/horovod/horovod/pull/2963))\r\n- Spark/Lightning: Added history to lightning estimator. ([#3214](https://github.com/horovod/horovod/pull/3214))\r\n\r\n### Changed\r\n\r\n- Moved to CMake version 3.13 with first-class CUDA language support and re-enabled parallelized builds. Uses a temporary installation of CMake if CMake 3.13 is not found. ([#3261](https://github.com/horovod/horovod/pull/3261), [#3371](https://github.com/horovod/horovod/pull/3371))\r\n- Moved released Docker image `horovod` and `horovod-cpu` to Ubuntu 20.04 and Python 3.8. ([#3393](https://github.com/horovod/horovod/pull/3393))\r\n- Spark Estimator: Don't shuffle row groups if training data requires non-shuffle ([#3369](https://github.com/horovod/horovod/pull/3369))\r\n- Spark/Lightning: Reduced memory footprint of async dataloader. ([#3239](https://github.com/horovod/horovod/pull/3239))\r\n- Elastic: Improved handling NCCL errors under elastic scenario. ([#3112](https://github.com/horovod/horovod/pull/3112))\r\n- Spark/Lightning: Do not overwrite model with checkpoint by default. ([#3201](https://github.com/horovod/horovod/pull/3201))\r\n- Make checkpoint name optional so that user can save to h5 format. ([#3411](https://github.com/horovod/horovod/pull/3411))\r\n\r\n### Deprecated\r\n\r\n- Deprecated ElasticRayExecutor APIs in favor of the new RayExecutor API. ([#3190](https://github.com/horovod/horovod/issues/3190))\r\n\r\n### Removed\r\n\r\n- Spark: Removed `h5py<3` constraint as this is not needed anymore for Tensorflow >2.5.0. ([#3301](https://github.com/horovod/horovod/pull/3301))\r\n\r\n### Fixed\r\n\r\n- Elastic Spark: Fixed indices in initial task-to-task registration. ([#3410](https://github.com/horovod/horovod/pull/3410))\r\n- PyTorch: Fixed GIL-related deadlock with PyTorch 1.10.1. ([#3352](https://github.com/horovod/horovod/issues/3352))\r\n- PyTorch: Fixed finalization of ProcessSetTable. ([#3351](https://github.com/horovod/horovod/pull/3351))\r\n- Fixed remote trainers to point to the correct shared lib path. ([#3258](https://github.com/horovod/horovod/pull/3258))\r\n- Fixed imports from `tensorflow.python.keras` with tensorflow 2.6.0+. ([#3403](https://github.com/horovod/horovod/pull/3403))\r\n- Fixed Adasum communicator init logic. ([#3379](https://github.com/horovod/horovod/pull/3379))\r\n- Lightning: Fixed resume logger. ([#3375](https://github.com/horovod/horovod/pull/3375))\r\n- Fixed the checkpoint directory structure for pytorch and pytorch lightning. ([#3362](https://github.com/horovod/horovod/pull/3362))\r\n- Fixed possible integer overflow in multiplication. ([#3368](https://github.com/horovod/horovod/pull/3368))\r\n- Fixed the `pytorch_lightning_mnist.py` example. ([#3245](https://github.com/horovod/horovod/pull/3245), [#3290](https://github.com/horovod/horovod/pull/3290))\r\n- Fixed barrier segmentation fault. ([#3313](https://github.com/horovod/horovod/pull/3313))\r\n- Fixed `hvd.barrier()` tensor queue management. ([#3300](https://github.com/horovod/horovod/pull/3300))\r\n- Fixed PyArrow \"list index out of range\" IndexError. ([#3274](https://github.com/horovod/horovod/pull/3274))\r\n- Elastic: Fixed all workers sometimes failing on elastic Horovod failure. ([#3264](https://github.com/horovod/horovod/issues/3264))\r\n- Spark/Lightning: Fixed setting `limit_train_batches` and `limit_val_batches`. ([#3237](https://github.com/horovod/horovod/pull/3237))\r\n- Elastic: Fixed ElasticSampler and `hvd.elastic.state` losing some indices of processed samples when nodes dropped. ([#3143](https://github.com/horovod/horovod/issues/3143))\r\n- Spark/Lightning: Fixed history metrics for estimator serialization. ([#3216](https://github.com/horovod/horovod/pull/3216))\r\n- Ray: Fixed RayExecutor to fail when `num_workers=0` and `num_hosts=None`. ([#3210](https://github.com/horovod/horovod/pull/3210))\r\n- Spark/Lightning: Fixed checkpoint callback `dirpath` typo. ([#3204](https://github.com/horovod/horovod/pull/3204))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.24.0",
        "name": "Elastic mode improvements, MXNet async dependency engine, fixes for latest PyTorch and TensorFlow versions",
        "release_id": 60812299,
        "tag": "v0.24.0",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.24.0",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/60812299",
        "value": "https://api.github.com/repos/horovod/horovod/releases/60812299",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.24.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2021-10-06T17:50:49Z",
        "date_published": "2021-10-06T17:52:54Z",
        "description": "### Added\r\n\r\n- Added process sets to concurrently run collective operations on subsets of Horovod processes in TensorFlow, PyTorch, and MXNet. ([#2839](https://github.com/horovod/horovod/pull/2839), [#3042](https://github.com/horovod/horovod/pull/3042), [#3043](https://github.com/horovod/horovod/pull/3043), [#3054](https://github.com/horovod/horovod/pull/3054), [#3083](https://github.com/horovod/horovod/pull/3083), [#3090](https://github.com/horovod/horovod/pull/3090))\r\n\r\n- Added XLA support for Allreduce via `tf.function(jit_compile=True)`. ([#3053](https://github.com/horovod/horovod/pull/3053))\r\n\r\n- Added fused buffer scaling and unpack/pack kernels on GPU. ([#2973](https://github.com/horovod/horovod/pull/2973))\r\n\r\n- Added support for NCCL on CUDA 11.4. ([#3182](https://github.com/horovod/horovod/issues/3182))\r\n\r\n- Added fp16 compression for MXNet. ([#2987](https://github.com/horovod/horovod/issues/2987))\r\n\r\n- Added terminate_on_nan flag to Spark Lightning estimator. ([#3088](https://github.com/horovod/horovod/issues/3088))\r\n\r\n- Added barrier() API to torch module to support simple synchronization among ranks and to achieve parity with PyTorch DDP and similar frameworks. [#3139](https://github.com/horovod/horovod/pull/3139)\r\n\r\n- Added params for customizing Tensorboard callback. ([#3153](https://github.com/horovod/horovod/issues/3153))\r\n\r\n- Added `hvd.cross_rank()` for keras. ([#3008](https://github.com/horovod/horovod/issues/3008))\r\n\r\n- Added barrier() API to torch module to support simple synchronization among ranks and to achieve parity with PyTorch DDP and similar frameworks. [#3139](https://github.com/horovod/horovod/pull/3139)\r\n\r\n### Changed\r\n\r\n- Implemented more asynchronous dependency handling on GPU. ([#2963](https://github.com/horovod/horovod/pull/2963))\r\n\r\n- Ray: RayExecutor will now use the current placement group instead of always creating a new one. ([#3134](https://github.com/horovod/horovod/pull/3134))\r\n\r\n- Lightning: turned off shuffling for validation dataset. ([#2974](https://github.com/horovod/horovod/pull/2974))\r\n\r\n- Ray: RayExecutor will use the current placement group if one exists. ([#3134](https://github.com/horovod/horovod/pull/3134))\r\n\r\n- Extended `hvd.join()` to return the last rank that joined. ([#3097](https://github.com/horovod/horovod/pull/3097))\r\n\r\n### Removed\r\n\r\n- Spark/Keras: remove bare Keras support. ([#3191](https://github.com/horovod/horovod/pull/3191))\r\n\r\n### Fixed\r\n\r\n- Fix Horovod develop/editable install mode and incremental builds. ([#3074](https://github.com/horovod/horovod/pull/3074))\r\n\r\n- Estimator/Lightning: use lightning datamodule. ([#3084](https://github.com/horovod/horovod/pull/3084))\r\n\r\n- Fix Horovod Spark StringType and numpy type mapping issue. ([#3146](https://github.com/horovod/horovod/pull/3146))\r\n\r\n- Fixed error in Keras LearningRateScheduler. ([#3135](https://github.com/horovod/horovod/pull/3135))\r\n\r\n- Fixed bug in Lightning Profiler on Ray. ([#3122](https://github.com/horovod/horovod/pull/3122))\r\n\r\n- Fixed torch op lazy release to prevent OOM in elastic training. ([#3110](https://github.com/horovod/horovod/pull/3110))\r\n\r\n- Lightning: Fixed usage of the checkpoint callback. ([#3186](https://github.com/horovod/horovod/pull/3186))\r\n\r\n- Fixed MPICH support to use Intel MPI's implementation. ([#3148](https://github.com/horovod/horovod/pull/3148))\r\n\r\n- Fixed race condition in PyTorch async dataloader. ([#3120](https://github.com/horovod/horovod/pull/3120))\r\n\r\n- Keras: Fixed learning rate scheduler. ([#3142](https://github.com/horovod/horovod/pull/3142), [#3135](https://github.com/horovod/horovod/pull/3135))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.23.0",
        "name": "Process sets, XLA support, improved GPU backend",
        "release_id": 50910322,
        "tag": "v0.23.0",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.23.0",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/50910322",
        "value": "https://api.github.com/repos/horovod/horovod/releases/50910322",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.23.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2021-06-10T15:56:27Z",
        "date_published": "2021-06-10T15:57:35Z",
        "description": "### Added\r\n\r\n- Estimator: added support for loading data from S3, GCS, ADLS, and other remote filesystems. ([#2927](https://github.com/horovod/horovod/issues/2927))\r\n\r\n- Estimator: added custom Spark data loader interface. ([#2938](https://github.com/horovod/horovod/issues/2923))\r\n\r\n- LightningEstimator: added support to supply a logger and associated parameter to control the frequency of logging. ([#2926](https://github.com/horovod/horovod/pull/2926))\r\n\r\n- Estimator: added check to ensure all ranks have the same device type. ([#2942](https://github.com/horovod/horovod/pull/2942))\r\n\r\n### Changed\r\n\r\n- Changed behavior from using TensorBoardLogger to now using it as a fallback if a logger is not supplied. ([#2926](https://github.com/horovod/horovod/pull/2926))\r\n\r\n- Ray: disabled capturing child tasks in placement group. ([#2920](https://github.com/horovod/horovod/pull/2920))\r\n\r\n### Fixed\r\n\r\n- Fixed `hvd.tensorflow.keras.Compression`, accidentally removed in v0.22.0. ([#2945](https://github.com/horovod/horovod/pull/2945))\r\n\r\n- TorchEstimator: fixed usage of `validation_steps` in place of `validation_steps_per_epoch`. ([#2918](https://github.com/horovod/horovod/pull/2918))\r\n\r\n- TensorFlow: fixed C++ API for TF v2.6.0. ([#2932](https://github.com/horovod/horovod/pull/2932))\r\n\r\n- PyTorch: fixed `sparse_allreduce_async` for PyTorch v0.10.0. ([#2965](https://github.com/horovod/horovod/pull/2965))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.22.1",
        "name": "Remote filesystem support, estimator fixes",
        "release_id": 44425986,
        "tag": "v0.22.1",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.22.1",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/44425986",
        "value": "https://api.github.com/repos/horovod/horovod/releases/44425986",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.22.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2021-05-18T23:26:46Z",
        "date_published": "2021-05-19T15:17:41Z",
        "description": "### Added\r\n\r\n- Added pytorch_lightning spark estimator which enables training pytorch_lightning models. ([#2713](https://github.com/horovod/horovod/pull/2713))\r\n\r\n- Added NVTX tracing hooks for profiling with Nsight Systems. ([#2723](https://github.com/horovod/horovod/pull/2723))\r\n\r\n- Added a generic `num_workers` API for ``RayExecutor`` ([#2870](https://github.com/horovod/horovod/pull/2870))\r\n\r\n- Supports Ray Client without code changes. ([#2882](https://github.com/horovod/horovod/pull/2882))\r\n\r\n- Supports inmemory cache option for Keras Estimator. ([#2896](https://github.com/horovod/horovod/pull/2896))\r\n\r\n- Added FP16 support for GPU tensor in mxnet. ([#2915](https://github.com/horovod/horovod/pull/2915))\r\n\r\n- Added response caching for allgather operations. ([#2872](https://github.com/horovod/horovod/pull/2872))\r\n\r\n- Estimator: add petastorm reader_pool_type into constructor ([#2903](https://github.com/horovod/horovod/pull/2903))\r\n\r\n### Changed\r\n\r\n- Changed `alltoall` to return the received splits as a second return value if non-uniform splits are sent. ([#2631](https://github.com/horovod/horovod/pull/2631))\r\n\r\n- Changed ``RayExecutor`` to use [Ray Placement Groups](https://docs.ray.io/en/master/placement-group.html) for worker colocation. ([#2824](https://github.com/horovod/horovod/pull/2824))\r\n\r\n- Changed ``Inmemory dataloader`` usage for Torch Estimator with petastorm v0.11.0 release. ([#2896](https://github.com/horovod/horovod/pull/2896))\r\n\r\n### Fixed\r\n\r\n- Changed RayExecutor to use Ray node ID to enable multi-container:single-host setups. ([#2883](https://github.com/horovod/horovod/pull/2882))\r\n\r\n- Support sparse gradients aggregation in TF1 Keras. ([#2879](https://github.com/horovod/horovod/pull/2879))\r\n\r\n- Respect `global_step` parameter for LegacyOptimizers when aggregating gradients.  ([#2879](https://github.com/horovod/horovod/pull/2879))\r\n\r\n- Fixed compatibility with PyTorch 1.9.0. ([#2829](https://github.com/horovod/horovod/pull/2829))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.22.0",
        "name": "PyTorch Lightning Estimator, Nsight profiling, PyTorch 1.9 support",
        "release_id": 43226883,
        "tag": "v0.22.0",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.22.0",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/43226883",
        "value": "https://api.github.com/repos/horovod/horovod/releases/43226883",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.22.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2020-11-23T23:55:33Z",
        "date_published": "2020-11-23T23:57:42Z",
        "description": "## Detailed Changes\r\n\r\n### Added\r\n\r\n- Added support for backward_passes_per_step > 1 for TF Keras graph mode. ([#2346](https://github.com/horovod/horovod/pull/2346))\r\n\r\n- Added support for backward_passes_per_step > 1 for TF Keras eager execution. ([#2371](https://github.com/horovod/horovod/pull/2371))\r\n\r\n- Added support for backward_passes_per_step > 1 for TF LegacyOptimizer in graph mode. ([#2401](https://github.com/horovod/horovod/pull/2401))\r\n\r\n- Added grouped allreduce to enable more efficient tensor fusion and deterministic training. ([#2453](https://github.com/horovod/horovod/pull/2453))\r\n\r\n- Add support for specifying `op` and `compression` in `horovod.tensorflow.keras.allreduce()`. ([#2423](https://github.com/horovod/horovod/pull/2423))\r\n\r\n- Adding support for batched D2D memcopy kernel on GPU. ([#2435](https://github.com/horovod/horovod/pull/2435))\r\n\r\n- Added schema inference in Spark Estimator without sampling. ([#2373](https://github.com/horovod/horovod/pull/2373))\r\n\r\n- Added `Store.create(\"dbfs:/\")` mapping to `DBFSLocalStore(\"/dbfs/...\")`. ([#2376](https://github.com/horovod/horovod/pull/2376))\r\n\r\n### Changed\r\n\r\n- Changed Keras callbacks to require parameter `initial_lr` of `LearningRateScheduleCallback` and `LearningRateWarmupCallback`. ([#2459](https://github.com/horovod/horovod/pull/2459))\r\n\r\n- Changed default cycle time from 5ms to 1ms and fusion threshold from 64MB to 128MB. ([#2468](https://github.com/horovod/horovod/pull/2468))\r\n\r\n### Fixed\r\n\r\n- Fixed support for TensorFlow v2.4.0. ([#2381](https://github.com/horovod/horovod/pull/2381))\r\n\r\n- Fixed averaging using CUDA half2 implementation one element half buffers. ([#2375](https://github.com/horovod/horovod/pull/2375))\r\n\r\n- Fixed `HOROVOD_THREAD_AFFINITY` when using oneCCL. ([#2350](https://github.com/horovod/horovod/pull/2350))\r\n\r\n- Added timeout to SSH check in horovodrun to prevent hanging. ([#2448](https://github.com/horovod/horovod/pull/2448))\r\n\r\n- Added `HOROVOD_GLOO_TIMEOUT_SECONDS` value to error messages. ([#2436](https://github.com/horovod/horovod/pull/2436))\r\n\r\n- Fixed race condition in dynamic timeline API. ([#2341](https://github.com/horovod/horovod/pull/2341))\r\n\r\n- Fixed --log-hide-timestamp to apply to driver logs with Gloo. ([#2388](https://github.com/horovod/horovod/pull/2388))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.21.0",
        "name": "Local Gradient Aggregation, Grouped Allreduce",
        "release_id": 34336229,
        "tag": "v0.21.0",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.21.0",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/34336229",
        "value": "https://api.github.com/repos/horovod/horovod/releases/34336229",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.21.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2020-10-01T14:49:20Z",
        "date_published": "2020-10-01T14:56:31Z",
        "description": "## Detailed Changes\r\n\r\n### Added\r\n\r\n- Added Elastic Ray integration. ([#2291](https://github.com/horovod/horovod/pull/2291))\r\n\r\n### Changed\r\n\r\n- Removed dependency on SSH access for Ray. ([#2275](https://github.com/horovod/horovod/pull/2275))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.20.3",
        "name": "Elastic Horovod on Ray",
        "release_id": 32051761,
        "tag": "v0.20.3",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.20.3",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/32051761",
        "value": "https://api.github.com/repos/horovod/horovod/releases/32051761",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.20.3"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2020-09-26T02:55:16Z",
        "date_published": "2020-09-26T02:58:05Z",
        "description": "## Detailed Changes\r\n\r\n### Fixed\r\n\r\n- Fixed building Horovod without HOROVOD_WITHOUT_MXNET when MXNet is not installed. ([#2334](https://github.com/horovod/horovod/pull/2334))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.20.2",
        "name": "Hotfix: build without MXNet installed",
        "release_id": 31837437,
        "tag": "v0.20.2",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.20.2",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/31837437",
        "value": "https://api.github.com/repos/horovod/horovod/releases/31837437",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.20.2"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2020-09-25T19:33:35Z",
        "date_published": "2020-09-25T19:38:01Z",
        "description": "## Detailed Changes\r\n\r\n### Added\r\n\r\n- Added Databricks storage `DBFSLocalStore` and support for GPU-aware scheduling to horovod.spark Estimator. ([#2234](https://github.com/horovod/horovod/pull/2234))\r\n\r\n- Added ElasticSampler and PyTorch Elastic ImageNet example. ([#2297](https://github.com/horovod/horovod/pull/2297))\r\n\r\n- Added ability to dynamically start and stop timeline programmatically. ([#2215](https://github.com/horovod/horovod/pull/2215))\r\n\r\n- Added support for Gloo on macOS. ([#2254](https://github.com/horovod/horovod/pull/2254))\r\n\r\n- Exposed name argument to TensorFlow allreduce operation. ([#2325](https://github.com/horovod/horovod/pull/2325))\r\n\r\n- Added option to strip outer name scope from Horovod ops in TensorFlow. ([#2328](https://github.com/horovod/horovod/pull/2328))\r\n\r\n### Fixed\r\n\r\n- Fixed usage of VERBOSE=1 when setting custom MAKEFLAGS. ([#2239](https://github.com/horovod/horovod/pull/2239))\r\n\r\n- Fixed bugs in Keras Elastic Callback classes. ([#2289](https://github.com/horovod/horovod/pull/2289))\r\n\r\n- Fixed RelWithDebInfo build and made it the default with -03 optimizations. ([#2305](https://github.com/horovod/horovod/pull/2305))\r\n\r\n- Fixed usage of tf.cond in TensorFlow alltoall gradient. ([#2327](https://github.com/horovod/horovod/pull/2327))\r\n\r\n- Fixed allreduce averaging for TF IndexedSlices in ROCm path. ([#2279](https://github.com/horovod/horovod/pull/2279))\r\n\r\n- Include stdexcept to handle certain compiler / frameworks that don't include it already. ([#2238](https://github.com/horovod/horovod/pull/2238))\r\n\r\n- Fixed Debug builds by setting compiler options based on CMake build type. ([#2263](https://github.com/horovod/horovod/pull/2263))\r\n\r\n- Skipped launching zero-sized send/recvs for NCCLAlltoall. ([#2273](https://github.com/horovod/horovod/pull/2273))\r\n\r\n- Fixed missing run in tf keras elastic mode. ([#2272](https://github.com/horovod/horovod/pull/2272))\r\n\r\n- Fixed loss function in TensorFlow2 elastic synthetic benchmark. ([#2265](https://github.com/horovod/horovod/pull/2265))\r\n\r\n- Fixed usage of HOROVOD_MIXED_INSTALL env var in alltoall tests. ([#2266](https://github.com/horovod/horovod/pull/2266))\r\n\r\n- Removed keras requirement from Ray example. ([#2262](https://github.com/horovod/horovod/pull/2262))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.20.1",
        "name": "Bugfixes, Databricks Runtime support for Estimators, ElasticSampler",
        "release_id": 31828806,
        "tag": "v0.20.1",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.20.1",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/31828806",
        "value": "https://api.github.com/repos/horovod/horovod/releases/31828806",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.20.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2020-09-03T23:26:25Z",
        "date_published": "2020-09-04T00:34:45Z",
        "description": "## Elastic Horovod API + Spark Auto-Scaling ([#1849](https://github.com/horovod/horovod/pull/1849), [#1956](https://github.com/horovod/horovod/pull/1956))\r\n\r\nElastic training enables Horovod to scale up and down the number of workers dynamically at runtime, without requiring a restart or resuming from checkpoints saved to durable storage. With elastic training, workers can come and go from the Horovod job without interrupting the training process.\r\n\r\nSupport for auto-scaling can be added to any existing Horovod script with just a few modifications:\r\n\r\n1. Decorate retryable functions with `@hvd.elastic.run`.\r\n2. Track state that needs to be kept in sync across workers in a `hvd.elastic.State` object.\r\n3. Perform all Horovod collective operations (allreduce, allgather, broadcast, etc.) inside the retryable functions.\r\n\r\nHere's an example for PyTorch:\r\n\r\n```\r\nimport torch\r\nimport horovod.torch as hvd\r\n\r\nhvd.init()\r\ntorch.cuda.set_device(hvd.local_rank())\r\n\r\nmodel = ...\r\ndataset = ...\r\n\r\n@hvd.elastic.run\r\ndef train(state):\r\n    for state.epoch in range(state.epoch, args.epochs + 1):\r\n        dataset.set_epoch(state.epoch)\r\n        dataset.set_batch_idx(state.batch_idx)\r\n        for state.batch_idx, (data, target) in enumerate(dataset):\r\n            state.optimizer.zero_grad()\r\n            output = state.model(data)\r\n            loss = F.nll_loss(output, target)\r\n            loss.backward()\r\n            state.optimizer.step()\r\n            state.commit()\r\n\r\noptimizer = optim.SGD(model.parameters(), lr * hvd.size())\r\noptimizer = hvd.DistributedOptimizer(optimizer)\r\n\r\ndef on_state_reset():\r\n    # adjust learning rate on reset\r\n    for param_group in optimizer.param_groups:\r\n        param_group['lr'] = lr * hvd.size()\r\n\r\nstate = hvd.elastic.TorchState(model, optimizer, epoch=1, batch_idx=0)\r\nstate.register_reset_callbacks([on_state_reset])\r\ntrain(state)\r\n```\r\n\r\nRun using `horovodrun` by specifying the minimum and maximum number of worker processes, as well as a \"host discovery script\" that will be used to find available workers to add at runtime:\r\n\r\n```\r\n$ horovodrun -np 8 --min-np 4 --max-np 12 --host-discovery-script discover_hosts.sh python train.py\r\n```\r\n\r\nElastic Horovod is supported natively with Spark auto-scaling using the `hvd.spark.run_elastic` API.\r\n\r\nFor more details, see [Elastic Horovod](https://horovod.readthedocs.io/en/v0.20.0/elastic_include.html).\r\n\r\n## Horovod on Ray ([#2218](https://github.com/horovod/horovod/pull/2218))\r\n\r\n[Ray](https://ray.io/) is a distributed execution framework that makes it easy to provision and scale distributed applications, and can now be used to execute Horovod jobs without needing to coordinate the workers by hand:\r\n\r\n```\r\nfrom horovod.ray import RayExecutor\r\n\r\n# Start the Ray cluster or attach to an existing Ray cluster\r\nray.init()\r\n\r\n# Start num_hosts * num_slots actors on the cluster\r\nexecutor = RayExecutor(\r\n    setting, num_hosts=num_hosts, num_slots=num_slots, use_gpu=True)\r\n\r\n# Launch the Ray actors on each machine\r\n# This will launch `num_slots` actors on each machine\r\nexecutor.start()\r\n\r\n# Using the stateless `run` method, a function can take in any args or kwargs\r\ndef train_fn():\r\n    hvd.init()\r\n    # Train the model on each worker here\r\n    ...\r\n\r\n# Execute the function on all workers at once\r\nresults = executor.run(train_fn)\r\n\r\nexecutor.shutdown()\r\n```\r\n\r\nHorovod now also integrates with [Ray Tune](https://docs.ray.io/en/latest/tune/) to scale up your hyperparameter search jobs.  Check out the example [here](https://github.com/ray-project/ray/blob/master/python/ray/tune/examples/horovod_simple.py).\r\n\r\nFor more details, see [Horovod on Ray](https://horovod.readthedocs.io/en/v0.20.0/ray_include.html).\r\n\r\n## All-to-All Operation ([#2143](https://github.com/horovod/horovod/pull/2143))\r\n\r\nThe all-to-all collective can be described as a combination of a scatter and gather, where each worker will scatter a tensor to each worker, while also gathering scattered data from other workers. This type of collective communication can arise in model-parallel training strategies.\r\n\r\nThe `hvd.alltoall` function takes the form `hvd.alltoall(tensor, splits=None)`,\r\nwhere `tensor` is a multi-dimensional tensor of data to scattered and `splits` is an optional 1D tensor of integers with length equal to the number of workers, describing how to split and distribute tensor. `splits` is applied along the first dimension of `tensor`. If splits is not provided, an equal splitting is assumed, where the first dimension is divided by the number of workers.\r\n\r\nThe implementation supports TensorFlow, PyTorch, and MXNet using the MPI backend, the CUDA-aware MPI backend via `HOROVOD_GPU_ALLTOALL=MPI`, and the NCCL backend via `HOROVOD_GPU_ALLTOALL=NCCL` / `HOROVOD_GPU_OPERATIONS=NCCL`.\r\n\r\n## Gradient Predivide Factor ([#1949](https://github.com/horovod/horovod/pull/1949))\r\n\r\nWe've added a `gradient_predivide_factor` parameter in the `DistributedOptimizer`, the purpose of which is to enable splitting the averaging before and after the allreduce. This can be useful in managing the numerical range for mixed precision computations. \r\n\r\nThe `gradient_predivide_factor` is applied as follows:\r\n\r\n```\r\n        If op == Average, gradient_predivide_factor splits the averaging\r\n        before and after the sum. Gradients are scaled by\r\n        1.0 / gradient_predivide_factor before the sum and\r\n        gradient_predivide_factor / size after the sum. \r\n```\r\n\r\nTo facilitate this, additional arguments (`prescale_factor` and `postscale_factor`) have been added to the basic `hvd.allreduce` functions, enabling the definition of multiplicative factors to scale the tensors before and after the allreduce respectively. For efficiency, the pre and post-scaling is implemented in the Horovod backend on the fused tensor buffer, rather than through framework level operations. For GPU, this required a CUDA kernel implementation to scale the GPU buffer which in turn, required adding compilation of CUDA code to the current build infrastructure.\r\n\r\nAs an additional general benefit from these changes, gradient averaging in the optimizer can now be carried out within the Horovod backend on the fused tensor buffer using the `postscale_factor` argument, rather than on a tensor by tensor basis at the framework level, decreasing the overhead of each allreduce call.\r\n\r\n## CMake Build System ([#2009](https://github.com/horovod/horovod/pull/2009))\r\n\r\n[CMake](https://cmake.org/), previously used to compile the optional [Gloo](https://horovod.readthedocs.io/en/v0.20.0/install_include.html#gloo) controller, is now required to install Horovod.  This change introduces a number of exciting benefits for Horovod developers and users:\r\n\r\n- Much faster installation times through a parallel task build\r\n- Incremental builds (almost instantaneous build when developing and making small changes at a time)\r\n- Separation of the build config phase with the build phase (less overhead for repeated builds)\r\n- Reuse `find_package` modules provided by CMake for MPI, CUDA, etc. to better handle a range of environment configurations\r\n- Libraries can be built outside of the python build process (no longer requiring `setup.py`)\r\n- Flexibility for the build system (make, ninja, IDEs, etc.)\r\n\r\n## Detailed Changes\r\n\r\n### Added\r\n\r\n- Added bare-metal elastic mode implementation to enable auto-scaling and fault tolerance. ([#1849](https://github.com/horovod/horovod/pull/1849))\r\n\r\n- Added Elastic Horovod support for Spark auto-scaling. ([#1956](https://github.com/horovod/horovod/pull/1956))\r\n\r\n- Added All-to-All operation for TensorFlow, PyTorch, and MXNet. ([#2143](https://github.com/horovod/horovod/pull/2143))\r\n\r\n- Added support for `gradient_predivide_factor` and averaging in Horovod backend. ([#1949](https://github.com/horovod/horovod/pull/1949))\r\n\r\n- Added NCCL implementation of the allgather operation. ([#1952](https://github.com/horovod/horovod/pull/1952))\r\n\r\n- Added `HOROVOD_GPU_OPERATIONS` installation variable to simplify enabling NCCL support for all GPU operations. ([#1960](https://github.com/horovod/horovod/pull/1960))\r\n\r\n- Added TensorFlow implementation of `SyncBatchNormalization` layer. ([#2075](https://github.com/horovod/horovod/pull/2075))\r\n\r\n- Added `hvd.is_initialized()` method. ([#2020](https://github.com/horovod/horovod/pull/2020))\r\n\r\n- Added `hvd.allgather_object` function for TensorFlow, PyTorch, and MXNet. ([#2166](https://github.com/horovod/horovod/pull/2166))\r\n\r\n- Added `hvd.broadcast_object` function for MXNet. ([#2122](https://github.com/horovod/horovod/pull/2122))\r\n\r\n- Added `label_shapes` parameter to KerasEstimator and TorchEstimator. ([#2140](https://github.com/horovod/horovod/pull/2140))\r\n\r\n- Added optional `modelCheckPoint` callback to KerasEstimator params. ([#2124](https://github.com/horovod/horovod/pull/2124))\r\n\r\n- Added `ssh_identity_file` argument to `horovodrun`. ([#2201](https://github.com/horovod/horovod/pull/2201))\r\n\r\n- Added support for `horovodrun` on `kubeflow/mpi-job`. ([#2199](https://github.com/horovod/horovod/pull/2199))\r\n\r\n- Added Ray integration. ([#2218](https://github.com/horovod/horovod/pull/2218))\r\n\r\n### Changed\r\n\r\n- Moved `horovod.run.runner.run` to `horovod.run`. ([#2099](https://github.com/horovod/horovod/pull/2099))\r\n\r\n- HOROVOD_THREAD_AFFINITY accepts multiple values, one for every Horovod rank ([#2131](https://github.com/horovod/horovod/pull/2131))\r\n\r\n- Migrated build system for native libraries to CMake ([#2009](https://github.com/horovod/horovod/pull/2009))\r\n\r\n### Deprecated\r\n\r\n- HOROVOD_CCL_BGT_AFFINITY is deprected. Use HOROVOD_THREAD_AFFINITY instead ([#2131](https://github.com/horovod/horovod/pull/2131))\r\n\r\n### Removed\r\n\r\n- Dropped support for Python 2. ([#1954](https://github.com/horovod/horovod/pull/1954))\r\n\r\n- Dropped support for TensorFlow < 1.15. ([#2169](https://github.com/horovod/horovod/pull/2169))\r\n\r\n- Dropped support for PyTorch < 1.2. ([#2086](https://github.com/horovod/horovod/pull/2086))\r\n\r\n### Fixed\r\n\r\n- Fixed MXNet allgather implementation to correctly handle resizing the output buffer. ([#2092](https://github.com/horovod/horovod/pull/2092))\r\n\r\n- Fixed Keras Spark Estimator incompatibility with TensorFlow 1.15 due to `tf.autograph`. ([#2069](https://github.com/horovod/horovod/pull/2069))\r\n\r\n- Fixed API compatibility with PyTorch 1.6. ([#2051](https://github.com/horovod/horovod/pull/2051))\r\n\r\n- Fixed Keras API compatibility with TensorFlow 2.4.0. ([#2178](https://github.com/horovod/horovod/pull/2178))\r\n\r\n- Fixed allgather gradient for TensorFlow 2 in cases where the tensor shape is not known during graph construction. ([#2121](https://github.com/horovod/horovod/pull/2121))\r\n\r\n- Fixed running using Gloo with an imbalanced number of workers per host. ([#2212](https://github.com/horovod/horovod/pull/2212))",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.20.0",
        "name": "Elastic Horovod, Ray integration, All-to-All, Gradient Predivide, CMake build system",
        "release_id": 30695280,
        "tag": "v0.20.0",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.20.0",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/30695280",
        "value": "https://api.github.com/repos/horovod/horovod/releases/30695280",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.20.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2020-06-24T16:42:09Z",
        "date_published": "2020-06-24T16:44:08Z",
        "description": "## Fixed\r\n\r\n- Added PYTHONPATH to mpirun env. (#2038)",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.19.5",
        "name": "Hotfix for adding PYTHONPATH to mpirun env",
        "release_id": 27882282,
        "tag": "v0.19.5",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.19.5",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/27882282",
        "value": "https://api.github.com/repos/horovod/horovod/releases/27882282",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.19.5"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2020-05-28T22:22:06Z",
        "date_published": "2020-05-28T22:28:14Z",
        "description": "## Fixed\r\n\r\n- Fixed Sync Batch Norm when using PyTorch 1.5. (#1980)\r\n- Fixed compatibility with mixed precision Keras policy in TensorFlow 2.2. (#1992)",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.19.4",
        "name": "Hotfix for sync batch norm in PyTorch 1.5, mixed precision in TensorFlow 2.2",
        "release_id": 27011219,
        "tag": "v0.19.4",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.19.4",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/27011219",
        "value": "https://api.github.com/repos/horovod/horovod/releases/27011219",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.19.4"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2020-05-22T16:35:29Z",
        "date_published": "2020-05-22T16:54:52Z",
        "description": "## Fixed\r\n\r\n- Fixed issue with network interface discovery causing `horovodrun` to fail during startup (#1974).\r\n",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.19.3",
        "name": "Hotfix for horovodrun network interface discovery process",
        "release_id": 26806230,
        "tag": "v0.19.3",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.19.3",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/26806230",
        "value": "https://api.github.com/repos/horovod/horovod/releases/26806230",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.19.3"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2020-05-13T20:27:30Z",
        "date_published": "2020-05-13T21:18:46Z",
        "description": "## Highlights\r\n\r\n- Added [Platform LSF](https://en.wikipedia.org/wiki/Platform_LSF) and `jsrun` support to `horovodrun`. (#1805)\r\n- Added support for running Horovod on Spark with Gloo in place of MPI. (#1807)\r\n- Added synchronous batch normalization for `horovod.torch` API. (#1923)\r\n\r\n## Additional changes\r\n\r\n- Added support for providing a set of inclusive NICs to `horovodrun`. (#1808)\r\n- Added optional `initial_lr` parameter to `LearningRateScheduleCallback`, deprecated implicit initialization. (#1933)\r\n- Changed Spark Estimators to use Petastorm `BatchDataLoader`. (#1879)\r\n- Changed Spark Estimators to use Petastorm's `make_reader` API. (#1804)\r\n- Improved latency of background thread loop. (#1880)\r\n- Enabled setting Horovod background thread affinity with all frameworks. (#1881)\r\n- Added `verbose` parameter to `SparkBackend`. (#1922)\r\n- Use parameter names when scheduling broadcasts in MXNet `broadcast_parameters`. (#1894)\r\n- Added metadata cache with calling `fit_on_parquet`. (#1826)\r\n- Added optional local version to package version. (#1925)\r\n\r\n## Bugfixes\r\n\r\n- Fixed module resolution for `tf.keras` optimizers when calling `hvd.load_model`. (#1935)\r\n- Modified `safe_shell_exec` to use multiprocessing spawn instead of fork to prevent deadlocks. (#1915)\r\n- Fixed multiprocessing to support Python 3.8. (#1904)\r\n- Added extra preprocessor guard for FMA optimization. (#1835)\r\n- Fixed exception in `KerasEstimator` when `num_proc` is larger than 4. (#1945)\r\n- Fixed memory leaks. (#1845)\r\n- Fixed a bug with sample weight in `TorchEstimator`. (#1790)\r\n- Removed `torchvision` from `pytorch` extra. (#1899)",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.19.2",
        "name": "Platform LSF support, Spark on Gloo, and Sync Batch Norm",
        "release_id": 26486190,
        "tag": "v0.19.2",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.19.2",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/26486190",
        "value": "https://api.github.com/repos/horovod/horovod/releases/26486190",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.19.2"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2020-03-16T19:51:26Z",
        "date_published": "2020-05-13T20:56:39Z",
        "description": "## TensorFlow\r\n\r\n- Added `_aggregate_gradients` in `DistributedOptimizer` to support Keras in TensorFlow 2.2. (#1784)\r\n\r\n## PyTorch\r\n\r\n- Removed uses of deprecated PyTorch C++ API to support PyTorch 1.6. (#1731)\r\n\r\n## Changes to `horovodrun`\r\n\r\n- Added process binding arguments to horovodrun. (#1767)\r\n- Added `--tcp` flag to horovodrun for TCP only communication. (#1744)\r\n\r\n## Changes to installer\r\n\r\n- Added `HOROVOD_BUILD_ARCH_FLAGS` to specify architecture-specific compiler flags. (#1751)\r\n- Added Python extras to enforce that Horovod is installed after other frameworks. (#1785)\r\n\r\n## API changes\r\n\r\n- Added `mpi_args` to `horovod.run.run`. (#1787)\r\n- Added support for data transformation before train and validation in `TorchEstimator` (#1750)\r\n\r\n## Bugs\r\n\r\n- Fixed bug in cache dump. (#1739)\r\n- Fixed root rank output handling in MXNet out-of-place broadcast. (#1740)\r\n- Fixed `data_type_to_str` for SparseVector and DenseVector. (#1780)",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.19.1",
        "name": "TensorFlow 2.2 compatibility, MPI args for horovodrun",
        "release_id": 26485437,
        "tag": "v0.19.1",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.19.1",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/26485437",
        "value": "https://api.github.com/repos/horovod/horovod/releases/26485437",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.19.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "tgaddair",
          "type": "User"
        },
        "date_created": "2020-01-14T16:22:59Z",
        "date_published": "2020-01-14T16:39:30Z",
        "description": "In version 0.19.0, Horovod adds tighter integration with Apache Spark, including a new high-level **Horovod Spark Estimator** framework and support for accelerator-aware task-level scheduling in the upcoming **Spark 3.0** release. This release also contains experimental new features including a **join operation** for PyTorch and the ability to launch Horovod jobs programmatically from environments like notebooks using a new **interactive run mode**.\r\n\r\n## Horovod Spark Estimators (#1554)\r\n\r\nTo bridge the gap between large-scale data processing in Spark and large-scale deep learning training with Horovod, we\u2019re introducing a new open source API called **Horovod Spark Estimators**.\r\n\r\nWith Horovod Spark Estimators, you can train your deep neural network directly on your existing Spark DataFrame, leveraging Horovod\u2019s ability to scale to hundreds of workers in parallel without any specialized code for distributed training:\r\n\r\n```\r\nfrom tensorflow import keras\r\nimport tensorflow as tf\r\nimport horovod.spark.keras as hvd\r\n\r\nmodel = keras.models.Sequential()\r\n    .add(keras.layers.Dense(8, input_dim=2))\r\n    .add(keras.layers.Activation('tanh'))\r\n    .add(keras.layers.Dense(1))\r\n    .add(keras.layers.Activation('sigmoid'))\r\n\r\n# NOTE: unscaled learning rate\r\noptimizer = keras.optimizers.SGD(lr=0.1)\r\nloss = 'binary_crossentropy'\r\n\r\nstore = HDFSStore('/user/username/experiments')\r\nkeras_estimator = hvd.KerasEstimator(\r\n    num_proc=4,\r\n    store=store,\r\n    model=model,\r\n    optimizer=optimizer,\r\n    loss=loss,\r\n    feature_cols=['features'],\r\n    label_cols=['y'],\r\n    batch_size=32,\r\n    epochs=10)\r\n\r\n\r\nkeras_model = keras_estimator.fit(train_df) \\\r\n    .setOutputCols(['predict'])\r\npredict_df = keras_model.transform(test_df)\r\n```\r\n\r\nHorovod Spark Estimators provide a single abstraction \u2014 the Estimator \u2014 which hides the complexity of gluing Spark DataFrames to a deep learning training script, reading data into a format interpretable by the training framework, and distributing the training using Horovod.  The user only needs to provide a model written in the deep learning framework of their choice, and the Estimator will do the work of fitting it to the DataFrame. \r\n\r\nAfter training, the Estimator returns a Transformer representation of the trained model. The model transformer can be used like any Spark ML transformer to make predictions on an input DataFrame, writing them as new columns in the output DataFrame.\r\n\r\nEstimators can be used to track experiment history through model checkpointing, hot start retraining, and metric logging (for Tensorboard) using the Estimator Store abstraction. Stores persist all training artifacts including intermediate representations of the training data. Horovod natively supports stores for HDFS and local filesystems.\r\n\r\nHorovod Spark Estimators are available for **Keras** (both tf.keras and standalone Keras) and **PyTorch**, with more frameworks (including pure TensorFlow) coming soon.\r\n\r\n## Spark 3.0 task-level GPU scheduling (#1584)\r\n\r\nApache Spark 3.0 introduces a new accelerator-aware scheduling capability, allowing a production ETL job running on CPUs to hand off data to Horovod running distributed deep learning training on GPUs within the same pipeline, breaking down the barriers between ETL and continuous model training.\r\n\r\nHorovod users can now request GPU resources directly from their Spark application, without assuming which tasks should map to which GPUs:\r\n\r\n```\r\nimport horovod.spark\r\n\r\ndef train():\r\n    from horovod.spark.task import get_available_devices\r\n    import horovod.tensorflow.keras as hvd\r\n\r\n    hvd.init()\r\n    \r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    config.gpu_options.visible_device_list = get_available_devices()[0]\r\n    K.set_session(tf.Session(config=config))\r\n\r\n    ...\r\n\r\nhorovod.spark.run(train)\r\n```\r\n\r\nCheck out the [keras_spark3_rossmann.py](https://github.com/horovod/horovod/blob/master/examples/keras_spark3_rossmann.py) script for a complete example.\r\n\r\nSpark 3.0 is currently in [preview release](https://spark.apache.org/docs/3.0.0-preview), with the full release forthcoming.\r\n\r\n## Join Operation for PyTorch (#1058)\r\n\r\nThe ability for different workers to train on a different number of batches in each epoch has been one of the most requested features for Horovod.  This problem frequently arises when a dataset doesn\u2019t evenly split among all workers, requiring the user to truncate any extra examples or risk deadlock during training.  \r\n\r\nWith the new join operation, users no longer need to worry about how evenly their dataset divides when training.  Just add a join step at the end of each epoch, and Horovod will train on any extra batches without causing the waiting workers to deadlock:\r\n\r\n```\r\nfor epoch in range(epochs):\r\n    for batch in dataset:\r\n        ...\r\n    hvd.join(device=hvd.local_rank())\r\n```\r\n\r\nThe join operation is currently supported in Horovod for PyTorch, with support for TensorFlow and Apache MXNet coming soon.\r\n\r\n## Interactive Run Mode (#1307)\r\n\r\nWith `horovod.spark.run`, Horovod was made to support launching training jobs programmatically by defining Python functions that were executed on Spark Executors.  Within Horovod Interactive Run Mode, we created a similar API that can launch training jobs on any visible hosts, similar to the command-line `horovodrun` tool:\r\n\r\n```\r\nfrom horovod.run import run as hvdrun\r\n\r\ndef train():\r\n    import horovod.tensorflow as hvd\r\n    hvd.init()\r\n    ...\r\n\r\nresults = hvdrun(train, np=2)\r\n```\r\n\r\nInteractive mode supports most of the functionality provided by `horovodrun`.  See the [API](https://horovod.readthedocs.io/en/latest/api.html#module-horovod.run) for a complete reference.\r\n\r\n## Bug Fixes and Improvements\r\n\r\n- Added NCCL implementation of `hvd.broadcast` when building with `HOROVOD_GPU_BROADCAST=NCCL` (#1579).\r\n- Fixed `hvd.allgather` to work with CUDA tensors when building with `HOROVOD_GPU_ALLGATHER=MPI` (#1480).\r\n- Fixed a crash bug in MXNet caused by early free of tensor object (#1639).\r\n- Added experimental implementation for the Adasum gradient aggregation method from Microsoft (full support coming in v0.20.0) (#1485).\r\n- Added support for Intel oneCCL to replace MLSL (#1566).\r\n- Added FP16 support in IBM DDL (#1465).\r\n- Improved support for running Horovod on Spark with YARN (#1525).\r\n- Added support for TensorFlow 2.0 learning rate schedules with `tf.keras` (#1588).\r\n- Added support for broadcasting Python objects with PyTorch (#1609).\r\n- Added thread pool for CUDA finalizer threads (#1562).\r\n- Fixed host file usage and parsing within `horovodrun` (#1607).",
        "html_url": "https://github.com/horovod/horovod/releases/tag/v0.19.0",
        "name": "Horovod Spark Estimators, Spark 3, Join, Interactive Run",
        "release_id": 22832109,
        "tag": "v0.19.0",
        "tarball_url": "https://api.github.com/repos/horovod/horovod/tarball/v0.19.0",
        "type": "Release",
        "url": "https://api.github.com/repos/horovod/horovod/releases/22832109",
        "value": "https://api.github.com/repos/horovod/horovod/releases/22832109",
        "zipball_url": "https://api.github.com/repos/horovod/horovod/zipball/v0.19.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Running Horovod",
        "parent_header": [
          "Horovod"
        ],
        "type": "Text_excerpt",
        "value": "The example commands below show how to run distributed training.\nSee `Run Horovod <docs/running.rst>`_ for more details, including RoCE/InfiniBand tweaks and tips for dealing with hangs.\n\n1. To run on a machine with 4 GPUs:\n\n   .. code-block:: bash\n\n        $ horovodrun -np 4 -H localhost:4 python train.py\n\n2. To run on 4 machines with 4 GPUs each:\n\n   .. code-block:: bash\n\n       $ horovodrun -np 16 -H server1:4,server2:4,server3:4,server4:4 python train.py\n\n3. To run using Open MPI without the ``horovodrun`` wrapper, see `Running Horovod with Open MPI <docs/mpi.rst>`_.\n\n4. To run in Docker, see `Horovod in Docker <docs/docker.rst>`_.\n\n5. To run on Kubernetes, see `Helm Chart <https://github.com/horovod/horovod/tree/master/docker/helm/>`_, `Kubeflow MPI Operator <https://github.com/kubeflow/mpi-operator/>`_, `FfDL <https://github.com/IBM/FfDL/tree/master/etc/examples/horovod/>`_, and `Polyaxon <https://docs.polyaxon.com/integrations/horovod/>`_.\n\n6. To run on Spark, see `Horovod on Spark <docs/spark.rst>`_.\n\n7. To run on Ray, see `Horovod on Ray <docs/ray.rst>`_.\n\n8. To run in Singularity, see `Singularity <https://github.com/sylabs/examples/tree/master/machinelearning/horovod>`_.\n\n9. To run in a LSF HPC cluster (e.g. Summit), see `LSF <docs/lsf.rst>`_.\n\n10. To run on Hadoop Yarn, see `TonY <https://github.com/linkedin/TonY/>`_.\n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "download",
    "requirements",
    "contact",
    "contributors",
    "faq",
    "support",
    "identifier"
  ],
  "somef_provenance": {
    "date": "2024-10-06 08:09:33",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 14190
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Usage",
        "parent_header": [
          "Horovod"
        ],
        "type": "Text_excerpt",
        "value": "To use Horovod, make the following additions to your program:\n\n1. Run ``hvd.init()`` to initialize Horovod.\n\n.. raw:: html\n\n    <p/>\n\n2. Pin each GPU to a single process to avoid resource contention.\n\n   With the typical setup of one GPU per process, set this to *local rank*. The first process on\n   the server will be allocated the first GPU, the second process will be allocated the second GPU, and so forth.\n\n.. raw:: html\n\n    <p/>\n\n\n3. Scale the learning rate by the number of workers.\n\n   Effective batch size in synchronous distributed training is scaled by the number of workers.\n   An increase in learning rate compensates for the increased batch size.\n\n.. raw:: html\n\n    <p/>\n\n\n4. Wrap the optimizer in ``hvd.DistributedOptimizer``.\n\n   The distributed optimizer delegates gradient computation to the original optimizer, averages gradients using **allreduce** or **allgather**, and then applies those averaged gradients.\n\n.. raw:: html\n\n    <p/>\n\n\n5. Broadcast the initial variable states from rank 0 to all other processes.\n\n   This is necessary to ensure consistent initialization of all workers when training is started with random weights or restored from a checkpoint.\n\n.. raw:: html\n\n    <p/>\n\n\n6. Modify your code to save checkpoints only on worker 0 to prevent other workers from corrupting them.\n\n.. raw:: html\n\n    <p/>\n\n\nExample using TensorFlow v1 (see the `examples <https://github.com/horovod/horovod/blob/master/examples/>`_ directory for full training examples):\n\n.. code-block:: python\n\n    import tensorflow as tf\n    import horovod.tensorflow as hvd\n\n\n    # Initialize Horovod\n    hvd.init()\n\n    # Pin GPU to be used to process local rank (one GPU per process)\n    config = tf.ConfigProto()\n    config.gpu_options.visible_device_list = str(hvd.local_rank())\n\n    # Build model...\n    loss = ...\n    opt = tf.train.AdagradOptimizer(0.01 * hvd.size())\n\n    # Add Horovod Distributed Optimizer\n    opt = hvd.DistributedOptimizer(opt)\n\n    # Add hook to broadcast variables from rank 0 to all other processes during\n    # initialization.\n    hooks = [hvd.BroadcastGlobalVariablesHook(0)]\n\n    # Make training operation\n    train_op = opt.minimize(loss)\n\n    # Save checkpoints only on worker 0 to prevent other workers from corrupting them.\n    checkpoint_dir = '/tmp/train_logs' if hvd.rank() == 0 else None\n\n    # The MonitoredTrainingSession takes care of session initialization,\n    # restoring from a checkpoint, saving to a checkpoint, and closing when done\n    # or an error occurs.\n    with tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,\n                                           config=config,\n                                           hooks=hooks) as mon_sess:\n      while not mon_sess.should_stop():\n        # Perform synchronous training.\n        mon_sess.run(train_op)\n\n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Getting Involved",
        "parent_header": [
          "Horovod"
        ],
        "type": "Text_excerpt",
        "value": "- `Community Slack <https://forms.gle/cPGvty5hp31tGfg79>`_ for collaboration and discussion\n- `Horovod Announce <https://lists.lfai.foundation/g/horovod-announce>`_ for updates on the project\n- `Horovod Technical-Discuss <https://lists.lfai.foundation/g/horovod-technical-discuss>`_ for public discussion\n- `Horovod Security <https://lists.lfai.foundation/g/horovod-security>`_ to report security vulnerabilities\n\n\n.. inclusion-marker-end-do-not-remove\n   Place contents above here if they should also appear in read-the-docs.\n   Contents below are already part of the read-the-docs table of contents.\n"
      },
      "source": "https://raw.githubusercontent.com/horovod/horovod/master/README.rst",
      "technique": "header_analysis"
    }
  ]
}