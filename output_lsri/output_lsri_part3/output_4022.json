{
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/AppliedBioinformatics/RefKA"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-04-17T12:48:10Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2021-07-06T19:40:42Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "A fast and efficient long-read genome assembly approach for large and complex genomes"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Introduction",
        "parent_header": [
          "RefKA: A fast and efficient long-read genome assembly approach for large and complex genomes"
        ],
        "type": "Text_excerpt",
        "value": "Recent advances in long-read sequencing have the potential to produce more complete genome assemblies using sequence reads which can span repetitive regions. However, overlap based assembly methods routinely used for this data require significant computing time and resources. We have developed RefKA, a reference-based approach for long read genome assembly. This approach relies on breaking up a closely related reference genome into bins, aligning k-mers unique to each bin with PacBio reads, and then assembling each bin in parallel followed by a final bin-stitching step. \n"
      },
      "source": "https://raw.githubusercontent.com/AppliedBioinformatics/RefKA/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.861398636891616,
      "result": {
        "original_header": "Process:",
        "type": "Text_excerpt",
        "value": "Use Jellyfish to find unique canonical kmers in the reference genome.  \n"
      },
      "source": "https://raw.githubusercontent.com/AppliedBioinformatics/RefKA/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9208556605415278,
      "result": {
        "original_header": "/usr/bin/env python2",
        "type": "Text_excerpt",
        "value": "**Mapping unikmers back to the reference and get position for each unikmer**\nsoap2 index the reference fasta \nThis script can create chromosome based directory and chop the input reference into pieces. However, the running time may be long. To save the time, chromosome specified fasta files and soap files are preferred. \nAccoding to SOAP3-dp, the chunks cannot contain more than 65,000 reads and the total length of all sequences is at most 4 billion. So you may adjust your parameter settings in using \u2018makeChunks.py\u2019 \nWhen using SOAP3-dp, please ensure \u201c.ini\u201d files in the same directory as your software. An example is \nThe command lines are:\n```bash\n#========== Modify the settings below ========\nnChunks=\"\" # num of total chunks\nnPieces=\"\" # num of pieces from the reference\noutDir=\"/path/to/outputDir\"\n#====== getting ids =======\nfor i in `seq 0 $nPieces`; do for j in `seq 1 $nChunks`; do cd \"$outDir/chunk.$j\"; cat soap.$i.chunk$j.reads | awk -v id=$i 'BEGIN{OFS=\"\\t\"}{print $1, id}'>> \"$outDir/all.txt\"; done; done\n```\n**Step 8: Pulling out long reads to each piece**\nA script used to pull out all long reads to each piece is:\n```python\n\n#!/usr/bin/env python2\n\n__author__= \"Andy Yuan\"\n__contact__=\"<yuxuan.yuan@outlook.com>\"\n\nimport os\nimport sys\nimport argparse\nfrom Bio import SeqIO\nfrom collections import defaultdict\n\ndef extract(fa, npieces, info, path):\n    \"\"\"\n    fa: the whole fasta dataset\n    npieces: total number of pieces from the reference\n    info: the read id and corresponsing soap id, such as all.txt file.\n    \"\"\"\n    fa = os.path.abspath(fa)\n    info = os.path.abspath(info)\n    path = os.path.abspath(path)\n    npieces=int(npieces)\n    faDict={}\n    infoDict=defaultdict(list)\n    with open (fa, 'r') as fasta:\n        for seqs in SeqIO.parse(fasta, 'fasta'):\n            seqID=seqs.id\n            seq=str(seqs.seq)\n            faDict[seqID]=seq\n    with open (info, 'r') as info:\n        for line in info:\n            line=line.strip()\n            try:\n                ID=line.split()[0]\n                soap=line.split()[1]\n                infoDict[int(soap)].append(ID)\n            except IndexError:\n                pass\n    for i in range(npieces):\n        with open ('soap.%s.fasta'%i, 'a') as myResult:\n            for j in infoDict[i]:\n                myResult.write('>%s\\n'%j)\n                myResult.write('%s\\n'%faDict[j])\n\nif __name__==\"__main__\":\n    \"\"\"\n    Parse the function and usage.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='\\nExtract sequences using the id from the whole dataset')\n    parser.add_argument('-v', action='version', version='1.0')\n    parser.add_argument('-f', dest='fasta', help='the whole dataset', type = str)\n    parser.add_argument('-n', dest='npieces', help='total number of pieces from specific chromosome', type = int)\n    parser.add_argument('-i', dest='info', help='the read id and corresponsing soap id from one chromosome, such as all.txt file', type = str)\n    parser.add_argument('-o', dest='output', help='the output directory', type=str)\n    args = parser.parse_args()\n    if None not in [args.fasta, args.npieces, args.info, args.output]:\n        try:\n             extract(args.fasta, args.npieces, args.infpo, args.output)\n        except:\n            print >> sys.stderr, '\\nSomething is wrong with you input, please check!\\n'\n    else:\n        print\n        parser.print_help()\n        print\n        sys.exit(0)\n```\n \nUse Kmer alignments to the reference genome and long sequencing reads to sort reads into bins.  \nInput: \n* Split chromosome list: txt file containing complete file paths to all split chromosome files in numerical order. \n* Soap3 alignment list: txt file containing complete file paths to all kmer alignments to reads\n* Read Index: this is a fastq SeqIO index of all long sequencing reads, and needs to be in the same location as a fasta file containings all of the long sequencing reads. \n```\npython makeFastaPerBucket_edited.py split_chromosome_list.txt soap3_alignment_list.txt read_index.idx\n\n```\n \nPolish stitched genome with ntEdit and Illumina reads. \n \n"
      },
      "source": "https://raw.githubusercontent.com/AppliedBioinformatics/RefKA/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/AppliedBioinformatics/RefKA/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 0
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/AppliedBioinformatics/RefKA/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "AppliedBioinformatics/RefKA"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "RefKA: A fast and efficient long-read genome assembly approach for large and complex genomes"
      },
      "source": "https://raw.githubusercontent.com/AppliedBioinformatics/RefKA/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9014344858537201,
      "result": {
        "original_header": "Process:",
        "type": "Text_excerpt",
        "value": "Use the python script to split the kmers into 500Kbp bins with 100kbp overlaps.  \n**Option 1: pretty slow** \n**Option 2: preferred** \n"
      },
      "source": "https://raw.githubusercontent.com/AppliedBioinformatics/RefKA/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9695793503671196,
      "result": {
        "original_header": "/usr/bin/env python2",
        "type": "Text_excerpt",
        "value": "```python\n__author__= \"Andy Yuan\"\n__contact__=\"<yuxuan.yuan@outlook.com>\"\n\nfrom Bio import SeqIO\nimport argparse\nimport sys\nimport os\n\ndef assignID(fasta, path):\n    fasta = os.path.abspath(fasta)\n    path = os.path.abspath(path)\n    if not os.path.isdir(path):\n        print >> sys.stderr, '\\nSomething is wrong with your output directory! Please check!'\n        sys.exit(1)\n    count=0\n    with open (fasta, 'r') as fa:\n        for seqs in SeqIO.parse (fa, 'fasta'):\n            seq = str(seqs.seq.upper())\n            seq_id =seqs.id\n            if seq_id == \"1\":\n                count+=1\n                fh=open('%s/uniq_kmer_ids.fasta'%path, 'a')\n                fh.write('>%s\\n'%count)\n                fh.write('%s\\n'%seq)\n\nif __name__==\"__main__\":\n    \"\"\"\n    Parse the function and usage.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='\\nAssign an id to unikmer: start from 1.')\n    parser.add_argument('-v', action='version', version='1.0')\n    parser.add_argument('-f', dest='fasta', help='unikmer fasta file', type = str)\n    parser.add_argument('-o', dest='output', help='the output directory', type=str)\n    args = parser.parse_args()\n    if None not in [args.fasta, args.output]:\n        try:\n            assignID(args.fasta, args.output)\n        except:\n            print >> sys.stderr, '\\nSomething is wrong with you input, please check!\\n'\n    else:\n        print\n        parser.print_help()\n        print\n        sys.exit(0)\n``` \nThe script used is:\n```python\n#/usr/bin/env python2\n\n__author__= \"Andy Yuan\"\n__contact__=\"<yuxuan.yuan@outlook.com>\"\n\nfrom Bio import SeqIO\nimport argparse\nimport sys\nimport os\n\ndef splitSOAP(fasta, soap, windowsize, stepsize, path):\n    fasta = os.path.abspath(fasta)\n    soap = os.path.abspath(soap)\n    path = os.path.abspath(path)\n    if not os.path.isdir(path):\n        print >> sys.stderr, '\\nSomething is wrong with your output directory! Please check!'\n        sys.exit(1)\n    windowsize=int(windowsize)\n    stepsize=int(stepsize)\n    with open (fasta, 'r') as fa:\n        for seqs in SeqIO.parse (fa, 'fasta'):\n            seq = str(seqs.seq.upper())\n            seq_id = seqs.id\n            piece_counter = 0\n            position = 1\n            while position < len(seq):\n                start, end = position, position+windowsize-1\n                position += windowsize - stepsize\n                with open (soap, 'r') as Soap:\n                    for line in Soap:\n                        line=line.strip()\n                        pos=int(line.split()[-1])\n                        chr=line.split()[3]\n                        if chr==seq_id and pos>=start and pos <=end:\n                            if not os.path.exists ('%s/%s'%(path,seq_id)):\n                                os.makedirs('%s/%s'%(path,seq_id))\n                            fh = open('%s/%s/uniq_kmer_id_%s.soap.%s'%(path,seq_id,seq_id,piece_counter), 'a')\n                            fh.write('%s\\n'%line)\n                        else:\n                            pass\n                piece_counter += 1\n\nif __name__==\"__main__\":\n    \"\"\"\n    Parse the function and usage.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='\\nSplit optimised soap file into pieces.')\n    parser.add_argument('-v', action='version', version='1.0')\n    parser.add_argument('-f', dest='fasta', help='a fasta format file that contains all genome sequences', type = str)\n    parser.add_argument('-S', dest='soap', help='optimised soap file', type = str)\n    parser.add_argument('-w', dest='windowsize', help='size of the target pieces (bp).Default: 500000', default=500000, type = int)\n    parser.add_argument('-s', dest='stepsize', help='stepsize to move the window (bp). Default: 50000', default=50000, type = int)\n    parser.add_argument('-o', dest='output', help='the output directory', type=str)\n    args = parser.parse_args()\n    if None not in [args.fasta, args.soap, args.output]:\n        if args.windowsize is None:\n            args.windowsize = 500000\n        if args.stepsize is None:\n            args.stepsize = 50000\n        splitSOAP(args.fasta, args.soap, args.windowsize, args.stepsize, args.output)\n    else:\n        print\n        parser.print_help()\n        print\n        sys.exit(0)\n```\n \nThe script used to split long reads dataset into chunks is: makeChunks.py\n```python\n#/usr/bin/env python2\n\n__author__= \"Andy Yuan\"\n__contact__=\"<yuxuan.yuan@outlook.com>\"\n\n# Created date: 01/09/2017\n# Last modification date: 03/09/2017\n\nimport os\nimport sys\nimport argparse\nfrom Bio import SeqIO\nfrom Bio.Seq import Seq\nfrom math import trunc\n\ndef makeChunks(fasta_file,nreads,ns,path):\n    \"\"\"\n    This script is used to split a given fasta file into tiny pieces.\n    \"\"\"\n    fasta_file = os.path.abspath(fasta_file)\n    path = os.path.abspath(path)\n    if not os.path.isdir(path):\n        print '\\nSomething is wrong with your output directory! Please check!'\n        sys.exit()\n    nreads=float(nreads)\n    ns=float(ns)\n    if nreads%ns == 0:\n        step=int(nreads/ns)\n    else:\n        step=int(trunc(nreads/ns)+1)\n    i=1\n    j=1\n\n    for seqs in SeqIO.parse(fasta_file, 'fasta'):\n        ID=seqs.id\n        if ID[0]==\"@\":\n            ID=ID[1:]\n        seq=str(seqs.seq.upper())\n        #start=(i-1)*step+1\n        #end=i*step\n        #if end >nreads:\n        #    end = int(nreads)\n        fd=open('%s/chunk_%s.fa'%(path,i),'a')\n        if j%step!=0:\n            fd.write('>%s\\n'%ID)\n            fd.write('%s\\n'%seq)\n        else:\n            fd.write('>%s\\n'%ID)\n            fd.write('%s\\n'%seq)\n            i+=1\n        j+=1\n\nif __name__==\"__main__\":\n    \"\"\"\n    Parse the function and usage.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='\\nSplit the raw nucleotide data to chunks.')\n    parser.add_argument('-v', action='version', version='1.0')\n    parser.add_argument('-f', dest='fasta', help='a fasta format file that contains all genome sequences', type = str)\n    parser.add_argument('-t', dest='nreads', help='total number of reads in the fasta file', type = int)\n    parser.add_argument('-p', dest='npieces', help='number of chunks to split to ', type=int)\n    parser.add_argument('-o', dest='output', help='the output directory', type=str)\n    args = parser.parse_args()\n    if None not in [args.fasta, args.nreads, args.npieces, args.output]:\n        try:\n            makeChunks(args.fasta, args.nreads, args.npieces, args.output)\n        except:\n            print >> sys.stderr, '\\nSomething is wrong with you input, please check!\\n'\n    else:\n        print\n        parser.print_help()\n        print\n        sys.exit()\n```\n \n\n```\n\n#========== Modify the settings below ========\nmyFasta=\"/path/to/your/fasta\"\noutDir=\"/path/to/your/outputDir\"\nnChunks=\"\" # num of total chunks\n#========= make chunks ===========\nnReads=`grep -c \">\" \"$myFasta\"`\npython makeChunks.py -f \"$myFasta\" -t $nReads -p $nChunks -o \"$outDIR\"\n``` \n    /path/to/SOAP3-dp/bin/soap3-dp-builder /path/to/chunk_1.fa \n\n    /path/to/SOAP3-dp/bin/BGS-Build /path/to/chunk_1.fa.index \nThe command lines are:\n```\n#========== Modify the settings below ========\nnChunks=\"\" # num of total chunks\nnPieces=\"\" # num of pieces from the reference\nindex=\"/path/to/soap3_index_Dir\"\noutDir=\"/path/to/outputDir\"\nSOAP3-dp=\"/path/to/SOAP3-dp_Dir\"\nunikmers=\"/path/to/unikmer_fasta\" # see below the format\n#========= start mapping ===========\nfor i in `seq 1 $nChunks`; do mkdir -p \"$outDir/chunk.$i\"; cd \"$outDir/chunk.$i\"; \"$SOAP3-dp/soap3-dp\" single \"$index/chunk_${i}.fa.index\" \"$unikmers\" -o uniq_kmer.soap.chunk.$i -L 41 -h 1 -s 0; cat uniq_kmer.soap.chunk.$i.gout.* | grep -v \"@\" | awk '{if($3!=\"*\") print $1, $3}' >uniq_kmer.soap.chunk.$i.ids; rm -rf uniq_kmer.soap.chunk.$i.gout.*; rm -rf uniq_kmer.soap.chunk.$i.dpout.*; rm -rf uniq_kmer.soap.chunk.$i.done; done\nfor i in `seq 1 $nChunks`; do cd \"$outDir/chunk.$i\"; for j in `seq 0 $nPieces`; do grep soap.${j}_kmerID uniq_kmer.soap.chunk.$i.ids | cut -d\" \" -f2 | sort | uniq >soap.$j.chunk${i}.reads; done; done\n```\nThe format of the unikmer_fasta file is:\n```\n>soap.0_kmerID_2396792851_pos_761\nAAACTATTGTTTTTCATCCTGTAGTCCCATTTAGAATTACA\n>soap.0_kmerID_3338392134_pos_762\nAACTATTGTTTTTCATCCTGTAGTCCCATTTAGAATTACAA\n>soap.0_kmerID_4867289698_pos_763\nACTATTGTTTTTCATCCTGTAGTCCCATTTAGAATTACAAA\n>soap.0_kmerID_2852094694_pos_764\nCTATTGTTTTTCATCCTGTAGTCCCATTTAGAATTACAAAA\n>soap.0_kmerID_2664500755_pos_765\nTATTGTTTTTCATCCTGTAGTCCCATTTAGAATTACAAAAC\n>soap.0_kmerID_4135776802_pos_766\nATTGTTTTTCATCCTGTAGTCCCATTTAGAATTACAAAACG\n>soap.0_kmerID_1626108915_pos_767\nTTGTTTTTCATCCTGTAGTCCCATTTAGAATTACAAAACGT\n>soap.0_kmerID_2534315346_pos_768\nTGTTTTTCATCCTGTAGTCCCATTTAGAATTACAAAACGTC\n```\nHow to create such a file\n \nIf you have a soap file like (lets\u2019s say uniq_kmer_id_chr1A.soap.0):\n```\n3072848642      CTAAACCCTAAACCCTAAACCCTAAACCCTAAACCCCTAAC       +       chr1A   1\n4903828995      TAAACCCTAAACCCTAAACCCTAAACCCTAAACCCCTAACC       -       chr1A   2\n4938805162      AAACCCTAAACCCTAAACCCTAAACCCTAAACCCCTAACCC       +       chr1A   3\n3182373650      AACCCTAAACCCTAAACCCTAAACCCTAAACCCCTAACCCT       +       chr1A   4\n2291210296      ACCCTAAACCCTAAACCCTAAACCCTAAACCCCTAACCCTA       +       chr1A   5\n3413536015      CCCTAAACCCTAAACCCTAAACCCTAAACCCCTAACCCTAA       +       chr1A   6\n2603724574      CCTAAACCCTAAACCCTAAACCCTAAACCCCTAACCCTAAA       +       chr1A   7\n2933780398      CTAAACCCTAAACCCTAAACCCTAAACCCCTAACCCTAAAC       +       chr1A   8\n3046773362      TAAACCCTAAACCCTAAACCCTAAACCCCTAACCCTAAACC       -       chr1A   9\n4335988878      AAACCCTAAACCCTAAACCCTAAACCCCTAACCCTAAACCC       +       chr1A   10\n2503614940      AACCCTAAACCCTAAACCCTAAACCCCTAACCCTAAACCCT       +       chr1A   11\n254313026       ACCCTAAACCCTAAACCCTAAACCCCTAACCCTAAACCCTA       +       chr1A   12\n1662286652      CCCTAAACCCTAAACCCTAAACCCCTAACCCTAAACCCTAA       +       chr1A   13\n```\nYou may change the file into a fasta using:\n \n```bash\ni=0\ncat uniq_kmer_id_chr1A.soap.0 | awk -v id=$i '{print \">soap.\"id\"_kmerID_\"$1\"_pos_\"$5\"\\n\"$2}' > uniq_kmer_id_chr1A.soap.$id.fasta\n```\nSo in the whole chromosome, you may use:\n```bash\n#========== Modify the settings below ========\nsoapFileDir=\"/path/to/soap_file_Dir\"\nchr=\"\" #name of chromosome, such as chr1A\noutDir=\"/path/to/outputDir\"\n#====== start converting =======\ncd $soapFileDir\nfor i in `ls -l | grep -v fasta | awk '{print $9}' | grep soap | rev | cut -d\".\" -f1 | rev | sort -n`; do echo $i; cat uniq_kmer_id_${chr}.soap.$i | awk -v id=$i '{print \">soap.\"id\"_kmerID_\"$1\"_pos_\"$5\"\\n\"$2}' >> \"$outDir/uniq_kmer_id_${chr}.soap.fasta\"; done\n```\nThe whole chromsome based fasta file is the one used in SOAP3-dp mapping\n**Step 7: Getting long reads id for each piece**\n \nThe command lines are:\n```bash\n#========== Modify the settings below ========\nnChunks=\"\" # num of total chunks\nnPieces=\"\" # num of pieces from the reference\noutDir=\"/path/to/outputDir\"\n#====== getting ids =======\nfor i in `seq 0 $nPieces`; do for j in `seq 1 $nChunks`; do cd \"$outDir/chunk.$j\"; cat soap.$i.chunk$j.reads | awk -v id=$i 'BEGIN{OFS=\"\\t\"}{print $1, id}'>> \"$outDir/all.txt\"; done; done\n```\n**Step 8: Pulling out long reads to each piece**\nA script used to pull out all long reads to each piece is:\n```python\n\n#!/usr/bin/env python2\n\n__author__= \"Andy Yuan\"\n__contact__=\"<yuxuan.yuan@outlook.com>\"\n\nimport os\nimport sys\nimport argparse\nfrom Bio import SeqIO\nfrom collections import defaultdict\n\ndef extract(fa, npieces, info, path):\n    \"\"\"\n    fa: the whole fasta dataset\n    npieces: total number of pieces from the reference\n    info: the read id and corresponsing soap id, such as all.txt file.\n    \"\"\"\n    fa = os.path.abspath(fa)\n    info = os.path.abspath(info)\n    path = os.path.abspath(path)\n    npieces=int(npieces)\n    faDict={}\n    infoDict=defaultdict(list)\n    with open (fa, 'r') as fasta:\n        for seqs in SeqIO.parse(fasta, 'fasta'):\n            seqID=seqs.id\n            seq=str(seqs.seq)\n            faDict[seqID]=seq\n    with open (info, 'r') as info:\n        for line in info:\n            line=line.strip()\n            try:\n                ID=line.split()[0]\n                soap=line.split()[1]\n                infoDict[int(soap)].append(ID)\n            except IndexError:\n                pass\n    for i in range(npieces):\n        with open ('soap.%s.fasta'%i, 'a') as myResult:\n            for j in infoDict[i]:\n                myResult.write('>%s\\n'%j)\n                myResult.write('%s\\n'%faDict[j])\n\nif __name__==\"__main__\":\n    \"\"\"\n    Parse the function and usage.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='\\nExtract sequences using the id from the whole dataset')\n    parser.add_argument('-v', action='version', version='1.0')\n    parser.add_argument('-f', dest='fasta', help='the whole dataset', type = str)\n    parser.add_argument('-n', dest='npieces', help='total number of pieces from specific chromosome', type = int)\n    parser.add_argument('-i', dest='info', help='the read id and corresponsing soap id from one chromosome, such as all.txt file', type = str)\n    parser.add_argument('-o', dest='output', help='the output directory', type=str)\n    args = parser.parse_args()\n    if None not in [args.fasta, args.npieces, args.info, args.output]:\n        try:\n             extract(args.fasta, args.npieces, args.infpo, args.output)\n        except:\n            print >> sys.stderr, '\\nSomething is wrong with you input, please check!\\n'\n    else:\n        print\n        parser.print_help()\n        print\n        sys.exit(0)\n```\n \nUse Canu to assemble bins separately.  \nInput: \n* Split chromosome list: txt file containing complete file paths to all split chromosomes in numerical order\n* Bin contig list: txt file containing complete file paths to all assembled bins in numerical order\n```\npython LiloAndStitch.py split_chromosome_list.txt bin_contig_list.txt output_log.log stitched_output.fasta\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/AppliedBioinformatics/RefKA/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.9107650565392388,
      "result": {
        "original_header": "Process:",
        "type": "Text_excerpt",
        "value": "Input: \n* Chromosome fasta file\n* Chromosome Kmer file (Soap output format)\n```\npython split_soap_file.py -f chromosome.fasta -S chromosome_kmer_file.soap -o output_directory\n```\n \n**Create unikmer fasta file** \n"
      },
      "source": "https://raw.githubusercontent.com/AppliedBioinformatics/RefKA/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8617896971899724,
      "result": {
        "original_header": "/usr/bin/env python2",
        "type": "Text_excerpt",
        "value": "```python\n__author__= \"Andy Yuan\"\n__contact__=\"<yuxuan.yuan@outlook.com>\"\n\nfrom Bio import SeqIO\nimport argparse\nimport sys\nimport os\n\ndef assignID(fasta, path):\n    fasta = os.path.abspath(fasta)\n    path = os.path.abspath(path)\n    if not os.path.isdir(path):\n        print >> sys.stderr, '\\nSomething is wrong with your output directory! Please check!'\n        sys.exit(1)\n    count=0\n    with open (fasta, 'r') as fa:\n        for seqs in SeqIO.parse (fa, 'fasta'):\n            seq = str(seqs.seq.upper())\n            seq_id =seqs.id\n            if seq_id == \"1\":\n                count+=1\n                fh=open('%s/uniq_kmer_ids.fasta'%path, 'a')\n                fh.write('>%s\\n'%count)\n                fh.write('%s\\n'%seq)\n\nif __name__==\"__main__\":\n    \"\"\"\n    Parse the function and usage.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='\\nAssign an id to unikmer: start from 1.')\n    parser.add_argument('-v', action='version', version='1.0')\n    parser.add_argument('-f', dest='fasta', help='unikmer fasta file', type = str)\n    parser.add_argument('-o', dest='output', help='the output directory', type=str)\n    args = parser.parse_args()\n    if None not in [args.fasta, args.output]:\n        try:\n            assignID(args.fasta, args.output)\n        except:\n            print >> sys.stderr, '\\nSomething is wrong with you input, please check!\\n'\n    else:\n        print\n        parser.print_help()\n        print\n        sys.exit(0)\n``` \n    sort -k9n uniq_kmer_ids.fasta.soap | awk 'BEGIN{OFS=\"\\t\"}''{print $1, $2, $7, $8, $9}' > uniq_kmer_id.fasta.optimised.soap \nThe script used is:\n```python\n#/usr/bin/env python2\n\n__author__= \"Andy Yuan\"\n__contact__=\"<yuxuan.yuan@outlook.com>\"\n\nfrom Bio import SeqIO\nimport argparse\nimport sys\nimport os\n\ndef splitSOAP(fasta, soap, windowsize, stepsize, path):\n    fasta = os.path.abspath(fasta)\n    soap = os.path.abspath(soap)\n    path = os.path.abspath(path)\n    if not os.path.isdir(path):\n        print >> sys.stderr, '\\nSomething is wrong with your output directory! Please check!'\n        sys.exit(1)\n    windowsize=int(windowsize)\n    stepsize=int(stepsize)\n    with open (fasta, 'r') as fa:\n        for seqs in SeqIO.parse (fa, 'fasta'):\n            seq = str(seqs.seq.upper())\n            seq_id = seqs.id\n            piece_counter = 0\n            position = 1\n            while position < len(seq):\n                start, end = position, position+windowsize-1\n                position += windowsize - stepsize\n                with open (soap, 'r') as Soap:\n                    for line in Soap:\n                        line=line.strip()\n                        pos=int(line.split()[-1])\n                        chr=line.split()[3]\n                        if chr==seq_id and pos>=start and pos <=end:\n                            if not os.path.exists ('%s/%s'%(path,seq_id)):\n                                os.makedirs('%s/%s'%(path,seq_id))\n                            fh = open('%s/%s/uniq_kmer_id_%s.soap.%s'%(path,seq_id,seq_id,piece_counter), 'a')\n                            fh.write('%s\\n'%line)\n                        else:\n                            pass\n                piece_counter += 1\n\nif __name__==\"__main__\":\n    \"\"\"\n    Parse the function and usage.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='\\nSplit optimised soap file into pieces.')\n    parser.add_argument('-v', action='version', version='1.0')\n    parser.add_argument('-f', dest='fasta', help='a fasta format file that contains all genome sequences', type = str)\n    parser.add_argument('-S', dest='soap', help='optimised soap file', type = str)\n    parser.add_argument('-w', dest='windowsize', help='size of the target pieces (bp).Default: 500000', default=500000, type = int)\n    parser.add_argument('-s', dest='stepsize', help='stepsize to move the window (bp). Default: 50000', default=50000, type = int)\n    parser.add_argument('-o', dest='output', help='the output directory', type=str)\n    args = parser.parse_args()\n    if None not in [args.fasta, args.soap, args.output]:\n        if args.windowsize is None:\n            args.windowsize = 500000\n        if args.stepsize is None:\n            args.stepsize = 50000\n        splitSOAP(args.fasta, args.soap, args.windowsize, args.stepsize, args.output)\n    else:\n        print\n        parser.print_help()\n        print\n        sys.exit(0)\n```\n \nThe script used to split long reads dataset into chunks is: makeChunks.py\n```python\n#/usr/bin/env python2\n\n__author__= \"Andy Yuan\"\n__contact__=\"<yuxuan.yuan@outlook.com>\"\n\n# Created date: 01/09/2017\n# Last modification date: 03/09/2017\n\nimport os\nimport sys\nimport argparse\nfrom Bio import SeqIO\nfrom Bio.Seq import Seq\nfrom math import trunc\n\ndef makeChunks(fasta_file,nreads,ns,path):\n    \"\"\"\n    This script is used to split a given fasta file into tiny pieces.\n    \"\"\"\n    fasta_file = os.path.abspath(fasta_file)\n    path = os.path.abspath(path)\n    if not os.path.isdir(path):\n        print '\\nSomething is wrong with your output directory! Please check!'\n        sys.exit()\n    nreads=float(nreads)\n    ns=float(ns)\n    if nreads%ns == 0:\n        step=int(nreads/ns)\n    else:\n        step=int(trunc(nreads/ns)+1)\n    i=1\n    j=1\n\n    for seqs in SeqIO.parse(fasta_file, 'fasta'):\n        ID=seqs.id\n        if ID[0]==\"@\":\n            ID=ID[1:]\n        seq=str(seqs.seq.upper())\n        #start=(i-1)*step+1\n        #end=i*step\n        #if end >nreads:\n        #    end = int(nreads)\n        fd=open('%s/chunk_%s.fa'%(path,i),'a')\n        if j%step!=0:\n            fd.write('>%s\\n'%ID)\n            fd.write('%s\\n'%seq)\n        else:\n            fd.write('>%s\\n'%ID)\n            fd.write('%s\\n'%seq)\n            i+=1\n        j+=1\n\nif __name__==\"__main__\":\n    \"\"\"\n    Parse the function and usage.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='\\nSplit the raw nucleotide data to chunks.')\n    parser.add_argument('-v', action='version', version='1.0')\n    parser.add_argument('-f', dest='fasta', help='a fasta format file that contains all genome sequences', type = str)\n    parser.add_argument('-t', dest='nreads', help='total number of reads in the fasta file', type = int)\n    parser.add_argument('-p', dest='npieces', help='number of chunks to split to ', type=int)\n    parser.add_argument('-o', dest='output', help='the output directory', type=str)\n    args = parser.parse_args()\n    if None not in [args.fasta, args.nreads, args.npieces, args.output]:\n        try:\n            makeChunks(args.fasta, args.nreads, args.npieces, args.output)\n        except:\n            print >> sys.stderr, '\\nSomething is wrong with you input, please check!\\n'\n    else:\n        print\n        parser.print_help()\n        print\n        sys.exit()\n```\n \nThe command lines are:\n```bash\n#========== Modify the settings below ========\nnChunks=\"\" # num of total chunks\nnPieces=\"\" # num of pieces from the reference\noutDir=\"/path/to/outputDir\"\n#====== getting ids =======\nfor i in `seq 0 $nPieces`; do for j in `seq 1 $nChunks`; do cd \"$outDir/chunk.$j\"; cat soap.$i.chunk$j.reads | awk -v id=$i 'BEGIN{OFS=\"\\t\"}{print $1, id}'>> \"$outDir/all.txt\"; done; done\n```\n**Step 8: Pulling out long reads to each piece**\nA script used to pull out all long reads to each piece is:\n```python\n\n#!/usr/bin/env python2\n\n__author__= \"Andy Yuan\"\n__contact__=\"<yuxuan.yuan@outlook.com>\"\n\nimport os\nimport sys\nimport argparse\nfrom Bio import SeqIO\nfrom collections import defaultdict\n\ndef extract(fa, npieces, info, path):\n    \"\"\"\n    fa: the whole fasta dataset\n    npieces: total number of pieces from the reference\n    info: the read id and corresponsing soap id, such as all.txt file.\n    \"\"\"\n    fa = os.path.abspath(fa)\n    info = os.path.abspath(info)\n    path = os.path.abspath(path)\n    npieces=int(npieces)\n    faDict={}\n    infoDict=defaultdict(list)\n    with open (fa, 'r') as fasta:\n        for seqs in SeqIO.parse(fasta, 'fasta'):\n            seqID=seqs.id\n            seq=str(seqs.seq)\n            faDict[seqID]=seq\n    with open (info, 'r') as info:\n        for line in info:\n            line=line.strip()\n            try:\n                ID=line.split()[0]\n                soap=line.split()[1]\n                infoDict[int(soap)].append(ID)\n            except IndexError:\n                pass\n    for i in range(npieces):\n        with open ('soap.%s.fasta'%i, 'a') as myResult:\n            for j in infoDict[i]:\n                myResult.write('>%s\\n'%j)\n                myResult.write('%s\\n'%faDict[j])\n\nif __name__==\"__main__\":\n    \"\"\"\n    Parse the function and usage.\n    \"\"\"\n    parser = argparse.ArgumentParser(description='\\nExtract sequences using the id from the whole dataset')\n    parser.add_argument('-v', action='version', version='1.0')\n    parser.add_argument('-f', dest='fasta', help='the whole dataset', type = str)\n    parser.add_argument('-n', dest='npieces', help='total number of pieces from specific chromosome', type = int)\n    parser.add_argument('-i', dest='info', help='the read id and corresponsing soap id from one chromosome, such as all.txt file', type = str)\n    parser.add_argument('-o', dest='output', help='the output directory', type=str)\n    args = parser.parse_args()\n    if None not in [args.fasta, args.npieces, args.info, args.output]:\n        try:\n             extract(args.fasta, args.npieces, args.infpo, args.output)\n        except:\n            print >> sys.stderr, '\\nSomething is wrong with you input, please check!\\n'\n    else:\n        print\n        parser.print_help()\n        print\n        sys.exit(0)\n```\n \nInput: \n* Split chromosome list: txt file containing complete file paths to all split chromosome files in numerical order. \n* Soap3 alignment list: txt file containing complete file paths to all kmer alignments to reads\n* Read Index: this is a fastq SeqIO index of all long sequencing reads, and needs to be in the same location as a fasta file containings all of the long sequencing reads. \n```\npython makeFastaPerBucket_edited.py split_chromosome_list.txt soap3_alignment_list.txt read_index.idx\n\n```\n \nInput: \n* Split chromosome list: txt file containing complete file paths to all split chromosomes in numerical order\n* Bin contig list: txt file containing complete file paths to all assembled bins in numerical order\n```\npython LiloAndStitch.py split_chromosome_list.txt bin_contig_list.txt output_log.log stitched_output.fasta\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/AppliedBioinformatics/RefKA/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/AppliedBioinformatics/RefKA/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "RefKA"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "AppliedBioinformatics"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 30746,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/AppliedBioinformatics/RefKA/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Requirements",
        "parent_header": [
          "RefKA: A fast and efficient long-read genome assembly approach for large and complex genomes"
        ],
        "type": "Text_excerpt",
        "value": "* Jellyfish\n* Soap Aligner\n* Soap3: GPU aligner\n* Canu\n"
      },
      "source": "https://raw.githubusercontent.com/AppliedBioinformatics/RefKA/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 14:54:13",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 3
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ]
}