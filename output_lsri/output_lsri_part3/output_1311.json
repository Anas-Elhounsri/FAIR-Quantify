{
  "application_domain": [
    {
      "confidence": 12.59,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    },
    {
      "confidence": 12.29,
      "result": {
        "type": "String",
        "value": "Audio"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "author": "Vasiliki Stoumpou and C\u00e9sar D. M. Vargas and Peter F. Schade and J. Lomax Boyd and Theodoros Giannakopoulos and Erich D. Jarvis",
        "doi": "10.1080/09524622.2022.2099973",
        "format": "bibtex",
        "title": "Analysis of Mouse Vocal Communication (AMVOC): a deep, unsupervised method for rapid detection, analysis and classification of ultrasonic vocalisations",
        "type": "Text_excerpt",
        "url": "https://doi.org/10.1080/09524622.2022.2099973",
        "value": "@article{doi:10.1080/09524622.2022.2099973,\n    url = {https://doi.org/10.1080/09524622.2022.2099973},\n    doi = {10.1080/09524622.2022.2099973},\n    publisher = {Taylor & Francis},\n    year = {2022},\n    pages = {1-31},\n    journal = {Bioacoustics},\n    title = {Analysis of Mouse Vocal Communication (AMVOC): a deep, unsupervised method for rapid detection, analysis and classification of ultrasonic vocalisations},\n    author = {Vasiliki Stoumpou and C\u00e9sar D. M. Vargas and Peter F. Schade and J. Lomax Boyd and Theodoros Giannakopoulos and Erich D. Jarvis},\n}"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/tyiannak/amvoc"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-03-22T22:26:36Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2023-10-29T22:24:51Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "A Python Tool for Analysis of Mouse Vocal Communication"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9583446808707073,
      "result": {
        "original_header": "Intro",
        "type": "Text_excerpt",
        "value": "amvoc is a tool used to analyze mouse vocal communication through analyzing\n audio recordings from mouse communication signals. It has been developed by [the Multimedia Analysis Group of the Computational Intelligence Lab (MagCIL)](http://magcil.github.io) of the NCSR \"Demokritos\" (Greece) in cooperation with the [Jarvis Neuroscience Lab](https://www.jarvislab.net) of the Rockefeller University. \n  \n"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9819729551460802,
      "result": {
        "original_header": "Offline functionality",
        "type": "Text_excerpt",
        "value": "The main GUI for analyzing mouse sounds is in the `main.py`. \nIt takes the WAV filename in which the recording to analyze is stored. \nBy analyzing, we basically mean detecting the USVs (ultrasonic vocalizations) \nproduced by the mouse, after processing the computed (or cached) spectrogram of \nthe recording. Start and end time of each USV are saved in a .csv file named \n\"offline_filename.csv\". \nTo just get the detected USVs, the user should run main.py as follows:\n```\npython3 main.py -i data/B148_test_small.wav -c n\n```\nOption -c declares whether the user would like to continue to the GUI, \nthrough the dash local address `http://127.0.0.1:8050/`. The purpose of the GUI \nis to visualize clusterings of the detected USVs of the recording. These\n clusterings are produced based on USVs' features, either \n hand-crafted or derived from a deep convolutional autoencoder, \n trained on a large number of USVs' spectrogram examples. \n One can use the GUI to (a) inspect the spectrogram of the whole input signal, \n (b) try various clustering parameters (clustering method, number of clusters, \n type of features to be used), \n (c) evaluate the clustering and (d) explore alternative clusterings \n after re-training the autoencoder used for the feature extraction \n through an active learning approach. \nOne can run the `main.py` script as follow:\n```\npython3 main.py -i data/B148_test_small.wav -c y -s 1\n```\nif the user wants to use the GUI and also the spectrogram of the whole signal \ndisplayed (not recommended for long recordings). \nThis functionality is set by parameter -s, which is optional.\n \n"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.908690909989003,
      "result": {
        "original_header": "Semisupervised Representation of Mice Vocalizations",
        "type": "Text_excerpt",
        "value": "The user can intervene in the re-training of the autoencoder used for feature extraction (Deep) from USVs in order to explore new clustering alternatives, by imposing pairwise constraints between USVs. This is achieved by clicking on the \"Retrain model\" button on the GUI. Pairs of detected USVs will subsequently pop up and the user can declare whether or not they should belong to the same cluster by clicking \"Yes\" or \"No\" respectively, in the pop-up window. If they want to continue the retraining procedure without annotating more pairs, they can click on the \"Stop\" button, and if they want to completely interrupt the retraining, they can click on the \"Cancel\" button. \nThe option \"Save clustering\" gives the opportunity to save the cluster label of each USV, the autoencoder model used as \"model_fileName_clusteringMethod_numClusters_featureType\" and also train a classifier, saved as \"clf_fileName_clusteringMethod_numClusters_featureType\", using USV representations and their corresponding labels as ground-truth data.  \n"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9833153537165803,
      "result": {
        "original_header": "Online functionality",
        "type": "Text_excerpt",
        "value": "AMVOC also includes an online functionality, i.e. the chance to detect USVs while recording the mouse vocal activity. This is achieved by running the main_live.py script, which detects USVs in online mode (every 750 ms) and saves their start and end times in a .csv file named \"realtime_filename.csv\". It just takes the WAV filename of the recording to be processed (or no filename \nif the signal is to be recorded from the soundcard).\n```\npython3 main_live.py -i data/B148_test_small.wav\n```\nReal-time classification of detected USVs is also provided, by using a classifier trained with clustering data (see above in `Vocalization Semisupervised Representation`) and the model used for feature extraction from this data. This can be done by first changing the \"model\" parameter in the config.json with the name of the new model and run main_live.py as:\n```\npython3 main_live.py -i data/B148_test_small.wav -c clf_B148_test_small_agg_6_deep\n\n```\nThe name of the classifier in the line above is an example. \n  \n \n"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8645938397672522,
      "result": {
        "original_header": "Vocalization detection evaluation and comparison",
        "type": "Text_excerpt",
        "value": "The detected vocalizations of a recording with our method \ncan be compared to the detected vocalizations of the same recording using some other method, or to annotated vocalizations (ground truth). \nThis comparison can be done using the `syllables_comp.py`, which takes the WAV filename of the recording, and the names of the two csv files to be compared.\n```\npython3 syllables_comp.py -i data/B148_test_small.wav -csv1 first_file.csv -csv2 second_file.csv\n```\nWe have annotated some intervals of multiple recordings in order to evaluate \nthe vocalization detection method and saved them in .csv files. \nThese can be found in the folder data/vocalizations_evaluation, along with the corresponding WAV recordings. Ground truth files have the name gt_{num}.csv, for example gt_1.csv. We have also included the detected vocalizations from 5 other methods: 2 versions of MSA, MUPET, VocalMat and DeepSqueak. Their csv files, e.g. deepsqueak_1.csv can be used for comparison with our results or the ground truth annotations.\nIf one wants to reproduce the results, she can run the `main.py` \n(offline detection) or main_live.py (online detection):\n```\npython3 main.py -i data/vocalizations_evaluation/1/rec_1.wav -c n\n\n```\nor\n \n```\n\npython3 main_live.py -i data/vocalizations_evaluation/1/rec_1.wav\n\n``` \nIn order to compare the e.g. online detected vocalizations with the actual ones, \nthe user should run the syllables_comp.py:\n```\npython3 syllables_comp.py -i data/vocalizations_evaluation/1/rec_1.wav -csv1 realtime_vocalizations.csv -csv2 data/vocalizations_evaluation/1/gt_1.csv\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9577778157273631,
      "result": {
        "original_header": "About",
        "type": "Text_excerpt",
        "value": "Amvoc has been developed by [the Multimedia Analysis Group of the Computational Intelligence Lab (MagCIL)](http://magcil.github.io) of the National Center for Scientific Research \"Demokritos\" in cooperation with the [Jarvis Neuroscience Lab](https://www.jarvislab.net) of the Rockefeller University. \n \n"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/tyiannak/amvoc/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tyiannak/amvoc/master/notebooks/training_task.ipynb"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/notebooks/training_task.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 2
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/tyiannak/amvoc/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "tyiannak/amvoc"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "amvoc: Python Tool for Analysis of Mouse Vocal Communication"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tyiannak/amvoc/master/run_online.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tyiannak/amvoc/master/run_offline.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tyiannak/amvoc/master/misc/screenshot.png"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tyiannak/amvoc/master/misc/screenshot3.png"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Setup",
        "parent_header": [
          "amvoc: Python Tool for Analysis of Mouse Vocal Communication"
        ],
        "type": "Text_excerpt",
        "value": "```\npip3 install -r requirements.txt\n``` \n\n"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8774207826031063,
      "result": {
        "original_header": "Vocalization detection evaluation and comparison",
        "type": "Text_excerpt",
        "value": "```\n\npython3 main_live.py -i data/vocalizations_evaluation/1/rec_1.wav\n\n``` \nIn order to compare the e.g. online detected vocalizations with the actual ones, \nthe user should run the syllables_comp.py:\n```\npython3 syllables_comp.py -i data/vocalizations_evaluation/1/rec_1.wav -csv1 realtime_vocalizations.csv -csv2 data/vocalizations_evaluation/1/gt_1.csv\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/tyiannak/amvoc/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "MIT License\n\nCopyright (c) 2020 Theodoros Giannakopoulos\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/tyiannak/amvoc/master/LICENSE",
      "technique": "file_exploration"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "amvoc"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "tyiannak"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 136638,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 80904,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 913,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tyiannak/amvoc/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-06 04:33:50",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 13
      },
      "technique": "GitHub_API"
    }
  ]
}