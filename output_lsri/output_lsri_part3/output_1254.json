{
  "application_domain": [
    {
      "confidence": 25.05,
      "result": {
        "type": "String",
        "value": "Natural Language Processing"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/eltonjrv/microbiome.westernu"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2017-02-15T23:08:45Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2021-11-16T11:41:10Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Microbiome Analysis applied to Vet Med"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9246456838272832,
      "result": {
        "original_header": "*Current affiliation: Leeds Omics, University of Leeds, UK.",
        "type": "Text_excerpt",
        "value": ">Reviewed and retested by Chayan Roy and Elton Vasconcelos (Sep/2020).\n>Reviewed by Elton Vasconcelos (Nov/2021) \nIf you use this pipeline (or part of it) on your research, please cite [Vasconcelos et al., 2021 - BMC Vet.Res.](https://bmcvetres.biomedcentral.com/articles/10.1186/s12917-021-02969-9) \n# 1. Debarcoding and Demultiplexing Raw Fastq Files (generated by the sequencing machine)\n##### This task is strictly dependent on both PCR and Sequencing protocols one has adopted in the Lab. \nIn the example below, we are referring to a two-round PCR method on which **two pairs of adapters** were used for a **paired-end sequencing**:\na) iNEXT barcodes (For_A-H - Rev_1-12) on the 1st round of amplifications, and \nb) Illumina overhang adapter sequences (For_i5 and Rev_i7) on the 2nd round.\nFor more details on this approach, please refer to [Faircloth & Glenn, 2012 - PLoS One](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0042543).\n>NOTE: If you have not employed this multiplexing approach or have all your fastq files already demultiplexed, please take a look at [qiime2 demultiplexing forum](https://forum.qiime2.org/t/demultiplexing-and-trimming-adapters-from-reads-with-q2-cutadapt/2313) or move straight to topic 2 below.  \n### 1.1. Preparing directories and importing raw fastq files to QIIME2\n1.1.1.  Creating a project parent directory for the whole analysis:\n```\n$ mkdir projectX\n```\n1.1.2. Entering this directory:\nBASH2*\n1.1.3. Creating a directory where all your raw fastq files will be placed in, and entering it:\nBASH3*\n1.1.4. Transferring your fastq files to the current work directory (raw_data/) with your most suitable/convenient command or tool (e.g. cp, scp, ftp, WinScp, FileZilla, etc ...).\n>NOTE: Since we don't know where you are pulling your data from, this step is intentionally left up to you. \n1.1.5.1. If you have only one pair of fastq files, rename R1 to forward.fastq.gz and R2 to reverse.fastq.gz, then move straight to sub-item 1.1.6:\n```\n$ mv your_R1_file.fastq.gz forward.fastq.gz\n$ mv your_R2_file.fastq.gz reverse.fastq.gz\n```\n1.1.5.2. If you have several (n) pairs of fastq files, it will be required to create several (n) experimental subdirectories within raw_data/; \n1.1.5.2.a. For example, the command below automatically creates 10 subdirectories named \"ExpN\", where N is a number from 1 to 10. Edit the command according to the total number of fastq files pairs you have on hands.\n```\n$ for i in `seq 1 10`; do mkdir Exp$i; done\n```\n1.1.5.2.b. Then move each pair of fastq files that were already placed within raw_data/ (sub-item 1.1.4) to each respective recently created subdirectory (Exp1 to Exp10), and rename the files. Below is an example of the commands' set for the Exp1 instance:\nBASH6*\n>NOTE: Since we don't know how your fastq files are originally named, we recommend that you move them separately with the individual commands set above. In case you are famliar with the Unix Shell, feel free to do it through a \"for\" loop. \n1.1.6. Moving back to the parent directory (projectX/), creating a directory for the demultiplexing job and entering it:\n```\n$ cd ../\n$ mkdir demux\n$ cd demux/\n```\n1.1.7. Activating qiime2 environment (once you have properly installed it through this [link](https://docs.qiime2.org/2021.8/install)):\nBASH8*\n1.1.8. Importing fastq files as qiime2 .qza format. \n1.1.8.1. For a single pair of fastq files (coming from 1.1.5.1 sub-item above), do the following:\n```\n$  qiime tools import --type MultiplexedPairedEndBarcodeInSequence --input-path ../raw_data/ --output-path multiplexed-seqs.qza\n```\n1.1.8.2. For several (n) pairs of fastq files (coming from 1.1.5.2 sub-item above), do this instead:\nBASH10* \n1.2.3. Actual debarcoding and demultiplexing process with qiime cutadapt. \n1.2.3.1. Debarcoding and demultiplexing a single pair of fastq files (multiplexed-seqs.qza file created on 1.1.8.1. step above):\n```\n$ for i in `ls iNextRev*tab`; do qiime cutadapt demux-paired --i-seqs multiplexed-seqs.qza --m-forward-barcodes-file $i --m-forward-barcodes-column iNext-For --m-reverse-barcodes-file $i --m-reverse-barcodes-column iNext-Rev --o-per-sample-sequences `echo $i | sed 's/\\-samples.*$//g'`-demultiplexed-seqs.qza --o-untrimmed-sequences `echo $i | sed 's/\\-samples.*$//g'`-untrimmed.qza --verbose; done\n```\n1.2.3.2. Debarcoding and demultiplexing several (n) pairs of fastq files (Exp\\*-multiplexed-seqs.qza files created on 1.1.8.2. step above):\nBASH14*\n1.2.4. Removing untrimmed.qza files:\nBASH15*\n1.2.5. Summarizing demultiplexed qza files as qzv ones in order to be visualized at https://view.qiime2.org/ (Important for sequenced reads quality control assessment/visualization on each sample):\nBASH16*\n1.2.6. Deactivating qiime2 environment:\nBASH17*\nor\nBASH18*\n1.2.7. Creating a directory to store and uncompress demultiplexed qza files, in order to have fastq files for topic 2 below:\nBASH19*\n1.2.8. Going back to the parent projectX/ dir:\nBASH20* \n# 2. Microbiome Sequencing Analyses\n2a) Since UPARSE will be the main microbiome analyzer used here, let's first create a \"uparse-run\" directory and then go within it:\n```\n$ mkdir uparse-run\n$ cd uparse-run/\n```\n2b) Creating an \"inputs\" directory and placing symbolic links of demultiplexed fastq files that will be used by UPARSE:\nBASH22*\n>NOTE: If you are coming straight to this topic because you already had demultiplexed your samples on your own, please disconsider the \"ln -s\" command above and just place your demultiplexed and compressed fastq files (\\*.fastq.gz) into the \"inputs\" dir. Please also make sure that your fastq.gz file names start with \"sampleID\" followed by \"\\_1\\_L001\\_R[12]\\_001.fastq.gz\". For example, take a look at our [sample metadata table](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-metadata.tsv) and see that, for the first sample, the paired file names will be the following: 865A1\\_1\\_L001\\_R1\\_001.fastq.gz and 865A1\\_1\\_L001\\_R2\\_001.fastq.gz.\n### 2.1. Trimming primers with Trimmomatic (this must be done prior running UPARSE)\n##### Please refer to http://www.usadellab.org/cms/?page=trimmomatic for Trimmomatic download and instructions\n2.1.1. Running trimmomatic\nBASH23*\n> NOTES:\nI) Please pay attention that you need to type your trimmmomatic installation full PATH after the \"java -jar\" command above.\nII) Edit the \"HEADCROP:20\" parameter according to your primers' average length. In this example, primers' length is ~ 20 bp. \n2.1.2. Removing unpaired reads after trimming:\n```\n$ rm *unpaired*\n```\n2.1.3. Creating a new directory where trimmed fastq files must be placed into:\nBASH25*\n2.1.4. Moving trimmed fastq files to that new directory:\nBASH26*\n### 2.2. Running UPARSE on sequenced and trimmed amplicons\n##### Please refer to https://www.drive5.com/usearch/download.html in order to download USEARCH tools\n2.2.1. Running the actual microbiome analyzer tool:\nBASH27*\n>NOTES:\nI) UPARSE pipeline does a series of tasks such as: mate joining and quality filtering of your sequenced amplicons, ZOTUs/ESVs assembly, taxonomic classification, and both alpha- and beta-diversity analyses on all your samples.\nII) \"run-uparse-amp250-450-ZOTUs.bash\" BASH script may be obtained [here](https://github.com/eltonjrv/microbiome.westernu/blob/master/run-uparse-amp250-450-ZOTUs.bash).\nIIa) One may tune parameters on each command in the script according to his/her needs.\nIIb) This script uses a customized RDP refDB, which we have added Ehrlichia_canis, Ehrlichia_chafeensis, Anaplasma_platys, Anaplasma_phagocytophilum, Mycoplasma_haemocanis and Mycoplasma_haematoparvum 16S rRNA sequences. In order to download it, please go [here](https://github.com/eltonjrv/microbiome.westernu/tree/refDB) and click on the \"Clone or download\" green button, then \"Download ZIP\". After unzipping the downloaded folder, uncompress the \"rdp_16s_extra_seqs.fa.gz\" file with \"gunzip\" command, and then place it into the directory where you will run this script (\"uparse-run/\" in this example). Line 52 from the \"\"run-uparse-amp250-450-ZOTUs.bash\"\" script will format that file in order to be used as a refDB (\\*.udb) for taxonomic classification purposes. If you want to use your own customized refDB fasta file, please edit script's lines 52 and 53.\nIIc) In order to play with different OTU clustering % identity thresholds (95, 97, and 99%), one must run the alternative \"run-uparse-amp250-450-OTUs95_97_99_100.bash\" script that is provided [here](https://github.com/eltonjrv/microbiome.westernu/blob/master/run-uparse-amp250-450-OTUs95_97_99_100.bash).\nIII) An \"outputs\" directory will be created and all uparse-generated files will be placed within it. \n\n# 3. ZOTU/ESV Table Customization for Diagnostics Purposes\n##### This topic is aimed at creating a customized ZOTU table which will make data visual inspection easier for clinicians\n3a) Move to the uparse-generated \"outputs\" directory:\n```\n$ cd outputs/\n```\n#### ATTENTION: A metadata file will also be needed (which I'll call \"samples-metadata.tsv\" in this tutorial). Please have your metadata file prepared as this [example](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-metadata.tsv) and place it within the current directory (outputs).\n>NOTES about the metadata table:\nI) Columns 1 and 4 are mandatory.\nII) Column 4 must contain any textual string that best describes your samples.\nIII) There must not be any colon \":\" nor blank spaces within your sample descriptions. \n### 3.1. Improving customized ZOTU table\n3.1.1. In case one wants to keep only the last taxonomic level assigned to each zotu, instead of seeing the whole taxonomic classification (from phylum to species), run the following:\n```\n$ sed 's/\\td\\:.*s\\:/\\ts\\:/g' zotus_table_uparse-customized.tsv |  sed 's/\\td\\:.*g\\:/\\tg\\:/g' | sed 's/\\td\\:.*f\\:/\\tf\\:/g' | sed 's/\\td\\:.*o\\:/\\to\\:/g' | sed 's/\\td\\:.*c\\:/\\tc\\:/g' | sed 's/\\td\\:.*p\\:/\\tp\\:/g' >zotus_table_uparse-customized.tsv2\n```\n### 3.2. Subtracting NTC-derived ZOTUs counts OR removing the whole NTC-derived ZOTUs content from all target samples.\n3.2.1. Subtracting NTC-derived ZOTUs counts from target samples\nBASH31*\n>NOTE: \"NTC-ZOTUs-subtraction.R\" script is provided [here](https://github.com/eltonjrv/microbiome.westernu/blob/bin/NTC-ZOTUs-subtraction.R). \n\n# 4. Phylogenetic Diversity Investigation on ToIs\n### 4.1. Preparing files for phylogenetic analyses of your ToI(s)\n4.1.1. Within the uparse-generated \"outputs/\" directory, create a new directory to work on tree-prep files, and go within it:\n```\n$ mkdir ToI_trees\n$ cd ToI_trees\n```\n4.1.2. Creating a symbolic link of both uparse-generated zotus.fa and zotus_table_uparse-customized.tsv files:\nBASH36*\n4.1.3. Catching ZOTU IDs of genus-level ToI from the customized ZOTU table (Ehrlichia and Bartonella are genus examples used herein as ToI):\nBASH37*\n4.1.4. Catching ToI-ZOTU sequences and placing them in separate fasta files:\t\nBASH38*\n>NOTES:\nI) Please download both seqs1.pl and seqtools.pl PERL scripts from the [bin branch](https://github.com/eltonjrv/microbiome.westernu/tree/bin) and place them in the current work directory (ToI_trees). \nII) One must edit line 9 from \"seqstools.pl\" in order to properly point to your BioPerl full PATH, as well as line 31 from \"seqs1.pl\", replacing the seqtools.pl correct location.\nIII) One must also have BioPerl properly installed on the system in order to run this tool. Please refer to bioperl.org for instructions on how to install BioPerl. \n### 4.2. Relying on SILVA type strains database for phylogenetic comparisons\n#### Download \"SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta.gz\" file from the [refDB branch](https://github.com/eltonjrv/microbiome.westernu/tree/refDB). \nThis file contains 16S rRNA sequences for 23,127 type strain bacteria. Therefore, it is a good initial source for a comparison against your ToI-ZOTUs on a rough phylogenetic view. Of course, one must use the whole SILVA (~ 700k 16S RNA sequences) as well as BlastN against NCBI-NT db for assurance about \"novel species/strains\" discovery.\n>NOTE: Go to [refDB branch](https://github.com/eltonjrv/microbiome.westernu/tree/refDB) and click on the \"Clone or download\" green button, then \"Download ZIP\".  \n4.3.2. Keeping the ToI-ZOTUs stretch only in the MSA, that is, the 16S rRNA target-amplicon region:\n```\n$ perl cutMSA.pl Ehr-4tree_aln.fa Zotu9 TGTGCCAG 360 for >Ehr-4tree_alnCut.fa \n$ perl cutMSA.pl Bart-4tree_aln.fa Zotu7 TATTGGA 360 for >Bart-4tree_alnCut.fa \n```\n>NOTE: The *ad hoc* \"cutMSA.pl\" PERL script may be obtained [here](https://github.com/eltonjrv/bioinfo.scripts/blob/master/cutMSA.pl). Please read \"cutMSA.pl\" initial commented lines for instructions on how to run it. Those final \"\\*\\_alnCut.fa\" outputs must be used as inputs on [MEGA](https://www.megasoftware.net/), so the user can perform his/her most convenient phylogenetic inference methods.\n \n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9891653055220839,
      "result": {
        "original_header": "1.2. Debarcoding and demultiplexing",
        "type": "Text_excerpt",
        "value": "1.2.3.1. Debarcoding and demultiplexing a single pair of fastq files (multiplexed-seqs.qza file created on 1.1.8.1. step above):\n```\n$ for i in `ls iNextRev*tab`; do qiime cutadapt demux-paired --i-seqs multiplexed-seqs.qza --m-forward-barcodes-file $i --m-forward-barcodes-column iNext-For --m-reverse-barcodes-file $i --m-reverse-barcodes-column iNext-Rev --o-per-sample-sequences `echo $i | sed 's/\\-samples.*$//g'`-demultiplexed-seqs.qza --o-untrimmed-sequences `echo $i | sed 's/\\-samples.*$//g'`-untrimmed.qza --verbose; done\n```\n1.2.3.2. Debarcoding and demultiplexing several (n) pairs of fastq files (Exp\\*-multiplexed-seqs.qza files created on 1.1.8.2. step above):\nBASH4*\n1.2.4. Removing untrimmed.qza files:\nBASH5*\n1.2.5. Summarizing demultiplexed qza files as qzv ones in order to be visualized at https://view.qiime2.org/ (Important for sequenced reads quality control assessment/visualization on each sample):\nBASH6*\n1.2.6. Deactivating qiime2 environment:\nBASH7*\nor\nBASH8*\n1.2.7. Creating a directory to store and uncompress demultiplexed qza files, in order to have fastq files for topic 2 below:\nBASH9*\n1.2.8. Going back to the parent projectX/ dir:\nBASH10*\n \n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.913403466231074,
      "result": {
        "original_header": "3.3.  Keeping taxa of interest only (ToIs)",
        "type": "Text_excerpt",
        "value": "3.3.1. Catching your taxa of interest within the zotu table:\n```\n$ grep -P 'Ehrlichia|Anaplasma|Bartonella|Mycoplasma|Rickettsia' zotus_table_uparse-customized-woNTCzotus.tsv >zotus_table_uparse-wTaxa-customized-woNTCzotus-ToIonly.tsv\n```\n>NOTE: Replace the name of above genera by the ones of your interest (keep both ' and | signs). \n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Please refer to http://www.usadellab.org/cms/?page=trimmomatic for Trimmomatic download and instructions",
        "parent_header": [
          "2. Microbiome Sequencing Analyses",
          "2.1. Trimming primers with Trimmomatic (this must be done prior running UPARSE)"
        ],
        "type": "Text_excerpt",
        "value": "2.1.1. Running trimmomatic\n```\n$ for i in `ls inputs/*R1*fastq.gz`; do R1=`echo $i | sed 's/inputs\\///g' | sed 's/\\.fastq\\.gz$//g'`; R2=`echo $i | sed 's/inputs\\///g' | sed 's/\\.fastq\\.gz$//g' | sed 's/_R1_/_R2_/g'`; java -jar /path/to/your/Trimmomatic-x.xx/trimmomatic-x.xx.jar PE -phred33 $i `echo $i | sed 's/_R1_/_R2_/g'` $R1.fq $R1.unpaired.fq $R2.fq $R2.unpaired.fq HEADCROP:20; done\n```\n> NOTES:\nI) Please pay attention that you need to type your trimmmomatic installation full PATH after the \"java -jar\" command above.\nII) Edit the \"HEADCROP:20\" parameter according to your primers' average length. In this example, primers' length is ~ 20 bp.\n\n2.1.2. Removing unpaired reads after trimming:\n```\n$ rm *unpaired*\n```\n2.1.3. Creating a new directory where trimmed fastq files must be placed into:\n```\n$ mkdir inputs-woPrimers\n```\n2.1.4. Moving trimmed fastq files to that new directory:\n```\n$ mv *fq inputs-woPrimers/\n```"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Please refer to https://www.drive5.com/usearch/download.html in order to download USEARCH tools",
        "parent_header": [
          "2. Microbiome Sequencing Analyses",
          "2.2. Running UPARSE on sequenced and trimmed amplicons"
        ],
        "type": "Text_excerpt",
        "value": "2.2.1. Running the actual microbiome analyzer tool:\n```\n$ bash run-uparse-amp250-450-ZOTUs.bash inputs-woPrimers/ 2>run-uparse.log\n```\n>NOTES:\nI) UPARSE pipeline does a series of tasks such as: mate joining and quality filtering of your sequenced amplicons, ZOTUs/ESVs assembly, taxonomic classification, and both alpha- and beta-diversity analyses on all your samples.\nII) \"run-uparse-amp250-450-ZOTUs.bash\" BASH script may be obtained [here](https://github.com/eltonjrv/microbiome.westernu/blob/master/run-uparse-amp250-450-ZOTUs.bash).\nIIa) One may tune parameters on each command in the script according to his/her needs.\nIIb) This script uses a customized RDP refDB, which we have added Ehrlichia_canis, Ehrlichia_chafeensis, Anaplasma_platys, Anaplasma_phagocytophilum, Mycoplasma_haemocanis and Mycoplasma_haematoparvum 16S rRNA sequences. In order to download it, please go [here](https://github.com/eltonjrv/microbiome.westernu/tree/refDB) and click on the \"Clone or download\" green button, then \"Download ZIP\". After unzipping the downloaded folder, uncompress the \"rdp_16s_extra_seqs.fa.gz\" file with \"gunzip\" command, and then place it into the directory where you will run this script (\"uparse-run/\" in this example). Line 52 from the \"\"run-uparse-amp250-450-ZOTUs.bash\"\" script will format that file in order to be used as a refDB (\\*.udb) for taxonomic classification purposes. If you want to use your own customized refDB fasta file, please edit script's lines 52 and 53.\nIIc) In order to play with different OTU clustering % identity thresholds (95, 97, and 99%), one must run the alternative \"run-uparse-amp250-450-OTUs95_97_99_100.bash\" script that is provided [here](https://github.com/eltonjrv/microbiome.westernu/blob/master/run-uparse-amp250-450-OTUs95_97_99_100.bash).\nIII) An \"outputs\" directory will be created and all uparse-generated files will be placed within it.\n\n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Download \"SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta.gz\" file from the <a href=\"https://github.com/eltonjrv/microbiome.westernu/tree/refDB\">refDB branch</a>.",
        "parent_header": [
          "4. Phylogenetic Diversity Investigation on ToIs",
          "4.2. Relying on SILVA type strains database for phylogenetic comparisons"
        ],
        "type": "Text_excerpt",
        "value": "This file contains 16S rRNA sequences for 23,127 type strain bacteria. Therefore, it is a good initial source for a comparison against your ToI-ZOTUs on a rough phylogenetic view. Of course, one must use the whole SILVA (~ 700k 16S RNA sequences) as well as BlastN against NCBI-NT db for assurance about \"novel species/strains\" discovery.\n>NOTE: Go to [refDB branch](https://github.com/eltonjrv/microbiome.westernu/tree/refDB) and click on the \"Clone or download\" green button, then \"Download ZIP\". \n\n4.2.1. After unzipping the downloaded folder, uncompress such file like the following:\n```\n$ gunzip SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta.gz\n```\n4.2.2. Catching ToI type strain sequences from SILVA:\n```\n$ perl geneSearcher.pl Ehrlichia SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta \n$ perl geneSearcher.pl Bartonella SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta \n```\n>NOTE: \"geneSearcher.pl\" PERL script may be obtained [here](https://github.com/eltonjrv/bioinfo.scripts/blob/master/geneSearcher.pl).\n\n4.2.3. Merging both ToI type strain sequences and ToI-ZOTUs in a single file:\n```\n$ cat Ehrlichia_from_SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta Ehr-zotus.fa >Ehr-4tree.fasta\n$ cat Bartonella_from_SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta Bart-zotus.fa >Bart-4tree.fasta\n```"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "header_analysis"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/eltonjrv/microbiome.westernu/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 0
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/eltonjrv/microbiome.westernu/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "eltonjrv/microbiome.westernu"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "############ Software to be installed prior executing this SOP ##################",
        "parent_header": [
          "**Standard Operating Procedure (SOP)** for microbiome analyses applied to vector-borne disease diagnostics using 16S rRNA Next-Generation Sequencing.",
          "Research Team: Elton Vasconcelos*, Chayan Roy, Joseph Geiger, Brian Oakley, Pedro Diniz.",
          "*Current affiliation: Leeds Omics, University of Leeds, UK."
        ],
        "type": "Text_excerpt",
        "value": "### **Standard Operating Procedure (SOP)** for microbiome analyses applied to vector-borne disease diagnostics using 16S rRNA Next-Generation Sequencing.\n#### Research Team: Elton Vasconcelos*, Chayan Roy, Joseph Geiger, Brian Oakley, Pedro Diniz.\n##### Affiliation: College of Veterinary Medicine at Western University of Health Sciences, Pomona, CA, USA. \n##### \\*Current affiliation: Leeds Omics, University of Leeds, UK.\n>Author: Elton Vasconcelos (Oct/2018)\n\n>Reviewed and retested by Chayan Roy and Elton Vasconcelos (Sep/2020).\n>Reviewed by Elton Vasconcelos (Nov/2021)\n\nIf you use this pipeline (or part of it) on your research, please cite [Vasconcelos et al., 2021 - BMC Vet.Res.](https://bmcvetres.biomedcentral.com/articles/10.1186/s12917-021-02969-9)\n\n################## Software to be installed prior executing this SOP ################## \n  - [QIIME2](https://docs.qiime2.org/2021.8/install)\\*\n  - [UPARSE](https://www.drive5.com/usearch/download.html)\n  - [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic)\n  - [Muscle](https://drive5.com/muscle5/)\n  - Perl and Bash (already installed on any Unix-based system: Mac or Linux)\n  - [Bioperl](https://bioperl.org/)\n  - [R](https://www.r-project.org/)\n \n \\* Only if you are running demultiplexing step (topic 1 below).\n\n#############################################################################\n\n# 1. Debarcoding and Demultiplexing Raw Fastq Files (generated by the sequencing machine)\n##### This task is strictly dependent on both PCR and Sequencing protocols one has adopted in the Lab. \nIn the example below, we are referring to a two-round PCR method on which **two pairs of adapters** were used for a **paired-end sequencing**:\na) iNEXT barcodes (For_A-H - Rev_1-12) on the 1st round of amplifications, and \nb) Illumina overhang adapter sequences (For_i5 and Rev_i7) on the 2nd round.\nFor more details on this approach, please refer to [Faircloth & Glenn, 2012 - PLoS One](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0042543).\n>NOTE: If you have not employed this multiplexing approach or have all your fastq files already demultiplexed, please take a look at [qiime2 demultiplexing forum](https://forum.qiime2.org/t/demultiplexing-and-trimming-adapters-from-reads-with-q2-cutadapt/2313) or move straight to topic 2 below. \n\n### 1.1. Preparing directories and importing raw fastq files to QIIME2\n1.1.1.  Creating a project parent directory for the whole analysis:\n```\n$ mkdir projectX\n```\n1.1.2. Entering this directory:\n```\n$ cd projectX/\n```\n1.1.3. Creating a directory where all your raw fastq files will be placed in, and entering it:\n```\n$ mkdir raw_data\n$ cd raw_data/\n```\n1.1.4. Transferring your fastq files to the current work directory (raw_data/) with your most suitable/convenient command or tool (e.g. cp, scp, ftp, WinScp, FileZilla, etc ...).\n>NOTE: Since we don't know where you are pulling your data from, this step is intentionally left up to you.\n\n1.1.5. Renaming your fastq files. \n\n1.1.5.1. If you have only one pair of fastq files, rename R1 to forward.fastq.gz and R2 to reverse.fastq.gz, then move straight to sub-item 1.1.6:\n```\n$ mv your_R1_file.fastq.gz forward.fastq.gz\n$ mv your_R2_file.fastq.gz reverse.fastq.gz\n```\n1.1.5.2. If you have several (n) pairs of fastq files, it will be required to create several (n) experimental subdirectories within raw_data/;\n\n1.1.5.2.a. For example, the command below automatically creates 10 subdirectories named \"ExpN\", where N is a number from 1 to 10. Edit the command according to the total number of fastq files pairs you have on hands.\n```\n$ for i in `seq 1 10`; do mkdir Exp$i; done\n```\n1.1.5.2.b. Then move each pair of fastq files that were already placed within raw_data/ (sub-item 1.1.4) to each respective recently created subdirectory (Exp1 to Exp10), and rename the files. Below is an example of the commands' set for the Exp1 instance:\n```\n$ mv your_first_fastq_pair_R[12]*.fastq.gz Exp1/\n$ cd Exp1/\n$ mv your_first_fastq_pair_R1.fastq.gz forward.fastq.gz\n$ mv your_first_fastq_pair_R2.fastq.gz reverse.fastq.gz\n$ cd ../\n```\n>NOTE: Since we don't know how your fastq files are originally named, we recommend that you move them separately with the individual commands set above. In case you are famliar with the Unix Shell, feel free to do it through a \"for\" loop.\n\n1.1.6. Moving back to the parent directory (projectX/), creating a directory for the demultiplexing job and entering it:\n```\n$ cd ../\n$ mkdir demux\n$ cd demux/\n```\n1.1.7. Activating qiime2 environment (once you have properly installed it through this [link](https://docs.qiime2.org/2021.8/install)):\n```\n$ source activate qiime2-2021.8\n```\n1.1.8. Importing fastq files as qiime2 .qza format.\n\n1.1.8.1. For a single pair of fastq files (coming from 1.1.5.1 sub-item above), do the following:\n```\n$  qiime tools import --type MultiplexedPairedEndBarcodeInSequence --input-path ../raw_data/ --output-path multiplexed-seqs.qza\n```\n1.1.8.2. For several (n) pairs of fastq files (coming from 1.1.5.2 sub-item above), do this instead:\n```\n$ for i in `ls -d ../raw_data/Exp*/`; do qiime tools import --type MultiplexedPairedEndBarcodeInSequence --input-path $i --output-path `echo $i | sed 's/.*\\///g'`-multiplexed-seqs.qza; done\n```\n\n### 1.2. Debarcoding and demultiplexing\n1.2.1. Preparing a sample-barcode map file (samples-map-BCseq.tab) by running a single-line *ad hoc* PERL script\n```\n$ perl -e 'open(FILE, \"samples-map.tab\"); open(FILE2, \"iNext-barcodes.tab\"); while(<FILE2>) {chomp($_); @array2 = split(/\\t/, $_); $hash{$array2[0]} = $array2[1];} while(<FILE>) {chomp($_); @array = split(/\\t/, $_); if($hash{$array[1]} ne \"\" && $hash{$array[2]} ne \"\") {print(\"$array[0]\\t$hash{$array[1]}\\t$hash{$array[2]}\\n\");} else {print(\"$_\\t**error** ->barcode sequence not found\\n\");}}' >samples-map-BCseq.tab\n```\n>NOTES: One must prepare and place both input files (iNext-barcodes.tab and samples-map.tab) into the current directory (projectX/demux/). Please see/download [iNext-barcodes.tab](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/iNext-barcodes.tab) and [samples-map.tab](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-map.tab) as guiding examples, in case you have employed a demultiplexing situation like ours. The generated output from the command above is [samples-map-BCseq.tab](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-map-BCseq.tab), which will be used in downstream steps. In case you already have a samples-map-BCseq.tab-like structure in your favorite spreadsheet editor (e.g. Excel), feel free to save it as a Plain Text file, naming it as samples-map-BCseq.tab, transferring it to the current dir (projectX/demux/) and skipping the 1.2.1 command above.\n\n1.2.2. Splitting samples-map-BCseq.tab into N different iNext-Rev barcode combinations (one map file for each iNext-Rev barcode):\n```\n$ for i in `grep -v '^\\#' samples-map-BCseq.tab | cut -f 3 | sort -u`; do grep -P \"Sample|$i\" samples-map-BCseq.tab >iNextRev_`echo $i`-samples-map-BCseq.tab; done\n```\n>NOTE: At the time this SOP was written, \"qiime cutadapt\" function considered only one iNext-Rev barcode per sample-barcode map file.\n\n1.2.3. Actual debarcoding and demultiplexing process with qiime cutadapt.\n\n1.2.3.1. Debarcoding and demultiplexing a single pair of fastq files (multiplexed-seqs.qza file created on 1.1.8.1. step above):\n```\n$ for i in `ls iNextRev*tab`; do qiime cutadapt demux-paired --i-seqs multiplexed-seqs.qza --m-forward-barcodes-file $i --m-forward-barcodes-column iNext-For --m-reverse-barcodes-file $i --m-reverse-barcodes-column iNext-Rev --o-per-sample-sequences `echo $i | sed 's/\\-samples.*$//g'`-demultiplexed-seqs.qza --o-untrimmed-sequences `echo $i | sed 's/\\-samples.*$//g'`-untrimmed.qza --verbose; done\n```\n1.2.3.2. Debarcoding and demultiplexing several (n) pairs of fastq files (Exp\\*-multiplexed-seqs.qza files created on 1.1.8.2. step above):\n```\n$ for j in `ls Exp*seqs.qza`; do for i in `ls iNextRev*tab`; do qiime cutadapt demux-paired --i-seqs $j --m-forward-barcodes-file $i --m-forward-barcodes-column iNext-For --m-reverse-barcodes-file $i --m-reverse-barcodes-column iNext-Rev --o-per-sample-sequences `echo $j | sed 's/\\-multiplexed.*$//g'`_`echo $i | sed 's/\\-samples.*$//g'`-demultiplexed-seqs.qza --o-untrimmed-sequences `echo $j | sed 's/\\-multiplexed.*$//g'`_`echo $i | sed 's/\\-samples.*$//g'`-untrimmed.qza --verbose; done; done\n```\n1.2.4. Removing untrimmed.qza files:\n```\n$ rm *untrimmed.qza\n```\n1.2.5. Summarizing demultiplexed qza files as qzv ones in order to be visualized at https://view.qiime2.org/ (Important for sequenced reads quality control assessment/visualization on each sample):\n```\n$ for i in `ls *demultiplexed-seqs.qza`; do qiime demux summarize --i-data $i --o-visualization `echo $i | sed 's/qza$/qzv/g'`; done\n```\n1.2.6. Deactivating qiime2 environment:\n```\n$ source deactivate\n```\nor\n```\n$ conda deactivate\n```\n1.2.7. Creating a directory to store and uncompress demultiplexed qza files, in order to have fastq files for topic 2 below:\n```\n$ mkdir demux-unzipped\n$ cd demux-unzipped/\n$ ln -s ../*demultiplexed-seqs.qza .\n$ ls | xargs -i unzip {}\n```\n1.2.8. Going back to the parent projectX/ dir:\n```\n$ cd ../../\n```\n\n# 2. Microbiome Sequencing Analyses\n2a) Since UPARSE will be the main microbiome analyzer used here, let's first create a \"uparse-run\" directory and then go within it:\n```\n$ mkdir uparse-run\n$ cd uparse-run/\n```\n2b) Creating an \"inputs\" directory and placing symbolic links of demultiplexed fastq files that will be used by UPARSE:\n```\n$ mkdir inputs\n$ cd inputs/\n$ ln -s ../../demux/demux-unzipped/*/data/*gz .\n$ cd ../\n```\n>NOTE: If you are coming straight to this topic because you already had demultiplexed your samples on your own, please disconsider the \"ln -s\" command above and just place your demultiplexed and compressed fastq files (\\*.fastq.gz) into the \"inputs\" dir. Please also make sure that your fastq.gz file names start with \"sampleID\" followed by \"\\_1\\_L001\\_R[12]\\_001.fastq.gz\". For example, take a look at our [sample metadata table](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-metadata.tsv) and see that, for the first sample, the paired file names will be the following: 865A1\\_1\\_L001\\_R1\\_001.fastq.gz and 865A1\\_1\\_L001\\_R2\\_001.fastq.gz.\n### 2.1. Trimming primers with Trimmomatic (this must be done prior running UPARSE)\n##### Please refer to http://www.usadellab.org/cms/?page=trimmomatic for Trimmomatic download and instructions\n2.1.1. Running trimmomatic\n```\n$ for i in `ls inputs/*R1*fastq.gz`; do R1=`echo $i | sed 's/inputs\\///g' | sed 's/\\.fastq\\.gz$//g'`; R2=`echo $i | sed 's/inputs\\///g' | sed 's/\\.fastq\\.gz$//g' | sed 's/_R1_/_R2_/g'`; java -jar /path/to/your/Trimmomatic-x.xx/trimmomatic-x.xx.jar PE -phred33 $i `echo $i | sed 's/_R1_/_R2_/g'` $R1.fq $R1.unpaired.fq $R2.fq $R2.unpaired.fq HEADCROP:20; done\n```\n> NOTES:\nI) Please pay attention that you need to type your trimmmomatic installation full PATH after the \"java -jar\" command above.\nII) Edit the \"HEADCROP:20\" parameter according to your primers' average length. In this example, primers' length is ~ 20 bp.\n\n2.1.2. Removing unpaired reads after trimming:\n```\n$ rm *unpaired*\n```\n2.1.3. Creating a new directory where trimmed fastq files must be placed into:\n```\n$ mkdir inputs-woPrimers\n```\n2.1.4. Moving trimmed fastq files to that new directory:\n```\n$ mv *fq inputs-woPrimers/\n```\n### 2.2. Running UPARSE on sequenced and trimmed amplicons\n##### Please refer to https://www.drive5.com/usearch/download.html in order to download USEARCH tools\n2.2.1. Running the actual microbiome analyzer tool:\n```\n$ bash run-uparse-amp250-450-ZOTUs.bash inputs-woPrimers/ 2>run-uparse.log\n```\n>NOTES:\nI) UPARSE pipeline does a series of tasks such as: mate joining and quality filtering of your sequenced amplicons, ZOTUs/ESVs assembly, taxonomic classification, and both alpha- and beta-diversity analyses on all your samples.\nII) \"run-uparse-amp250-450-ZOTUs.bash\" BASH script may be obtained [here](https://github.com/eltonjrv/microbiome.westernu/blob/master/run-uparse-amp250-450-ZOTUs.bash).\nIIa) One may tune parameters on each command in the script according to his/her needs.\nIIb) This script uses a customized RDP refDB, which we have added Ehrlichia_canis, Ehrlichia_chafeensis, Anaplasma_platys, Anaplasma_phagocytophilum, Mycoplasma_haemocanis and Mycoplasma_haematoparvum 16S rRNA sequences. In order to download it, please go [here](https://github.com/eltonjrv/microbiome.westernu/tree/refDB) and click on the \"Clone or download\" green button, then \"Download ZIP\". After unzipping the downloaded folder, uncompress the \"rdp_16s_extra_seqs.fa.gz\" file with \"gunzip\" command, and then place it into the directory where you will run this script (\"uparse-run/\" in this example). Line 52 from the \"\"run-uparse-amp250-450-ZOTUs.bash\"\" script will format that file in order to be used as a refDB (\\*.udb) for taxonomic classification purposes. If you want to use your own customized refDB fasta file, please edit script's lines 52 and 53.\nIIc) In order to play with different OTU clustering % identity thresholds (95, 97, and 99%), one must run the alternative \"run-uparse-amp250-450-OTUs95_97_99_100.bash\" script that is provided [here](https://github.com/eltonjrv/microbiome.westernu/blob/master/run-uparse-amp250-450-OTUs95_97_99_100.bash).\nIII) An \"outputs\" directory will be created and all uparse-generated files will be placed within it.\n\n\n# 3. ZOTU/ESV Table Customization for Diagnostics Purposes\n##### This topic is aimed at creating a customized ZOTU table which will make data visual inspection easier for clinicians\n3a) Move to the uparse-generated \"outputs\" directory:\n```\n$ cd outputs/\n```\n#### ATTENTION: A metadata file will also be needed (which I'll call \"samples-metadata.tsv\" in this tutorial). Please have your metadata file prepared as this [example](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-metadata.tsv) and place it within the current directory (outputs).\n>NOTES about the metadata table:\nI) Columns 1 and 4 are mandatory.\nII) Column 4 must contain any textual string that best describes your samples.\nIII) There must not be any colon \":\" nor blank spaces within your sample descriptions.\n\n3b) There will already be two important uparse-generated files within the \"outputs\" dir (\"zotus_table_uparse.tsv\" and \"zotus.sintax\") . Once you have those two files plus your \"samples-metadata.tsv\" ready, run the following command:\n```\n$ bash customize-OTUtable.bash zotus_table_uparse.tsv zotus.sintax samples-metadata.tsv\n```\n>NOTES:\nI) \"customize-OTUtable.bash\" script may be obtained [here](https://github.com/eltonjrv/microbiome.westernu/blob/bin/customize-OTUtable.bash).\nII) There are two other embedded scripts that must also be placed within the current directory where you'll run customize-OTUtable.bash: [customize-OTUtable.R](https://github.com/eltonjrv/microbiome.westernu/blob/bin/customize-OTUtable.R) and [sampleID-to-sampleDescription.pl](https://github.com/eltonjrv/microbiome.westernu/blob/bin/sampleID-to-sampleDescription.pl).\nIII) A \"zotus_table_uparse-customized.tsv\" main output file is created with the above command.\n\n### 3.1. Improving customized ZOTU table\n3.1.1. In case one wants to keep only the last taxonomic level assigned to each zotu, instead of seeing the whole taxonomic classification (from phylum to species), run the following:\n```\n$ sed 's/\\td\\:.*s\\:/\\ts\\:/g' zotus_table_uparse-customized.tsv |  sed 's/\\td\\:.*g\\:/\\tg\\:/g' | sed 's/\\td\\:.*f\\:/\\tf\\:/g' | sed 's/\\td\\:.*o\\:/\\to\\:/g' | sed 's/\\td\\:.*c\\:/\\tc\\:/g' | sed 's/\\td\\:.*p\\:/\\tp\\:/g' >zotus_table_uparse-customized.tsv2\n```\n### 3.2. Subtracting NTC-derived ZOTUs counts OR removing the whole NTC-derived ZOTUs content from all target samples.\n3.2.1. Subtracting NTC-derived ZOTUs counts from target samples\n```\n$ Rscript NTC-ZOTUs-subtraction.R zotus_table_uparse-customized.tsv\n```\n>NOTE: \"NTC-ZOTUs-subtraction.R\" script is provided [here](https://github.com/eltonjrv/microbiome.westernu/blob/bin/NTC-ZOTUs-subtraction.R).\n\n3.2.2. Alternatively, one may want to remove the whole NTC-derived ZOTUs content. So, first catch all zotus present in negative control samples:\n```\n$ grep '_neg_' zotus_table_uparse-customized.tsv2 | cut -f 2 | sort -u >ZotusOnNTC.txt\n```\n>NOTE: On the command above, replace '\\_neg\\_' by any other tag that characterizes the negative control in your sample descriptions (e.g. NTC, water, blank, etc ...)\n\n3.2.3. Then, with the following PERL code, create a new zotu table without any NTC-derived zotu:\n```\n$  perl -e 'open(FILE, \"zotus_table_uparse-customized.tsv2\"); open(FILE2, \"ZotusOnNTC.txt\"); while(<FILE2>){chomp($_); $hash{$_} = 1;} while(<FILE>){chomp($_); @array = split(/\\t/, $_); if($hash{$array[1]} eq \"\"){print(\"$_\\n\");}}' >zotus_table_uparse-customized-woNTCzotus.tsv\n```\n### 3.3.  Keeping taxa of interest only (ToIs)\n3.3.1. Catching your taxa of interest within the zotu table:\n```\n$ grep -P 'Ehrlichia|Anaplasma|Bartonella|Mycoplasma|Rickettsia' zotus_table_uparse-customized-woNTCzotus.tsv >zotus_table_uparse-wTaxa-customized-woNTCzotus-ToIonly.tsv\n```\n>NOTE: Replace the name of above genera by the ones of your interest (keep both ' and | signs).\n\n\n# 4. Phylogenetic Diversity Investigation on ToIs\n### 4.1. Preparing files for phylogenetic analyses of your ToI(s)\n4.1.1. Within the uparse-generated \"outputs/\" directory, create a new directory to work on tree-prep files, and go within it:\n```\n$ mkdir ToI_trees\n$ cd ToI_trees\n```\n4.1.2. Creating a symbolic link of both uparse-generated zotus.fa and zotus_table_uparse-customized.tsv files:\n```\n$ ln -s ../zotus.fa .\n$ ln -s ../zotus_table_uparse-customized.tsv .\n```\n4.1.3. Catching ZOTU IDs of genus-level ToI from the customized ZOTU table (Ehrlichia and Bartonella are genus examples used herein as ToI):\n```\n$ grep 'g:Ehrlichia' zotus_table_uparse-customized.tsv | cut -f 2 | sort -u >Ehr-zotus.nam\n$ grep 'g:Bartonella' zotus_table_uparse-customized.tsv | cut -f 2 | sort -u >Bart-zotus.nam\n```\n4.1.4. Catching ToI-ZOTU sequences and placing them in separate fasta files:\t\n```\n$ perl seqs1.pl -outfmt fasta -incl Ehr-zotus.nam -seq zotus.fa >Ehr-zotus.fa\n$ perl seqs1.pl -outfmt fasta -incl Bart-zotus.nam -seq zotus.fa >Bart-zotus.fa\n```\n>NOTES:\nI) Please download both seqs1.pl and seqtools.pl PERL scripts from the [bin branch](https://github.com/eltonjrv/microbiome.westernu/tree/bin) and place them in the current work directory (ToI_trees). \nII) One must edit line 9 from \"seqstools.pl\" in order to properly point to your BioPerl full PATH, as well as line 31 from \"seqs1.pl\", replacing the seqtools.pl correct location.\nIII) One must also have BioPerl properly installed on the system in order to run this tool. Please refer to bioperl.org for instructions on how to install BioPerl.\n\n### 4.2. Relying on SILVA type strains database for phylogenetic comparisons\n#### Download \"SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta.gz\" file from the [refDB branch](https://github.com/eltonjrv/microbiome.westernu/tree/refDB). \nThis file contains 16S rRNA sequences for 23,127 type strain bacteria. Therefore, it is a good initial source for a comparison against your ToI-ZOTUs on a rough phylogenetic view. Of course, one must use the whole SILVA (~ 700k 16S RNA sequences) as well as BlastN against NCBI-NT db for assurance about \"novel species/strains\" discovery.\n>NOTE: Go to [refDB branch](https://github.com/eltonjrv/microbiome.westernu/tree/refDB) and click on the \"Clone or download\" green button, then \"Download ZIP\". \n\n4.2.1. After unzipping the downloaded folder, uncompress such file like the following:\n```\n$ gunzip SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta.gz\n```\n4.2.2. Catching ToI type strain sequences from SILVA:\n```\n$ perl geneSearcher.pl Ehrlichia SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta \n$ perl geneSearcher.pl Bartonella SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta \n```\n>NOTE: \"geneSearcher.pl\" PERL script may be obtained [here](https://github.com/eltonjrv/bioinfo.scripts/blob/master/geneSearcher.pl).\n\n4.2.3. Merging both ToI type strain sequences and ToI-ZOTUs in a single file:\n```\n$ cat Ehrlichia_from_SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta Ehr-zotus.fa >Ehr-4tree.fasta\n$ cat Bartonella_from_SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta Bart-zotus.fa >Bart-4tree.fasta\n```\n### 4.3. Running a global Multiple Sequence Alignment (MSA) with muscle:\n4.3.1. Running muscle in order to get an MSA in fasta format\n```\n$ muscle -in Ehr-4tree.fasta  -out Ehr-4tree_aln.fa\n$ muscle -in Bart-4tree.fasta -out Bart-4tree_aln.fa\n```\n>NOTE: Refer to https://www.drive5.com/muscle/downloads.htm for instructions on how to download and install muscle.\n\n4.3.2. Keeping the ToI-ZOTUs stretch only in the MSA, that is, the 16S rRNA target-amplicon region:\n```\n$ perl cutMSA.pl Ehr-4tree_aln.fa Zotu9 TGTGCCAG 360 for >Ehr-4tree_alnCut.fa \n$ perl cutMSA.pl Bart-4tree_aln.fa Zotu7 TATTGGA 360 for >Bart-4tree_alnCut.fa \n```\n>NOTE: The *ad hoc* \"cutMSA.pl\" PERL script may be obtained [here](https://github.com/eltonjrv/bioinfo.scripts/blob/master/cutMSA.pl). Please read \"cutMSA.pl\" initial commented lines for instructions on how to run it. Those final \"\\*\\_alnCut.fa\" outputs must be used as inputs on [MEGA](https://www.megasoftware.net/), so the user can perform his/her most convenient phylogenetic inference methods.\n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "1.1. Preparing directories and importing raw fastq files to QIIME2",
        "parent_header": [
          "1. Debarcoding and Demultiplexing Raw Fastq Files (generated by the sequencing machine)"
        ],
        "type": "Text_excerpt",
        "value": "1.1.1.  Creating a project parent directory for the whole analysis:\n```\n$ mkdir projectX\n```\n1.1.2. Entering this directory:\n```\n$ cd projectX/\n```\n1.1.3. Creating a directory where all your raw fastq files will be placed in, and entering it:\n```\n$ mkdir raw_data\n$ cd raw_data/\n```\n1.1.4. Transferring your fastq files to the current work directory (raw_data/) with your most suitable/convenient command or tool (e.g. cp, scp, ftp, WinScp, FileZilla, etc ...).\n>NOTE: Since we don't know where you are pulling your data from, this step is intentionally left up to you.\n\n1.1.5. Renaming your fastq files. \n\n1.1.5.1. If you have only one pair of fastq files, rename R1 to forward.fastq.gz and R2 to reverse.fastq.gz, then move straight to sub-item 1.1.6:\n```\n$ mv your_R1_file.fastq.gz forward.fastq.gz\n$ mv your_R2_file.fastq.gz reverse.fastq.gz\n```\n1.1.5.2. If you have several (n) pairs of fastq files, it will be required to create several (n) experimental subdirectories within raw_data/;\n\n1.1.5.2.a. For example, the command below automatically creates 10 subdirectories named \"ExpN\", where N is a number from 1 to 10. Edit the command according to the total number of fastq files pairs you have on hands.\n```\n$ for i in `seq 1 10`; do mkdir Exp$i; done\n```\n1.1.5.2.b. Then move each pair of fastq files that were already placed within raw_data/ (sub-item 1.1.4) to each respective recently created subdirectory (Exp1 to Exp10), and rename the files. Below is an example of the commands' set for the Exp1 instance:\n```\n$ mv your_first_fastq_pair_R[12]*.fastq.gz Exp1/\n$ cd Exp1/\n$ mv your_first_fastq_pair_R1.fastq.gz forward.fastq.gz\n$ mv your_first_fastq_pair_R2.fastq.gz reverse.fastq.gz\n$ cd ../\n```\n>NOTE: Since we don't know how your fastq files are originally named, we recommend that you move them separately with the individual commands set above. In case you are famliar with the Unix Shell, feel free to do it through a \"for\" loop.\n\n1.1.6. Moving back to the parent directory (projectX/), creating a directory for the demultiplexing job and entering it:\n```\n$ cd ../\n$ mkdir demux\n$ cd demux/\n```\n1.1.7. Activating qiime2 environment (once you have properly installed it through this [link](https://docs.qiime2.org/2021.8/install)):\n```\n$ source activate qiime2-2021.8\n```\n1.1.8. Importing fastq files as qiime2 .qza format.\n\n1.1.8.1. For a single pair of fastq files (coming from 1.1.5.1 sub-item above), do the following:\n```\n$  qiime tools import --type MultiplexedPairedEndBarcodeInSequence --input-path ../raw_data/ --output-path multiplexed-seqs.qza\n```\n1.1.8.2. For several (n) pairs of fastq files (coming from 1.1.5.2 sub-item above), do this instead:\n```\n$ for i in `ls -d ../raw_data/Exp*/`; do qiime tools import --type MultiplexedPairedEndBarcodeInSequence --input-path $i --output-path `echo $i | sed 's/.*\\///g'`-multiplexed-seqs.qza; done\n```\n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "ATTENTION: A metadata file will also be needed (which I'll call \"samples-metadata.tsv\" in this tutorial). Please have your metadata file prepared as this <a href=\"https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-metadata.tsv\">example</a> and place it within the current directory (outputs).",
        "parent_header": [
          "3. ZOTU/ESV Table Customization for Diagnostics Purposes"
        ],
        "type": "Text_excerpt",
        "value": ">NOTES about the metadata table:\nI) Columns 1 and 4 are mandatory.\nII) Column 4 must contain any textual string that best describes your samples.\nIII) There must not be any colon \":\" nor blank spaces within your sample descriptions.\n\n3b) There will already be two important uparse-generated files within the \"outputs\" dir (\"zotus_table_uparse.tsv\" and \"zotus.sintax\") . Once you have those two files plus your \"samples-metadata.tsv\" ready, run the following command:\n```\n$ bash customize-OTUtable.bash zotus_table_uparse.tsv zotus.sintax samples-metadata.tsv\n```\n>NOTES:\nI) \"customize-OTUtable.bash\" script may be obtained [here](https://github.com/eltonjrv/microbiome.westernu/blob/bin/customize-OTUtable.bash).\nII) There are two other embedded scripts that must also be placed within the current directory where you'll run customize-OTUtable.bash: [customize-OTUtable.R](https://github.com/eltonjrv/microbiome.westernu/blob/bin/customize-OTUtable.R) and [sampleID-to-sampleDescription.pl](https://github.com/eltonjrv/microbiome.westernu/blob/bin/sampleID-to-sampleDescription.pl).\nIII) A \"zotus_table_uparse-customized.tsv\" main output file is created with the above command.\n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "4.1. Preparing files for phylogenetic analyses of your ToI(s)",
        "parent_header": [
          "4. Phylogenetic Diversity Investigation on ToIs"
        ],
        "type": "Text_excerpt",
        "value": "4.1.1. Within the uparse-generated \"outputs/\" directory, create a new directory to work on tree-prep files, and go within it:\n```\n$ mkdir ToI_trees\n$ cd ToI_trees\n```\n4.1.2. Creating a symbolic link of both uparse-generated zotus.fa and zotus_table_uparse-customized.tsv files:\n```\n$ ln -s ../zotus.fa .\n$ ln -s ../zotus_table_uparse-customized.tsv .\n```\n4.1.3. Catching ZOTU IDs of genus-level ToI from the customized ZOTU table (Ehrlichia and Bartonella are genus examples used herein as ToI):\n```\n$ grep 'g:Ehrlichia' zotus_table_uparse-customized.tsv | cut -f 2 | sort -u >Ehr-zotus.nam\n$ grep 'g:Bartonella' zotus_table_uparse-customized.tsv | cut -f 2 | sort -u >Bart-zotus.nam\n```\n4.1.4. Catching ToI-ZOTU sequences and placing them in separate fasta files:\t\n```\n$ perl seqs1.pl -outfmt fasta -incl Ehr-zotus.nam -seq zotus.fa >Ehr-zotus.fa\n$ perl seqs1.pl -outfmt fasta -incl Bart-zotus.nam -seq zotus.fa >Bart-zotus.fa\n```\n>NOTES:\nI) Please download both seqs1.pl and seqtools.pl PERL scripts from the [bin branch](https://github.com/eltonjrv/microbiome.westernu/tree/bin) and place them in the current work directory (ToI_trees). \nII) One must edit line 9 from \"seqstools.pl\" in order to properly point to your BioPerl full PATH, as well as line 31 from \"seqs1.pl\", replacing the seqtools.pl correct location.\nIII) One must also have BioPerl properly installed on the system in order to run this tool. Please refer to bioperl.org for instructions on how to install BioPerl.\n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9931467141383905,
      "result": {
        "original_header": "*Current affiliation: Leeds Omics, University of Leeds, UK.",
        "type": "Text_excerpt",
        "value": "################## Software to be installed prior executing this SOP ################## \n  - [QIIME2](https://docs.qiime2.org/2021.8/install)\\*\n  - [UPARSE](https://www.drive5.com/usearch/download.html)\n  - [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic)\n  - [Muscle](https://drive5.com/muscle5/)\n  - Perl and Bash (already installed on any Unix-based system: Mac or Linux)\n  - [Bioperl](https://bioperl.org/)\n  - [R](https://www.r-project.org/)\n \n \\* Only if you are running demultiplexing step (topic 1 below). \n### 1.1. Preparing directories and importing raw fastq files to QIIME2\n1.1.1.  Creating a project parent directory for the whole analysis:\n```\n$ mkdir projectX\n```\n1.1.2. Entering this directory:\nBASH2*\n1.1.3. Creating a directory where all your raw fastq files will be placed in, and entering it:\nBASH3*\n1.1.4. Transferring your fastq files to the current work directory (raw_data/) with your most suitable/convenient command or tool (e.g. cp, scp, ftp, WinScp, FileZilla, etc ...).\n>NOTE: Since we don't know where you are pulling your data from, this step is intentionally left up to you. \n1.1.5.1. If you have only one pair of fastq files, rename R1 to forward.fastq.gz and R2 to reverse.fastq.gz, then move straight to sub-item 1.1.6:\n```\n$ mv your_R1_file.fastq.gz forward.fastq.gz\n$ mv your_R2_file.fastq.gz reverse.fastq.gz\n```\n1.1.5.2. If you have several (n) pairs of fastq files, it will be required to create several (n) experimental subdirectories within raw_data/; \n1.1.6. Moving back to the parent directory (projectX/), creating a directory for the demultiplexing job and entering it:\n```\n$ cd ../\n$ mkdir demux\n$ cd demux/\n```\n1.1.7. Activating qiime2 environment (once you have properly installed it through this [link](https://docs.qiime2.org/2021.8/install)):\nBASH8*\n1.1.8. Importing fastq files as qiime2 .qza format. \n# 2. Microbiome Sequencing Analyses\n2a) Since UPARSE will be the main microbiome analyzer used here, let's first create a \"uparse-run\" directory and then go within it:\n```\n$ mkdir uparse-run\n$ cd uparse-run/\n```\n2b) Creating an \"inputs\" directory and placing symbolic links of demultiplexed fastq files that will be used by UPARSE:\nBASH22*\n>NOTE: If you are coming straight to this topic because you already had demultiplexed your samples on your own, please disconsider the \"ln -s\" command above and just place your demultiplexed and compressed fastq files (\\*.fastq.gz) into the \"inputs\" dir. Please also make sure that your fastq.gz file names start with \"sampleID\" followed by \"\\_1\\_L001\\_R[12]\\_001.fastq.gz\". For example, take a look at our [sample metadata table](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-metadata.tsv) and see that, for the first sample, the paired file names will be the following: 865A1\\_1\\_L001\\_R1\\_001.fastq.gz and 865A1\\_1\\_L001\\_R2\\_001.fastq.gz.\n### 2.1. Trimming primers with Trimmomatic (this must be done prior running UPARSE)\n##### Please refer to http://www.usadellab.org/cms/?page=trimmomatic for Trimmomatic download and instructions\n2.1.1. Running trimmomatic\nBASH23*\n> NOTES:\nI) Please pay attention that you need to type your trimmmomatic installation full PATH after the \"java -jar\" command above.\nII) Edit the \"HEADCROP:20\" parameter according to your primers' average length. In this example, primers' length is ~ 20 bp. \n2.1.2. Removing unpaired reads after trimming:\n```\n$ rm *unpaired*\n```\n2.1.3. Creating a new directory where trimmed fastq files must be placed into:\nBASH25*\n2.1.4. Moving trimmed fastq files to that new directory:\nBASH26*\n### 2.2. Running UPARSE on sequenced and trimmed amplicons\n##### Please refer to https://www.drive5.com/usearch/download.html in order to download USEARCH tools\n2.2.1. Running the actual microbiome analyzer tool:\nBASH27*\n>NOTES:\nI) UPARSE pipeline does a series of tasks such as: mate joining and quality filtering of your sequenced amplicons, ZOTUs/ESVs assembly, taxonomic classification, and both alpha- and beta-diversity analyses on all your samples.\nII) \"run-uparse-amp250-450-ZOTUs.bash\" BASH script may be obtained [here](https://github.com/eltonjrv/microbiome.westernu/blob/master/run-uparse-amp250-450-ZOTUs.bash).\nIIa) One may tune parameters on each command in the script according to his/her needs.\nIIb) This script uses a customized RDP refDB, which we have added Ehrlichia_canis, Ehrlichia_chafeensis, Anaplasma_platys, Anaplasma_phagocytophilum, Mycoplasma_haemocanis and Mycoplasma_haematoparvum 16S rRNA sequences. In order to download it, please go [here](https://github.com/eltonjrv/microbiome.westernu/tree/refDB) and click on the \"Clone or download\" green button, then \"Download ZIP\". After unzipping the downloaded folder, uncompress the \"rdp_16s_extra_seqs.fa.gz\" file with \"gunzip\" command, and then place it into the directory where you will run this script (\"uparse-run/\" in this example). Line 52 from the \"\"run-uparse-amp250-450-ZOTUs.bash\"\" script will format that file in order to be used as a refDB (\\*.udb) for taxonomic classification purposes. If you want to use your own customized refDB fasta file, please edit script's lines 52 and 53.\nIIc) In order to play with different OTU clustering % identity thresholds (95, 97, and 99%), one must run the alternative \"run-uparse-amp250-450-OTUs95_97_99_100.bash\" script that is provided [here](https://github.com/eltonjrv/microbiome.westernu/blob/master/run-uparse-amp250-450-OTUs95_97_99_100.bash).\nIII) An \"outputs\" directory will be created and all uparse-generated files will be placed within it. \n3b) There will already be two important uparse-generated files within the \"outputs\" dir (\"zotus_table_uparse.tsv\" and \"zotus.sintax\") . Once you have those two files plus your \"samples-metadata.tsv\" ready, run the following command:\n```\n$ bash customize-OTUtable.bash zotus_table_uparse.tsv zotus.sintax samples-metadata.tsv\n```\n>NOTES:\nI) \"customize-OTUtable.bash\" script may be obtained [here](https://github.com/eltonjrv/microbiome.westernu/blob/bin/customize-OTUtable.bash).\nII) There are two other embedded scripts that must also be placed within the current directory where you'll run customize-OTUtable.bash: [customize-OTUtable.R](https://github.com/eltonjrv/microbiome.westernu/blob/bin/customize-OTUtable.R) and [sampleID-to-sampleDescription.pl](https://github.com/eltonjrv/microbiome.westernu/blob/bin/sampleID-to-sampleDescription.pl).\nIII) A \"zotus_table_uparse-customized.tsv\" main output file is created with the above command. \n3.2.2. Alternatively, one may want to remove the whole NTC-derived ZOTUs content. So, first catch all zotus present in negative control samples:\n```\n$ grep '_neg_' zotus_table_uparse-customized.tsv2 | cut -f 2 | sort -u >ZotusOnNTC.txt\n```\n>NOTE: On the command above, replace '\\_neg\\_' by any other tag that characterizes the negative control in your sample descriptions (e.g. NTC, water, blank, etc ...) \n\n# 4. Phylogenetic Diversity Investigation on ToIs\n### 4.1. Preparing files for phylogenetic analyses of your ToI(s)\n4.1.1. Within the uparse-generated \"outputs/\" directory, create a new directory to work on tree-prep files, and go within it:\n```\n$ mkdir ToI_trees\n$ cd ToI_trees\n```\n4.1.2. Creating a symbolic link of both uparse-generated zotus.fa and zotus_table_uparse-customized.tsv files:\nBASH36*\n4.1.3. Catching ZOTU IDs of genus-level ToI from the customized ZOTU table (Ehrlichia and Bartonella are genus examples used herein as ToI):\nBASH37*\n4.1.4. Catching ToI-ZOTU sequences and placing them in separate fasta files:\t\nBASH38*\n>NOTES:\nI) Please download both seqs1.pl and seqtools.pl PERL scripts from the [bin branch](https://github.com/eltonjrv/microbiome.westernu/tree/bin) and place them in the current work directory (ToI_trees). \nII) One must edit line 9 from \"seqstools.pl\" in order to properly point to your BioPerl full PATH, as well as line 31 from \"seqs1.pl\", replacing the seqtools.pl correct location.\nIII) One must also have BioPerl properly installed on the system in order to run this tool. Please refer to bioperl.org for instructions on how to install BioPerl. \n### 4.2. Relying on SILVA type strains database for phylogenetic comparisons\n#### Download \"SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta.gz\" file from the [refDB branch](https://github.com/eltonjrv/microbiome.westernu/tree/refDB). \nThis file contains 16S rRNA sequences for 23,127 type strain bacteria. Therefore, it is a good initial source for a comparison against your ToI-ZOTUs on a rough phylogenetic view. Of course, one must use the whole SILVA (~ 700k 16S RNA sequences) as well as BlastN against NCBI-NT db for assurance about \"novel species/strains\" discovery.\n>NOTE: Go to [refDB branch](https://github.com/eltonjrv/microbiome.westernu/tree/refDB) and click on the \"Clone or download\" green button, then \"Download ZIP\".  \n4.2.1. After unzipping the downloaded folder, uncompress such file like the following:\n```\n$ gunzip SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta.gz\n```\n4.2.2. Catching ToI type strain sequences from SILVA:\nBASH40*\n>NOTE: \"geneSearcher.pl\" PERL script may be obtained [here](https://github.com/eltonjrv/bioinfo.scripts/blob/master/geneSearcher.pl). \n4.2.3. Merging both ToI type strain sequences and ToI-ZOTUs in a single file:\n```\n$ cat Ehrlichia_from_SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta Ehr-zotus.fa >Ehr-4tree.fasta\n$ cat Bartonella_from_SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta Bart-zotus.fa >Bart-4tree.fasta\n```\n### 4.3. Running a global Multiple Sequence Alignment (MSA) with muscle:\n4.3.1. Running muscle in order to get an MSA in fasta format\nBASH42*\n>NOTE: Refer to https://www.drive5.com/muscle/downloads.htm for instructions on how to download and install muscle. \n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.999999468810329,
      "result": {
        "original_header": "2. Microbiome Sequencing Analyses",
        "type": "Text_excerpt",
        "value": "2a) Since UPARSE will be the main microbiome analyzer used here, let's first create a \"uparse-run\" directory and then go within it:\n```\n$ mkdir uparse-run\n$ cd uparse-run/\n```\n2b) Creating an \"inputs\" directory and placing symbolic links of demultiplexed fastq files that will be used by UPARSE:\nBASH2*\n>NOTE: If you are coming straight to this topic because you already had demultiplexed your samples on your own, please disconsider the \"ln -s\" command above and just place your demultiplexed and compressed fastq files (\\*.fastq.gz) into the \"inputs\" dir. Please also make sure that your fastq.gz file names start with \"sampleID\" followed by \"\\_1\\_L001\\_R[12]\\_001.fastq.gz\". For example, take a look at our [sample metadata table](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-metadata.tsv) and see that, for the first sample, the paired file names will be the following: 865A1\\_1\\_L001\\_R1\\_001.fastq.gz and 865A1\\_1\\_L001\\_R2\\_001.fastq.gz. \n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8100562169082713,
      "result": {
        "original_header": "This topic is aimed at creating a customized ZOTU table which will make data visual inspection easier for clinicians",
        "type": "Text_excerpt",
        "value": "3a) Move to the uparse-generated \"outputs\" directory:\n```\n$ cd outputs/\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.848373762070835,
      "result": {
        "original_header": "3.2. Subtracting NTC-derived ZOTUs counts OR removing the whole NTC-derived ZOTUs content from all target samples.",
        "type": "Text_excerpt",
        "value": "3.2.1. Subtracting NTC-derived ZOTUs counts from target samples\n```\n$ Rscript NTC-ZOTUs-subtraction.R zotus_table_uparse-customized.tsv\n```\n>NOTE: \"NTC-ZOTUs-subtraction.R\" script is provided [here](https://github.com/eltonjrv/microbiome.westernu/blob/bin/NTC-ZOTUs-subtraction.R). \n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8355775382641681,
      "result": {
        "original_header": "*Current affiliation: Leeds Omics, University of Leeds, UK.",
        "type": "Text_excerpt",
        "value": "3b) There will already be two important uparse-generated files within the \"outputs\" dir (\"zotus_table_uparse.tsv\" and \"zotus.sintax\") . Once you have those two files plus your \"samples-metadata.tsv\" ready, run the following command:\n```\n$ bash customize-OTUtable.bash zotus_table_uparse.tsv zotus.sintax samples-metadata.tsv\n```\n>NOTES:\nI) \"customize-OTUtable.bash\" script may be obtained [here](https://github.com/eltonjrv/microbiome.westernu/blob/bin/customize-OTUtable.bash).\nII) There are two other embedded scripts that must also be placed within the current directory where you'll run customize-OTUtable.bash: [customize-OTUtable.R](https://github.com/eltonjrv/microbiome.westernu/blob/bin/customize-OTUtable.R) and [sampleID-to-sampleDescription.pl](https://github.com/eltonjrv/microbiome.westernu/blob/bin/sampleID-to-sampleDescription.pl).\nIII) A \"zotus_table_uparse-customized.tsv\" main output file is created with the above command. \n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8204664456136888,
      "result": {
        "original_header": "3.2. Subtracting NTC-derived ZOTUs counts OR removing the whole NTC-derived ZOTUs content from all target samples.",
        "type": "Text_excerpt",
        "value": "3.2.3. Then, with the following PERL code, create a new zotu table without any NTC-derived zotu:\n```\n$  perl -e 'open(FILE, \"zotus_table_uparse-customized.tsv2\"); open(FILE2, \"ZotusOnNTC.txt\"); while(<FILE2>){chomp($_); $hash{$_} = 1;} while(<FILE>){chomp($_); @array = split(/\\t/, $_); if($hash{$array[1]} eq \"\"){print(\"$_\\n\");}}' >zotus_table_uparse-customized-woNTCzotus.tsv\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/eltonjrv/microbiome.westernu/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "microbiome.westernu"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "eltonjrv"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 41870,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "M",
        "size": 3702,
        "type": "Programming_language",
        "value": "M"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "This task is strictly dependent on both PCR and Sequencing protocols one has adopted in the Lab.",
        "parent_header": [
          "1. Debarcoding and Demultiplexing Raw Fastq Files (generated by the sequencing machine)"
        ],
        "type": "Text_excerpt",
        "value": "In the example below, we are referring to a two-round PCR method on which **two pairs of adapters** were used for a **paired-end sequencing**:\na) iNEXT barcodes (For_A-H - Rev_1-12) on the 1st round of amplifications, and \nb) Illumina overhang adapter sequences (For_i5 and Rev_i7) on the 2nd round.\nFor more details on this approach, please refer to [Faircloth & Glenn, 2012 - PLoS One](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0042543).\n>NOTE: If you have not employed this multiplexing approach or have all your fastq files already demultiplexed, please take a look at [qiime2 demultiplexing forum](https://forum.qiime2.org/t/demultiplexing-and-trimming-adapters-from-reads-with-q2-cutadapt/2313) or move straight to topic 2 below. \n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "header_analysis"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "############ Software to be installed prior executing this SOP ##################",
        "parent_header": [
          "**Standard Operating Procedure (SOP)** for microbiome analyses applied to vector-borne disease diagnostics using 16S rRNA Next-Generation Sequencing.",
          "Research Team: Elton Vasconcelos*, Chayan Roy, Joseph Geiger, Brian Oakley, Pedro Diniz.",
          "*Current affiliation: Leeds Omics, University of Leeds, UK."
        ],
        "type": "Text_excerpt",
        "value": "### **Standard Operating Procedure (SOP)** for microbiome analyses applied to vector-borne disease diagnostics using 16S rRNA Next-Generation Sequencing.\n#### Research Team: Elton Vasconcelos*, Chayan Roy, Joseph Geiger, Brian Oakley, Pedro Diniz.\n##### Affiliation: College of Veterinary Medicine at Western University of Health Sciences, Pomona, CA, USA. \n##### \\*Current affiliation: Leeds Omics, University of Leeds, UK.\n>Author: Elton Vasconcelos (Oct/2018)\n\n>Reviewed and retested by Chayan Roy and Elton Vasconcelos (Sep/2020).\n>Reviewed by Elton Vasconcelos (Nov/2021)\n\nIf you use this pipeline (or part of it) on your research, please cite [Vasconcelos et al., 2021 - BMC Vet.Res.](https://bmcvetres.biomedcentral.com/articles/10.1186/s12917-021-02969-9)\n\n################## Software to be installed prior executing this SOP ################## \n  - [QIIME2](https://docs.qiime2.org/2021.8/install)\\*\n  - [UPARSE](https://www.drive5.com/usearch/download.html)\n  - [Trimmomatic](http://www.usadellab.org/cms/?page=trimmomatic)\n  - [Muscle](https://drive5.com/muscle5/)\n  - Perl and Bash (already installed on any Unix-based system: Mac or Linux)\n  - [Bioperl](https://bioperl.org/)\n  - [R](https://www.r-project.org/)\n \n \\* Only if you are running demultiplexing step (topic 1 below).\n\n#############################################################################\n\n# 1. Debarcoding and Demultiplexing Raw Fastq Files (generated by the sequencing machine)\n##### This task is strictly dependent on both PCR and Sequencing protocols one has adopted in the Lab. \nIn the example below, we are referring to a two-round PCR method on which **two pairs of adapters** were used for a **paired-end sequencing**:\na) iNEXT barcodes (For_A-H - Rev_1-12) on the 1st round of amplifications, and \nb) Illumina overhang adapter sequences (For_i5 and Rev_i7) on the 2nd round.\nFor more details on this approach, please refer to [Faircloth & Glenn, 2012 - PLoS One](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0042543).\n>NOTE: If you have not employed this multiplexing approach or have all your fastq files already demultiplexed, please take a look at [qiime2 demultiplexing forum](https://forum.qiime2.org/t/demultiplexing-and-trimming-adapters-from-reads-with-q2-cutadapt/2313) or move straight to topic 2 below. \n\n### 1.1. Preparing directories and importing raw fastq files to QIIME2\n1.1.1.  Creating a project parent directory for the whole analysis:\n```\n$ mkdir projectX\n```\n1.1.2. Entering this directory:\n```\n$ cd projectX/\n```\n1.1.3. Creating a directory where all your raw fastq files will be placed in, and entering it:\n```\n$ mkdir raw_data\n$ cd raw_data/\n```\n1.1.4. Transferring your fastq files to the current work directory (raw_data/) with your most suitable/convenient command or tool (e.g. cp, scp, ftp, WinScp, FileZilla, etc ...).\n>NOTE: Since we don't know where you are pulling your data from, this step is intentionally left up to you.\n\n1.1.5. Renaming your fastq files. \n\n1.1.5.1. If you have only one pair of fastq files, rename R1 to forward.fastq.gz and R2 to reverse.fastq.gz, then move straight to sub-item 1.1.6:\n```\n$ mv your_R1_file.fastq.gz forward.fastq.gz\n$ mv your_R2_file.fastq.gz reverse.fastq.gz\n```\n1.1.5.2. If you have several (n) pairs of fastq files, it will be required to create several (n) experimental subdirectories within raw_data/;\n\n1.1.5.2.a. For example, the command below automatically creates 10 subdirectories named \"ExpN\", where N is a number from 1 to 10. Edit the command according to the total number of fastq files pairs you have on hands.\n```\n$ for i in `seq 1 10`; do mkdir Exp$i; done\n```\n1.1.5.2.b. Then move each pair of fastq files that were already placed within raw_data/ (sub-item 1.1.4) to each respective recently created subdirectory (Exp1 to Exp10), and rename the files. Below is an example of the commands' set for the Exp1 instance:\n```\n$ mv your_first_fastq_pair_R[12]*.fastq.gz Exp1/\n$ cd Exp1/\n$ mv your_first_fastq_pair_R1.fastq.gz forward.fastq.gz\n$ mv your_first_fastq_pair_R2.fastq.gz reverse.fastq.gz\n$ cd ../\n```\n>NOTE: Since we don't know how your fastq files are originally named, we recommend that you move them separately with the individual commands set above. In case you are famliar with the Unix Shell, feel free to do it through a \"for\" loop.\n\n1.1.6. Moving back to the parent directory (projectX/), creating a directory for the demultiplexing job and entering it:\n```\n$ cd ../\n$ mkdir demux\n$ cd demux/\n```\n1.1.7. Activating qiime2 environment (once you have properly installed it through this [link](https://docs.qiime2.org/2021.8/install)):\n```\n$ source activate qiime2-2021.8\n```\n1.1.8. Importing fastq files as qiime2 .qza format.\n\n1.1.8.1. For a single pair of fastq files (coming from 1.1.5.1 sub-item above), do the following:\n```\n$  qiime tools import --type MultiplexedPairedEndBarcodeInSequence --input-path ../raw_data/ --output-path multiplexed-seqs.qza\n```\n1.1.8.2. For several (n) pairs of fastq files (coming from 1.1.5.2 sub-item above), do this instead:\n```\n$ for i in `ls -d ../raw_data/Exp*/`; do qiime tools import --type MultiplexedPairedEndBarcodeInSequence --input-path $i --output-path `echo $i | sed 's/.*\\///g'`-multiplexed-seqs.qza; done\n```\n\n### 1.2. Debarcoding and demultiplexing\n1.2.1. Preparing a sample-barcode map file (samples-map-BCseq.tab) by running a single-line *ad hoc* PERL script\n```\n$ perl -e 'open(FILE, \"samples-map.tab\"); open(FILE2, \"iNext-barcodes.tab\"); while(<FILE2>) {chomp($_); @array2 = split(/\\t/, $_); $hash{$array2[0]} = $array2[1];} while(<FILE>) {chomp($_); @array = split(/\\t/, $_); if($hash{$array[1]} ne \"\" && $hash{$array[2]} ne \"\") {print(\"$array[0]\\t$hash{$array[1]}\\t$hash{$array[2]}\\n\");} else {print(\"$_\\t**error** ->barcode sequence not found\\n\");}}' >samples-map-BCseq.tab\n```\n>NOTES: One must prepare and place both input files (iNext-barcodes.tab and samples-map.tab) into the current directory (projectX/demux/). Please see/download [iNext-barcodes.tab](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/iNext-barcodes.tab) and [samples-map.tab](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-map.tab) as guiding examples, in case you have employed a demultiplexing situation like ours. The generated output from the command above is [samples-map-BCseq.tab](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-map-BCseq.tab), which will be used in downstream steps. In case you already have a samples-map-BCseq.tab-like structure in your favorite spreadsheet editor (e.g. Excel), feel free to save it as a Plain Text file, naming it as samples-map-BCseq.tab, transferring it to the current dir (projectX/demux/) and skipping the 1.2.1 command above.\n\n1.2.2. Splitting samples-map-BCseq.tab into N different iNext-Rev barcode combinations (one map file for each iNext-Rev barcode):\n```\n$ for i in `grep -v '^\\#' samples-map-BCseq.tab | cut -f 3 | sort -u`; do grep -P \"Sample|$i\" samples-map-BCseq.tab >iNextRev_`echo $i`-samples-map-BCseq.tab; done\n```\n>NOTE: At the time this SOP was written, \"qiime cutadapt\" function considered only one iNext-Rev barcode per sample-barcode map file.\n\n1.2.3. Actual debarcoding and demultiplexing process with qiime cutadapt.\n\n1.2.3.1. Debarcoding and demultiplexing a single pair of fastq files (multiplexed-seqs.qza file created on 1.1.8.1. step above):\n```\n$ for i in `ls iNextRev*tab`; do qiime cutadapt demux-paired --i-seqs multiplexed-seqs.qza --m-forward-barcodes-file $i --m-forward-barcodes-column iNext-For --m-reverse-barcodes-file $i --m-reverse-barcodes-column iNext-Rev --o-per-sample-sequences `echo $i | sed 's/\\-samples.*$//g'`-demultiplexed-seqs.qza --o-untrimmed-sequences `echo $i | sed 's/\\-samples.*$//g'`-untrimmed.qza --verbose; done\n```\n1.2.3.2. Debarcoding and demultiplexing several (n) pairs of fastq files (Exp\\*-multiplexed-seqs.qza files created on 1.1.8.2. step above):\n```\n$ for j in `ls Exp*seqs.qza`; do for i in `ls iNextRev*tab`; do qiime cutadapt demux-paired --i-seqs $j --m-forward-barcodes-file $i --m-forward-barcodes-column iNext-For --m-reverse-barcodes-file $i --m-reverse-barcodes-column iNext-Rev --o-per-sample-sequences `echo $j | sed 's/\\-multiplexed.*$//g'`_`echo $i | sed 's/\\-samples.*$//g'`-demultiplexed-seqs.qza --o-untrimmed-sequences `echo $j | sed 's/\\-multiplexed.*$//g'`_`echo $i | sed 's/\\-samples.*$//g'`-untrimmed.qza --verbose; done; done\n```\n1.2.4. Removing untrimmed.qza files:\n```\n$ rm *untrimmed.qza\n```\n1.2.5. Summarizing demultiplexed qza files as qzv ones in order to be visualized at https://view.qiime2.org/ (Important for sequenced reads quality control assessment/visualization on each sample):\n```\n$ for i in `ls *demultiplexed-seqs.qza`; do qiime demux summarize --i-data $i --o-visualization `echo $i | sed 's/qza$/qzv/g'`; done\n```\n1.2.6. Deactivating qiime2 environment:\n```\n$ source deactivate\n```\nor\n```\n$ conda deactivate\n```\n1.2.7. Creating a directory to store and uncompress demultiplexed qza files, in order to have fastq files for topic 2 below:\n```\n$ mkdir demux-unzipped\n$ cd demux-unzipped/\n$ ln -s ../*demultiplexed-seqs.qza .\n$ ls | xargs -i unzip {}\n```\n1.2.8. Going back to the parent projectX/ dir:\n```\n$ cd ../../\n```\n\n# 2. Microbiome Sequencing Analyses\n2a) Since UPARSE will be the main microbiome analyzer used here, let's first create a \"uparse-run\" directory and then go within it:\n```\n$ mkdir uparse-run\n$ cd uparse-run/\n```\n2b) Creating an \"inputs\" directory and placing symbolic links of demultiplexed fastq files that will be used by UPARSE:\n```\n$ mkdir inputs\n$ cd inputs/\n$ ln -s ../../demux/demux-unzipped/*/data/*gz .\n$ cd ../\n```\n>NOTE: If you are coming straight to this topic because you already had demultiplexed your samples on your own, please disconsider the \"ln -s\" command above and just place your demultiplexed and compressed fastq files (\\*.fastq.gz) into the \"inputs\" dir. Please also make sure that your fastq.gz file names start with \"sampleID\" followed by \"\\_1\\_L001\\_R[12]\\_001.fastq.gz\". For example, take a look at our [sample metadata table](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-metadata.tsv) and see that, for the first sample, the paired file names will be the following: 865A1\\_1\\_L001\\_R1\\_001.fastq.gz and 865A1\\_1\\_L001\\_R2\\_001.fastq.gz.\n### 2.1. Trimming primers with Trimmomatic (this must be done prior running UPARSE)\n##### Please refer to http://www.usadellab.org/cms/?page=trimmomatic for Trimmomatic download and instructions\n2.1.1. Running trimmomatic\n```\n$ for i in `ls inputs/*R1*fastq.gz`; do R1=`echo $i | sed 's/inputs\\///g' | sed 's/\\.fastq\\.gz$//g'`; R2=`echo $i | sed 's/inputs\\///g' | sed 's/\\.fastq\\.gz$//g' | sed 's/_R1_/_R2_/g'`; java -jar /path/to/your/Trimmomatic-x.xx/trimmomatic-x.xx.jar PE -phred33 $i `echo $i | sed 's/_R1_/_R2_/g'` $R1.fq $R1.unpaired.fq $R2.fq $R2.unpaired.fq HEADCROP:20; done\n```\n> NOTES:\nI) Please pay attention that you need to type your trimmmomatic installation full PATH after the \"java -jar\" command above.\nII) Edit the \"HEADCROP:20\" parameter according to your primers' average length. In this example, primers' length is ~ 20 bp.\n\n2.1.2. Removing unpaired reads after trimming:\n```\n$ rm *unpaired*\n```\n2.1.3. Creating a new directory where trimmed fastq files must be placed into:\n```\n$ mkdir inputs-woPrimers\n```\n2.1.4. Moving trimmed fastq files to that new directory:\n```\n$ mv *fq inputs-woPrimers/\n```\n### 2.2. Running UPARSE on sequenced and trimmed amplicons\n##### Please refer to https://www.drive5.com/usearch/download.html in order to download USEARCH tools\n2.2.1. Running the actual microbiome analyzer tool:\n```\n$ bash run-uparse-amp250-450-ZOTUs.bash inputs-woPrimers/ 2>run-uparse.log\n```\n>NOTES:\nI) UPARSE pipeline does a series of tasks such as: mate joining and quality filtering of your sequenced amplicons, ZOTUs/ESVs assembly, taxonomic classification, and both alpha- and beta-diversity analyses on all your samples.\nII) \"run-uparse-amp250-450-ZOTUs.bash\" BASH script may be obtained [here](https://github.com/eltonjrv/microbiome.westernu/blob/master/run-uparse-amp250-450-ZOTUs.bash).\nIIa) One may tune parameters on each command in the script according to his/her needs.\nIIb) This script uses a customized RDP refDB, which we have added Ehrlichia_canis, Ehrlichia_chafeensis, Anaplasma_platys, Anaplasma_phagocytophilum, Mycoplasma_haemocanis and Mycoplasma_haematoparvum 16S rRNA sequences. In order to download it, please go [here](https://github.com/eltonjrv/microbiome.westernu/tree/refDB) and click on the \"Clone or download\" green button, then \"Download ZIP\". After unzipping the downloaded folder, uncompress the \"rdp_16s_extra_seqs.fa.gz\" file with \"gunzip\" command, and then place it into the directory where you will run this script (\"uparse-run/\" in this example). Line 52 from the \"\"run-uparse-amp250-450-ZOTUs.bash\"\" script will format that file in order to be used as a refDB (\\*.udb) for taxonomic classification purposes. If you want to use your own customized refDB fasta file, please edit script's lines 52 and 53.\nIIc) In order to play with different OTU clustering % identity thresholds (95, 97, and 99%), one must run the alternative \"run-uparse-amp250-450-OTUs95_97_99_100.bash\" script that is provided [here](https://github.com/eltonjrv/microbiome.westernu/blob/master/run-uparse-amp250-450-OTUs95_97_99_100.bash).\nIII) An \"outputs\" directory will be created and all uparse-generated files will be placed within it.\n\n\n# 3. ZOTU/ESV Table Customization for Diagnostics Purposes\n##### This topic is aimed at creating a customized ZOTU table which will make data visual inspection easier for clinicians\n3a) Move to the uparse-generated \"outputs\" directory:\n```\n$ cd outputs/\n```\n#### ATTENTION: A metadata file will also be needed (which I'll call \"samples-metadata.tsv\" in this tutorial). Please have your metadata file prepared as this [example](https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-metadata.tsv) and place it within the current directory (outputs).\n>NOTES about the metadata table:\nI) Columns 1 and 4 are mandatory.\nII) Column 4 must contain any textual string that best describes your samples.\nIII) There must not be any colon \":\" nor blank spaces within your sample descriptions.\n\n3b) There will already be two important uparse-generated files within the \"outputs\" dir (\"zotus_table_uparse.tsv\" and \"zotus.sintax\") . Once you have those two files plus your \"samples-metadata.tsv\" ready, run the following command:\n```\n$ bash customize-OTUtable.bash zotus_table_uparse.tsv zotus.sintax samples-metadata.tsv\n```\n>NOTES:\nI) \"customize-OTUtable.bash\" script may be obtained [here](https://github.com/eltonjrv/microbiome.westernu/blob/bin/customize-OTUtable.bash).\nII) There are two other embedded scripts that must also be placed within the current directory where you'll run customize-OTUtable.bash: [customize-OTUtable.R](https://github.com/eltonjrv/microbiome.westernu/blob/bin/customize-OTUtable.R) and [sampleID-to-sampleDescription.pl](https://github.com/eltonjrv/microbiome.westernu/blob/bin/sampleID-to-sampleDescription.pl).\nIII) A \"zotus_table_uparse-customized.tsv\" main output file is created with the above command.\n\n### 3.1. Improving customized ZOTU table\n3.1.1. In case one wants to keep only the last taxonomic level assigned to each zotu, instead of seeing the whole taxonomic classification (from phylum to species), run the following:\n```\n$ sed 's/\\td\\:.*s\\:/\\ts\\:/g' zotus_table_uparse-customized.tsv |  sed 's/\\td\\:.*g\\:/\\tg\\:/g' | sed 's/\\td\\:.*f\\:/\\tf\\:/g' | sed 's/\\td\\:.*o\\:/\\to\\:/g' | sed 's/\\td\\:.*c\\:/\\tc\\:/g' | sed 's/\\td\\:.*p\\:/\\tp\\:/g' >zotus_table_uparse-customized.tsv2\n```\n### 3.2. Subtracting NTC-derived ZOTUs counts OR removing the whole NTC-derived ZOTUs content from all target samples.\n3.2.1. Subtracting NTC-derived ZOTUs counts from target samples\n```\n$ Rscript NTC-ZOTUs-subtraction.R zotus_table_uparse-customized.tsv\n```\n>NOTE: \"NTC-ZOTUs-subtraction.R\" script is provided [here](https://github.com/eltonjrv/microbiome.westernu/blob/bin/NTC-ZOTUs-subtraction.R).\n\n3.2.2. Alternatively, one may want to remove the whole NTC-derived ZOTUs content. So, first catch all zotus present in negative control samples:\n```\n$ grep '_neg_' zotus_table_uparse-customized.tsv2 | cut -f 2 | sort -u >ZotusOnNTC.txt\n```\n>NOTE: On the command above, replace '\\_neg\\_' by any other tag that characterizes the negative control in your sample descriptions (e.g. NTC, water, blank, etc ...)\n\n3.2.3. Then, with the following PERL code, create a new zotu table without any NTC-derived zotu:\n```\n$  perl -e 'open(FILE, \"zotus_table_uparse-customized.tsv2\"); open(FILE2, \"ZotusOnNTC.txt\"); while(<FILE2>){chomp($_); $hash{$_} = 1;} while(<FILE>){chomp($_); @array = split(/\\t/, $_); if($hash{$array[1]} eq \"\"){print(\"$_\\n\");}}' >zotus_table_uparse-customized-woNTCzotus.tsv\n```\n### 3.3.  Keeping taxa of interest only (ToIs)\n3.3.1. Catching your taxa of interest within the zotu table:\n```\n$ grep -P 'Ehrlichia|Anaplasma|Bartonella|Mycoplasma|Rickettsia' zotus_table_uparse-customized-woNTCzotus.tsv >zotus_table_uparse-wTaxa-customized-woNTCzotus-ToIonly.tsv\n```\n>NOTE: Replace the name of above genera by the ones of your interest (keep both ' and | signs).\n\n\n# 4. Phylogenetic Diversity Investigation on ToIs\n### 4.1. Preparing files for phylogenetic analyses of your ToI(s)\n4.1.1. Within the uparse-generated \"outputs/\" directory, create a new directory to work on tree-prep files, and go within it:\n```\n$ mkdir ToI_trees\n$ cd ToI_trees\n```\n4.1.2. Creating a symbolic link of both uparse-generated zotus.fa and zotus_table_uparse-customized.tsv files:\n```\n$ ln -s ../zotus.fa .\n$ ln -s ../zotus_table_uparse-customized.tsv .\n```\n4.1.3. Catching ZOTU IDs of genus-level ToI from the customized ZOTU table (Ehrlichia and Bartonella are genus examples used herein as ToI):\n```\n$ grep 'g:Ehrlichia' zotus_table_uparse-customized.tsv | cut -f 2 | sort -u >Ehr-zotus.nam\n$ grep 'g:Bartonella' zotus_table_uparse-customized.tsv | cut -f 2 | sort -u >Bart-zotus.nam\n```\n4.1.4. Catching ToI-ZOTU sequences and placing them in separate fasta files:\t\n```\n$ perl seqs1.pl -outfmt fasta -incl Ehr-zotus.nam -seq zotus.fa >Ehr-zotus.fa\n$ perl seqs1.pl -outfmt fasta -incl Bart-zotus.nam -seq zotus.fa >Bart-zotus.fa\n```\n>NOTES:\nI) Please download both seqs1.pl and seqtools.pl PERL scripts from the [bin branch](https://github.com/eltonjrv/microbiome.westernu/tree/bin) and place them in the current work directory (ToI_trees). \nII) One must edit line 9 from \"seqstools.pl\" in order to properly point to your BioPerl full PATH, as well as line 31 from \"seqs1.pl\", replacing the seqtools.pl correct location.\nIII) One must also have BioPerl properly installed on the system in order to run this tool. Please refer to bioperl.org for instructions on how to install BioPerl.\n\n### 4.2. Relying on SILVA type strains database for phylogenetic comparisons\n#### Download \"SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta.gz\" file from the [refDB branch](https://github.com/eltonjrv/microbiome.westernu/tree/refDB). \nThis file contains 16S rRNA sequences for 23,127 type strain bacteria. Therefore, it is a good initial source for a comparison against your ToI-ZOTUs on a rough phylogenetic view. Of course, one must use the whole SILVA (~ 700k 16S RNA sequences) as well as BlastN against NCBI-NT db for assurance about \"novel species/strains\" discovery.\n>NOTE: Go to [refDB branch](https://github.com/eltonjrv/microbiome.westernu/tree/refDB) and click on the \"Clone or download\" green button, then \"Download ZIP\". \n\n4.2.1. After unzipping the downloaded folder, uncompress such file like the following:\n```\n$ gunzip SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta.gz\n```\n4.2.2. Catching ToI type strain sequences from SILVA:\n```\n$ perl geneSearcher.pl Ehrlichia SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta \n$ perl geneSearcher.pl Bartonella SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta \n```\n>NOTE: \"geneSearcher.pl\" PERL script may be obtained [here](https://github.com/eltonjrv/bioinfo.scripts/blob/master/geneSearcher.pl).\n\n4.2.3. Merging both ToI type strain sequences and ToI-ZOTUs in a single file:\n```\n$ cat Ehrlichia_from_SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta Ehr-zotus.fa >Ehr-4tree.fasta\n$ cat Bartonella_from_SILVA_132_SSURef_NR99_13_12_17_opt-typeStrains.fasta Bart-zotus.fa >Bart-4tree.fasta\n```\n### 4.3. Running a global Multiple Sequence Alignment (MSA) with muscle:\n4.3.1. Running muscle in order to get an MSA in fasta format\n```\n$ muscle -in Ehr-4tree.fasta  -out Ehr-4tree_aln.fa\n$ muscle -in Bart-4tree.fasta -out Bart-4tree_aln.fa\n```\n>NOTE: Refer to https://www.drive5.com/muscle/downloads.htm for instructions on how to download and install muscle.\n\n4.3.2. Keeping the ToI-ZOTUs stretch only in the MSA, that is, the 16S rRNA target-amplicon region:\n```\n$ perl cutMSA.pl Ehr-4tree_aln.fa Zotu9 TGTGCCAG 360 for >Ehr-4tree_alnCut.fa \n$ perl cutMSA.pl Bart-4tree_aln.fa Zotu7 TATTGGA 360 for >Bart-4tree_alnCut.fa \n```\n>NOTE: The *ad hoc* \"cutMSA.pl\" PERL script may be obtained [here](https://github.com/eltonjrv/bioinfo.scripts/blob/master/cutMSA.pl). Please read \"cutMSA.pl\" initial commented lines for instructions on how to run it. Those final \"\\*\\_alnCut.fa\" outputs must be used as inputs on [MEGA](https://www.megasoftware.net/), so the user can perform his/her most convenient phylogenetic inference methods.\n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "4.3. Running a global Multiple Sequence Alignment (MSA) with muscle:",
        "parent_header": [
          "4. Phylogenetic Diversity Investigation on ToIs"
        ],
        "type": "Text_excerpt",
        "value": "4.3.1. Running muscle in order to get an MSA in fasta format\n```\n$ muscle -in Ehr-4tree.fasta  -out Ehr-4tree_aln.fa\n$ muscle -in Bart-4tree.fasta -out Bart-4tree_aln.fa\n```\n>NOTE: Refer to https://www.drive5.com/muscle/downloads.htm for instructions on how to download and install muscle.\n\n4.3.2. Keeping the ToI-ZOTUs stretch only in the MSA, that is, the 16S rRNA target-amplicon region:\n```\n$ perl cutMSA.pl Ehr-4tree_aln.fa Zotu9 TGTGCCAG 360 for >Ehr-4tree_alnCut.fa \n$ perl cutMSA.pl Bart-4tree_aln.fa Zotu7 TATTGGA 360 for >Bart-4tree_alnCut.fa \n```\n>NOTE: The *ad hoc* \"cutMSA.pl\" PERL script may be obtained [here](https://github.com/eltonjrv/bioinfo.scripts/blob/master/cutMSA.pl). Please read \"cutMSA.pl\" initial commented lines for instructions on how to run it. Those final \"\\*\\_alnCut.fa\" outputs must be used as inputs on [MEGA](https://www.megasoftware.net/), so the user can perform his/her most convenient phylogenetic inference methods.\n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "contact",
    "contributors",
    "documentation",
    "license",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 04:20:18",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 0
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "non-software"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "ATTENTION: A metadata file will also be needed (which I'll call \"samples-metadata.tsv\" in this tutorial). Please have your metadata file prepared as this <a href=\"https://github.com/eltonjrv/microbiome.westernu/blob/accFiles/samples-metadata.tsv\">example</a> and place it within the current directory (outputs).",
        "parent_header": [
          "3. ZOTU/ESV Table Customization for Diagnostics Purposes"
        ],
        "type": "Text_excerpt",
        "value": ">NOTES about the metadata table:\nI) Columns 1 and 4 are mandatory.\nII) Column 4 must contain any textual string that best describes your samples.\nIII) There must not be any colon \":\" nor blank spaces within your sample descriptions.\n\n3b) There will already be two important uparse-generated files within the \"outputs\" dir (\"zotus_table_uparse.tsv\" and \"zotus.sintax\") . Once you have those two files plus your \"samples-metadata.tsv\" ready, run the following command:\n```\n$ bash customize-OTUtable.bash zotus_table_uparse.tsv zotus.sintax samples-metadata.tsv\n```\n>NOTES:\nI) \"customize-OTUtable.bash\" script may be obtained [here](https://github.com/eltonjrv/microbiome.westernu/blob/bin/customize-OTUtable.bash).\nII) There are two other embedded scripts that must also be placed within the current directory where you'll run customize-OTUtable.bash: [customize-OTUtable.R](https://github.com/eltonjrv/microbiome.westernu/blob/bin/customize-OTUtable.R) and [sampleID-to-sampleDescription.pl](https://github.com/eltonjrv/microbiome.westernu/blob/bin/sampleID-to-sampleDescription.pl).\nIII) A \"zotus_table_uparse-customized.tsv\" main output file is created with the above command.\n"
      },
      "source": "https://raw.githubusercontent.com/eltonjrv/microbiome.westernu/SOP/README.md",
      "technique": "header_analysis"
    }
  ]
}