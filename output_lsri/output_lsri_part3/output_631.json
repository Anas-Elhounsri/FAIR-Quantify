{
  "application_domain": [
    {
      "confidence": 12.01,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/liulizhi1996/HPODNets"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2021-01-29T07:21:11Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-03-30T13:25:47Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "HPODNets: deep graph convolutional networks for predicting human protein-phenotype associations"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9149888630985687,
      "result": {
        "original_header": "HPODNets",
        "type": "Text_excerpt",
        "value": "**HPODNets: deep graph convolutional networks for predicting human protein-phenotype associations** \n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8980512718352288,
      "result": {
        "original_header": "Temporal validation",
        "type": "Text_excerpt",
        "value": "- `split_dataset_temporal.py`: We still focus on HPO term in PA sub-ontology. We use the proteins added before a certain previous time as the training set, and the proteins added after that time as the test set. The generated dataset pickle file is organized as \n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8497113554287574,
      "result": {
        "original_header": "HumanNet",
        "type": "Text_excerpt",
        "value": "- `humannet.py`: First, open [https://www.inetbio.org/humannet/download.php](https://www.inetbio.org/humannet/download.php), and then click `HumanNet-XN` in the `Integrated Networks` section. Next, open [https://www.uniprot.org/uniprot/?query=*&fil=reviewed%3Ayes+AND+organism%3A%22Homo+sapiens+%28Human%29+%5B9606%5D%22](https://www.uniprot.org/uniprot/?query=*&fil=reviewed%3Ayes+AND+organism%3A%22Homo+sapiens+%28Human%29+%5B9606%5D%22). Click the `Columns` button in the middle of the page, and click the crosses in the upper right corner of all the dotted boxes in the `Columns to be displayed` of the new page. Then enter `GeneID` in the `Search:` search box in the middle of the `Add more columns` column, and click the associated word that pops up. At this point, click the `Save` button on the far right. Jump back to the original page, now only `Entry` and `Cross-reference (GeneID)` are left in the form. Click the `Download` button in the middle of the page, select `Format: Tab-separated`, and then click the `Go` button to download the file. Rename the file to `entrez2uniprot.txt` and place it under `data/feature/HumanNet/raw`. After the download is complete, in the UniProt page just now, click the `Columns` button again, in the new page, click `Reset to default` on the right side (*note: do not click the word default, but click Reset*), and then click ` Save`. Now, the UniProt interface is restored to its original appearance. Finally, run `humannet.py` and then get the processed PPI network in json format.\n \n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Download the prediction results",
        "parent_header": [
          "HPODNets"
        ],
        "type": "Text_excerpt",
        "value": "We upload the prediction results of 15,033 proteins stored in STRING, GeneMANIA-Net and HumanNet databases that have not been annotated using the HPO annotations of 4,424 proteins released in October 2020 as the training set. The data is available at:\n\n[https://doi.org/10.6084/m9.figshare.14222732](https://doi.org/10.6084/m9.figshare.14222732)\n\nThe file is organized in .json format, where the key is the HPO term, and the values are proteins and their corresponding predictive scores. The file is so large (1.98 GB), and you are free to download it."
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/liulizhi1996/HPODNets/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/liulizhi1996/HPODNets/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "liulizhi1996/HPODNets"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "HPODNets"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9976792327325051,
      "result": {
        "original_header": "Preprocessing",
        "type": "Text_excerpt",
        "value": "- `extract_gene_id.py`: First, please download gene annotations file from [http://compbio.charite.de/jenkins/job/hpo.annotations.monthly/](http://compbio.charite.de/jenkins/job/hpo.annotations.monthly/) with all sources and all frequencies: `ALL_SOURCES_ALL_FREQUENCIES_phenotype_to_genes.txt`. Then run the script, you will get a .txt file containing all gene ids. Finally, please upload this file to [http://www.uniprot.org/mapping/](http://www.uniprot.org/mapping/) to map Entrez Gene ID to UniProt ID. \n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9988066676784918,
      "result": {
        "original_header": "STRING",
        "type": "Text_excerpt",
        "value": "- `string.py`: Please firstly open [https://string-db.org/cgi/download.pl](https://string-db.org/cgi/download.pl) and choose \"organism\" as \"Homo sapiens\", then download \"9606.protein.links.v11.0.txt.gz\" (version number may change). Meanwhile, download mapping file under \"ACCESSORY DATA\" category, or open website\n[https://string-db.org/mapping\\_files/uniprot\\_mappings/](https://string-db.org/mapping_files/uniprot_mappings/) to download it. After downloading, you can run this code to get a json file containing PPI data organized as \n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9974076953391585,
      "result": {
        "original_header": "GeneMANIA-Net",
        "type": "Text_excerpt",
        "value": "- `genemania.py`: First, please download `COMBINED.DEFAULT_NETWORKS.BP_COMBINING.txt` from [http://genemania.org/data/current/Homo\\_sapiens.COMBINED/](http://genemania.org/data/current/Homo_sapiens.COMBINED/), and then download `identifier_mappings.txt` from [http://genemania.org/data/current/Homo_sapiens/](http://genemania.org/data/current/Homo_sapiens/). Run the `genemania.py` and you will obtain the PPI network as a json file.\n \n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8882884653312588,
      "result": {
        "original_header": "Preprocessing",
        "type": "Text_excerpt",
        "value": "- `extract_gene_id.py`: First, please download gene annotations file from [http://compbio.charite.de/jenkins/job/hpo.annotations.monthly/](http://compbio.charite.de/jenkins/job/hpo.annotations.monthly/) with all sources and all frequencies: `ALL_SOURCES_ALL_FREQUENCIES_phenotype_to_genes.txt`. Then run the script, you will get a .txt file containing all gene ids. Finally, please upload this file to [http://www.uniprot.org/mapping/](http://www.uniprot.org/mapping/) to map Entrez Gene ID to UniProt ID. \n- `create_annotation.py`: After generating Gene ID mapping file, you can run this script to generate HPO term-protein association file. The output json file contains associated proteins of each HPO term, like \n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.884846294802751,
      "result": {
        "original_header": "Cross-validation",
        "type": "Text_excerpt",
        "value": "\t```\n\tstore = {\n\t\t\"annotation\": full dataset\n    \t\"mask\": [\n\t\t\t{\n\t\t\t\t\"train\": training mask of 1st fold\n\t\t\t\t\"test\": test mask of 1st fold\n\t\t\t},\n\t\t\t...\n\t\t]\n\t}\n\t```\n \n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9335240286594302,
      "result": {
        "original_header": "Temporal validation",
        "type": "Text_excerpt",
        "value": "\t```\n    store = {\n        \"annotation\": full dataset\n        \"mask\": {\n            \"train\": training mask\n            \"test\": test mask\n        }\n    }\n\t```\n \n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8945607646835818,
      "result": {
        "original_header": "STRING",
        "type": "Text_excerpt",
        "value": "- `string.py`: Please firstly open [https://string-db.org/cgi/download.pl](https://string-db.org/cgi/download.pl) and choose \"organism\" as \"Homo sapiens\", then download \"9606.protein.links.v11.0.txt.gz\" (version number may change). Meanwhile, download mapping file under \"ACCESSORY DATA\" category, or open website\n[https://string-db.org/mapping\\_files/uniprot\\_mappings/](https://string-db.org/mapping_files/uniprot_mappings/) to download it. After downloading, you can run this code to get a json file containing PPI data organized as \n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9174165071246739,
      "result": {
        "original_header": "GeneMANIA-Net",
        "type": "Text_excerpt",
        "value": "- `genemania.py`: First, please download `COMBINED.DEFAULT_NETWORKS.BP_COMBINING.txt` from [http://genemania.org/data/current/Homo\\_sapiens.COMBINED/](http://genemania.org/data/current/Homo_sapiens.COMBINED/), and then download `identifier_mappings.txt` from [http://genemania.org/data/current/Homo_sapiens/](http://genemania.org/data/current/Homo_sapiens/). Run the `genemania.py` and you will obtain the PPI network as a json file.\n \n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/liulizhi1996/HPODNets/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "graph-convolutional-network, human-phenotype-ontology"
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "HPODNets"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "liulizhi1996"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 89092,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Dependencies",
        "parent_header": [
          "HPODNets"
        ],
        "type": "Text_excerpt",
        "value": "Our model is implemented by Python 3.6 with Pytorch 1.4.0 and Pytorch-geometric 1.5.0, and run on Nvidia GPU with CUDA 10.0.\n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Run the model",
        "parent_header": [
          "HPODNets"
        ],
        "type": "Text_excerpt",
        "value": "- `main.py`: Run the `main.py` script, and you will obtain the prediction results. Be careful to the cuda device ID!\n"
      },
      "source": "https://raw.githubusercontent.com/liulizhi1996/HPODNets/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 01:46:05",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 6
      },
      "technique": "GitHub_API"
    }
  ]
}