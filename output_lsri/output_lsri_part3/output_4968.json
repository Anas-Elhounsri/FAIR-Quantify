{
  "application_domain": [
    {
      "confidence": 21.41,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/bihealth/WiPP"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2018-08-23T13:41:35Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2023-10-17T07:22:10Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "large scale GC-MS data preprocessing workflow"
      },
      "technique": "GitHub_API"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/bihealth/WiPP/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 3
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/bihealth/WiPP/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "bihealth/WiPP"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "WiPP - A Workflow for improved Peak Picking - Quick Start Guide"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/bihealth/WiPP/master/run_WiPP.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
        "type": "Text_excerpt",
        "value": "**WiPP** is an open source large scale GC-MS data preprocessing workflow built in Python 3 that uses machine learning to optimise, automate and combine the peak detection process of commonly used peak picking algorithms.\n\n**WiPP** has been developed as a collaborative effort between the Berlin Institute of Health (BIH) Metabolomics platform, the BIH Core Unit Bioinformatics, the INRA Plateforme d'Exploration du M\u00e9tabolisme, and the INRA Laboratoire d'Etude des R\u00e9sidus et Contaminants dans les Aliments.\n\nThis document aims to help you get started with **WiPP** and brings you through the minimum requirements to install, set up, and run the software. However, we strongly recommend you to read through the [complete user guide](documentation/USERGUIDE.md) for a full and advanced use of **WiPP**.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Operating System Compatibility",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide"
        ],
        "type": "Text_excerpt",
        "value": "**WiPP** has been tested successfully with:\n- Ubuntu 16 (Xenial Xerus)\n- CentOS 7.6.1810 (Core)\n\nUbuntu 18 (Bionic Beaver) is not supported yet due to lacking support of incorporated R packages.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Note",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running your own project"
        ],
        "type": "Text_excerpt",
        "value": "# WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide\n**WiPP** is an open source large scale GC-MS data preprocessing workflow built in Python 3 that uses machine learning to optimise, automate and combine the peak detection process of commonly used peak picking algorithms.\n\n**WiPP** has been developed as a collaborative effort between the Berlin Institute of Health (BIH) Metabolomics platform, the BIH Core Unit Bioinformatics, the INRA Plateforme d'Exploration du M\u00e9tabolisme, and the INRA Laboratoire d'Etude des R\u00e9sidus et Contaminants dans les Aliments.\n\nThis document aims to help you get started with **WiPP** and brings you through the minimum requirements to install, set up, and run the software. However, we strongly recommend you to read through the [complete user guide](documentation/USERGUIDE.md) for a full and advanced use of **WiPP**.\n\n## License\n**WiPP** v 1.0 is release under the [MIT License](LICENSE.md).\n\n## Operating System Compatibility\n**WiPP** has been tested successfully with:\n- Ubuntu 16 (Xenial Xerus)\n- CentOS 7.6.1810 (Core)\n\nUbuntu 18 (Bionic Beaver) is not supported yet due to lacking support of incorporated R packages.\n\n## Requirements\n- conda ([Bioconda website - Python 3.x](https://conda.io/en/latest/miniconda.html))\n- libnetcdf11 ([Ubuntu packages website](https://packages.ubuntu.com/xenial/libs/libnetcdf11))\n\n> ### Note\n> If you install conda from scratch, remember to `source ~/.bashrc` or open a new terminal before installing WiPP.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide"
        ],
        "type": "Text_excerpt",
        "value": "You can install **WiPP** using the following command:\n```bash\ngit clone https://github.com/bihealth/WiPP.git\ncd WiPP\nmake\n```\nNow you are ready to run **WiPP**!\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Change into example_project directory",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running a test project"
        ],
        "type": "Text_excerpt",
        "value": "The pipeline needs to be run from the project directory (the one that contains the `config.yaml` file). Use the following command to change to the example project directory:\n```bash\ncd ./projects/example_project\n```\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Generate training data",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running a test project"
        ],
        "type": "Text_excerpt",
        "value": "From there, you can now run the first part of the pipeline, the generation of the training data.\nYou can adjust the number of cores using the inline paramter `-n <CORES>`:\n```bash\n../../run_WiPP.sh tr -n 4 \n```\n> ### Note\n> Running this for the first time takes a while: another conda environment is created\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Annotate detected peaks",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running a test project"
        ],
        "type": "Text_excerpt",
        "value": "Run the following command to start the annotation:\n```bash\n../../run_WiPP.sh an\n```\nThe script opens a simple visualization tool using the default pdf viewer, and will wait for you to assign a class to the peak. By default, seven different classes are available for you to choose from.\nOnce you have annotated the required amount of peaks for each algorithm (only 25 for the example project), the tool will automatically close.\nYou are now ready to launch the last part of the pipeline.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Do the actual peack picking",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running a test project"
        ],
        "type": "Text_excerpt",
        "value": "Many sequential substeps are in fact happening during this final step of the pipeline such as classifier hyperparameter optimisation, peak detection algorithms parameter optimisation, peak detection on the full dataset, output classification and result integration.\nRun it with the following command and adjust the number of cores using the inline parameter `-n <CORES>`:\n```bash\n../../run_WiPP.sh pp -n 4\n```\n\n> ### Note\n> As you annotated a very small amount of peaks, your classifier is likely not to be accurate. For this reason, we provide a trained classifier. In order to use this example classifier, please uncomment the line `path: example_classifier` (by removing the `#` symbol) in the `classifier` block of the `config.yaml` file. You can follow the same procedure for your data if you want to use a specific classifier for several projects. Please note that the example classifier is provided to help you test the tool and is specific to the test data, do not use it for your own project.\n\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Input files",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running your own project"
        ],
        "type": "Text_excerpt",
        "value": "Let's create and structure the directory for your data files. **WiPP** supports mzML, mzData and NetCDF file formats.\n\nYou can create the `Input_folder` directory anywhere you want as long as it is accessible to the tool. You can also name it the way you want, but for clarity purposes, we will call it `Input_folder` in this tutorial. \n\nHere is the structure:\n```\nInput_folder/\n\tcondition_1/\n\tcondition_2/\n\t...\n\tcondition_n/\n\tTraining_samples/\n\tOptimization_samples/\n\tWash/\n```\n\nAll your files should be separated into subdirectories corresponding to the experimental conditions of your study (here, condition 1 to n).\n\nThree extra subdirectories are necessary to run the pipeline.\n\nThe `Wash` directory is optional but if present should contain the wash or blank sample files. Here, we assume that these samples contain the alkanes that are used for Retention Index calculation.\n\nThe `Training_samples` directory should contain a subset of the pooled samples or a representative subset of the samples of each biological condition. If you do not have pooled samples, we recommend using a minimum of 2 sample for each condition.\n\nThe `Optimization_samples` directory is similar to the `Training_samples` directory, just make sure to choose different samples as we do not want the parameters to be optimized using the same data the classifier was trained on.\n\n> ### Note\n> The sample files used for training and optimization should still be present in your sample directories, the files in the `Training_samples` and `Optimization_samples` directories are only copies.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Pipeline settings",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running your own project"
        ],
        "type": "Text_excerpt",
        "value": "This tutorial only shows the minimum requirements to run the pipeline, to learn more about all pipeline settings, have a look at the pipeline settings section of the [complete user guide](documentation/USERGUIDE.md).\n\nAll general pipeline settings are stored in the `config.yaml` of the individual project folder and need to be created for every new project. You can have a look at the [example config](projects/example_project/config.yaml) from the example_project.\n\n> ### Note\n> By default, in the example project, we run only one peak picking algorithm to speed up the test. Do not forget to enable in the `config.yaml` file the peak picking algorithms you want to use.\n\nFirst, you need to define the absolute path to your `Input_folder`. This parameter can be found in the `static_data` block under the name `absolute_path`. You also need to specify the resolution of your data by setting the `high_resolution` parameter to `True` or `False` (in this context, any data with a mass resolution higher than 1 Da is considered high resolution).\n\nNext, if you have a `Wash` directory for retention index calculation, go to the `retention_index` block. You need to define the relative path from the `Input_folder` to the `Wash` folder. In our example, this parameter would look like this `./Wash`.\nYou also have to define the alkanes present in your samples as a simple list `c10,c12,c16,[...],c32,c34`.\nIf you do not have blank samples containing alkanes for retention index calculation, you can leave this section as it is in the example file.\n\nThe last two compulsory parameters that you need to define are the relatives path from the `Input_folder` to your `Training_samples` and `Optimization_samples` directories, respectively found in `training_data-general` and `optimization-general` setting blocks.\n\nThat's all for the basic settings, you are now ready to run the pipeline.\n\n> ### Note\n> Keep in mind that those parameters are the only one required to run the pipeline, but there is a lot more you can do to precisely tune the pipeline. Have a read through the [complete user guide](documentation/USERGUIDE.md) to learn more.\n "
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/bihealth/WiPP/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "Copyright 2019,  Berlin Institute of Health (BIH)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/LICENSE.md",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide"
        ],
        "type": "Text_excerpt",
        "value": "**WiPP** v 1.0 is release under the [MIT License](LICENSE.md).\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "WiPP"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "bihealth"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 147146,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "R",
        "size": 25036,
        "type": "Programming_language",
        "value": "R"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 1559,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Makefile",
        "size": 1412,
        "type": "Programming_language",
        "value": "Makefile"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "NBMueller",
          "type": "User"
        },
        "date_created": "2018-09-21T07:34:28Z",
        "date_published": "2018-09-25T21:09:49Z",
        "description": "Initial WiPP (Workflow for improved Peak Picking) release",
        "html_url": "https://github.com/bihealth/WiPP/releases/tag/v1.0",
        "name": "Initial release for conda packaginf",
        "release_id": 13091213,
        "tag": "v1.0",
        "tarball_url": "https://api.github.com/repos/bihealth/WiPP/tarball/v1.0",
        "type": "Release",
        "url": "https://api.github.com/repos/bihealth/WiPP/releases/13091213",
        "value": "https://api.github.com/repos/bihealth/WiPP/releases/13091213",
        "zipball_url": "https://api.github.com/repos/bihealth/WiPP/zipball/v1.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Requirements",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide"
        ],
        "type": "Text_excerpt",
        "value": "- conda ([Bioconda website - Python 3.x](https://conda.io/en/latest/miniconda.html))\n- libnetcdf11 ([Ubuntu packages website](https://packages.ubuntu.com/xenial/libs/libnetcdf11))\n\n> ### Note\n> If you install conda from scratch, remember to `source ~/.bashrc` or open a new terminal before installing WiPP.\n\n## Installation\nYou can install **WiPP** using the following command:\n```bash\ngit clone https://github.com/bihealth/WiPP.git\ncd WiPP\nmake\n```\nNow you are ready to run **WiPP**!\n\n## Running a test project\n\n### Change into example_project directory\nThe pipeline needs to be run from the project directory (the one that contains the `config.yaml` file). Use the following command to change to the example project directory:\n```bash\ncd ./projects/example_project\n```\n\n### Generate training data\nFrom there, you can now run the first part of the pipeline, the generation of the training data.\nYou can adjust the number of cores using the inline paramter `-n <CORES>`:\n```bash\n../../run_WiPP.sh tr -n 4 \n```\n> ### Note\n> Running this for the first time takes a while: another conda environment is created\n\n### Annotate detected peaks\nRun the following command to start the annotation:\n```bash\n../../run_WiPP.sh an\n```\nThe script opens a simple visualization tool using the default pdf viewer, and will wait for you to assign a class to the peak. By default, seven different classes are available for you to choose from.\nOnce you have annotated the required amount of peaks for each algorithm (only 25 for the example project), the tool will automatically close.\nYou are now ready to launch the last part of the pipeline.\n\n### Do the actual peack picking\nMany sequential substeps are in fact happening during this final step of the pipeline such as classifier hyperparameter optimisation, peak detection algorithms parameter optimisation, peak detection on the full dataset, output classification and result integration.\nRun it with the following command and adjust the number of cores using the inline parameter `-n <CORES>`:\n```bash\n../../run_WiPP.sh pp -n 4\n```\n\n> ### Note\n> As you annotated a very small amount of peaks, your classifier is likely not to be accurate. For this reason, we provide a trained classifier. In order to use this example classifier, please uncomment the line `path: example_classifier` (by removing the `#` symbol) in the `classifier` block of the `config.yaml` file. You can follow the same procedure for your data if you want to use a specific classifier for several projects. Please note that the example classifier is provided to help you test the tool and is specific to the test data, do not use it for your own project.\n\n\n## Running your own project\nTo run your own project, you have to do some prelimitary work and decisions, which is described in this section.\nTo actually run **WiPP** subsequently, you have to follow the same steps as in the example_project.\n\n### Input files\nLet's create and structure the directory for your data files. **WiPP** supports mzML, mzData and NetCDF file formats.\n\nYou can create the `Input_folder` directory anywhere you want as long as it is accessible to the tool. You can also name it the way you want, but for clarity purposes, we will call it `Input_folder` in this tutorial. \n\nHere is the structure:\n```\nInput_folder/\n\tcondition_1/\n\tcondition_2/\n\t...\n\tcondition_n/\n\tTraining_samples/\n\tOptimization_samples/\n\tWash/\n```\n\nAll your files should be separated into subdirectories corresponding to the experimental conditions of your study (here, condition 1 to n).\n\nThree extra subdirectories are necessary to run the pipeline.\n\nThe `Wash` directory is optional but if present should contain the wash or blank sample files. Here, we assume that these samples contain the alkanes that are used for Retention Index calculation.\n\nThe `Training_samples` directory should contain a subset of the pooled samples or a representative subset of the samples of each biological condition. If you do not have pooled samples, we recommend using a minimum of 2 sample for each condition.\n\nThe `Optimization_samples` directory is similar to the `Training_samples` directory, just make sure to choose different samples as we do not want the parameters to be optimized using the same data the classifier was trained on.\n\n> ### Note\n> The sample files used for training and optimization should still be present in your sample directories, the files in the `Training_samples` and `Optimization_samples` directories are only copies.\n\n### Pipeline settings\n\nThis tutorial only shows the minimum requirements to run the pipeline, to learn more about all pipeline settings, have a look at the pipeline settings section of the [complete user guide](documentation/USERGUIDE.md).\n\nAll general pipeline settings are stored in the `config.yaml` of the individual project folder and need to be created for every new project. You can have a look at the [example config](projects/example_project/config.yaml) from the example_project.\n\n> ### Note\n> By default, in the example project, we run only one peak picking algorithm to speed up the test. Do not forget to enable in the `config.yaml` file the peak picking algorithms you want to use.\n\nFirst, you need to define the absolute path to your `Input_folder`. This parameter can be found in the `static_data` block under the name `absolute_path`. You also need to specify the resolution of your data by setting the `high_resolution` parameter to `True` or `False` (in this context, any data with a mass resolution higher than 1 Da is considered high resolution).\n\nNext, if you have a `Wash` directory for retention index calculation, go to the `retention_index` block. You need to define the relative path from the `Input_folder` to the `Wash` folder. In our example, this parameter would look like this `./Wash`.\nYou also have to define the alkanes present in your samples as a simple list `c10,c12,c16,[...],c32,c34`.\nIf you do not have blank samples containing alkanes for retention index calculation, you can leave this section as it is in the example file.\n\nThe last two compulsory parameters that you need to define are the relatives path from the `Input_folder` to your `Training_samples` and `Optimization_samples` directories, respectively found in `training_data-general` and `optimization-general` setting blocks.\n\nThat's all for the basic settings, you are now ready to run the pipeline.\n\n> ### Note\n> Keep in mind that those parameters are the only one required to run the pipeline, but there is a lot more you can do to precisely tune the pipeline. Have a read through the [complete user guide](documentation/USERGUIDE.md) to learn more.\n \n### Running the pipeline\nFollow the same steps as in the [example_project](#running-a-test-project).\n\n> ### Note\n> Peak annotation usually takes several hours (1200 peaks per algorithm by default), but you only need to do that once per instrument/protocol. Once trained, the SVM classifiers can be reused on other datasets generated with the same instrument and data acquisition method.\n\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Note",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running your own project"
        ],
        "type": "Text_excerpt",
        "value": "# WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide\n**WiPP** is an open source large scale GC-MS data preprocessing workflow built in Python 3 that uses machine learning to optimise, automate and combine the peak detection process of commonly used peak picking algorithms.\n\n**WiPP** has been developed as a collaborative effort between the Berlin Institute of Health (BIH) Metabolomics platform, the BIH Core Unit Bioinformatics, the INRA Plateforme d'Exploration du M\u00e9tabolisme, and the INRA Laboratoire d'Etude des R\u00e9sidus et Contaminants dans les Aliments.\n\nThis document aims to help you get started with **WiPP** and brings you through the minimum requirements to install, set up, and run the software. However, we strongly recommend you to read through the [complete user guide](documentation/USERGUIDE.md) for a full and advanced use of **WiPP**.\n\n## License\n**WiPP** v 1.0 is release under the [MIT License](LICENSE.md).\n\n## Operating System Compatibility\n**WiPP** has been tested successfully with:\n- Ubuntu 16 (Xenial Xerus)\n- CentOS 7.6.1810 (Core)\n\nUbuntu 18 (Bionic Beaver) is not supported yet due to lacking support of incorporated R packages.\n\n## Requirements\n- conda ([Bioconda website - Python 3.x](https://conda.io/en/latest/miniconda.html))\n- libnetcdf11 ([Ubuntu packages website](https://packages.ubuntu.com/xenial/libs/libnetcdf11))\n\n> ### Note\n> If you install conda from scratch, remember to `source ~/.bashrc` or open a new terminal before installing WiPP.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Change into example_project directory",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running a test project"
        ],
        "type": "Text_excerpt",
        "value": "The pipeline needs to be run from the project directory (the one that contains the `config.yaml` file). Use the following command to change to the example project directory:\n```bash\ncd ./projects/example_project\n```\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Generate training data",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running a test project"
        ],
        "type": "Text_excerpt",
        "value": "From there, you can now run the first part of the pipeline, the generation of the training data.\nYou can adjust the number of cores using the inline paramter `-n <CORES>`:\n```bash\n../../run_WiPP.sh tr -n 4 \n```\n> ### Note\n> Running this for the first time takes a while: another conda environment is created\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Annotate detected peaks",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running a test project"
        ],
        "type": "Text_excerpt",
        "value": "Run the following command to start the annotation:\n```bash\n../../run_WiPP.sh an\n```\nThe script opens a simple visualization tool using the default pdf viewer, and will wait for you to assign a class to the peak. By default, seven different classes are available for you to choose from.\nOnce you have annotated the required amount of peaks for each algorithm (only 25 for the example project), the tool will automatically close.\nYou are now ready to launch the last part of the pipeline.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Do the actual peack picking",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running a test project"
        ],
        "type": "Text_excerpt",
        "value": "Many sequential substeps are in fact happening during this final step of the pipeline such as classifier hyperparameter optimisation, peak detection algorithms parameter optimisation, peak detection on the full dataset, output classification and result integration.\nRun it with the following command and adjust the number of cores using the inline parameter `-n <CORES>`:\n```bash\n../../run_WiPP.sh pp -n 4\n```\n\n> ### Note\n> As you annotated a very small amount of peaks, your classifier is likely not to be accurate. For this reason, we provide a trained classifier. In order to use this example classifier, please uncomment the line `path: example_classifier` (by removing the `#` symbol) in the `classifier` block of the `config.yaml` file. You can follow the same procedure for your data if you want to use a specific classifier for several projects. Please note that the example classifier is provided to help you test the tool and is specific to the test data, do not use it for your own project.\n\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Running your own project",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide"
        ],
        "type": "Text_excerpt",
        "value": "To run your own project, you have to do some prelimitary work and decisions, which is described in this section.\nTo actually run **WiPP** subsequently, you have to follow the same steps as in the example_project.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Input files",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running your own project"
        ],
        "type": "Text_excerpt",
        "value": "Let's create and structure the directory for your data files. **WiPP** supports mzML, mzData and NetCDF file formats.\n\nYou can create the `Input_folder` directory anywhere you want as long as it is accessible to the tool. You can also name it the way you want, but for clarity purposes, we will call it `Input_folder` in this tutorial. \n\nHere is the structure:\n```\nInput_folder/\n\tcondition_1/\n\tcondition_2/\n\t...\n\tcondition_n/\n\tTraining_samples/\n\tOptimization_samples/\n\tWash/\n```\n\nAll your files should be separated into subdirectories corresponding to the experimental conditions of your study (here, condition 1 to n).\n\nThree extra subdirectories are necessary to run the pipeline.\n\nThe `Wash` directory is optional but if present should contain the wash or blank sample files. Here, we assume that these samples contain the alkanes that are used for Retention Index calculation.\n\nThe `Training_samples` directory should contain a subset of the pooled samples or a representative subset of the samples of each biological condition. If you do not have pooled samples, we recommend using a minimum of 2 sample for each condition.\n\nThe `Optimization_samples` directory is similar to the `Training_samples` directory, just make sure to choose different samples as we do not want the parameters to be optimized using the same data the classifier was trained on.\n\n> ### Note\n> The sample files used for training and optimization should still be present in your sample directories, the files in the `Training_samples` and `Optimization_samples` directories are only copies.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Pipeline settings",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running your own project"
        ],
        "type": "Text_excerpt",
        "value": "This tutorial only shows the minimum requirements to run the pipeline, to learn more about all pipeline settings, have a look at the pipeline settings section of the [complete user guide](documentation/USERGUIDE.md).\n\nAll general pipeline settings are stored in the `config.yaml` of the individual project folder and need to be created for every new project. You can have a look at the [example config](projects/example_project/config.yaml) from the example_project.\n\n> ### Note\n> By default, in the example project, we run only one peak picking algorithm to speed up the test. Do not forget to enable in the `config.yaml` file the peak picking algorithms you want to use.\n\nFirst, you need to define the absolute path to your `Input_folder`. This parameter can be found in the `static_data` block under the name `absolute_path`. You also need to specify the resolution of your data by setting the `high_resolution` parameter to `True` or `False` (in this context, any data with a mass resolution higher than 1 Da is considered high resolution).\n\nNext, if you have a `Wash` directory for retention index calculation, go to the `retention_index` block. You need to define the relative path from the `Input_folder` to the `Wash` folder. In our example, this parameter would look like this `./Wash`.\nYou also have to define the alkanes present in your samples as a simple list `c10,c12,c16,[...],c32,c34`.\nIf you do not have blank samples containing alkanes for retention index calculation, you can leave this section as it is in the example file.\n\nThe last two compulsory parameters that you need to define are the relatives path from the `Input_folder` to your `Training_samples` and `Optimization_samples` directories, respectively found in `training_data-general` and `optimization-general` setting blocks.\n\nThat's all for the basic settings, you are now ready to run the pipeline.\n\n> ### Note\n> Keep in mind that those parameters are the only one required to run the pipeline, but there is a lot more you can do to precisely tune the pipeline. Have a read through the [complete user guide](documentation/USERGUIDE.md) to learn more.\n "
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Running the pipeline",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running your own project"
        ],
        "type": "Text_excerpt",
        "value": "Follow the same steps as in the [example_project](#running-a-test-project).\n\n> ### Note\n> Peak annotation usually takes several hours (1200 peaks per algorithm by default), but you only need to do that once per instrument/protocol. Once trained, the SVM classifiers can be reused on other datasets generated with the same instrument and data acquisition method.\n\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "download",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 18:15:42",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 5
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
        "type": "Text_excerpt",
        "value": "**WiPP** is an open source large scale GC-MS data preprocessing workflow built in Python 3 that uses machine learning to optimise, automate and combine the peak detection process of commonly used peak picking algorithms.\n\n**WiPP** has been developed as a collaborative effort between the Berlin Institute of Health (BIH) Metabolomics platform, the BIH Core Unit Bioinformatics, the INRA Plateforme d'Exploration du M\u00e9tabolisme, and the INRA Laboratoire d'Etude des R\u00e9sidus et Contaminants dans les Aliments.\n\nThis document aims to help you get started with **WiPP** and brings you through the minimum requirements to install, set up, and run the software. However, we strongly recommend you to read through the [complete user guide](documentation/USERGUIDE.md) for a full and advanced use of **WiPP**.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Operating System Compatibility",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide"
        ],
        "type": "Text_excerpt",
        "value": "**WiPP** has been tested successfully with:\n- Ubuntu 16 (Xenial Xerus)\n- CentOS 7.6.1810 (Core)\n\nUbuntu 18 (Bionic Beaver) is not supported yet due to lacking support of incorporated R packages.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Note",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running your own project"
        ],
        "type": "Text_excerpt",
        "value": "# WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide\n**WiPP** is an open source large scale GC-MS data preprocessing workflow built in Python 3 that uses machine learning to optimise, automate and combine the peak detection process of commonly used peak picking algorithms.\n\n**WiPP** has been developed as a collaborative effort between the Berlin Institute of Health (BIH) Metabolomics platform, the BIH Core Unit Bioinformatics, the INRA Plateforme d'Exploration du M\u00e9tabolisme, and the INRA Laboratoire d'Etude des R\u00e9sidus et Contaminants dans les Aliments.\n\nThis document aims to help you get started with **WiPP** and brings you through the minimum requirements to install, set up, and run the software. However, we strongly recommend you to read through the [complete user guide](documentation/USERGUIDE.md) for a full and advanced use of **WiPP**.\n\n## License\n**WiPP** v 1.0 is release under the [MIT License](LICENSE.md).\n\n## Operating System Compatibility\n**WiPP** has been tested successfully with:\n- Ubuntu 16 (Xenial Xerus)\n- CentOS 7.6.1810 (Core)\n\nUbuntu 18 (Bionic Beaver) is not supported yet due to lacking support of incorporated R packages.\n\n## Requirements\n- conda ([Bioconda website - Python 3.x](https://conda.io/en/latest/miniconda.html))\n- libnetcdf11 ([Ubuntu packages website](https://packages.ubuntu.com/xenial/libs/libnetcdf11))\n\n> ### Note\n> If you install conda from scratch, remember to `source ~/.bashrc` or open a new terminal before installing WiPP.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Change into example_project directory",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running a test project"
        ],
        "type": "Text_excerpt",
        "value": "The pipeline needs to be run from the project directory (the one that contains the `config.yaml` file). Use the following command to change to the example project directory:\n```bash\ncd ./projects/example_project\n```\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Generate training data",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running a test project"
        ],
        "type": "Text_excerpt",
        "value": "From there, you can now run the first part of the pipeline, the generation of the training data.\nYou can adjust the number of cores using the inline paramter `-n <CORES>`:\n```bash\n../../run_WiPP.sh tr -n 4 \n```\n> ### Note\n> Running this for the first time takes a while: another conda environment is created\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Annotate detected peaks",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running a test project"
        ],
        "type": "Text_excerpt",
        "value": "Run the following command to start the annotation:\n```bash\n../../run_WiPP.sh an\n```\nThe script opens a simple visualization tool using the default pdf viewer, and will wait for you to assign a class to the peak. By default, seven different classes are available for you to choose from.\nOnce you have annotated the required amount of peaks for each algorithm (only 25 for the example project), the tool will automatically close.\nYou are now ready to launch the last part of the pipeline.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Do the actual peack picking",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running a test project"
        ],
        "type": "Text_excerpt",
        "value": "Many sequential substeps are in fact happening during this final step of the pipeline such as classifier hyperparameter optimisation, peak detection algorithms parameter optimisation, peak detection on the full dataset, output classification and result integration.\nRun it with the following command and adjust the number of cores using the inline parameter `-n <CORES>`:\n```bash\n../../run_WiPP.sh pp -n 4\n```\n\n> ### Note\n> As you annotated a very small amount of peaks, your classifier is likely not to be accurate. For this reason, we provide a trained classifier. In order to use this example classifier, please uncomment the line `path: example_classifier` (by removing the `#` symbol) in the `classifier` block of the `config.yaml` file. You can follow the same procedure for your data if you want to use a specific classifier for several projects. Please note that the example classifier is provided to help you test the tool and is specific to the test data, do not use it for your own project.\n\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Input files",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running your own project"
        ],
        "type": "Text_excerpt",
        "value": "Let's create and structure the directory for your data files. **WiPP** supports mzML, mzData and NetCDF file formats.\n\nYou can create the `Input_folder` directory anywhere you want as long as it is accessible to the tool. You can also name it the way you want, but for clarity purposes, we will call it `Input_folder` in this tutorial. \n\nHere is the structure:\n```\nInput_folder/\n\tcondition_1/\n\tcondition_2/\n\t...\n\tcondition_n/\n\tTraining_samples/\n\tOptimization_samples/\n\tWash/\n```\n\nAll your files should be separated into subdirectories corresponding to the experimental conditions of your study (here, condition 1 to n).\n\nThree extra subdirectories are necessary to run the pipeline.\n\nThe `Wash` directory is optional but if present should contain the wash or blank sample files. Here, we assume that these samples contain the alkanes that are used for Retention Index calculation.\n\nThe `Training_samples` directory should contain a subset of the pooled samples or a representative subset of the samples of each biological condition. If you do not have pooled samples, we recommend using a minimum of 2 sample for each condition.\n\nThe `Optimization_samples` directory is similar to the `Training_samples` directory, just make sure to choose different samples as we do not want the parameters to be optimized using the same data the classifier was trained on.\n\n> ### Note\n> The sample files used for training and optimization should still be present in your sample directories, the files in the `Training_samples` and `Optimization_samples` directories are only copies.\n"
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Pipeline settings",
        "parent_header": [
          "WiPP - A **W**orkflow for **i**mproved **P**eak **P**icking - Quick Start Guide",
          "Running your own project"
        ],
        "type": "Text_excerpt",
        "value": "This tutorial only shows the minimum requirements to run the pipeline, to learn more about all pipeline settings, have a look at the pipeline settings section of the [complete user guide](documentation/USERGUIDE.md).\n\nAll general pipeline settings are stored in the `config.yaml` of the individual project folder and need to be created for every new project. You can have a look at the [example config](projects/example_project/config.yaml) from the example_project.\n\n> ### Note\n> By default, in the example project, we run only one peak picking algorithm to speed up the test. Do not forget to enable in the `config.yaml` file the peak picking algorithms you want to use.\n\nFirst, you need to define the absolute path to your `Input_folder`. This parameter can be found in the `static_data` block under the name `absolute_path`. You also need to specify the resolution of your data by setting the `high_resolution` parameter to `True` or `False` (in this context, any data with a mass resolution higher than 1 Da is considered high resolution).\n\nNext, if you have a `Wash` directory for retention index calculation, go to the `retention_index` block. You need to define the relative path from the `Input_folder` to the `Wash` folder. In our example, this parameter would look like this `./Wash`.\nYou also have to define the alkanes present in your samples as a simple list `c10,c12,c16,[...],c32,c34`.\nIf you do not have blank samples containing alkanes for retention index calculation, you can leave this section as it is in the example file.\n\nThe last two compulsory parameters that you need to define are the relatives path from the `Input_folder` to your `Training_samples` and `Optimization_samples` directories, respectively found in `training_data-general` and `optimization-general` setting blocks.\n\nThat's all for the basic settings, you are now ready to run the pipeline.\n\n> ### Note\n> Keep in mind that those parameters are the only one required to run the pipeline, but there is a lot more you can do to precisely tune the pipeline. Have a read through the [complete user guide](documentation/USERGUIDE.md) to learn more.\n "
      },
      "source": "https://raw.githubusercontent.com/bihealth/WiPP/master/README.md",
      "technique": "header_analysis"
    }
  ]
}