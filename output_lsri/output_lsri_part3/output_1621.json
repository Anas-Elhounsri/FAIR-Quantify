{
  "acknowledgement": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Acknowledgement",
        "parent_header": [
          "9. Acknowledgement, Copyright and general"
        ],
        "type": "Text_excerpt",
        "value": "We acknowledge funding by the National Institutes of Health through the NHGRI (U41HG006941). The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.\n\n* We thank Sumir Panji and Nicola Mulder for their support and leadership\n* We thank Fourie Joubert at the University of Pretoria for hosting our initial hackathon.\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Authors",
        "parent_header": [
          "9. Acknowledgement, Copyright and general",
          "Acknowledgement"
        ],
        "type": "Text_excerpt",
        "value": "Current team: Scott Hazelhurst, Jean-Tristan Brandenburg, Lindsay Clark, Obokula Smile, Michael Ebo Turkson, Michael Thompson, \n\nH3ABioNet Pipelines team leadership: Christopher Fields,  Shakuntala Baichoo, Sumir Panji, Gerrit Botha.\n\n\nPast members and contributors: Lerato E. Magosi, Shaun Aron, Rob Clucas,  Eugene de Beste, Aboyomini Mosaku, Don Armstrong and the Wits Bioinformatics team\n\nWe thank Harry Noyes from the University of Liverpool and Ayton Meintjes from UCT who both spent significant effort being testers of the pipleine, and the many users at the Sydney Brenner Institute for Molecular Bioscience for their patience and suggestion.\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Funding",
        "parent_header": [
          "9. Acknowledgement, Copyright and general",
          "Acknowledgement"
        ],
        "type": "Text_excerpt",
        "value": "We acknowledge the support from the NIH NHGRI H3ABioNet (U24HG006941)   and AWI-Gen   (U54HG006938)\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "format": "cff",
        "type": "File_dump",
        "value": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n- family-names: \"Brandenburg\"\n  given-names: \"Jean-Tristan\"\n  orcid: https://orcid.org/0000-0003-0197-2648\n- family-names: \"Clark\"\n  given-names: \"Lindsay\"\n  orcid: https://orcid.org 0000-0002-3881-9252\n- family-names: \"Botha\"\n  given-names: \"Gerrit\"\n  orcid: \n- family-names: \"Panji\"\n  given-names: \"Sumir\"\n  orcid:  https://orcid.org/0000-0003-0447-0598\n- family-names: \"Baichoo\"\n  given-names: \"Shakuntala\"\n  orcid:  https://orcid.org/0000-0002-9335-1939\n- family-names: \"Fields\"\n  given-names: \"Christopher J.\"\n  orcid:  https://orcid.org/ 0000-0002-0581-149X\n- family-names: \"Hazelhurst\"\n  given-names: Scott\n  orcid: https://orcid.org/0000-0002-0581-149X\ntitle: \"H3AGWAS : A portable workflow for Genome Wide Association Studies\"\ndoi: https://doi.org/10.1186/s12859-022-05034-w\nversion: 3.0\ndate-released: 2019-12-18\nurl: \"https://github.com/h3abionet/h3agwas\"\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/CITATION.cff",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Citing this workflow",
        "parent_header": [
          "9. Acknowledgement, Copyright and general"
        ],
        "type": "Text_excerpt",
        "value": "If you use this workflow, please cite the following paper\n\n* Studies Jean-Tristan Brandenburg, Lindsay Clark, Gerrit Botha, Sumir Panji, Shakuntala Baichoo, Christopher J. Fields, Scott Hazelhurst. (2022). H3AGWAS : A portable workflow for Genome Wide Association Studies bioRxiv 2022.05.02.490206; doi: https://doi.org/10.1101/2022.05.02.490206 \n\nand \n\n\n* Baichoo S, Souilmi Y, Panji S, Botha G, Meintjes A, Hazelhurst S, Bendou H, De Beste E, Mpangase P, Souiai O, Alghali M, Yi L, O'Connor B, Crusoe M, Armstrong D, Aron S, Joubert D, Ahmed A, Mbiyavanga M, Van Heusden P, Magosi, L, Zermeno, J, Mainzer L, Fadlelmola F, Jongeneel CV, and Mulder N. (2018) Developing reproducible bioinformatics analysis workflows for heterogenous computing environments to support African genomics, *BMC Bioinformatics* **19**, 457, 13 pages, doi:10.1186/s12859-018-2446-1.\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/h3abionet/h3agwas"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2016-08-22T15:32:08Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-16T07:23:18Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "GWAS Pipeline for H3Africa"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Brief introduction",
        "parent_header": [
          "Nextflow update"
        ],
        "type": "Text_excerpt",
        "value": "In addition to this README we have a detailed tutorial and videos \n* These can be found at http://www.bioinf.wits.ac.za/gwas\n* found example data-set and example in  [h3agwas-examples github](https://github.com/h3abionet/h3agwas-examples)\n\npipeline do different step of GWAS :\n * [Format output illumina in plink format](call2plink/README.md)\n * [Quality control of array input illuminat in plink format](qc/README.md)\n * [Association using different software : gcta, plink, gemma, Bolt-LMM, FastLMM and GxE with gemma and plink](assoc/README.md)\n * Post meta analyses script :\n   * [Meta analysis and Multi-Trait Analysis of GWAS ](meta/README.md)\n   * [Computation of heritabilities or variance explained of phenotype using summary statistisc or genetics](heritabilities/README.md)\n   * [Finemapping, cojo extraction of windows or conditional analysis using gemma](finemapping/README.md)\n   * [Annotation and plot of result](utils/annotation/README.md)\n * [Simulation of dataset](utils/build_example_data/README.md)\n * [Format data differents dataset](formatdata//README.md) :\n   * plink in vcf to prepared your data at imputation;\n   * vcf in plink, bgen, bimbam after imputation;\n   * summary statistics between build;\n * h3agwas doesn't perform imputation, specific pipeline can be found in [h3abionet](https://github.com/h3abionet/chipimputation).\n\n<img src=\"helperfiles/script_pipeline_all.png\" title=\"overview of pipeline in gwas studies\">\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9392827102101302,
      "result": {
        "original_header": "Nextflow update",
        "type": "Text_excerpt",
        "value": "for instance :\n```\nNXF_VER=22.10.8 ./nextflow run h3abionet/h3agwas/assoc/main.nf \\\n --input_dir data/imputed/ --input_pat imput_data \\\n --data data/pheno/pheno_test.all --pheno pheno_qt1,pheno_qt2 \\\n --output_dir assoc --output assoc \\\n --gemma 1 --sample_snps_rel 1 --linear 1 \\\n  -profile singularity\n``` \n\nThe major change from Version 2 to Version 3 is the reorganisation of the repo so that the different workflows are in separate directories. \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9684419993983936,
      "result": {
        "original_header": "Background",
        "type": "Text_excerpt",
        "value": "\nH3Agwas is a simple human GWAS analysis workflow for data quality control (QC) and basic association testing developed by [H3ABioNet](https://www.h3abionet.org/). It is an extension of the [witsGWAS pipeline](http://magosil86.github.io/witsGWAS/) for human genome-wide association studies built at the [Sydney Brenner Institute for Molecular Bioscience](https://www.wits.ac.za/research/sbimb/). H3Agwas uses Nextflow as the basis for workflow managment and has been dockerised to facilitate portability. \n\nThe original version of the H3Agwas was published in June 2017 with minor updates and bug fixes through the rest of the year. Based on experience with large data sets, the pipelines were considerably revised with additional features, reporting and a slightly different workflow.   \n_Please ignore the Wiki in this version which refers to version 1_ \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9280141925935574,
      "result": {
        "original_header": "Questions and feedback",
        "type": "Text_excerpt",
        "value": "Problems with the workflow should be raised as an [issue](https://github.com/h3abionet/h3agwas/issues) on this  GitHub repo. (If you think the probem is the workflow) \nWe welcome contributions to this repository via [pull requests](https://github.com/h3abionet/h3agwas/pulls) on GitHub.  Please strive to match the repository style, as well as the [nf-core guidelines](https://nf-co.re/developers/guidelines) inasmuch as they apply. \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.944998255831069,
      "result": {
        "original_header": "Goals of the h3agwas pipeline",
        "type": "Text_excerpt",
        "value": "The goals of this pipeline is to have a portable and robust pipeline\nfor performing a genome-wide association study \n1. `call2plink`.  Conversion of Illumina genotyping reports with TOP/BOTTOM or FORWARD/REVERSE  calls into PLINK format, aligning the calls.\n   * see [README of call2plink](call2plink/) \n2. `qc`: Quality control of the data. This is the focus of the pipeline. It takes as input PLINK data and has the following functions\n   * see [README of qc](qc/) \n3. `assoc`: Association study. A simple analysis association study is done. The purpose of this is to give users an introduction to their data. Real studies, particularly those of the H3A consortium will have to handle compex co-variates and particular population study. We encourage users of our pipeline to submit their analysis for the use of other scientists.\n   * see [README of assoc/](assoc/)\n  * Basic PLINK association tests, producing manhattan and qqplots\n  * CMH association test - Association analysis, accounting for clusters\n  * permutation testing\n  * logistic regression\n  * Efficient Mixed Model Association testing with gemma, boltlmm or fastlmm\n  * Gene environment association with gemma or plink \n4. `meta` : meta analyse or mtag :\n    * `meta/meta-assoc.nf` : do meta analysis with summary statistics \n    * `meta/mtag.nf` : do mtag analysis with summary statistics  \n5. `heritabilities`\n    *  \u0300heritabilities/esth2-assoc.nf` : estimate heritability and co-heritabilie with gcta, ldsc, gemma and bolt \n6. `finemapping` :\n    * `finemapping/main.nf` : performed fine-mapping on full summary statistics, using plink, gcta, FINEMAP...\n    * `finemapping/finemap_region.nf.nf` : performed fine-mapping on specific region\n    * `finemapping/cond-assoc` : performed conditional association on specificcs SNPs\n    * `finemapping/cojo-assoc.nf` : do Conditional & joint (COJO) analysis of GWAS summary statistics without individual-level genotype data with gcta \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9660993686473914,
      "result": {
        "original_header": "Design principles",
        "type": "Text_excerpt",
        "value": "The goal of the H3ABionet GWAS pipeline is to provide a portable and robust pipeline for reproducble genome-wide association studies. \n\nA GWAS requires a complex set of analyses with complex dependancies\nbetween the analyses. We want to support GWAS work by supporting\n* reproducibility -- we can rerun the entire analysis from start to finish;\n* reusability -- we can run the entire analysis with different parameters in an efficient and consistent way;\n* portability -- we can run the analysis on a laptop, on a server, on a cluster, in the cloud. The same workflow can be used for all environments, even if the time taken may change; \n* The anticipated users are heterogeneous both in terms of their bioinformatics needs and the computing environments they will use.\n* Each GWAS is different -- it must be customisable to allow the bioinformaticists to set different parameters. \n\nThere are two key technologies that we use, Nextflow and Docker, both of which support our design principles. \n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9766195358761708,
      "result": {
        "original_header": "Nextflow",
        "type": "Text_excerpt",
        "value": "[Nextflow](https://www.nextflow.io/) is a workflow language designed at the Centre for Genomic Regulation, Barcelona. Although it is a general workflow language for science, it comes out of a bioinformmatics group and strongly supports bioinformatics.  \nOur pipeline is built using Nextflow. However, users do not need to know anything about Nextflow. Obviously if you can do some programming you can customise and extend the pipelines, but you do not need to know Nextflow yourself.  \nNextlow is very easy to install and is highly portable. It supports partial execution and pipelines that scale.  Nextflow supports our worklow requirements very well.\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9357358528320072,
      "result": {
        "original_header": "Docker",
        "type": "Text_excerpt",
        "value": "A GWAS requires several software tools to be installed. Using Docker we can simplify the installation. Essentially, Docker wraps up all software dependancies into _containers_. Instead of installing all the dependancies, you can install Docker, easily and then install our containers. (In fact you don't need to explicitly install our containers, Nextflow and our workflow will do that for you automatically). \nWe expect that many of our users will use Docker. However, we recognise that this won't be suitable for everyone because many high performance computing centres do not support Docker for security reasons. It is possible to run our pipeline without Docker and will give  instructions about which software needs to be installed.\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8724404951113001,
      "result": {
        "original_header": "Singularity",
        "type": "Text_excerpt",
        "value": "Similarily we support Singularity. Although it's a new feature, we've tested it two different organisaitons and it's worked flawlessly\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9001017981966205,
      "result": {
        "original_header": "4 The Nextflow configuration file",
        "type": "Text_excerpt",
        "value": "Nextflow uses parameters that are passed to it and contents of a\nconfiguration file to guide its behaviour. By default, the\nconfiguration file used in _nextflow.config_. This includes\nspecifiying \n* where the inputs come from and outputs go to;\n* what the parameters of the various programs/steps. For example, in QC you can specify the what missingness cut-offs you want;\n* the mode of operation -- for example, are you running it on a cluster? Using Docker? \nTo run your workflow, you need to modify the nextflow.config file, and\nthen run nexflow. Remember, that to make your workflow truly\nreproducible you need to save a copy of the _config_ file. For this\nreason although you can specify many parameters from the command line,\nwe recommend using the config file since this makes your runs\nreproducible.  It may be useful to use git or similar tool to archive\nyour config files.\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9379913729613801,
      "result": {
        "original_header": "4.2 Creating an auxiliary nextflow .config file",
        "type": "Text_excerpt",
        "value": "There is a template of a nextflow.config file called aux.config.template. This is a read only file. Make a copy of it, call it _aux.config_ (or some suitable name).  This file contains all the options a user is likely to want to change. It does not specify options like the names of docker containers etc. Of course, you can if you wish modify the nextflow.config file, but we recommend against it. Your auxiliary file should supplement the nextflow.config file. \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8851098469251379,
      "result": {
        "original_header": "4.3 Specifying options",
        "type": "Text_excerpt",
        "value": "When you run the the scripts there are a number of different options that you might want to use. These options are specified by using  the `-flag` or `--flag` notation. The flags with a single hyphen (e.g. `-resume`) are standard Nextflow options applicable to all Nextflow scripts. The flags with a double hyphen (e.g., `--pi_hat`) are options that are specific to _our_ scripts.  *Take care not to mix this up as it's an easy error to make, and may cause silent errors to occur.* \nsets the maximim allowable per-SNP misisng to 4%. However, this should only be used when debugging and playing round. Rather, keep the options in the auxiliary config file that you save. By putting options on the command line you reduce reproducibility. (Using the parameters that change the mode of the running -- e.g. whether using docker or whether to produce a time line only affects time taken and auxiliary data rather than the substantive results). \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9779861479328709,
      "result": {
        "original_header": "4.4 Partial execution and resuming execution",
        "type": "Text_excerpt",
        "value": "Often a workflow may fail in the middle of execution because there's a problem with data (perhaps a typo in the name of a file), or you may want to run the workflow with slightly different parameters. Nextflow is very good in detecting what parts of the workflow need to re-executed -- use the `-resume` option. \n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.965814993449338,
      "result": {
        "original_header": "4.6 Workflow overview, and timing",
        "type": "Text_excerpt",
        "value": "Nextflow provides [several options](https://www.nextflow.io/docs/latest/tracing.html) for visualising and tracing workflow. See the Nextflow documentation for details. Two of the options are: \n* A nice graphic of a run of your workflow \n* A timeline of your workflow and individual processes (produced as an html file). \n    This is useful for seeing how long different parts of your process took. Also useful is peak virtual memory used, which you may need to know if running on very large data to ensure you have a big enough machine and specify the right parmeters. \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9401362785796292,
      "result": {
        "original_header": "7.1 updateFam.py",
        "type": "Text_excerpt",
        "value": "Can be used to update fam files. You probably won't need it, but others might find it useful. The intended application might be that there's been a mix-up of sample IDs and you want to correct.  The program takes four parameters: the original sample sheet, a new sample sheet (only has to include those elements that have changed), the original fam file, and then the base of a newfam file name.  The program takes the plate and well as the authorative ID of a sample. For every row in the updated sheet, the program finds the plate and well, looks up the corresponded entry in the original sheet, and then replaces that associated ID in the fam file. For example, if we have \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9332500030394932,
      "result": {
        "original_header": "7.2 getRunsTimes.pl (By Harry Noyes)",
        "type": "Text_excerpt",
        "value": "Nextflow has great options for showing resourc usage. However, you have to remember to set those option when you run.  It's easy to forget to do this. This very useful script by Harry Noyes (harry@liverpool.ac.uk) parses the .nextflow.log file  for you\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8705870901074757,
      "result": {
        "original_header": "7.3 make_ref.py",
        "type": "Text_excerpt",
        "value": "Makes a reference genome in a format the the pipeline can use. The first argument is a directory that contains FASTA files for each chromosome; the second is the strand report, the third is the manifest report, the fourt in the base of othe output files. \n\nThe program checks each SNP given in the manifest file by the chromosome and position number and then checks that the probe given in the manifest file actually matches the reference genome at that point. Minor slippage is acceptable because of indels. \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9275282493464257,
      "result": {
        "original_header": "7.4 plates.py",
        "type": "Text_excerpt",
        "value": "This is used to depict where on the plates particular samples are. This is very useful for looking at problems in the data. If for example you find a bunch of sex mismatches this is most likely due to misplating. This script is a quick way of looking at the problem and seeing whether the errors are close together or spread out. There are two input arguments \n* A file with the IDs of the individuals -- assuming that the first token on each line is an individual\n* A sample sheet that gives the plating of each sample \nIn our example, we assumed the ID can found in the column \"Institute Sample Label\" but from the position 18 (indexed from 0) in the string. Change as appropriate for you\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "main documentation",
        "parent_header": [
          "Nextflow update",
          "Outline of documentation"
        ],
        "type": "Text_excerpt",
        "value": "1. Features"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "format": "wiki",
        "type": "Url",
        "value": "https://github.com/h3abionet/h3agwas/wiki"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "download": [
    {
      "confidence": 1,
      "result": {
        "original_header": "3.1.1 If you downloaded using Nextflow",
        "parent_header": [
          "3. Quick start example",
          "3.1 Running on your local computer"
        ],
        "type": "Text_excerpt",
        "value": "We also assume the _sample_ directory with data is in the current working directory\n\n`nextflow run h3abionet/h3agwas/qc/main.nf --input_dir=s3://h3abionet/sample`\n\nIf you have downloaded the sample data and the directory with the sample data is a sub-directory of your working directory, you could just say: `nextflow run h3abionet/h3agwas/qc/main.nf --input_dir=s3://h3abionet/sample`\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "3.1.2 If you downloaded using Git",
        "parent_header": [
          "3. Quick start example",
          "3.1 Running on your local computer"
        ],
        "type": "Text_excerpt",
        "value": "Change directory to the directory in which the workflow was downloaded\n\n`nextflow run  qc`\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Downloading the results from blob storage",
        "parent_header": [
          "5. Running the workflow in different environments",
          "5.8 Running on Azure Batch"
        ],
        "type": "Text_excerpt",
        "value": "Follow the instructions above for generating a SAS token, with the only difference being that now you need \"Read\" and \"List\" permissions.  Then use the SAS token with `azcopy` to download the pipeline output.  For example:\n\n```\nazcopy_windows_amd64_10.9.0\\azcopy.exe copy --recursive \"https://batchstore.blob.core.windows.net/container/output?sp=rl&st=2021-05-17T19:55:25Z&se=2021-05-18T03:55:25Z&spr=https&sv=2020-02-10&sr=c&sig=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" output_azure_2021-05-17\n```\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Download",
        "parent_header": [
          "9. Acknowledgement, Copyright and general",
          "Acknowledgement"
        ],
        "type": "Text_excerpt",
        "value": "`git clone https://github.com/h3abionet/h3agwas`\n\n\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/h3abionet/h3agwas/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "faq": [
    {
      "confidence": 1,
      "result": {
        "original_header": "6. Dealing with errors",
        "type": "Text_excerpt",
        "value": "One problem with our current workflow is that error messages can be obscure. Errors can be caused by\n* bugs in our code\n* your doing something odd\n\nThere are two related problems. When a Nextflow script fails for some reason, Nextflow prints out in _great_ detail what went wrong. Second, we don't always catch mistakes that the user makes gracefully.\n\nFirst, don't panic. Take a breath and read through the error message to see if you can find a sensible error message there. \n\nA typical error message looks something like this\n\n```\nCommand exit status:\n  1\n\nCommand output:\n  (empty)\n\nCommand error:\n  Traceback (most recent call last):\n    File \".command.sh\", line 577, in <module>\n      bfrm, btext = getBatchAnalysis()\n    File \".command.sh\", line 550, in getBatchAnalysis\n      result = miss_vals(ifrm,bfrm,args.batch_col,args.sexcheck_report)\n    File \".command.sh\", line 188, in miss_vals\n      g  = pd.merge(pfrm,ifrm,left_index=True,right_index=True,how='inner').groupby(pheno_col)\n    File \"/usr/local/python36/lib/python3.6/site-packages/pandas/core/generic.py\", line 5162, in groupby\n      **kwargs)\n    File \"/usr/local/python36/lib/python3.6/site-packages/pandas/core/groupby.py\", line 1848, in groupby\n      return klass(obj, by, **kwds)\n    File \"/usr/local/python36/lib/python3.6/site-packages/pandas/core/groupby.py\", line 516, in __init__\n      mutated=self.mutated)\n    File \"/usr/local/python36/lib/python3.6/site-packages/pandas/core/groupby.py\", line 2934, in _get_grouper\n      raise KeyError(gpr)\n\nColumn 'batches' unknown\n\nWork dir:\n  /project/h3abionet/h3agwas/test/work/cf/335b6d21ad75841e1e806178933d3d\n\nTip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`\n\n -- Check '.nextflow.log' file for details\nWARN: Killing pending tasks (1)\n\n```\n\nBuried in this is an error message that might help (did you say there was a column _batches_ in the manifest?) If you're comfortable, you can change directory to the specified directory and explore. There'll you find\n* Any input files for the process that failed\n* Any output files that might have been created\n* The script that was executed can be found in `.command.sh`\n* Output and error can be found as `.command.out` and `.command.err`\n\nIf you spot the error, you can re-run the workflow (from the original directory), appending `-resume`.  Nextflow will re-run your workflow as needed -- any steps that finished successfully will not need to be re-run.\n\nIf you are still stuck you can ask for help at two places\n\n\n* [H3ABioNet Help desk](https://helpdesk.h3abionet.org)   Help on how to do GWAS\n\n\n* On GitHub -- need a GitHub account if you have a GitHub account: problems with the workflow \n\n   https://github.com/h3abionet/h3agwas/issues\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "8.1 Resource allocation problems",
        "parent_header": [
          "8 Troubleshooting"
        ],
        "type": "Text_excerpt",
        "value": "If you get an error like this\n\n```\nProcess `doGemmaChro (18)` terminated with an error exit status (137)\n```\n\nthe typical reason is that you have not allocated enough memory for the process and the operating system has killed the process.\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 65
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/h3abionet/h3agwas/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "h3abionet/h3agwas"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "H3Agwas Pipeline Version 3"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/helperfiles/fake.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/assoc/bin/chi2.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/assoc/bin/logistic.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/assoc/bin/fisher.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/assoc/bin/linear.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/assoc/templates/test.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "identifier": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://zenodo.org/doi/10.5281/zenodo.3235520"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/helperfiles/script_pipeline_all.png"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "2.1 Background",
        "parent_header": [
          "2. Installing H3Agwas"
        ],
        "type": "Text_excerpt",
        "value": "The h3agwas pipeline can be run in different environments; the requirements differ. The different modes are described in detail below\n* Running on Docker/Singularity. This is the easiest way of running h3agwas. We have a set of Docker containers that have all the required executables and libraries.\n* Running natively on a local computer -- this is requires a number of external executables and libraries to be installed..\n* Running with a scheduler -- Nextflow supports a range of schedulers. Our pipeline supports using docker or running natively.\n* Running on Amazon EC2.  You need to have Amazon AWS credentials (and a credit card). Our EC2 pipeline uses Docker so this is very easy to run.\n* We have also used Docker swarm. If you have a Docker swarm it's easy to do.\n\nWe now explore these in details\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "2.3 Installing with Docker or Singularity",
        "parent_header": [
          "2. Installing H3Agwas"
        ],
        "type": "Text_excerpt",
        "value": "If you install Docker or Singularity, you do not need to install all the other dependencies. Docker is available on most major platforms.  See [the Docker documentation](https://docs.docker.com/) for installation for your platform.  Singularity works very well on Linux.\n    \nThat's it. \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "2.4  Installing software dependencies to run natively",
        "parent_header": [
          "2. Installing H3Agwas"
        ],
        "type": "Text_excerpt",
        "value": "This requires a standard Linux installation or macOS. It requires _bash_ to be available as the shell of the user running the pipeline.\n\nThe following code needs to be installed and placed in a directory on the user's PATH.\n\n* plink 1.9 [Currently, it will not work on plink 2, though it is on our list of things to fix. It probably will work on plink 1.05 but just use plink 1.0]\n* LaTeX. A standard installation of texlive should have all the packages you need. If you are installing a lightweight TeX version, you need the following pacakges which are part of texlive.: fancyhdr, datetime, geometry, graphicx, subfig, listings, longtable, array, booktabs, float, url.\n* python 3.6 or later. pandas, numpy, scipy, matplotlib and openpyxl need to be installed. You can instally these by saying: `pip3 install pandas`  etc\n\nIf you want to run other scripts than qc then you should install various software of association, finemaping etc...\n\n[you can find in h3agwas-example a dictatorial to installsoftwares on ubuntu platform](https://github.com/h3abionet/h3agwas-examples/tree/main/requirement/ubuntu)\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "2.5 Installing the workflow",
        "parent_header": [
          "2. Installing H3Agwas"
        ],
        "type": "Text_excerpt",
        "value": "There are two approaches: let Nextflow manage this for you; or download using Git. The former is easier; you need to use Git if you want to change the workflow\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "2.5.1 Managing using Nextflow",
        "parent_header": [
          "2. Installing H3Agwas",
          "2.5 Installing the workflow"
        ],
        "type": "Text_excerpt",
        "value": "To download the workflow you can say\n\n`nextflow pull h3abionet/h3agwas`\n\nIf we update the workflow, the next time you run it, you will get a warning message. You can do another pull to bring it up to date.\n\nIf you manage the workflow this way, you will run the scripts, as follows\n* `nextflow run h3abionet/h3agwas/call2plink/main.nf ..... `\n* `nextflow run h3abionet/h3agwas/qc/main.nf ..... `\n* `nextflow run h3abionet/h3agwas/assoc/main.nf ..... `\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "2.5.2 Managing with Git",
        "parent_header": [
          "2. Installing H3Agwas",
          "2.5 Installing the workflow"
        ],
        "type": "Text_excerpt",
        "value": "Change directory where you want to install the software and say\n\n    git clone https://github.com/h3abionet/h3agwas.git\n\nThis will create a directory called _h3agwas_ with all the necesssary code.\nIf you manage the workflow this way, you will run the scripts this way:\n* `nextflow run SOME-PATH/call2plink ..... `\n* `nextflow run SOME-PATH/qc ..... `\n* `nextflow run SOME-PATH/assoc ..... `\n\nwhere _SOME-PATH_ is a relative or absolute path to where the workflow was downloaded.\n\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9960920050469879,
      "result": {
        "original_header": "Nextflow update",
        "type": "Text_excerpt",
        "value": "***Nextflow 23.X  by default doesn't support anymore dsl1 to run pipeline you need to run NXF_VER=22.10.8 nextflow ...*** \n\nThis means that instead of running `nextflow run h3abionet/h3agwas/assoc.nf`, you should run `nextflow run h3abionet/h3agwas/assoc/main.nf` \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9561505114490033,
      "result": {
        "original_header": "What's new :",
        "type": "Text_excerpt",
        "value": " \n * Version 1.3 updated see [What's new](News.md)\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9973869244047961,
      "result": {
        "original_header": "Background",
        "type": "Text_excerpt",
        "value": "\nWe have moved all scripts from Python 2 to Python 3, so you will need to have Python 3 installed.   \n_Please ignore the Wiki in this version which refers to version 1_ \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8812866783867266,
      "result": {
        "original_header": "Questions and feedback",
        "type": "Text_excerpt",
        "value": "If you need help with using the workflow, please log a call with the [H3A Help Desk](https://www.h3abionet.org/categories/communications/helpdesk). \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8887826188653933,
      "result": {
        "original_header": "Nextflow",
        "type": "Text_excerpt",
        "value": "Our pipeline is built using Nextflow. However, users do not need to know anything about Nextflow. Obviously if you can do some programming you can customise and extend the pipelines, but you do not need to know Nextflow yourself.  \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9999999755168577,
      "result": {
        "original_header": "Docker",
        "type": "Text_excerpt",
        "value": "A GWAS requires several software tools to be installed. Using Docker we can simplify the installation. Essentially, Docker wraps up all software dependancies into _containers_. Instead of installing all the dependancies, you can install Docker, easily and then install our containers. (In fact you don't need to explicitly install our containers, Nextflow and our workflow will do that for you automatically). \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9999999996571773,
      "result": {
        "original_header": "4 The Nextflow configuration file",
        "type": "Text_excerpt",
        "value": "To run your workflow, you need to modify the nextflow.config file, and\nthen run nexflow. Remember, that to make your workflow truly\nreproducible you need to save a copy of the _config_ file. For this\nreason although you can specify many parameters from the command line,\nwe recommend using the config file since this makes your runs\nreproducible.  It may be useful to use git or similar tool to archive\nyour config files.\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9999439167999518,
      "result": {
        "original_header": "4.1 Specifiying an alternative configuration file",
        "type": "Text_excerpt",
        "value": "You can use the _-c_ option specify another configuration file in addition to the nextflow.config file\n```nextflow run -c data1.config qc```\n\n**This is highly recommended.** We recommend that you keep the `nextflow.config` file as static as possible, perhaps not even modifying it from the default config. Then  for any\n run or data set, have a much smaller config file that only specifies the changes you want made. The base `nextflow.config` file will typically contain config options that are best set by the H3Agwas developers (e.g., the names of the docker containers) or default GWAS options that are unlikely to change. In your separate config file, you will specify the run-specific options, such as data sets, directories or particular GWAS parameters you want. Both configuration files should be specified. For example, suppose I create a sub-directory within the directory where the nextflow file is (probably called h3agwas). Within the h3agwas directory I keep my nexflow.config file and the nextflow file itself. From the sub-directory, I run the workflow by saying:\n```nextflow run  -c data1.config ../qc```\n \nThis will automatically use the `nextflow.config` file in either the current or parent directory. Note that the the config files are processed in order: if an option is set into two config files, the latter one takes precedence. \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.977360688579334,
      "result": {
        "original_header": "4.2 Creating an auxiliary nextflow .config file",
        "type": "Text_excerpt",
        "value": "There is a template of a nextflow.config file called aux.config.template. This is a read only file. Make a copy of it, call it _aux.config_ (or some suitable name).  This file contains all the options a user is likely to want to change. It does not specify options like the names of docker containers etc. Of course, you can if you wish modify the nextflow.config file, but we recommend against it. Your auxiliary file should supplement the nextflow.config file. \nThen fill in the details in the config that are required for your run. These are expained in more detail below. \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9978917919273674,
      "result": {
        "original_header": "4.3 Specifying options",
        "type": "Text_excerpt",
        "value": "When you run the the scripts there are a number of different options that you might want to use. These options are specified by using  the `-flag` or `--flag` notation. The flags with a single hyphen (e.g. `-resume`) are standard Nextflow options applicable to all Nextflow scripts. The flags with a double hyphen (e.g., `--pi_hat`) are options that are specific to _our_ scripts.  *Take care not to mix this up as it's an easy error to make, and may cause silent errors to occur.* \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9980872937847791,
      "result": {
        "original_header": "4.5 Cleaning up",
        "type": "Text_excerpt",
        "value": "If you want to clean up your work directory, say `nextflow clean`.\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9295183871618468,
      "result": {
        "original_header": "4.6 Workflow overview, and timing",
        "type": "Text_excerpt",
        "value": "    `nextflow run <pipeline name> -with-timeline time.html` \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9400526732429753,
      "result": {
        "original_header": "7.2 getRunsTimes.pl (By Harry Noyes)",
        "type": "Text_excerpt",
        "value": "Nextflow has great options for showing resourc usage. However, you have to remember to set those option when you run.  It's easy to forget to do this. This very useful script by Harry Noyes (harry@liverpool.ac.uk) parses the .nextflow.log file  for you\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9610878025659525,
      "result": {
        "original_header": "7.3 make_ref.py",
        "type": "Text_excerpt",
        "value": "\n`python3 make_ref.py auxfiles/37/ H3Africa_2017_20021485_A3_StrandReport_FT.txt H3Africa_2017_20021485_A3.csv h3aref201812` \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8299969545356103,
      "result": {
        "original_header": "7.4 plates.py",
        "type": "Text_excerpt",
        "value": "You may need to change this line\n```\nbatches['ID'] = batches['Institute Sample Label'].apply(lambda x:x[18:])\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.851232143111992,
      "result": {
        "original_header": "Goals of the h3agwas pipeline",
        "type": "Text_excerpt",
        "value": "7. `utils/build_example_data` \n   * `utils/build_example_data/main.nf` : extract specifics positions from vcf file and used by default gwas catalog and phenotype to simulate dataset using gcta \n   * `utils/build_example_data/simul-assoc_gcta.nf` : simul phenotype using plink file, gcta and gwas catalog\n   * `utils/build_example_data/simul-assoc_phenosim.nf` : simulation of phenotype using phenosim using random positions \n8. `utils/permutation` \n  * `utils/permutation/permutation-assoc.nf`: do a permutation test to reevaluate p.value with gemma \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9031140397394983,
      "result": {
        "original_header": "7.3 make_ref.py",
        "type": "Text_excerpt",
        "value": "\n`python3 make_ref.py auxfiles/37/ H3Africa_2017_20021485_A3_StrandReport_FT.txt H3Africa_2017_20021485_A3.csv h3aref201812` \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/h3abionet/h3agwas/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "(C) University of the Witwatersrand, Johannesburg, 2016-2018 on behalf of the H3ABioNet Consortium\n\nMIT Licence\n\nCopyright (c) 2018 H3ABioNet\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/LICENSE",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Licence",
        "parent_header": [
          "9. Acknowledgement, Copyright and general",
          "Acknowledgement"
        ],
        "type": "Text_excerpt",
        "value": "This software is licensed under the MIT Licence.\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "logo": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/helperfiles/H3ABioNetlogo2.jpg"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "h3agwas"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "h3abionet"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Nextflow",
        "size": 579044,
        "type": "Programming_language",
        "value": "Nextflow"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 497245,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Perl",
        "size": 228507,
        "type": "Programming_language",
        "value": "Perl"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "R",
        "size": 152974,
        "type": "Programming_language",
        "value": "R"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 850,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Raku",
        "size": 550,
        "type": "Programming_language",
        "value": "Raku"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jeantristanb",
          "type": "User"
        },
        "date_created": "2024-03-20T09:00:28Z",
        "date_published": "2024-04-14T11:30:42Z",
        "description": "Regenie integration and other, see News.MD",
        "html_url": "https://github.com/h3abionet/h3agwas/releases/tag/V3.6.0",
        "name": "V3.6.0",
        "release_id": 151065224,
        "tag": "V3.6.0",
        "tarball_url": "https://api.github.com/repos/h3abionet/h3agwas/tarball/V3.6.0",
        "type": "Release",
        "url": "https://api.github.com/repos/h3abionet/h3agwas/releases/151065224",
        "value": "https://api.github.com/repos/h3abionet/h3agwas/releases/151065224",
        "zipball_url": "https://api.github.com/repos/h3abionet/h3agwas/zipball/V3.6.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jeantristanb",
          "type": "User"
        },
        "date_created": "2022-11-17T14:26:12Z",
        "date_published": "2022-11-22T10:22:41Z",
        "description": "bug correction, see  new.Md",
        "html_url": "https://github.com/h3abionet/h3agwas/releases/tag/v3.4.0",
        "name": "V3.5.0",
        "release_id": 83850473,
        "tag": "v3.4.0",
        "tarball_url": "https://api.github.com/repos/h3abionet/h3agwas/tarball/v3.4.0",
        "type": "Release",
        "url": "https://api.github.com/repos/h3abionet/h3agwas/releases/83850473",
        "value": "https://api.github.com/repos/h3abionet/h3agwas/releases/83850473",
        "zipball_url": "https://api.github.com/repos/h3abionet/h3agwas/zipball/v3.4.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jeantristanb",
          "type": "User"
        },
        "date_created": "2022-07-21T08:16:01Z",
        "date_published": "2022-07-21T08:57:58Z",
        "description": "Version updated after comment of article :  https://doi.org/10.1101/2022.05.02.490206",
        "html_url": "https://github.com/h3abionet/h3agwas/releases/tag/v3.3.0",
        "name": "v3.3.0",
        "release_id": 72503752,
        "tag": "v3.3.0",
        "tarball_url": "https://api.github.com/repos/h3abionet/h3agwas/tarball/v3.3.0",
        "type": "Release",
        "url": "https://api.github.com/repos/h3abionet/h3agwas/releases/72503752",
        "value": "https://api.github.com/repos/h3abionet/h3agwas/releases/72503752",
        "zipball_url": "https://api.github.com/repos/h3abionet/h3agwas/zipball/v3.3.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "shaze",
          "type": "User"
        },
        "date_created": "2022-05-10T11:29:30Z",
        "date_published": "2022-05-10T11:32:07Z",
        "description": "Version updated and partially described in bioRxiv article doi: https://doi.org/10.1101/2022.05.02.490206",
        "html_url": "https://github.com/h3abionet/h3agwas/releases/tag/v3.2.0",
        "name": "v3.2.0",
        "release_id": 66463939,
        "tag": "v3.2.0",
        "tarball_url": "https://api.github.com/repos/h3abionet/h3agwas/tarball/v3.2.0",
        "type": "Release",
        "url": "https://api.github.com/repos/h3abionet/h3agwas/releases/66463939",
        "value": "https://api.github.com/repos/h3abionet/h3agwas/releases/66463939",
        "zipball_url": "https://api.github.com/repos/h3abionet/h3agwas/zipball/v3.2.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "shaze",
          "type": "User"
        },
        "date_created": "2019-05-30T13:53:03Z",
        "date_published": "2019-05-30T14:31:16Z",
        "description": "Bug fixes, improved documentation, expanded association testing",
        "html_url": "https://github.com/h3abionet/h3agwas/releases/tag/v2.2.1",
        "name": "Updated version for DOI",
        "release_id": 17682447,
        "tag": "v2.2.1",
        "tarball_url": "https://api.github.com/repos/h3abionet/h3agwas/tarball/v2.2.1",
        "type": "Release",
        "url": "https://api.github.com/repos/h3abionet/h3agwas/releases/17682447",
        "value": "https://api.github.com/repos/h3abionet/h3agwas/releases/17682447",
        "zipball_url": "https://api.github.com/repos/h3abionet/h3agwas/zipball/v2.2.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "shaze",
          "type": "User"
        },
        "date_created": "2019-05-30T13:53:03Z",
        "date_published": "2019-05-30T14:21:43Z",
        "description": "Bug fixes\r\nExtended documentation\r\nMuch extended assocation testing",
        "html_url": "https://github.com/h3abionet/h3agwas/releases/tag/v2.2",
        "name": "Revised pipeline",
        "release_id": 17682176,
        "tag": "v2.2",
        "tarball_url": "https://api.github.com/repos/h3abionet/h3agwas/tarball/v2.2",
        "type": "Release",
        "url": "https://api.github.com/repos/h3abionet/h3agwas/releases/17682176",
        "value": "https://api.github.com/repos/h3abionet/h3agwas/releases/17682176",
        "zipball_url": "https://api.github.com/repos/h3abionet/h3agwas/zipball/v2.2"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "shaze",
          "type": "User"
        },
        "date_created": "2018-08-21T19:02:18Z",
        "date_published": "2018-08-21T19:22:30Z",
        "html_url": "https://github.com/h3abionet/h3agwas/releases/tag/2.1",
        "name": "August 2018",
        "release_id": 12508244,
        "tag": "2.1",
        "tarball_url": "https://api.github.com/repos/h3abionet/h3agwas/tarball/2.1",
        "type": "Release",
        "url": "https://api.github.com/repos/h3abionet/h3agwas/releases/12508244",
        "value": "https://api.github.com/repos/h3abionet/h3agwas/releases/12508244",
        "zipball_url": "https://api.github.com/repos/h3abionet/h3agwas/zipball/2.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "shaze",
          "type": "User"
        },
        "date_created": "2018-02-06T09:39:40Z",
        "date_published": "2018-02-06T09:41:06Z",
        "html_url": "https://github.com/h3abionet/h3agwas/releases/tag/2.0",
        "name": "Version 2.0",
        "release_id": 9549804,
        "tag": "2.0",
        "tarball_url": "https://api.github.com/repos/h3abionet/h3agwas/tarball/2.0",
        "type": "Release",
        "url": "https://api.github.com/repos/h3abionet/h3agwas/releases/9549804",
        "value": "https://api.github.com/repos/h3abionet/h3agwas/releases/9549804",
        "zipball_url": "https://api.github.com/repos/h3abionet/h3agwas/zipball/2.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "shaze",
          "type": "User"
        },
        "date_created": "2018-01-24T12:37:35Z",
        "date_published": "2018-01-31T19:15:38Z",
        "html_url": "https://github.com/h3abionet/h3agwas/releases/tag/1.0",
        "name": "Master of version 1: December 2017",
        "release_id": 9474036,
        "tag": "1.0",
        "tarball_url": "https://api.github.com/repos/h3abionet/h3agwas/tarball/1.0",
        "type": "Release",
        "url": "https://api.github.com/repos/h3abionet/h3agwas/releases/9474036",
        "value": "https://api.github.com/repos/h3abionet/h3agwas/releases/9474036",
        "zipball_url": "https://api.github.com/repos/h3abionet/h3agwas/zipball/1.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "2.2 Pre-requisites",
        "parent_header": [
          "2. Installing H3Agwas"
        ],
        "type": "Text_excerpt",
        "value": "**All** modes of h3agwas have the following requirements\n* Java 8 or later\n* Nextflow. To install Nextflow, run the command below. It creates a _nextflow_ executable in the directory you ran the command. Move the executable to a directory on the system or user PATH and make it executable. You need to be running Nextflow 27 (January 2018) or later.\n    `curl -fsSL get.nextflow.io | bash`\n\n  If you don't have curl (you can use wget)\n\n* Git (this probably is already installed)\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "2.4  Installing software dependencies to run natively",
        "parent_header": [
          "2. Installing H3Agwas"
        ],
        "type": "Text_excerpt",
        "value": "This requires a standard Linux installation or macOS. It requires _bash_ to be available as the shell of the user running the pipeline.\n\nThe following code needs to be installed and placed in a directory on the user's PATH.\n\n* plink 1.9 [Currently, it will not work on plink 2, though it is on our list of things to fix. It probably will work on plink 1.05 but just use plink 1.0]\n* LaTeX. A standard installation of texlive should have all the packages you need. If you are installing a lightweight TeX version, you need the following pacakges which are part of texlive.: fancyhdr, datetime, geometry, graphicx, subfig, listings, longtable, array, booktabs, float, url.\n* python 3.6 or later. pandas, numpy, scipy, matplotlib and openpyxl need to be installed. You can instally these by saying: `pip3 install pandas`  etc\n\nIf you want to run other scripts than qc then you should install various software of association, finemaping etc...\n\n[you can find in h3agwas-example a dictatorial to installsoftwares on ubuntu platform](https://github.com/h3abionet/h3agwas-examples/tree/main/requirement/ubuntu)\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "2.4  Installing software dependencies to run natively",
        "parent_header": [
          "2. Installing H3Agwas"
        ],
        "type": "Text_excerpt",
        "value": "This requires a standard Linux installation or macOS. It requires _bash_ to be available as the shell of the user running the pipeline.\n\nThe following code needs to be installed and placed in a directory on the user's PATH.\n\n* plink 1.9 [Currently, it will not work on plink 2, though it is on our list of things to fix. It probably will work on plink 1.05 but just use plink 1.0]\n* LaTeX. A standard installation of texlive should have all the packages you need. If you are installing a lightweight TeX version, you need the following pacakges which are part of texlive.: fancyhdr, datetime, geometry, graphicx, subfig, listings, longtable, array, booktabs, float, url.\n* python 3.6 or later. pandas, numpy, scipy, matplotlib and openpyxl need to be installed. You can instally these by saying: `pip3 install pandas`  etc\n\nIf you want to run other scripts than qc then you should install various software of association, finemaping etc...\n\n[you can find in h3agwas-example a dictatorial to installsoftwares on ubuntu platform](https://github.com/h3abionet/h3agwas-examples/tree/main/requirement/ubuntu)\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "3.1 Running on your local computer",
        "parent_header": [
          "3. Quick start example"
        ],
        "type": "Text_excerpt",
        "value": "This requires that all software dependancies have been installed (see later for singularity or docker) \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "3.3 Running with Docker on your local computer",
        "parent_header": [
          "3. Quick start example"
        ],
        "type": "Text_excerpt",
        "value": "Just add `-profile docker` to your run command -- for example, \n\n* `nextflow run  qc -profile docker`   or\n* `nextflow run h3abionet/h3agwas/qc/main.nf --input_dir=s3://h3abionet/sample`\n\nPlease note that the _first_ time you run the workflow using Docker,  the Docker images will be downloaded. *Warning:* This will take about 1GB of bandwidth which will consume bandwidth and will take time depending on your network connection. It is only the first time that the workflow runs that the image will be downloaded.\n\n\nMore options are shown later.\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "3.4 Running multiple workflows at the same time",
        "parent_header": [
          "3. Quick start example"
        ],
        "type": "Text_excerpt",
        "value": "You may at some point want to run multiple, _independent_ executions of the workflows at the same time (e.g. different data). This is possible. However, each run should be started in a different working directory. You can refer to the scripts and even the data in the same diretory, but the directories from which you run the `nextflow run` command should be different.\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "5. Running the workflow in different environments",
        "type": "Text_excerpt",
        "value": "In the  quick start we gave an overview of running our workflows in different environments. Here we go through all the options, in a little more detail\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "5.1 Running natively on a machine",
        "parent_header": [
          "5. Running the workflow in different environments"
        ],
        "type": "Text_excerpt",
        "value": "This option requires that all dependancies have been installed. You run the code by saying\n\n````\nnextflow run qc\n````\n\nYou can add that any extra parameters at the end.\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "5.2 Running  on a local machine with Docker",
        "parent_header": [
          "5. Running the workflow in different environments"
        ],
        "type": "Text_excerpt",
        "value": "This requires the user to have docker installed.\n\nRun by `nextlow run qc -profile docker`\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "5.3 Running on a cluster",
        "parent_header": [
          "5. Running the workflow in different environments"
        ],
        "type": "Text_excerpt",
        "value": "Nextflow supports execution on clusters using standard resource managers, including Torque/PBS, SLURM and SGE. Log on to the head node of the cluster, and execute the workflow as shown below. Nextflow submits the jobs to the cluster on your behalf, taking care of any dependancies. If your job is likely to run for a long time because you've got really large data sets, use a tool like _screen_ to allow you to control your session without timing out.\n\nOur workflow has pre-built configuration for SLURM and Torque/PBS. If you use another scheduler that Nextflow supports you'll need to do a _little_ more (see later): see https://www.nextflow.io/docs/latest/executor.html for details\n\nTo run using Torque/PBS, log into the head node. Edit the _nextflow.config_ file, and change the `queue` variable to be the queue you will run jobs on (if you're not sure of this, ask your friendly sysadmin). Then when you run, our workflow, use the `-profile pbs` option -- typically you would say something like `nextflow run -c my.config qc -profile pbs`. Note that the `-profile pbs` only uses a single \"-\".\n\nSimilarily, if you run SLURM, set the _queue_ variable, and use the `-profile slurm` option.\n\nTo use only of the other schedulers supported by Nextflow, add the following sub-stanza to your nextflow.config file inside of the _profile_ stanza:\n\n\n\n```\n    myscheduler {\n        process.executor = 'myscheduler'\n\tprocess.queue = queue\n    }\n```\n\nwhere `myscheduler` is one of: nqsii, htcondor, sge, lsf.  \n\n\nand  then use this as the profile.\n\n\nWe assume all the data is visible to all nodes in the swarm. Log into the head node of the Swarm and run your chosed workflow -- for example\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "5.4 Running on Docker Swarm",
        "parent_header": [
          "5. Running the workflow in different environments"
        ],
        "type": "Text_excerpt",
        "value": "We have tested our workflow on different Docker Swarms. How to set up Docker Swarm is beyond the scope of this tutorial, but if you have a Docker Swarm, it is easy to run. From the head node of your Docker swarm, run\n\n```\nnextflow run qc -profile dockerSwarm\n```\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "5.5 Singularity",
        "parent_header": [
          "5. Running the workflow in different environments"
        ],
        "type": "Text_excerpt",
        "value": "\nOur workflows now run easily with Singularity.\n\n`nextflow run qc -profile singularity`\n\nor\n\n`nextflow run qc -profile pbsSingularity`\n\nBy default the user's ${HOME}/.singularity will be used as the cache for Singularity images. If you want to use something else, change the `singularity.cacheDir` parameter in the config file.\n\n\nIf you have a cluster which runs Docker, you can get the best of both worlds by editing the queue variable in the _pbsDocker_ stanza, and then running\n\n```\nnextflow run qc -profile option\n```\n\nwhere _option_ is one of _pbsDocker_, _pbsSingularity_, _slurmDocker_ or _slurmSingularity_. If you use a different scheduler, read the Nextflow documentation on schedulers, and then use what we have in the _nextflow.config_ file as a template to tweak.\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "5.5 Other container services",
        "parent_header": [
          "5. Running the workflow in different environments"
        ],
        "type": "Text_excerpt",
        "value": "We are unlikely to support udocker unless Nextflow does. See this link for a discussion https://www.nextflow.io/blog/2016/more-fun-containers-hpc.html\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "5.6 Running on Amazon EC2",
        "parent_header": [
          "5. Running the workflow in different environments"
        ],
        "type": "Text_excerpt",
        "value": "\nNextflow supports execution on Amazon EC2. Of course, you can do your own custom thing on Amazon EC2, but there is direct support from Nextflow and  we provide an Amazon AMI that allows you to use Amazon very easilyl. This discussion assumes you are familiar with Amazon and EC2 and shows you how to run the workflow on EC2:\n\n\n1. We assume you have an Amazon AWS account and have some familiariy with EC2. The easiest way to run is by building am Amazon Elastic File System (EFS) which persists between runs. Each time you run,  you attach the EFS to the cluster you use. We assume you have\n* Your Amazon accessKey and secretKey\n* you have the ID of your EFS\n* you have the ID of the subnet you will use for your Amazon EC2.\n\n    Edit the  nextflow config file to add your keys to the _aws_ stanza, as well as changing the AMI ID, sharedStorageID, the mount and subnet ID. *BUT see point 8 below for a better way of doing things*.\n    ```\n    aws {\n       accessKey ='AAAAAAAAAAAAAAAAA'\n       secretKey = 'raghdkGAHHGH13hg3hGAH18382GAJHAJHG11'\n       region    ='eu-west-1'\n    }\n\n    cloud {\n             ...\n             ...\n             ... other options\n             imageId = \"ami-710b9108\"      // AMI which has cloud-init installed\n             sharedStorageId   = \"fs-XXXXXXXXX\"   // Set a common mount point for images\n             sharedStorageMount = \"/mnt/shared \n   \t     subnetId = \"subnet-XXXXXXX\" \n    }\n     ```\n\n    Note that the AMI is the H3ABionet AMI ID, which you should use. The other information such as the keys, sharedStorageID and subnetID you have to set to what you have.\n\nThe instructions below assume you are using nextflow. If you launch the machine directly, the user will be `ec2-user`; if you use the instructions below, you will be told who the user on Amazon instance is (probably the same userid as your own machine).\n\n2. Create the cloud. For the simple example, you only need to have one machine. If you have many, big files adjust accordingly.\n\n    `nextflow cloud create h3agwascloud -c 1`\n \n    The name of the cluster is your choice (_h3agwascloud_ is your choice).\n    \n\n3.  If successful, you will be given the ID of the headnode of the cluster to log in. You should see a message like,\n\n```\n> cluster name: h3agwascloud\n> instances count: 1\n> Launch configuration:\n - bootStorageSize: '20GB'\n - driver: 'aws'\n - imageId: 'ami-710b9108'\n - instanceType: 'm4.xlarge'\n - keyFile: /home/user/.ssh/id_rsa.pub\n - sharedStorageId: 'fs-e17f461c'\n - sharedStorageMount: '/mnt/shared'\n - subnetId: 'subnet-b321c8c2'\n - userName: 'scott'\n - autoscale:\n   - enabled: true\n   - maxInstances: 5\n   - terminateWhenIdle: true\n\n\n\nPlease confirm you really want to launch the cluster with above configuration [y/n] y\nLaunching master node -- Waiting for `running` status.. ready.\nLogin in the master node using the following command: \n  ssh -i /home/scott/.ssh/id_rsa scott@ec2-54-246-155-85.eu-west-1.compute.amazonaws.com\n```\n\n4. ssh into the head node of your Amazon cluster. The EFS is mounted onto `/mnt/shared`. In our example, we will analyse the files _sampleA.{bed,bim,fam}_ in the /mnt/shared/input directory  The  _nextflow_ binary will be found in your home directory. (Note that you can choose to mount the EFS on another mount point by modifying the nextflow option `sharedStorageMount`;\n\n5. For real runs, upload any data you need. I suggest you put in the /mnt/shared directory, and do not put any data output on the home directory. Yo\n\n5. Run the workflow -- you can run directly from github. The AMI doesn't have any of the bioinformatics software installed. \n\n    Specify the docker profile and nextflow will run using Docker, fetching any necessary images.\n\n    Do `nextflow pull h3abionet/h3agwas`\n\n    This pull is not strictly necessary the first time you run the job, but it's a  good practice to get into to check if there are updates.\n\n6. Then run the workflow\n\n    `nextflow run  h3abionet/h3agwas -profile docker --input_dir=/mnt/shared/XXXXX/projects/h3abionet/h3agwas/input/ --work_dir=/mnt/shared `\n\n   You will need to replace XXXXX with your userid -- the local copy of the repo is found in the `/mnt/shared/XXXXX/projects/h3abionet/h3agwas/` directory. But we want the work directory to be elsewhere.\n\n   Of course, you can also use other parameters (e.g. -resume or --work_dir). For your own run you will want to use your nextflow.config file.\n\n\n   __ Need to change : By default, running the workflow like this runs the `qc` script. If you want to run one of the other scripts you would say `nextflow run  h3abionet/h3agwas/topbottom.nf` or `nextflow run h3abionet/h3agwas/assoc.nf` etc. __\n\n\n7. The output of the default runcan be found in` /mnt/shared/output`. The file sampleA.pdf is a report of the analysis that was done.\n\n8. Remember to shutdown the Amazon cluster to avoid unduly boosting Amazon's share price.\n\n    `nextflow cloud shutdown h3agwascloud`\n\n\n9. _Security considerations_: Note that your Amazon credentials should be kept confidential. Practically this means adding the credentials to your _nextflow.config_ file is a bad idea, especially if you put that under git control or if you share your nextflow scripts. So a better way of handling this is to  put confidential information in a separate file that you don't share. So I have a file called _scott.aws_\nwhich has the following:\n```\naws {\n    accessKey ='APT3YGD76GNbOP1HSTYU4'\n    secretKey = 'WHATEVERYOURSECRETKEYISGOESHERE'\n    region    ='eu-west-1'\n}\n\ncloud {\n            sharedStorageId   = \"fs-XXXXXX\"   \n\t    subnetId = \"subnet-XXXXXX\" \n}\n\n```\nThen when you create your cloud you say this on your local machine\n  \n  `nextflow -c scott.aws -c run10.config cloud create scottcluster -c 5`\n\nNote there are two uses of `-c`. The positions of these arguments are crucial. The first are arguments to _nextflow_ itself and gives the configuration files that nextflow to use. The second is an argument to _cloud create_ which says how many nodes should be created. \n\nThe _scott.aws_ file is not shared or put under git control. The _nextflow.config_ and _run10.config_ files can be archived, put under git control and so on because you _want_ to share and archive this information with o thers.\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "5.7 Running on AWS Batch",
        "parent_header": [
          "5. Running the workflow in different environments"
        ],
        "type": "Text_excerpt",
        "value": "AWS Batch is a service layered on top of EC2 by Amazon which may make it easier and / or cheaper than using EC2. My personal view is that if you are only our pipeline on Amazon and you have reasonable Linux experience then the EC2 implementation above is probably easier. However, if you use or plan to use AWS Batch for other services then, AWS Batch is a definite option.\n\nSee our [batch documentation](https://github.com/h3abionet/h3agwas/blob/master/Readme_AWS_Batch.md)\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Uploading your data",
        "parent_header": [
          "5. Running the workflow in different environments",
          "5.8 Running on Azure Batch"
        ],
        "type": "Text_excerpt",
        "value": "Uploading your data to Blob Storage is optional.  You can read and write to local storage while using Azure batch computing, although Nextflow will still need a work directory in Blob Storage.\n\nThe easiest way to get your data onto and off of Azure Blob Storage is generally to use the [AzCopy](https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10) program.  It can be downloaded as an executable file that does not require installation (although you may want to add its location to your `PATH` variable).\n\nYou will need a [SAS (shared access signature) token](https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview) to transfer data.  This is a temporary code that grants you customizable permissions.  Go to https://portal.azure.com/#home, select \"Storage Accounts\", and click on the storage account you want to use (e.g. `batchstore`).  Then click \"Containers\".  Click the name of the container you want to use (e.g. `container`).  If there are any files there already, you will see them.  (You can also upload and download from here, although I couldn't figure out how to make folders, which is why I recommend `azcopy`.)  Then in the left pane click \"Shared access signature\".  You'll see a dialogue that gives you options for customizing your SAS.  Be sure to add \"create\" and \"list\" permissions.  I did not need to add an IP address when I tried it.  Click \"Generate SAS token and URL\".  Copy the Blob SAS URL.\n\nYou can test out `azcopy` with the `list` command.  Here is an example on my Windows machine, where I had not updated my `PATH` variable.\n\n```\nazcopy_windows_amd64_10.9.0\\azcopy.exe list \"https://batchstore.blob.core.windows.net/container?sp=rcl&st=2021-03-25T14:42:42Z&se=2021-03-25T22:42:42Z&spr=https&sv=2020-02-10&sr=c&sig=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx%3D\"\n```\n\nIf there are any files there already, you should see them listed.  Now you can try the `copy` command.  Use the `--recursive` flag to upload an entire folder (the `sample` folder in the example below).\n\n```\nazcopy_windows_amd64_10.9.0\\azcopy.exe copy --recursive sample \"https://batchstore.blob.core.windows.net/container?sp=rawdl&st=2021-03-24T20:46:30Z&se=2021-03-25T04:46:30Z&spr=https&sv=2020-02-10&sr=c&sig=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx%3D\"\n```\n\nNow you can run `list` again or look in the web browser to see that the files have been uploaded.\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Access keys",
        "parent_header": [
          "5. Running the workflow in different environments",
          "5.8 Running on Azure Batch"
        ],
        "type": "Text_excerpt",
        "value": "To run the Nextflow pipeline with Azure Batch, you will need access keys both for your batch account and your storage account.  At https://portal.azure.com/#home, go to Batch Accounts and then the name of your account (e.g. `h3abionet`), then click \"Keys\" on the left pane.  Here you should be able to copy a key for your batch account as well as your storage account.  Paste them into a plain text file to hang onto it for now.\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Auxilliary config file",
        "parent_header": [
          "5. Running the workflow in different environments",
          "5.8 Running on Azure Batch"
        ],
        "type": "Text_excerpt",
        "value": "You should create an auxilliary config file with your credentials.  I named mine `lindsay.azure.config`.  (You could instead edit the `nextflow.config` file, but that's a little more dangerous in terms of accidentally sharing your keys via GitHub.)  You should specify either `endpoint` or `location`, but not both.  An example file is below.\n\n```\nazure {\n  storage {\n    accountName = \"batchstore\"\n    accountKey = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx==\"\n  }\n  batch {\n    endpoint='https://h3abionet.eastus.batch.azure.com'\n    accountName = 'h3abionet'\n    accountKey = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx=='\n    autoPoolMode = true\n    allowPoolCreation = true\n    deletePoolsOnCompletion = true\n  }\n}\n```\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Running the pipeline",
        "parent_header": [
          "5. Running the workflow in different environments",
          "5.8 Running on Azure Batch"
        ],
        "type": "Text_excerpt",
        "value": "Once all of the above has been set up, you can run the pipeline.  Below is an example command using Blob Storage.\n\n```\nnextflow run h3abionet/h3agwas/qc/main.nf -c lindsay.azure.config -w az://container/workdir -profile azurebatch --work_dir az://container --input_dir az://container/sample\n```\n\nHere is an alternative using local storage.\n\n```\nnextflow run h3abionet/h3agwas/qc/main.nf -c lindsay.azure.config -w az://container//workdir2 -profile azurebatch --work_dir . --input_dir ./sample\n```\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "contact",
    "contributors",
    "support",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 05:38:59",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 104
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Example and data-set",
        "parent_header": [
          "Nextflow update"
        ],
        "type": "Text_excerpt",
        "value": "Data-set example can be found in [h3agwas-examples github](https://github.com/h3abionet/h3agwas-examples)\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "3. Quick start example",
        "type": "Text_excerpt",
        "value": "This section shows a simple run of the `qc` pipeline that\nshould run out of the box if you have installed the software or\nDocker or Singularity. More details and general configuration will be shown later.\n\nThis section illustrates how to run the pipeline on a small sample data\nfile with default parameters.  For real runs, the data to be analysed\nand the various parameters to be used are specified in the\n_nextflow.config_ files in assoc, qc and call2plink folder.  The details will be explained in another\nsection.\n\n\nOur quick start example will fetch the data from an Amazon S3 bucket,\nbut if you'd prefer then you use locally installed sample. If you have\ndownloaded the software using Git, you can find the sample data in the\ndirectory. Otherwise you can download the files from\nhttp://www.bioinf.wits.ac.za/gwas/sample.zip and unzip The sample data\nto be used is in the _input_ directory (in PLINK format as\n_sampleA.bed_, _sampleA.bim_, _sampleA.fam_). The default\n_nextflow.config_ file uses this, and so you can run the workflow\nthrough with this example. Note that this is a very small PLINK data\nset with no X-chromosome information and no sex checking is done.\n\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "3.2 Remarks",
        "parent_header": [
          "3. Quick start example"
        ],
        "type": "Text_excerpt",
        "value": "The workflow runs and output goes to the _output_ directory. In the\n_sampleA.pdf_ file, a record of the analysis can be found.\n\nIn order, to run the workflow on another PLINK data set, say _mydata.{bed,bim,fam}_, say\n\n`nextflow run  qc --input_pat mydata`\n\n(or `nextflow run  h3abionet/h3agwas/qc --input_pat mydata` : **for simplicity for the rest of the tutorial we'll only present the one way of running the workflow -- you should use the method that is appropriate for you**)\n\nIf the data is another directory, and you want to the data to go elsehwere:\n\n`nextflow run  qc --input_pat mydata --input_dir /data/project10/ --output_dir ~/results `\n\nThere are many other options that can be passed on the the command-line. Options can also be given in the _config_ file (explained below). We recommend putting options in the configuration file since these can be archived, which makes the workflow more portable\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "workflows": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/finemapping/cojo-assoc.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/finemapping/finemap_region.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/finemapping/cond-assoc.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/finemapping/cond-gcta.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/finemapping/main.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/call2plink/main.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/qc/main.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/heritabilities/main.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/assoc/main.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/meta/mtag-assoc.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/meta/meta-assoc.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/formatdata/format_gwasfile.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/formatdata/vcf_in_plink.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/utils/permutation/main.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/utils/build_example_data/simul-assoc_gcta.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/utils/build_example_data/main.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/utils/build_example_data/simul-assoc_phenosim.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/utils/annotation/annot-assoc.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/h3agwas/master/replication/gwascat/main.nf"
      },
      "technique": "file_exploration"
    }
  ]
}