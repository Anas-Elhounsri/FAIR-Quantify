{
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Reference",
        "parent_header": [
          "Description of scripts"
        ],
        "type": "Text_excerpt",
        "value": "If you found this code useful, please cite:\n\n```@article{tuckute2021real,\n  title={Real-Time Decoding of Attentional States Using Closed-Loop EEG Neurofeedback},\n  author={Tuckute, Greta and Hansen, Sofie Therese and Kjaer, Troels Wesenberg and Hansen, Lars Kai},\n  journal={Neural Computation},\n  pages={1--38},\n  year={2021}\n}\n```\n\n*Cognitive Systems, Department of Applied Mathematics and Computer Science, Technical University of Denmark, 2018-19* \n\nIn collaboration with Dr. [Sofie Therese Hansen](https://github.com/STherese) and Professor Lars Kai Hansen.\n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/gretatuckute/ClosedLoop"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-03-27T10:15:52Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-06-28T16:25:53Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Framework for building a closed loop neurofeedback system."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "settings.py",
        "parent_header": [
          "Description of scripts"
        ],
        "type": "Text_excerpt",
        "value": "Set up experimental, recording, preprocessing and classification parameters.\n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "runSystem.py",
        "parent_header": [
          "Description of scripts"
        ],
        "type": "Text_excerpt",
        "value": "*runSystem.py* must be started before *runExperiment.py*, since it waits for a marker (trigger point for stimuli onset) from the experimental script which is called in *runExperiment.py*.\n\n*runSystem.py* will stream the EEG data. The EEG sampling rate, number of channels and epoch length can be changed in this script. Based on the experimental structure, the script will change the system states among:\n\n1) \u2018stable\u2019 (Data acquisition: Recording of EEG data. No feedback provided)\n2) \u2018train\u2019 (Classification: Identification of artifactual data components and training of the decoding classifier)\n3) \u2018feedback\u2019 (Neurofeedback: Update of the stimulus on a trial-to-trial basis based on the decoding classifier)\n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "realtimeFunctions.py",
        "parent_header": [
          "Description of scripts"
        ],
        "type": "Text_excerpt",
        "value": "Functions for working with real-time EEG data in Python:\nStandardizing, scaling, artifact correction (SSP projection), preprocessing, classification.\nThe functions are called in *runClosedLoop.py*.\n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "streamFunctions.py",
        "parent_header": [
          "Description of scripts"
        ],
        "type": "Text_excerpt",
        "value": "Functions for finding the EEG stream and the experimental stream containing markers (trigger points for stimuli onset, from experimental script: *experimentFunctions.py*), saving data, writing log files and changing system states for the neurofeedback system.\nThe functions are called in *runClosedLoop.py*.\n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "runExperiment.py",
        "parent_header": [
          "Description of scripts"
        ],
        "type": "Text_excerpt",
        "value": "*runExperiment.py* starts the experimental script. As explained in **Experimental paradigm**, the paradigm consists of behavioral days (day 1 and 3), and a neurofeedback day (day 2). \n\nFor the behavioral experiment, the function *runBehDay* from *experimentFunctions.py* is used.  \nFor the neurofeedback experiment, the function *runNFday* from *experimentFunctions.py* is used.\n\nA simple .txt file named \u201cfeedback_subjID_01.txt\u201d has to be located in the subject\u2019s folder containing 1 in the first row and their own subject ID in the second line (example provided in \\ClosedLoop\\subjectsData\\01\\). \nThis feedback .txt file provides an opportunity to make participants function as controls, and hence receive yoked, sham neurofeedback (feedback based on another participant\u2019s brain response). In this case, the .txt file has to contain 0 in the first row, and the subject ID of the matched neurofeedback participant in the second row.\n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "experimentFunctions.py",
        "parent_header": [
          "Description of scripts"
        ],
        "type": "Text_excerpt",
        "value": "Functions for running the PsychoPy experimental script and generating stimuli for the experiment.\nThe functions are called in *runExperiment.py*.\n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Dependencies/acknowledgements:",
        "parent_header": [
          "Description of scripts"
        ],
        "type": "Text_excerpt",
        "value": "- [PsychoPy](https://www.psychopy.org/)\n- [MNE](https://mne-tools.github.io/stable/index.html) \n- [Lab Streaming Layer](https://github.com/sccn/labstreaminglayer)\n- [NumPy](https://www.numpy.org/)\n- [Scikit-Learn](https://scikit-learn.org/stable/)\n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9639579693313176,
      "result": {
        "original_header": "Closed-Loop EEG Neurofeedback System",
        "type": "Text_excerpt",
        "value": "This GitHub contains code for running a closed-loop, real-time EEG neurofeedback system.  \nA video demo of the neurofeedback system is available [here](https://www.youtube.com/watch?v=Ns8AHIg_Wtc&feature=youtu.be). Sample data is available [here](https://data.mendeley.com/datasets/sfwkmvmmd5/1). \nThe framework is:\n1. Written using exclusively Python source code for a high degree of modularity and transparency.\n2. A fully automated, real-time implementation without the need for any manual throughout the neurofeedback session.\n3. Not dependent on any preliminary EEG recordings prior to the neurofeedback session.\n4. Feasible to run using consumer-grade, portable EEG acquisition equipment with dry electrodes.\n5. Robust to decoding of cognitive states across intra- and inter-individidual variability. \nThe framework is initially designed for a visual attention paradigm ([deBettencourt et al., 2015](https://www.nature.com/articles/nn.3940)), where the subjective attentional state of the participant is decoded in real-time and used to update the subsequent visual stimuli to modulate the attentional state using feedback. \n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8562849326238695,
      "result": {
        "original_header": "Stimuli",
        "type": "Text_excerpt",
        "value": "The current paradigm uses composite images (an overlay of two images from different categories). Any image stimuli can be used. The stimuli are provided by a .csv file, as illustrated below: \nThe number of rows correspond to the number of experimental trials. \nFor the current paradigm, the .csv file can be generated using the script *prepareImageStimuli.py*. This requires images in the folders in \\ClosedLoop\\imageStimuli\\. The script assumes four different image categories: male, female, indoor, outdoor (*prepareImageStimuli.py*). For the face images we used the [FERET database](https://www.nist.gov/itl/iad/image-group/color-feret-database) and for the scene images we used the [SUN database](https://groups.csail.mit.edu/vision/SUN/).\nThe .csv file will be saved in  \\ClosedLoop\\subjectsData\\subjectID\\ (corresponding to the subjectID specified in *settings.py*. \nNon-composite images can also be generated using the function *createNonFusedIndices* in *experimentFunctions.py* and run using the function *prepNonFusedImage* instead of fuseImage in either runBehDay for the behavioral paradigm, and runNFday for the neurofeedback paradigm.\n \n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9550195287102448,
      "result": {
        "original_header": "Experimental paradigm",
        "type": "Text_excerpt",
        "value": "Participants had to respond to and, by extension, focus their attention towards subcategories of faces: female and male, and scenes: indoor and outdoor. Stimuli were composite images of the two categories (equal image mixture ratio, alpha, during training blocks). During feedback blocks, the decoded task-relevant EEG representation was used to continuously update the image mixture of the stimuli in a closed-loop manner. If participants attended well (high levels of task-relevant information in their brain) the task-relevant image became easier to see, and vice versa (see [deBettencourt et al., 2015](https://www.nature.com/articles/nn.3940)). \nThe experimental structure for the neurofeedback day is as follows: \nParticipants completed six task runs consisting of eight blocks each. Every block (graded box) consisted of 50 trials displayed successively for one second each. The blocks with a red, dashed outline are denoted as 'stable' blocks, and the blue blocks are denoted as 'feedback' blocks.  \nAll experimental parameters (e.g. number of runs, blocks, images within each block, stimuli presentation time) can be changed in *settings.py*.   \nFor the neurofeedback paradigm, the first 12 blocks (illustrated as boxes with a red, dashed outline) are used for recording EEG data. A classifier is trained based on these blocks, and used to provide feedback in the subsequent neurofeedback blocks (illustrated as boxes with a blue, dashed outline). This is followed by runs consisting of 4 blocks of recording EEG (\u2018stable\u2019 blocks) and 4 blocks of providing feedback (\u2018feedback\u2019 blocks). \n \n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9353460942372089,
      "result": {
        "original_header": "Saving data",
        "type": "Text_excerpt",
        "value": "The data will be stored and saved in the subject's folder \\ClosedLoop\\subjectsData\\subjectID\\. The .csv file containing image stimuli also has to be located in this folder (see **Stimuli**). \n*runExperiment.py* will save the following files after a completed neurofeedback session:\n-\tsubject_01_EEG_TIMESTAMP.csv: All of the recorded, raw EEG data. \n-\tsubject_01_marker_TIMESTAMP.csv: All the markers (time points of stimuli/EEG epoch onset) for experimental trials. \n-\tstream_logfile_subject_01_TIMETAMP.log: A log of the EEG streaming (created by runClosedLoop.py and streamFunctions.py).\n-\timageTime_subjID_01_day_2_TIMESTAMP.csv: All time points of stimuli onsets. Same clock as in the two files below.\n-\tPsychoPyLog_subjID_01_day_2_TIMESTAMP.csv: Log of all changes in the experimental window (using [PsychoPy logging](http://www.psychopy.org/coder/codeLogging.html)).\n-\tKeypress_subjID_01_day_2_TIMESTAMP.csv: Time points for all recorded keypresses during the experiment.\n-\talpha_subjID_01.csv: The computed alpha values (image mixture interpolation factor, see **Experimental paradigm**) used to update image stimuli during \u2018feedback\u2019 blocks.\n-\tMEAN_alpha_subjID_01_day_2_TIMESTAMP.csv: The mean alpha values used to update image stimuli during \u2018feedback\u2019 blocks. \n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/gretatuckute/ClosedLoop/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 12
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/gretatuckute/ClosedLoop/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "gretatuckute/ClosedLoop"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Closed-Loop EEG Neurofeedback System"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop//master/systemComponents.png"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop//master/createIndices_example.PNG"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop//master/experimentalParadigm.png"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.8893379459886511,
      "result": {
        "original_header": "Closed-Loop EEG Neurofeedback System",
        "type": "Text_excerpt",
        "value": "The system can easily be adapted for various visual paradigms (see **Stimuli**).\n \n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/gretatuckute/ClosedLoop/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "ClosedLoop"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "gretatuckute"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 94879,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Running the system",
        "parent_header": [
          "Closed-Loop EEG Neurofeedback System"
        ],
        "type": "Text_excerpt",
        "value": "1) Set up experimental, recording, preprocessing and classification parameters in *settings.py*.\n2) Start recording using the EEG equipment (we used [Enobio 32](https://www.neuroelectrics.com/products/enobio/). The most widely used M/EGG hardware devices are supported though Lab Streaming Layer, LSL).\n3)\tStart *runSystem.py* (the script will wait for initialization of step 3). \n4)\tIn a different terminal/console, start *runExperiment.py* (initializes the experimental script). \n\nVisualization of the major components of the system:\n\n![](systemComponents.png)\n\nThe system requires images for the visual paradigm (see **Stimuli**). The recorded EEG data will be saved in the \\ClosedLoop\\subjectsData folder (see \u201cSaving data\u201d).\n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Additional scripts (not necessary for running the system)",
        "parent_header": [
          "Description of scripts"
        ],
        "type": "Text_excerpt",
        "value": "1)\t*randomizeParticipants.py*: Functions for automated randomization of participants into 'feedback' participants or matched controls. Generates the \u201cfeedback_subjID_X.txt file in the subject's folder \\ClosedLoop\\subjectData\\subjectID\\, denoting whether a participant is feedback (first row containing 1) or control (first row containing 0).\n\n2) Offline analysis: Code for offline analyses of data (EEG data as well as behavioral data) can be found in the [ClosedLoop_Offline GitHub](https://github.com/gretatuckute/ClosedLoop_Offline).\n"
      },
      "source": "https://raw.githubusercontent.com/gretatuckute/ClosedLoop/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 17:27:39",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 19
      },
      "technique": "GitHub_API"
    }
  ]
}