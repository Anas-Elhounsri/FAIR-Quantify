{
  "citation": [
    {
      "confidence": 1,
      "result": {
        "format": "bibtex",
        "type": "File_dump",
        "value": "@article{10.1371/journal.pcbi.1008640,\n    doi = {10.1371/journal.pcbi.1008640},\n    author = {Ras, Verena AND Botha, Gerrit AND Aron, Shaun AND Lennard, Katie AND Allali, Imane AND Claassen-Weitz, Shantelle AND Mwaikono, Kilaza Samson AND Kennedy, Dane AND Holmes, Jessica R. AND Rendon, Gloria AND Panji, Sumir AND Fields, Christopher J AND Mulder, Nicola},\n    journal = {PLOS Computational Biology},\n    publisher = {Public Library of Science},\n    title = {Using a multiple-delivery-mode training approach to develop local capacity and infrastructure for advanced bioinformatics in Africa},\n    year = {2021},\n    month = {02},\n    volume = {17},\n    url = {https://doi.org/10.1371/journal.pcbi.1008640},\n    pages = {1-11},\n    abstract = {With more microbiome studies being conducted by African-based research groups, there is an increasing demand for knowledge and skills in the design and analysis of microbiome studies and data. However, high-quality bioinformatics courses are often impeded by differences in computational environments, complicated software stacks, numerous dependencies, and versions of bioinformatics tools along with a lack of local computational infrastructure and expertise. To address this, H3ABioNet developed a 16S rRNA Microbiome Intermediate Bioinformatics Training course, extending its remote classroom model. The course was developed alongside experienced microbiome researchers, bioinformaticians, and systems administrators, who identified key topics to address. Development of containerised workflows has previously been undertaken by H3ABioNet, and Singularity containers were used here to enable the deployment of a standard replicable software stack across different hosting sites. The pilot ran successfully in 2019 across 23 sites registered in 11 African countries, with more than 200 participants formally enrolled and 106 volunteer staff for onsite support. The pulling, running, and testing of the containers, software, and analyses on various clusters were performed prior to the start of the course by hosting classrooms. The containers allowed the replication of analyses and results across all participating classrooms running a cluster and remained available posttraining ensuring analyses could be repeated on real data. Participants thus received the opportunity to analyse their own data, while local staff were trained and supported by experienced experts, increasing local capacity for ongoing research support. This provides a model for delivering topic-specific bioinformatics courses across Africa and other remote/low-resourced regions which overcomes barriers such as inadequate infrastructures, geographical distance, and access to expertise and educational materials.},\n    number = {2},\n\n}"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/CITATION.bib",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Credits",
        "parent_header": [
          "TADA - Targeted Amplicon Diversity Analysis using DADA2, implemented in Nextflow"
        ],
        "type": "Text_excerpt",
        "value": "The initial implementation of the DADA2 pipeline as a Nextflow workflow (https://github.com/HPCBio/16S-rDNA-dada2-pipeline) was done by Chris Fields from the High Performance Computating in Biology group at the University of Illinois (http://www.hpcbio.illinois.edu). Please remember to cite the authors of [DADA2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4927377/) when using this pipeline. Further development to the Nextflow workflow and containerisation in Docker and Singularity for implementation on UCT's HPC was done by Dr Katie Lennard and Gerrit Botha, with inspiration and code snippets from Phil Ewels http://nf-co.re/\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/h3abionet/TADA"
      },
      "technique": "GitHub_API"
    }
  ],
  "contributors": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Contributors",
        "parent_header": [
          "TADA - Targeted Amplicon Diversity Analysis using DADA2, implemented in Nextflow"
        ],
        "type": "Text_excerpt",
        "value": "The following have contributed to the development, testing, and deployment of this workflow. For the most up-to-date listing see the [Contributors](https://github.com/h3abionet/TADA/graphs/contributors) link.\n\n* [Katie Lennard](https://github.com/kviljoen)\n* [Gerrit Botha](https://github.com/grbot)\n* [Chris Fields](https://github.com/cjfields)\n* [Jessica Holmes](https://github.com/jrkirk61)\n* [Gloria Rendon](https://github.com/grendon)\n* [Lindsay Clark](https://github.com/lvclark)\n* [Wojtek Ba\u017cant](https://github.com/wbazant)\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-10-31T14:34:23Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-07-06T14:17:13Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "TADA - Targeted Amplicon Diversity Analysis - a DADA2-focused Nextflow workflow for any targeted amplicon region"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9191087344043464,
      "result": {
        "original_header": "TADA - Targeted Amplicon Diversity Analysis using DADA2, implemented in Nextflow",
        "type": "Text_excerpt",
        "value": "*NOTE:* We are working on a DSL2 implementation using the nf-core tools on separate branches.  Based on some differences in focus we don't currently anticipate combining this with the nf-core `ampliseq` workflow, though we may revisit this in the future.  In the meantime: We will continue to address critical bugs on this branch, but the majority of effort will be in converting the workflow to DSL2 \nA dada2-based workflow using the Nextflow workflow manager for Targeted Amplicon Diversity Analysis.\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/h3abionet/TADA/tree/main/docs"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Documentation",
        "parent_header": [
          "TADA - Targeted Amplicon Diversity Analysis using DADA2, implemented in Nextflow"
        ],
        "type": "Text_excerpt",
        "value": "The h3abionet/TADA pipeline comes with documentation about the pipeline, found in the `docs/` directory:\n\n1. [Installation](docs/installation.md)\n2. [Running the pipeline](docs/usage.md)\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/h3abionet/TADA/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 14
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/h3abionet/TADA/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "h3abionet/TADA"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "TADA - Targeted Amplicon Diversity Analysis using DADA2, implemented in Nextflow"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_build_file": [
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/TADA/main/dockerfiles/qiime2/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/dockerfiles/qiime2/Dockerfile",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/TADA/main/dockerfiles/R/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/dockerfiles/R/Dockerfile",
      "technique": "file_exploration"
    }
  ],
  "identifier": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://doi.org/10.5281/zenodo.4208836"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/TADA/main/./assets/HPCBio.png"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# uct-cbio/16S-rDNA-dada2-pipeline Installation\n\nTo start using the uct-cbio/16S-rDNA-dada2-pipeline, follow the steps below:\n\n1. [Install Nextflow](#install-nextflow)\n2. [Install the pipeline](#install-the-pipeline)\n\n## 1) Install NextFlow (UCT users see below instead)\nNextflow runs on most POSIX systems (Linux, Mac OSX etc). It can be installed by running the following commands:\n\n```bash\n# Make sure that Java v7+ is installed:\njava -version\n\n# Install Nextflow\ncurl -fsSL get.nextflow.io | bash\n\n# Add Nextflow binary to your PATH:\nmv nextflow ~/bin\n# OR system-wide installation:\nsudo mv nextflow /usr/local/bin\n```\n\n### For Univeristy of Cape Town users working on HPC (hex):\n```\n#From your home directory on hex install nextflow\ncurl -fsSL get.nextflow.io | bash\n\n#Add the following to ~/.bashrc:\nJAVA_HOME=/opt/exp_soft/java/jdk1.8.0_31/\nJAVA_CMD=/opt/exp_soft/java/jdk1.8.0_31/bin/java\nexport PATH=$PATH:/opt/exp_soft/cbio/nextflow\n\n#Do not run nextflow from the headnode, it requires substantial memory to run java. Please therefore first start an interactive job as follows: \nqsub -I -q UCTlong -l nodes=1:series600:ppn=1 -d `pwd`\n```\n\n**You need NextFlow version >= 0.24 to run this pipeline.**\n\nSee [nextflow.io](https://www.nextflow.io/) and [NGI-NextflowDocs](https://github.com/SciLifeLab/NGI-NextflowDocs) for further instructions on how to install and configure Nextflow.\n\n## 2) Install the Pipeline\nThis pipeline itself needs no installation - NextFlow will automatically fetch it from GitHub if `uct-cbio/16S-rDNA-dada2-pipeline` is specified as the pipeline name when executing `nextflow run uct-cbio/16S-rDNA-dada2-pipeline`. If for some reason you need to use the development branch, this can be specified as `nextflow run uct-cbio/16S-rDNA-dada2-pipeline -r dev`\n\n### Offline use\n\nIf you need to run the pipeline on a system with no internet connection, you will need to download the files yourself from GitHub and run them directly:\n\n```bash\nwget https://github.com/uct-cbio/16S-rDNA-dada2-pipeline/archive/master.zip\nunzip master.zip -d /my-pipelines/\ncd /my_data/\nnextflow run /my-pipelines/16S-rDNA-dada2-pipeline-master\n```\n\n---\n\n[![UCT Computational Biology](/assets/cbio_logo.png)](http://www.cbio.uct.ac.za/)\n\n---\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/docs/installation.md",
      "technique": "file_exploration"
    },
    {
      "confidence": 0.8188351635099891,
      "result": {
        "original_header": "Badges",
        "type": "Text_excerpt",
        "value": "| fair-software.nl recommendations                        |                             |\n| ------------------------------------------------------- | --------------------------- |\n|(1/5) code repository                                    |[![GitHub Repo Status](https://img.shields.io/badge/github-repo-000.svg?logo=github&labelColor=gray&color=blue)](https://github.com/h3abionet/TADA)|\n|(2/5) license                                            |[![GitHub License Status](https://img.shields.io/github/license/h3abionet/TADA)](https://github.com/h3abionet/TADA)|\n|(3/5) community registry                                 | [bio.tools Registry](https://bio.tools/tada-amplicon) |\n|(4/5) citation                                           |[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4208836.svg)](https://doi.org/10.5281/zenodo.4208836)|\n|(5/5) checklist                                          |[![Codacy Badge](https://app.codacy.com/project/badge/Grade/06b6121f3ece46db88c663d1b6ebd563)](https://www.codacy.com/gh/h3abionet/TADA/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=h3abionet/TADA&amp;utm_campaign=Badge_Grade) |\n|overall                                                  | [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) |\n**GitHub Actions**\n|Docker build                                             | [![GitHub Docker Status](https://github.com/h3abionet/TADA/actions/workflows/docker.yml/badge.svg)](https://github.com/h3abionet/TADA/actions?query=workflow%3A%22Docker%22)|\n|Continuous integration                                   | [![GitHub CI Status](https://github.com/h3abionet/TADA/actions/workflows/ci.yml/badge.svg)](https://github.com/h3abionet/TADA/actions?query=workflow%3A%22CI%22)|\n \n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/h3abionet/TADA/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "MIT License\n\nCopyright (c) 2018 UCT CBIO\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/LICENSE.md",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License",
        "parent_header": [
          "TADA - Targeted Amplicon Diversity Analysis using DADA2, implemented in Nextflow"
        ],
        "type": "Text_excerpt",
        "value": "This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "logo": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/TADA/main/./assets/cbio_logo.png"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "TADA"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "h3abionet"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Nextflow",
        "size": 109823,
        "type": "Programming_language",
        "value": "Nextflow"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "R",
        "size": 57867,
        "type": "Programming_language",
        "value": "R"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 12356,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Groovy",
        "size": 8974,
        "type": "Programming_language",
        "value": "Groovy"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "HTML",
        "size": 3871,
        "type": "Programming_language",
        "value": "HTML"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Perl",
        "size": 3780,
        "type": "Programming_language",
        "value": "Perl"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 1787,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Dockerfile",
        "size": 1036,
        "type": "Programming_language",
        "value": "Dockerfile"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "cjfields",
          "type": "User"
        },
        "date_created": "2020-11-02T22:09:50Z",
        "date_published": "2020-11-02T22:18:17Z",
        "description": "This is a snapshot release for Zenodo for referencing the current code in the v0.5 workflow.  This has been used for some production work with standard Illumina-based amplicons as well as testing with long read data.",
        "html_url": "https://github.com/h3abionet/TADA/releases/tag/v0.5-beta",
        "name": "Beta v0.5 release for Zenodo",
        "release_id": 33376553,
        "tag": "v0.5-beta",
        "tarball_url": "https://api.github.com/repos/h3abionet/TADA/tarball/v0.5-beta",
        "type": "Release",
        "url": "https://api.github.com/repos/h3abionet/TADA/releases/33376553",
        "value": "https://api.github.com/repos/h3abionet/TADA/releases/33376553",
        "zipball_url": "https://api.github.com/repos/h3abionet/TADA/zipball/v0.5-beta"
      },
      "technique": "GitHub_API"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Prerequisites",
        "parent_header": [
          "TADA - Targeted Amplicon Diversity Analysis using DADA2, implemented in Nextflow"
        ],
        "type": "Text_excerpt",
        "value": "Nextflow (>=20.11.0) with either Singularity (>3.4.1) or Docker (>20.10.1, though we recommend the latest based on security updates).  We don't directly support non-containerized options (locally installed tools, bioconda) though these could work\nbased on configuration.\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "download",
    "contact",
    "faq",
    "support",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 01:48:56",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 19
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Basic usage:",
        "parent_header": [
          "TADA - Targeted Amplicon Diversity Analysis using DADA2, implemented in Nextflow"
        ],
        "type": "Text_excerpt",
        "value": "The latest help menu can be accessed using `nextflow run h3abionet/TADA --help`.  \n\n```\n  Usage:\n\n  This pipeline can be run specifying parameters in a config file or with command line flags.\n  The typical example for running the pipeline with command line flags is as follows:\n  \n    nextflow run h3abionet/TADA --reads '*_R{1,2}.fastq.gz' --trimFor 24 --trimRev 25 \\\n      --reference 'gg_13_8_train_set_97.fa.gz' -profile uct_hex\n\n  The typical command for running the pipeline with your own config (instead of command line flags) is as follows:\n  \n    nextflow run h3abionet/TADA -c dada2_user_input.config -profile uct_hex\n  \n  where 'dada2_user_input.config' is the configuration file (see example 'dada2_user_input.config')\n  \n  NB: '-profile uct_hex' still needs to be specified from the command line\n\n  Parameters\n  ----------\n\n  Mandatory arguments:\n    -profile                      Hardware config to use. Currently profile available for UCT's HPC 'uct_hex' and UIUC's 'uiuc_singularity' - create your own if necessary\n                                  NB -profile should always be specified on the command line, not in the config file      \n    \n  Input (mandatory): Additionally, only one of the following must be specified:\n    --reads                       Path to FASTQ read input data.  If the data are single-end, set '--single-end' to true.\n    --input                       Path to a sample sheet (CSV); sample sheet columns must have a headers with 'id,fastq_1,fastq_2'.  \n    --seqTables                   Path to input R/dada2 sequence tables. Only sequence tables with the original ASV sequences as the identifier are supported\n\n  Output location:\n    --outdir                      The output directory where the results will be saved\n\n  Read preparation parameters:\n    --trimFor                     integer. Headcrop of read1 (set 0 if no trimming is needed)\n    --trimRev                     integer. Headcrop of read2 (set 0 if no trimming is needed)\n    --truncFor                    integer. Truncate read1 here (i.e. if you want to trim 10bp off the end of a 250bp R1, truncFor should be set to 240). Enforced before trimFor/trimRev\n    --truncRev                    integer. Truncate read2 here ((i.e. if you want to trim 10bp off the end of a 250bp R2, truncRev should be set to 240). Enforced before trimFor/trimRev\n    --maxEEFor                    integer. After truncation, R1 reads with higher than maxEE \"expected errors\" will be discarded. EE = sum(10^(-Q/10)), default=2\n    --maxEERev                    integer. After truncation, R1 reads with higher than maxEE \"expected errors\" will be discarded. EE = sum(10^(-Q/10)), default=2\n    --truncQ                      integer. Truncate reads at the first instance of a quality score less than or equal to truncQ; default=2\n    --maxN                        integer. Discard reads with more than maxN number of Ns in read; default=0\n    --maxLen                      integer. Maximum length of trimmed sequence; maxLen is enforced before trimming and truncation; default=Inf (no maximum)\n    --minLen                      integer. Minimum length enforced after trimming and truncation; default=50\n    --rmPhiX                      {\"T\",\"F\"}. remove PhiX from read\n\n    In addition due to modifications needed for variable-length sequences (ITS), the following are also supported.  Note if these are set,\n    one should leave '--trimFor/--trimRev' set to 0.\n\n    --fwdprimer                   Provided when sequence-specific trimming is required (e.g. ITS sequences using cutadapt).  Experimental\n    --revprimer                   Provided when sequence-specific trimming is required (e.g. ITS sequences using cutadapt).  Experimental\n\n  Read merging:\n    --minOverlap                  integer. minimum length of the overlap required for merging R1 and R2; default=20 (dada2 package default=12)\n    --maxMismatch                 integer. The maximum mismatches allowed in the overlap region; default=0\n    --trimOverhang                {\"T\",\"F\"}. If \"T\" (true), \"overhangs\" in the alignment between R1 and R2 are trimmed off.\n                                  \"Overhangs\" are when R2 extends past the start of R1, and vice-versa, as can happen when reads are longer than the amplicon and read into the other-direction                                               primer region. Default=\"F\" (false)\n\n  Error models:\n    --qualityBinning              Binned quality correction (e.g. NovaSeq/NextSeq).  default: false\n    --errorModel                  NYI. Error model to use (one of 'illumina', 'illumina-binned', 'pacbio-ccs', 'custom'). This will replace\n                                  '--qualityBinning'\n\n  Denoising using dada:\n    --dadaOpt.XXX                 Set as e.g. --dadaOpt.HOMOPOLYMER_GAP_PENALTY=-1 Global defaults for the dada function, see ?setDadaOpt in R for available options and their defaults\n    --pool                        Should sample pooling be used to aid identification of low-abundance ASVs? Options are\n                                  pseudo pooling: \"pseudo\", true: \"T\", false: \"F\"\n\n  Merging arguments (optional):\n    --minOverlap                  The minimum length of the overlap required for merging R1 and R2; default=20 (dada2 package default=12)\n    --maxMismatch                 The maximum mismatches allowed in the overlap region; default=0.\n    --trimOverhang                If \"T\" (true), \"overhangs\" in the alignment between R1 and R2 are trimmed off. \"Overhangs\" are when R2 extends past the start of R1, and vice-versa, as can happen\n                                  when reads are longer than the amplicon and read into the other-direction primer region. Default=\"F\" (false)\n    --minMergedLen                Minimum length of fragment *after* merging; default = 0 (no minimum)\n    --maxMergedLen                Maximum length of fragment *after* merging; default = 0 (no maximum)\n\n  ASV identifiers:\n    --idType                      The ASV IDs are renamed to simplify downstream analysis, in particular with downstream tools.  The\n                                  default is \"md5\" which will run MD5 on the sequence and generate a QIIME2-like unique hash.  Alternatively, \n                                  this can be set to \"ASV\", which simply renames the sequences in sequencial order.  \n\n  Taxonomic arguments.  If unset, taxonomic assignment is skipped\n    --taxassignment               Taxonomic assignment method.  default = 'rdp'\n    --reference                   Path to taxonomic database to be used for annotation (e.g. gg_13_8_train_set_97.fa.gz). default = false\n    --species                     Specify path to fasta file. See dada2 addSpecies() for more detail. default = false\n    --minBoot                     Minimum bootstrap value.  default = 50\n    --taxLevels                   Listing of taxonomic levels for 'assignTaxonomy'. Experimental.\n\n  Chimera detection:\n    --skipChimeraDetection        Skip chimera detection/removal; default = false\n    --removeBimeraDenovoOpts      Additional removeBimeraDenovo options; default = ''\n\n  ASV multiple sequence alignment:\n    --skipAlignment               Skip alignment step; note this also skips ML phylogenetic analysis. default = false\n    --aligner                     Aligner to use, options are 'DECIPHER' or 'infernal'. default = 'DECIPHER'\n    --infernalCM                  Covariance model (Rfam-compliant) to use.  default = false.\n\n  Phylogenetic analysis:\n    --runTree                     Tool for ML phylogenetic analysis.  Options are 'phangorn' and 'fasttree'. default = 'phangorn'\n\n  Additional output:\n    --toBIOM                      Generate a BIOM v1 compliant output. default = true\n    --toQIIME2                    Generate QZA artifacts for all data for use in QIIME2. default = false\n\n  Sample names:\n    --sampleRegex                 Modify sample names based on a regular expression. default = false.  Note this option\n                                  is deprecated in favor of using a sample sheet.\n\n  Additional options:\n    --email                       Set this parameter to your e-mail address to get a summary e-mail with details of the run\n                                  sent to you when the workflow exits\n    -name                         Name for the pipeline run. If not specified, Nextflow will automatically generate a random mnemonic.\n\n  Help:\n    --help                        Will print out summary above when executing nextflow run uct-cbio/16S-rDNA-dada2-pipeline\n```\n"
      },
      "source": "https://raw.githubusercontent.com/h3abionet/TADA/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "workflows": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/TADA/main/pacbio.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/h3abionet/TADA/main/main.nf"
      },
      "technique": "file_exploration"
    }
  ]
}