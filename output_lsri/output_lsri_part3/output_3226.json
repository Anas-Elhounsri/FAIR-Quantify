{
  "application_domain": [
    {
      "confidence": 30.16,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/BenevolentAI/DeeplyTough"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-04-03T12:03:29Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-23T15:57:55Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "DeeplyTough: Learning Structural Comparison of Protein Binding Sites"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9868034206245709,
      "result": {
        "original_header": "DeeplyTough",
        "type": "Text_excerpt",
        "value": "This is the official PyTorch implementation of our paper *DeeplyTough: Learning Structural Comparison of Protein Binding Sites*, available from <https://pubs.acs.org/doi/abs/10.1021/acs.jcim.9b00554>. \n"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9418901850477592,
      "result": {
        "original_header": "Evaluation",
        "type": "Text_excerpt",
        "value": "We provide pre-trained networks in the `networks` directory in this repository. The following commands assume a GPU and a 4-core CPU available; use `--device 'cpu'` if there is no GPU and set `--nworkers` parameter accordingly if there are fewer cores available. \nEach of these commands will output to `$DEEPLYTOUGH/results` a CSV file with the resulting similarity scores (negative distances) as well as a pickle file with more detailed results (please see the code). The CSV files are already provided in this repository for conveniency. \n"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9689424493134479,
      "result": {
        "original_header": "Training",
        "type": "Text_excerpt",
        "value": "Training requires a GPU with >=11GB of memory and takes about 1.5 days on recent hardware. In addition, at least a 4-core CPU is recommended due to volumetric input pre-processing being an expensive task. \nNote that due to non-determinism inherent to the currently established process of training deep networks, it is nearly impossible to exactly reproduce the pre-trained networks in `networks` directory. \n"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9950173097990649,
      "result": {
        "original_header": "Changelog",
        "type": "Text_excerpt",
        "value": "- 23.02.2020: Updated code to follow our revised [JCIM paper](https://pubs.acs.org/doi/abs/10.1021/acs.jcim.9b00554), in particular away moving from UniProt-based splitting strategy as in our [BioRxiv](https://www.biorxiv.org/content/10.1101/600304v1) paper to sequence-based clustering approach whereby protein structures sharing more than 30% sequence identity are always allocated to the same testing/training set. We have also made data pre-processing more robust and frozen the versions of several dependencies. The old code is kept in `old_bioarxiv_version` branch, though note the legacy splitting behavior can be turned on also in the current `master` by setting `--db_split_strategy` command line argument in the scripts to `uniprot_folds` instead of `seqclust`.\n- 08.12.2020: pinned versions of requirements and updated DockerFile and README to reflect build instructions\n- 28.09.2021: replaced conda htmd with source build in dockerfile to relieve dependency solver (patched: 2.12.2021, also added biopython fn to remove non-protein atoms instead of VMD which is deprecated)\n \n"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/BenevolentAI/DeeplyTough/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 37
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/BenevolentAI/DeeplyTough/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "BenevolentAI/DeeplyTough"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "DeeplyTough"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_build_file": [
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/Dockerfile",
      "technique": "file_exploration"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/datasets_downloader.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/overview.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Code setup",
        "parent_header": [
          "DeeplyTough",
          "Setup"
        ],
        "type": "Text_excerpt",
        "value": "The software is ready for Docker: the image can be created from `Dockerfile` by running `docker build -t deeplytough .` (image size ~4.7GB so you may have to increase the disk space available to docker). The DeeplyTough tool is then accessible within `deeplytough` conda environment inside the container with `source activate deeplytough`.\n\nAlternatively, environment `deeplytough` can be created inside local [conda](https://conda.io/en/latest/miniconda.html) by executing the following steps from the root of this repository (linux only): \n\n```bash\n# create new python 3 env and activate\nconda create -y -n deeplytough python=3.6\nconda activate deeplytough\n\n# install legacy version of htmd from source\ncurl -LO https://github.com/Acellera/htmd/archive/refs/tags/1.13.10.tar.gz && \\\n    tar -xvzf 1.13.10.tar.gz && rm 1.13.10.tar.gz && cd htmd-1.13.10 && \\\n    python setup.py install && \\\n    cd .. && \\\n    rm -rf htmd-1.13.10;\n\n# install remaining python3 reqs\napt-get -y install openbabel\npip install --upgrade pip && pip install -r requirements.txt && pip install --ignore-installed llvmlite==0.28\n\n# install legacy se3nn library from source\ngit clone https://github.com/mariogeiger/se3cnn && cd se3cnn && git reset --hard 6b976bea4ea17e1bd5655f0f030c6e2bb1637b57 && mv experiments se3cnn; sed -i \"s/exclude=\\['experiments\\*'\\]//g\" setup.py && python setup.py install && cd .. && rm -rf se3cnn\ngit clone https://github.com/AMLab-Amsterdam/lie_learn && cd lie_learn && python setup.py install && cd .. && rm -rf lie_learn\n\n# create python2 env used for protein structure preprocessing\nconda create -y -n deeplytough_mgltools python=2.7\nconda install -y -n deeplytough_mgltools -c bioconda mgltools=1.5.6\n```\n"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Training and benchmark datasets",
        "parent_header": [
          "DeeplyTough",
          "Setup",
          "Dataset setup"
        ],
        "type": "Text_excerpt",
        "value": "The tool comes with built-in support for three datasets: TOUGH-M1 (Govindaraj and Brylinski, 2018), Vertex (Chen et al., 2016), and ProSPECCTs (Ehrt et al., 2018). These datasets must be downloaded if one wishes to either retrain the network or evaluate on one of these benchmarks. The datasets can be prepared in two steps:\n\n1. Set `STRUCTURE_DATA_DIR` environment variable to a directory that will contain the datasets (about 27 GB): `export STRUCTURE_DATA_DIR=/path_to_a_dir`\n2. Run `datasets_downloader.sh` from the root of this repository and get yourself a coffee\n\nThis will download PDB files, extracted pockets and pre-process input features. It will also download lists of pocket pairs provided by the respective dataset authors. By downloading Prospeccts, you accept their [terms of use](http://www.ccb.tu-dortmund.de/ag-koch/prospeccts/license_en.pdf).\n\nNote that this is a convenience and we also provide code for data pre-processing: in case one wishes to start from the respective base datasets, pre-processing may be triggered using the `--db_preprocessing 1` flag when running any of our training and evaluation scripts. For the TOUGH-M1 dataset in particular, fpocket2 is required and can be installed as follows:\n```bash\ncurl -O -L https://netcologne.dl.sourceforge.net/project/fpocket/fpocket2.tar.gz && tar -xvzf fpocket2.tar.gz && rm fpocket2.tar.gz && cd fpocket2 && sed -i 's/\\$(LFLAGS) \\$\\^ -o \\$@/\\$\\^ -o \\$@ \\$(LFLAGS)/g' makefile && make && mv bin/fpocket bin/fpocket2 && mv bin/dpocket bin/dpocket2 && mv bin/mdpocket bin/mdpocket2 && mv bin/tpocket bin/tpocket2\n```\n"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Custom datasets",
        "parent_header": [
          "DeeplyTough",
          "Setup",
          "Dataset setup"
        ],
        "type": "Text_excerpt",
        "value": "The tool also supports an easy way of computing pocket distances for a user-defined set of pocket pairs. This requires providing i) a set of PDB structures, ii) pockets in PDB format (extracted around bound ligands or detected using any pocket detection algorithm), iii) a CSV file defining the pairing. A toy custom dataset example is provided in `datasets/custom`. The CSV file contains a quadruplet on each line indicating pairs to evaluate: `relative_path_to_pdbA, relative_path_to_pocketA, relative_path_to_pdbB, relative_path_to_pocketB`, where paths are relative to the directory containing the CSV file and the pdb extension may be omitted. `STRUCTURE_DATA_DIR` environment variable must be set to the parent directory containing the custom dataset (in the example `/path_to_this_repository/datasets`).\n"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Environment setup",
        "parent_header": [
          "DeeplyTough",
          "Setup"
        ],
        "type": "Text_excerpt",
        "value": "To run the evaluation and training scripts, please first set the `DEEPLYTOUGH` environment variable to the directory containing this repository and then update the `PYTHONPATH` and `PATH` variables respectively:\n```bash\nexport DEEPLYTOUGH=/path_to_this_repository\nexport PYTHONPATH=$DEEPLYTOUGH/deeplytough:$PYTHONPATH\nexport PATH=$DEEPLYTOUGH/fpocket2/bin:$PATH\n```\n"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9984039036088038,
      "result": {
        "original_header": "Evaluation",
        "type": "Text_excerpt",
        "value": "We provide pre-trained networks in the `networks` directory in this repository. The following commands assume a GPU and a 4-core CPU available; use `--device 'cpu'` if there is no GPU and set `--nworkers` parameter accordingly if there are fewer cores available. \n"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8404619434576254,
      "result": {
        "original_header": "Changelog",
        "type": "Text_excerpt",
        "value": "- 23.02.2020: Updated code to follow our revised [JCIM paper](https://pubs.acs.org/doi/abs/10.1021/acs.jcim.9b00554), in particular away moving from UniProt-based splitting strategy as in our [BioRxiv](https://www.biorxiv.org/content/10.1101/600304v1) paper to sequence-based clustering approach whereby protein structures sharing more than 30% sequence identity are always allocated to the same testing/training set. We have also made data pre-processing more robust and frozen the versions of several dependencies. The old code is kept in `old_bioarxiv_version` branch, though note the legacy splitting behavior can be turned on also in the current `master` by setting `--db_split_strategy` command line argument in the scripts to `uniprot_folds` instead of `seqclust`.\n- 08.12.2020: pinned versions of requirements and updated DockerFile and README to reflect build instructions\n- 28.09.2021: replaced conda htmd with source build in dockerfile to relieve dependency solver (patched: 2.12.2021, also added biopython fn to remove non-protein atoms instead of VMD which is deprecated)\n \n"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/BenevolentAI/DeeplyTough/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "3d-models, deep-learning, drug-discovery, metric-learning, protein-structure"
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "(c) BenevolentAI Limited 2019. All rights reserved.\nFor licensing enquiries, please contact hello@benevolent.ai\n"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/LICENCE",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License Terms",
        "parent_header": [
          "DeeplyTough"
        ],
        "type": "Text_excerpt",
        "value": "(c) BenevolentAI Limited 2019. All rights reserved.<br>\nFor licensing enquiries, please contact hello@benevolent.ai\n"
      },
      "source": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "DeeplyTough"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "BenevolentAI"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 109775,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Dockerfile",
        "size": 2559,
        "type": "Programming_language",
        "value": "Dockerfile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 1732,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/BenevolentAI/DeeplyTough/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "usage",
    "faq",
    "support",
    "identifier",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 12:00:10",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 155
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ]
}