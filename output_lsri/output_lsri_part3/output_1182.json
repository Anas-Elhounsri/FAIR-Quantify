{
  "application_domain": [
    {
      "confidence": 11.3,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/s-a-nersisyan/ExhauFS"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2021-02-09T15:03:30Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2023-12-12T14:18:24Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Introduction",
        "type": "Text_excerpt",
        "value": "<img align=\"right\" width=\"400px\" src=\"https://github.com/s-a-nersisyan/ExhauFS/blob/v-novosad-dev/img/flowchart.png?raw=true\">\n<div>\n<p>The main idea behind ExhauFS is the exhaustive search of feature subsets to construct the most powerful classification and survival regression models. Since computational complexity of such approach grows exponentially with respect to combination length, we first narrow down features list in order to make the search practically feasible. Briefly, a pipeline is implemented as follows:</p>\n<ol>\n  <li><i>Feature pre-selection:</i> filter features by specified method.</li>\n  <li><i>Feature selection:</i> select \"best\" <b>n</b> features for exhaustive search.</li>\n  <li><i>Exhaustive search:</i> iterate through all possible <b>k</b>-element feature subsets and fit classification/regression models.</li>\n  <li><i>Evaluation:</i> evaluate each model and make summary of all passed feature subsets</li>\n</ol>\nValues <b>n</b> and <b>k</b> also define running time of the pipeline (there are <b>C<sub>n</sub><sup>k</sup></b> feature subsets). And, for example, iterating through all 8-gene signatures composed of <b>n = 20</b> genes is possible (see example breast cancer data below), while search over <b>n = 1000</b> genes is impossible due to time constraints.\n\nInput data can consist from different batches (datasets), and each dataset should be labeled by one of the following types:\n<ol>\n<li><i>Training set:</i> samples from training datasets will be used for tuning classification/regression models. At least one such dataset is required; if multiple given, the union will be used.</li>\n<li><i>Filtration set:</i> all tuned models will be first evaluated on training and filtration sets. If specified thresholds for accuracy are reached, model will be evaluated on validation (test) sets. The use of filtration sets is optional.</li>\n<li><i>Validation (test) set:</i> performance of models that passed filtration thresholds then evaluated on validation sets. At least one such dataset is required; if multiple given, model will be evaluated on all test sets independently.</li>\n</ol>\n\n</div>\n"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9866598876127789,
      "result": {
        "original_header": "ExhauFS",
        "type": "Text_excerpt",
        "value": "Exhaustive feature selection for classification and survival analysis. Please cite this paper if you are using ExhauFS in your work: \n"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.881970563395936,
      "result": {
        "original_header": "Functions and classes",
        "type": "Text_excerpt",
        "value": "<details>\n  <summary>Feature pre-selectors</summary>\n  \n  - <details>\n    <summary>from_file</summary> \n    \n    Pre-select features from a given file\n    \n    __name__: from_file     \n    __kwargs__:   \n    BASH1*\n    </details>\n  - <details>\n    <summary>f_test</summary> \n    \n    Pre-select features without difference between different datasets and types\n    \n    __name__: f_test     \n    </details>\n</details>\n \n<details>\n  <summary>Feature selectors</summary>\n  \n  - <details>\n    <summary>from_file</summary> \n     \n    Select first n features from a given file\n    \n    __name__: from_file   \n    __kwargs__:   \n    BASH1*\n    </details>\n  - <details>\n    <summary>t_test</summary> \n    \n    Select n features with the lowest p-values according to t-test\n    \n    __name__: t_test    \n    __kwargs__: \n    BASH2*\n    </details>\n  - <details>\n    <summary>spearman_correlation</summary> \n    \n    Select n features with the highest correlation with target label\n    \n    __name__: spearman_correlation  \n    __kwargs__: \n    BASH2* \n    </details>\n  - <details>\n    <summary>median</summary> \n    \n    Select n features with the highest median value  \n    __name__: median  \n    __kwargs__: \n    BASH2* \n    </details> \n  ##### Classification specific selectors:\n  - <details>\n    <summary>l1_logistic_regression</summary> \n       \n    Select n features with the highest concordance index on one-factor Cox regression.\n    \n    __name__: l1_logistic_regression  \n    __kwargs__: \n    ```json\n    {\n      \"C_low\": 0, // minumum inverse l1 penalizer value\n      \"C_high\": 1000000, // maximum inverse l1 penalizer value\n      \"max_iter\": 1000,  // maximum number of iteration until non-convergance error\n      \"use_filtration\": false // whether to use filtration dataset with training dataset\n    }\n    ``` \n    </details>\n        \n  ##### Regression specific selectors:\n  - <details>\n    <summary>cox_concordance</summary> \n       \n    Select n features with the highest concordance index on one-factor Cox regression.\n    \n    __name__: cox_concordance  \n    __kwargs__: \n    BASH2* \n    </details>\n  - <details>\n    <summary>cox_dynamic_auc</summary> \n    \n    Select n features with the highest time-dependent auc on one-factor Cox regression.\n  \n    __name__: cox_dynamic_auc   \n    __kwargs__: \n    BASH4*\n    </details>\n  - <details>\n    <summary>cox_hazard_ratio</summary> \n    \n    Select n features with the highest hazard ratio on one-factor Cox regression.\n    \n    __name__: cox_hazard_ratio \n    __kwargs__: \n    BASH2*   \n    </details>\n  - <details>\n    <summary>cox_likelihood</summary> \n    \n    Select n features with the highest log-likelihood on one-factor Cox regression.\n    \n    __name__: cox_likelihood  \n    __kwargs__: \n    BASH2* \n    </details>\n  - <details>\n    <summary>l1_cox</summary> \n       \n    Select n features with sparse L1-penalized Cox model.\n    \n    __name__: l1_cox  \n    __kwargs__: \n    BASH5* \n    </details>\n</details> \n<details>\n  <summary>Classifiers</summary>\n  \n  - [SVC](#https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n  - [KNeighborsClassifier](#https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n  - [RandomForestClassifier](#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n  - [XGBClassifier](#https://xgboost.readthedocs.io/en/latest/python/python_api.html)\n  \n  As a `model_kwargs` value - use parameters from the documentation of chosen model.\n  \n  #### Accuracy scores\n  - TPR\n  - FPR\n  - TNR\n  - min_TPR_TNR\n</details>\n<details>\n  <summary>Regressors</summary>\n  \n  - CoxRegression\n  \n  #### Accuracy scores\n  - concordance_index\n  - dynamic_auc\n  - hazard_ratio\n  - logrank\n</details>\n \n"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/s-a-nersisyan/ExhauFS/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/s-a-nersisyan/ExhauFS/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "s-a-nersisyan/ExhauFS"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "ExhauFS"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/v-novosad-dev/img/flowchart.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "ExhauFS installation:",
        "parent_header": [
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "`pip3 install exhaufs`\n"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 1: data preparation",
        "parent_header": [
          "Running ExhauFS"
        ],
        "type": "Text_excerpt",
        "value": "Before running the tool, you should prepare three csv tables containing actual data, its annotation and *n* / *k* grid. Both for classification and survival analysis data table should contain numerical values associated with samples (rows) and features (columns):\n\n<details>\n  <summary>Example</summary>\n  \n  |            | Feature 1 | Feature 2 |\n  | ---------- | --------- | --------- |\n  | Sample 1   | 17.17     | 365.1     |\n  | Sample 2   | 56.99     | 123.9     |\n  | ...        |           |           |\n  | Sample 98  | 22.22     | 123.4     |\n  | Sample 99  | 23.23     | 567.8     |\n  | ...        |           |           |\n  | Sample 511 | 10.82     | 665.8     |\n  | Sample 512 | 11.11     | 200.2     |\n</details>\n\n\nAnnotation table format is different for classification and survival analysis. For classification it should contain binary indicator of sample class (e.g., 1 for recurrent tumor and 0 for non-recurrent), dataset (batch) label and dataset type (Training/Filtration/Validation).  \nIt is important that `Class = 1` represents \"Positives\" and `Class = 0` are \"negatives\", otherwise accuracy scores may be calculated incorrectly.   \nNote that annotation should be present for each sample listed in the data table in the same order:\n\n<details>\n  <summary>Example</summary>\n  \n  |            | Class | Dataset  | Dataset type |\n  | ---------- | ----- | -------- | ------------ |\n  | Sample 1   | 1     | GSE3494  | Training     |\n  | Sample 2   | 0     | GSE3494  | Training     |\n  | ...        |       |          |              |\n  | Sample 98  | 0     | GSE12093 | Filtration   |\n  | Sample 99  | 0     | GSE12093 | Filtration   |\n  | ...        |       |          |              |\n  | Sample 511 | 1     | GSE1456  | Validation   |\n  | Sample 512 | 1     | GSE1456  | Validation   |\n</details>\n\n\nFor survival analysis, annotation table should contain binary event indicator and time to event:\n<details>\n  <summary>Example</summary>\n  \n  |            | Event | Time to event | Dataset  | Dataset type |\n  | ---------- | ----- | ------------- | -------- | ------------ |\n  | Sample 1   | 1     | 100.1         | GSE3494  | Training     |\n  | Sample 2   | 0     | 500.2         | GSE3494  | Training     |\n  | ...        |       |               |          |              |\n  | Sample 98  | 0     | 623.9         | GSE12093 | Filtration   |\n  | Sample 99  | 0     | 717.1         | GSE12093 | Filtration   |\n  | ...        |       |               |          |              |\n  | Sample 511 | 1     | 40.5          | GSE1456  | Validation   |\n  | Sample 512 | 1     | 66.7          | GSE1456  | Validation   |\n</details>\n\n\nTable with *n* / *k* grid for exhaustive feature selection:  \n*n* is a number of selected features, *k* is a length of each features subset.  \n\nIf you are not sure what values for *n* *k* to use, see [Step 3: defining a *n*, *k* grid](#step-3-defining-a-n-k-grid)  \n\n<details>\n  <summary>Example</summary> \n   \n  | n   | k   |  \n  | --- | --- |  \n  | 100 | 1   |  \n  | 100 | 2   |  \n  | ... | ... |  \n  | 20  | 5   |  \n  | 20  | 10  |  \n  | 20  | 15  |  \n</details>\n\n"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/s-a-nersisyan/ExhauFS/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "Copyright (c) 2018 The Python Packaging Authority\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/LICENSE",
      "technique": "file_exploration"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "ExhauFS"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "s-a-nersisyan"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 97233,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Makefile",
        "size": 236,
        "type": "Programming_language",
        "value": "Makefile"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_documentation": [
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "https://xgboost.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Prerequisites:",
        "parent_header": [
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "Make sure you have installed all of the following prerequisites on your development machine:\n  - python3.6+  \n  - pip3\n\n"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 2: creating configuration file",
        "parent_header": [
          "Running ExhauFS"
        ],
        "type": "Text_excerpt",
        "value": "Configuration file is a json file containing all customizable parameters for the model (classification and survival analysis)  \n\n<details>\n  <summary>Available parameters</summary> \n\n  \ud83d\udd34!NOTE! - All paths to files / directories can be either relative to the configuration file directory or absolute paths \n  * `data_path`\n      Path to csv table of the data.\n\n  * `annotation_path`\n      Path to csv table of the data annotation.\n\n  * `n_k_path`\n      Path to a *n*/*k* grid file.\n\n  * `output_dir`\n      Path to directory for output files. If it doesn't exist, it will be created.\n\n  * `feature_pre_selector`  \n      Name of feature pre-selection function from [feature pre-selectors section](#functions-and-classes).\n\n  * `feature_pre_selector_kwargs`  \n      Object/Dictionary of keyword arguments for feature pre-selector function.\n\n  * `feature_selector`  \n      Name of feature selection function from [feature selectors section](#functions-and-classes).\n\n  * `feature_selector_kwargs`  \n      Object/Dictionary of keyword arguments for feature selector function. Boolean `use_filtration` indicates whether to use *Filtration* dataset besides *Training* dataset for the selector function.\n\n  * `preprocessor`\n      Name of class for data preprocessing from [sklearn.preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html).\n\n  * `preprocessor_kwargs`\n      Object/Dictionary of keyword arguments for preprocessor class initialization.  \n      If you are using `sklearn` model, use `kwargs` parameters from the documentation of the model.\n\n  * `model`  \n      Name of class for classification / survival analysis from [Classifiers / Regressors section](#functions-and-classes).\n\n  * `model_kwargs`\n      Object/Dictionary of keyword arguments for model initialization.  \n      If you are using `sklearn` model, use `kwargs` parameters from the documentation of the model.\n\n  * `model_CV_ranges`\n      Object/Dictionary defining model parameters which should be cross-validated. Keys are parameter names, values are lists for grid search.\n\n  * `model_CV_folds`\n      Number of folds for K-Folds cross-validation.\n\n  * `scoring_functions`\n      List with names for scoring functions (from [Accuracy scores section](#functions-and-classes)) which will be calculated for each model. If you need to pass parameters to the function (e.g. `year` in dynamic auc score), you can use object {\"name\": `function name`, \"kwargs\": `parameters object`}.\n\n  * `main_scoring_function`\n      Key from scoring_functions dict defining the \"main\" scoring function which will be optimized during cross-validation and will be used for model filtering.\n\n  * `main_scoring_threshold`\n      A number defining threshold for model filtering: models with score below this threshold on training/filtration sets will not be further evaluated.\n\n  * `n_processes`\n      Number of processes / threads to run on.\n  \n  * `random_state`\n      Random seed (set to an arbitrary integer for reproducibility).\n\n  * `verbose`\n      If *true*, print running time for each pair of *n*, *k*.\n</details>\n\n"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 3: defining a *n*, *k* grid",
        "parent_header": [
          "Running ExhauFS"
        ],
        "type": "Text_excerpt",
        "value": "To estimate running time of the exhaustive pipeline and define adequate *n* / *k* values you can run:  \n```bash\nexhaufs estimate regressors|classifiers -c <config_file> --max_k <max_k> --max_estimated_time <max_estimated_time>\n```\nwhere\n* `config_file` is the path to json configuration file.\n* `max_k` is the maximum length of each features subset.\n* `max_estimated_time` is the time constraint (in hours) for a pipeline running time on one pair of (n, k).\n\nAbove script calculates maximum possible values *n* / *k* for each *k*=`1...max_k` such that pipeline running time for each pair (*n*, *k*) is less then `max_estimated_time`\n"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 4: running the exhaustive pipeline",
        "parent_header": [
          "Running ExhauFS"
        ],
        "type": "Text_excerpt",
        "value": "When input data, configuration file and *n* / *k* grid are ready,  \nthe exhaustive pipeline can be executed as follows -  \n* __Classifiers__:\n```bash\nexhaufs build classifiers -c <config_file>\n```\n* __Regressors__:\n```bash\nexhaufs build regressors -c <config_file>\n```\n\nThis will generate multiple files in the specified output folder:\n* `models.csv`: this file contains all models (classifiers or regressors) which passed the filtration together with their quality metrics.\n* `summary_n_k.csv`: for each pair of *n*, *k* three numbers are given: number of models which passed the filtration,\nnumber of models which showed reliable performance (i.e., passed quality thresholds) on the validation set and\ntheir ratio (in %). Low percentage of validation-reliable models together with high number of \nfiltration-reliable models is usually associated with overfitting.\n* `summary_features.csv`: for each pair (n, k), for each feature, percentage of models carrying this feature \nis listed (only models which passed the filtration are considered).\n"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "download",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 03:55:47",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 10
      },
      "technique": "GitHub_API"
    }
  ],
  "support": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 5: generating report for a single model",
        "parent_header": [
          "Running ExhauFS"
        ],
        "type": "Text_excerpt",
        "value": "To get detailed report on the specific model (== specific set of features): \n* Create configuration file (use ./examples/make_<u>(classifier | regressor)</u>_summary/config.json as a template) and set following key parameters:\n    * `data_path` - path to dataset used for search of classifiers or regressors\n    * `annotation_path` - path to annotation file\n    * `output_dir` - path to output directory for detailed report\n    * `features_subset` - set of features belonging to the classifier or regressor of interest;\n    * `saving_format` - either \"tiff\" or \"pdf\": format of the saved plots documents;\n* * For classifier run `exhaufs summary classifiers -c <config_file>`   \n  * For regressor run `exhaufs summary regressors -c <config_file>`    \n* Check the detailed report in the `output_dir`\n\nIf you also have time-to-event data for classification problem, you can make Kaplan-Meier plots based on the classifier predictions.  \nTo do so you can run `exhaufs km_plot -c <config_file>` and check the `output_dir` directory.  \nYou can also specify `KM_x_label` and `KM_y_label` in the config to change plot axis names.  \n"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Tutorial",
        "type": "Text_excerpt",
        "value": "In this section we illustrate the main functionality of ExhauFS, and\ntogether with that show how to reproduce the results present in \nthe manuscript.\n\n<details>\n  <summary>A toy example (classification)</summary>\n  \n  We illustrate ExhauFS basics by using a small [cervical cancer toy dataset](https://archive.ics.uci.edu/ml/datasets/Cervical+Cancer+Behavior+Risk) with 72 samples and 19 features. All necessary data for this example can be found in [`tutorial/cervical_cancer`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/cervical_cancer) directory.  \n  \n  We start from [`data.csv`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/cervical_cancer/data.csv) and [`annotation.csv`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/cervical_cancer/annotation.csv) files: the first one contains data matrix and the\n  second one maps each sample to class label and dataset type (training or validation). In this\n  example we brute force all existing feature triples - this information is reflected in [`n_k.csv`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/cervical_cancer/n_k.csv)\n  file (n = 19 is the total number of features). Prior to ExhauFS run we should also create\n  a configuration file. Here we use random forest classifier and standard accuracy metrics \n  ([`config_for_build_classifiers.json`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/cervical_cancer/config_for_build_classifiers.json)). Since all 19 features are used, we do not specify any\n  feature selector and pre-selector. In order to get only highly accurate classifiers, we impose\n  0.9 threshold on the minimum of sensitivity (TPR) and specificity (TNR) on the training set.\n  \n  Now we are ready to execute ExhauFS:\n  \n  `exhaufs build classifiers -c config_for_build_classifiers.json`\n  \n  Output files are located in [`results_build_classifiers`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/cervical_cancer/results_build_classifiers) directory.\n  In this example, we focus only on two reports:\n  - [`models.csv`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/cervical_cancer/results_build_classifiers/models.csv)\n  \n  This file contains accuracy metrics for all models which passed 0.9 threshold filtration on the training set.\n  The file is sorted according to the classifier accuracy on the training set, so \n  we can see that almost all models have sensitivity and specificity equal to 1.0.\n  Among these models there are multiple cases with particularly high accuracy on the validation set, e.g.  \n  - [`summary_features.csv`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/cervical_cancer/results_build_classifiers/summary_features.csv)\n  \n  Here we see a number of occurrences of each feature in the set of\n  constructed models which passed 0.9 accuracy threshold. The most\n  important features could be picked, e.g., by taking rows with FDR < 0.05.\n  \n  Let us take a closer look to the particular classifier built on\n  perception_vulnerability, socialSupport_instrumental and empowerment_desires features.\n  To do that, we should create an\n  additional configuration file with `features_subset` parameter set to the desired\n  triple ([`config_for_summary_classifiers.json`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/cervical_cancer/config_for_summary_classifiers.json)). To run ExhauFS in the summary mode,\n  simply execute the following command:\n  \n  `exhaufs summary classifiers -c config_for_summary_classifiers.json`\n  \n  Note, that we do not specify any feature selection/pre-selection or \n  accuracy threshold parameters for the summary mode. The most important of\n  generated files are:\n  - [`report.txt`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/cervical_cancer/results_summary_classifiers/report.txt): accuracy scores for the training and the validation datasets.\n  - [`ROC_CervicalCancerBehaviorRisk_Training.pdf`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/cervical_cancer/results_summary_classifiers/ROC_CervicalCancerBehaviorRisk_Training.pdf): ROC curve for the training set.\n  The red dot stands for actual sensitivity and specificity. For example, the classifier\n  does not work ideally on the training set despite AUC equals 1.\n  - [`ROC_CervicalCancerBehaviorRisk_Validation.pdf`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/cervical_cancer/results_summary_classifiers/ROC_CervicalCancerBehaviorRisk_Validation.pdf): ROC curve for validation set.\n</details>\n\n<details>\n  <summary>Breast cancer (classification)</summary>\n  \n  As a real-life example of the classification module of the tool we used multi-cohort breast cancer dataset.\n  The objective is to predict whether a patient will have cancer recurrence within first 5 years after the surgery\n  based on gene expression profile in the removed tumor (see our manuscript for the details). \n  Configuration and output files for this example are\n  located in [`tutorial/breast_cancer`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/breast_cancer) folder, input data can be downloaded [here](https://eduhseru-my.sharepoint.com/:f:/g/personal/snersisyan_hse_ru/EihEOok4stJFnCjGxqL14qgBSqxLzUy7hBThWp4_jE3HKw?e=bges1q). The microarray data are split into independent training, filtration and validation sets.\n \n  The following options are used ([`config_for_build_classifiers.json`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/breast_cancer/config_for_build_classifiers.json)):\n  - `\"feature_pre_selector\": \"f_test\"` - this is for pre-selection of genes whose expression distribution is similar in training and filtration datasets (the batch effect removal approach).\n  - `\"feature_selector\": \"t_test\"` - top n most differentially expressed genes are selected. Additional option `\"use_filtration\": true` means that Student's t-test will be applied to the union of training and filtration sets.\n  - `\"preprocessor\": \"StandardScaler\"` - prior classifier fitting, data are centered and scaled (z-score transformation).\n  - `\"model\": \"SVC\"` - Support Vector machine Classifier (SVC) is used. Additional arguments are used to specify linear kernel, normalization for unbalanced classes and a cross-validation grid for penalty parameter (C) estimation.\n\n  Classifier construction with the given [`n_k.csv`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/breast_cancer/n_k.csv) file could be done by the same command\n  as in the previous toy example (however, this will take several days to finish).\n  Here we review two output reports which were not covered in the toy example:\n  \n  - [`summary_n_k.csv`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/breast_cancer/results_build_classifiers/summary_n_k.csv)\n  \n  For each *n, k* pair the number of classifiers which passed the 0.65 accuracy threshold \n  on the training and the filtration sets is presented (num_training_reliable). All\n  these classifiers were evaluated on the validation set; num_validation_reliable and\n  percentage_reliable columns contain the fraction of these classifiers which also\n  passed 0.65 accuracy threshold on the validation set. For all values of k above\n  10 we see almost 100% passability, which means the absence of overfitting and successful\n  victory over the batch effects (all classifiers which demonstrated reliable performance\n  on the training and the filtration sets were also good on the validation one).\n  \n  - [`sorted_features.txt`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/breast_cancer/results_build_classifiers/sorted_features.txt)\n\n  This is a technical though useful file: the list of pre-selected genes is sorted according to the rate of\n  differential expression (`t_test` feature selection). Each pipeline iteration begins from\n  the selection of the first *n* entries from this file.\n \t\n  As in the previous toy example, let us take a closer look to the single gene signature\n  (see [`config_for_summary_classifiers.json`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/breast_cancer/config_for_summary_classifiers.json)). The following output files were not\n  covered in the toy example:\n  - [`feature_importances.pdf`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/breast_cancer/results_summary_classifiers/feature_importances.pdf) (contains coefficients of the linear SVM classifier)\n  - [`model.pkl`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/breast_cancer/results_summary_classifiers/model.pkl) (Python-pickled classifier and pre-processor objects)\n\n  ExhauFS also allows one to evaluate constucted classifiers on time-to-event data.\n  For example, let us evaluate the same ten-gene signature on \n  additional RNA-seq TCGA-BRCA dataset. To do that, we should include to desired feature \n  subset and pickled model path to the configuration file ([`config_for_km_plot.json`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/breast_cancer/config_for_km_plot.json)).\n  The analysis could be done by running\n\n  `exhaufs km_plot -c config_for_km_plot.json`\n  \n  This will generate the Kaplan-Meier plot ([`results_km_plot/KM_TCGA-BRCA_Validation.pdf`](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/breast_cancer/results_km_plot/KM_TCGA-BRCA_Validation.pdf)).\n</details>\n\n<details>\n  <summary>Colorectal cancer (survival regression)</summary>\n  \n  As a real-life example of the regression part of the tool we used colorectal cancer dataset.  \n  \n  Transformed data and config used for pipeline can be found [here](https://eduhseru-my.sharepoint.com/:f:/g/personal/snersisyan_hse_ru/Est199kj_IhNtZMf0cM2T-0BvvzM2amkZD22uvhtvUTMXA?e=R79vhh).  \n\n  Same with classification, the main objective was to analyse contribution of different feature [pre]selection techniques and accuracy scores using Cox Regression as a main model.  \n  We achieved best results using `concordance_index` as a feature selector and as a main scoring function.  \n  \n  Again, same with classification, firstly we need to make *n, k* grid table for the pipeline.  \n  As a result of `exhaufs estimate regressors -c confifg_for_estimate_regressors.json --max_estimated_time 3 --max_k 20` we got the [estimated_times.csv](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/colorectal_cancer/results_estimated_times/estimated_times.csv) table with *n/k* grid and predicted running time for each pair of values.\n  \n  Same with examples above, we can build regression models or make summary for one specific set of features as follows:\n  - `exhaufs build regressors -c confifg_for_build_regressors.json` will produce same files as for classification task.\n  - `exhaufs summary regressors -c confifg_for_summary_regressors.json` will produce a detailed report for the specified set of features and also a Kaplan-Meier plots for each dataset type.\n  \n  Where [confifg_for_build_regressors.json](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/colorectal_cancer/confifg_for_build_regressors.json) and [confifg_for_summary_regressors.json](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/colorectal_cancer/confifg_for_summary_regressors.json) can be found in the [tutorial/colorectal_cancer](https://github.com/s-a-nersisyan/ExhauFS/blob/main/tutorial/colorectal_cancer) directory.\n</details>\n"
      },
      "source": "https://raw.githubusercontent.com/s-a-nersisyan/ExhauFS/main/README.md",
      "technique": "header_analysis"
    }
  ]
}