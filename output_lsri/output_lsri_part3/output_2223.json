{
  "application_domain": [
    {
      "confidence": 0.9409692370393522,
      "result": {
        "type": "String",
        "value": "Semantic web"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/trance-project/trance"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2018-12-12T10:15:05Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2023-12-12T16:33:20Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 0.9945605407426242,
      "result": {
        "original_header": "TRAnsforming Nested Collections Efficiently: a framework for processing nested collection queries",
        "type": "Text_excerpt",
        "value": "The framework is organized into two subfolders:\n* `compiler` contains all the components for running the standard and shredded pipeline, both skew-unaware and skew-aware. This goes from NRC to generated code.\n* `executor` contains plan operator implementations specific to the targets of the code generator. We support Scala 2.12 based versions of Spark (2.4.2, 3.1.1, etc). This organization avoids running the compiler with Spark-specific dependencies.\n \n"
      },
      "source": "https://raw.githubusercontent.com/jacmarjorie/trance/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9798705669173122,
      "result": {
        "original_header": "Additional Configurations",
        "type": "Text_excerpt",
        "value": "| Parameter | Type | Description | Default |\n| ---- | ---- | --- | --- |\n| datapath | String | Path to TPC-H `.tbl` files. Only required for TPC-H queries. | `/path/to/executor/spark/data/tpch` |\n| minPartitions | Int | The minimum number of partitions. | 400 |\n| maxPartitions | Int | The maximum number of partitions. Also used for `spark.sql.shuffle`. | 1000 |\n| heavyKeyStrategy | String | The strategy for heavy key calculuation. Can be either `sample` or `slice`. | `sample` |\n| sample | Int | The number of values to slice in the `slice` procedure. | 1000 |\n| threshold | Double | Threshold used for heavy key procedure. For `sample` this is the percent of values that should be reached to be considered heavy. For `slice` this is the scale above average to be reached to be considered heavy. | .0025 | \n"
      },
      "source": "https://raw.githubusercontent.com/jacmarjorie/trance/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/jacmarjorie/trance/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 2
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/trance-project/trance/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "trance-project/trance"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "TraNCE"
      },
      "source": "https://raw.githubusercontent.com/jacmarjorie/trance/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jacmarjorie/trance/master/compiler/compile.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/trance-project/trance/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "MIT License\n\nCopyright (c) 2020 University of Oxford (University of Oxford means the\nChancellor, Masters and Scholars of the University of Oxford, having an\nadministrative office at Wellington Square, Oxford OX1 2JD, UK).\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/jacmarjorie/trance/master/LICENSE",
      "technique": "file_exploration"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "trance"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "trance-project"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Scala",
        "size": 1100678,
        "type": "Programming_language",
        "value": "Scala"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 216,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jacmarjorie/trance/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2011.06381"
      },
      "source": "https://raw.githubusercontent.com/jacmarjorie/trance/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "System Requirements",
        "parent_header": [
          "TraNCE"
        ],
        "type": "Text_excerpt",
        "value": "The framework has been tested with `Scala 2.12` and associated Spark versions (2.4.2, 3.1.1 etc). \n"
      },
      "source": "https://raw.githubusercontent.com/jacmarjorie/trance/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "installation",
    "citation",
    "acknowledgement",
    "run",
    "download",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 08:12:37",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 11
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Example Usage",
        "parent_header": [
          "TraNCE"
        ],
        "type": "Text_excerpt",
        "value": "This example runs the skew-unaware and skew-aware shredded pipeline for the running example:\n\n```\nFor c in COP Union\n  {( c_name := c.c_name,\n     c_orders := For o in c.c_orders Union\n    {( o_orderdate := o.o_orderdate,\n       o_parts := ReduceByKey([p_name], [l_quantity],\n      For l in o.o_parts Union\n        For p in P Union\n          If (l.l_partkey = p.p_partkey) Then\n            {( p_name := p.p_name,\n               l_quantity := l.l_quantity )}\n```\n\nNRC queries can currently be described directly in Scala. The above is the running example as defined \nin Scala [here](https://github.com/jacmarjorie/shredder/blob/master/compiler/src/main/scala/framework/examples/tpch/NestedToNested.scala#L286-L309). `compilers/src/main/scala/framework/examples/` contains more examples on how to do this.\n\nThis example writes the Spark application code into the `executor/spark` package, \nwhich will then be compiled into an application \njar along with additional Spark-specific dependencies. This is because it will need to \naccess the same `spark.sql.implicits` as the functions inside this package. \n\nRun sbt in the `compiler` directory:\n\n```\ncd compiler\nsbt run\n```\n\nSelect to run the first application `[1] framework.generator.spark.App`. This will generate two files:\n* skew-unaware, shredded pipeline: `../executor/spark/src/main/scala/sparkutils/generated/ShredTest2NNLUnshredSpark.scala`\n* skew-aware, shredded pipeline: `../executor/spark/src/main/scala/sparkutils/generated/ShredTest2NNLUnshredSkewSpark.scala`\n\nNavigate to `executor/spark`. This example assumes that the application \nwill be ran from this directory; if you execute from any other directory, \nupdate the `datapath` configuration variable in `data.flat` \nto point to `executor/spark/data/tpch` directory. See [below](#additional-configurations) \nfor further details on config parameters.\n\nTo continue, compile the package:\n\n```\ncd executor/spark\necho \"datapath=$PWD/data/tpch\" > data.flat\nsbt package\n```\n\nOnce compiled, the application can be compiled with spark-submit. For example, to run the application \ncorresponding to `ShredTest2NNLUnshredSkewSpark.scala` as a local spark job do:\n\n```\nspark-submit --class sparkutils.generated.ShredTest2NNLUnshredSkewSpark \\\n  --master \"local[*]\" target/scala-2.12/sparkutils_2.12-0.1.jar\n```\n\nIf you would like to print to console, you can edit the generated \napplication file - just uncomment the line `Test2NNL.print` at the \nbottom of the application file, then redirect to stdout: \n\n```\nspark-submit --class sparkutils.generated.ShredTest2NNLUnshredSkewSpark \\\n  --master \"local[*]\" target/scala-2.12/sparkutils_2.12-0.1.jar > out\n```\n"
      },
      "source": "https://raw.githubusercontent.com/jacmarjorie/trance/master/README.md",
      "technique": "header_analysis"
    }
  ]
}