{
  "acknowledgement": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Acknowledgements",
        "parent_header": [
          "Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue"
        ],
        "type": "Text_excerpt",
        "value": "* This code began with [batra-mlp-lab/visdial-challenge-starter-pytorch][10]. We thank the developers for doing most of the heavy-lifting.\n\n\n[1]: https://visualdialog.org/data\n[2]: https://s3.amazonaws.com/visual-dialog/data/v1.0/2019/visdial_1.0_word_counts_train.json\n[3]: https://github.com/peteanderson80/bottom-up-attention\n[4]: https://github.com/jz462/Large-Scale-VRD.pytorch\n[5]: https://github.com/jcjohnson/densecap\n[6]: https://allennlp.org/elmo\n[7]: https://github.com/stanfordnlp/GloVe\n[8]: https://github.com/JXZe/DualVD/blob/master/visdialch/utils/checkpointing.py\n[9]: https://www.github.com/lanpa/tensorboardX\n[10]: https://github.com/batra-mlp-lab/visdial-challenge-starter-pytorch\n\n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "application_domain": [
    {
      "confidence": 15.83,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "author": "Yu, Jing and Jiang, Xiaoze and Qin, Zengchang and Zhang, Weifeng and Hu, Yue and Wu, Qi",
        "format": "bibtex",
        "title": "Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue",
        "type": "Text_excerpt",
        "value": "@article{yu2020learning,\n    year = {2020},\n    pages = {220--233},\n    volume = {30},\n    journal = {IEEE Transactions on Image Processing},\n    author = {Yu, Jing and Jiang, Xiaoze and Qin, Zengchang and Zhang, Weifeng and Hu, Yue and Wu, Qi},\n    title = {Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue},\n}"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/JXZe/Learning_DualVD"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-10-19T05:05:14Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2022-11-17T03:14:00Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 0.9985808530256798,
      "result": {
        "original_header": "Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue",
        "type": "Text_excerpt",
        "value": "This is a PyTorch implementation for [Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue, IEEE Transactions on Image Processing](https://ieeexplore.ieee.org/document/9247486). \n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9426527699385304,
      "result": {
        "type": "Text_excerpt",
        "value": "If you use this code in your research, please consider citing: \n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9596416512087104,
      "result": {
        "original_header": "Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue",
        "type": "Text_excerpt",
        "value": "A previous version of our dual encoding model was published in AAAI 2020: DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue. [[Paper]](https://arxiv.org/abs/1911.07251) [[Code]](https://github.com/JXZe/DualVD) \n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9635437840296892,
      "result": {
        "original_header": "Training",
        "type": "Text_excerpt",
        "value": "The code have an `--overfit` flag, which can be useful for rapid debugging. It takes a batch of 5 examples and overfits the model on them.\n \n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/JXZe/Learning_DualVD/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 2
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/JXZe/Learning_DualVD/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "JXZe/Learning_DualVD"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/image/visual_result.png"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9347369855361712,
      "result": {
        "original_header": "Training",
        "type": "Text_excerpt",
        "value": "\nTrain the DualVD model as:\n```sh\npython train.py --config-yml configs/lf_disc_faster_rcnn_x101_bs32.yml --gpu-ids 0 1 # provide more ids for multi-GPU execution other args...\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9981694531633994,
      "result": {
        "original_header": "Logging",
        "type": "Text_excerpt",
        "value": "Use [Tensorboard][9] for logging training progress. Recommended: execute `tensorboard --logdir /path/to/save_dir --port 8008` and visit `localhost:8008` in the browser. \n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.999169029871636,
      "result": {
        "original_header": "Evaluation",
        "type": "Text_excerpt",
        "value": "Evaluation of a trained model checkpoint can be done as follows:\n```sh\npython evaluate.py --config-yml /path/to/config.yml --load-pthpath /path/to/checkpoint.pth --split val --gpu-ids 0\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8464458952066723,
      "result": {
        "original_header": "Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue",
        "type": "Text_excerpt",
        "value": "![alt text](image/visual_result.png)\n<p align=\"center\">Example results from the VisDial v1.0 validation dataset.</p> \n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8228709951823983,
      "result": {
        "original_header": "Data",
        "type": "Text_excerpt",
        "value": "1. Download the VisDial v1.0 dialog json files and images from [here][1].\n2. Download the word counts file for VisDial v1.0 train split from [here][2]. \nThey are used to build the vocabulary.\n3. Use Faster-RCNN to extract image features from [here][3].\n4. Use Large-Scale-VRD to extract visual relation embedding from [here][4].\n5. Use Densecap to extract local captions from [here][5].\n6. Generate ELMo word vectors from [here][6].\n7. Download pre-trained GloVe word vectors from [here][7]. \n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8664515969768949,
      "result": {
        "original_header": "Training",
        "type": "Text_excerpt",
        "value": "\nTrain the DualVD model as:\n```sh\npython train.py --config-yml configs/lf_disc_faster_rcnn_x101_bs32.yml --gpu-ids 0 1 # provide more ids for multi-GPU execution other args...\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8201275201380053,
      "result": {
        "original_header": "Saving model checkpoints",
        "type": "Text_excerpt",
        "value": "This script will save model checkpoints at every epoch as per path specified by `--save-dirpath`. Refer [visdialch/utils/checkpointing.py][8] for more details on how checkpointing is managed.\n \n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9216050636242972,
      "result": {
        "original_header": "Evaluation",
        "type": "Text_excerpt",
        "value": "Evaluation of a trained model checkpoint can be done as follows:\n```sh\npython evaluate.py --config-yml /path/to/config.yml --load-pthpath /path/to/checkpoint.pth --split val --gpu-ids 0\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/JXZe/Learning_DualVD/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Learning_DualVD"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "JXZe"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 97783,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1911.07251"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Requirements",
        "parent_header": [
          "Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue"
        ],
        "type": "Text_excerpt",
        "value": "This code is implemented using PyTorch v1.0, and provides out of the box support with CUDA 9 and CuDNN 7. \n\n```sh\nconda create -n visdialch python=3.6\nconda activate visdialch  # activate the environment and install all dependencies\ncd DualVD/\npip install -r requirements.txt\n```\n\n\n\n"
      },
      "source": "https://raw.githubusercontent.com/JXZe/Learning_DualVD/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "run",
    "download",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 11:31:10",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 7
      },
      "technique": "GitHub_API"
    }
  ]
}