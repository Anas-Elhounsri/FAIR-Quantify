{
  "acknowledgement": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Acknowledgments",
        "parent_header": [
          "EndoL2H: Deep Super-Resolution for Capsule Endoscopy"
        ],
        "type": "Text_excerpt",
        "value": "This repository is based on [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix).\n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "application_domain": [
    {
      "confidence": 67.17,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Reference",
        "parent_header": [
          "EndoL2H: Deep Super-Resolution for Capsule Endoscopy"
        ],
        "type": "Text_excerpt",
        "value": "If you find our work useful in your research please consider citing our paper:\n\n```\n@article{almalioglu2020endol2h,\n    title={EndoL2H: Deep Super-Resolution for Capsule Endoscopy},\n    author={Yasin Almalioglu and Kutsev Bengisu Ozyoruk and Abdulkadir Gokce and Kagan Incetan and Guliz Irem Gokceler and Muhammed Ali Simsek and Kivanc Ararat and Richard J. Chen and Nichalos J. Durr and Faisal Mahmood and Mehmet Turan},\n    journal={arXiv preprint arXiv:2002.05459},\n    year={2020}\n}\n```\n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Yasin Almalioglu and Kutsev Bengisu Ozyoruk and Abdulkadir Gokce and Kagan Incetan and Guliz Irem Gokceler and Muhammed Ali Simsek and Kivanc Ararat and Richard J. Chen and Nichalos J. Durr and Faisal Mahmood and Mehmet Turan",
        "format": "bibtex",
        "title": "EndoL2H: Deep Super-Resolution for Capsule Endoscopy",
        "type": "Text_excerpt",
        "value": "@article{almalioglu2020endol2h,\n    year = {2020},\n    journal = {arXiv preprint arXiv:2002.05459},\n    author = {Yasin Almalioglu and Kutsev Bengisu Ozyoruk and Abdulkadir Gokce and Kagan Incetan and Guliz Irem Gokceler and Muhammed Ali Simsek and Kivanc Ararat and Richard J. Chen and Nichalos J. Durr and Faisal Mahmood and Mehmet Turan},\n    title = {EndoL2H: Deep Super-Resolution for Capsule Endoscopy},\n}"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/CapsuleEndoscope/EndoL2H"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-06-15T13:09:44Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-21T09:02:06Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "EndoL2H: Deep Super-Resolution for Capsule Endoscopy"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Summary of Our Work",
        "parent_header": [
          "EndoL2H: Deep Super-Resolution for Capsule Endoscopy",
          "Overview"
        ],
        "type": "Text_excerpt",
        "value": "  \nA conditional GAN combined with spatial attention unit maps low resolution(LR) endoscopic images to diagnostically relevant  high resolution(HR) endoscopic images. Unlike an unconditional GAN, both the generator and discriminator observe the input LR images.\n\n<p align=\"center\">\n<img src='imgs/systemSummary.png' width=384/> \n</p>\n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.944093232409451,
      "result": {
        "original_header": "EndoL2H: Deep Super-Resolution for Capsule Endoscopy",
        "type": "Text_excerpt",
        "value": "Code, dataset, and trained models for \"EndoL2H: Deep Super-Resolution for Capsule Endoscopy\" \nIf you use this code, please cite: \n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9980790369720594,
      "result": {
        "original_header": "Overview",
        "type": "Text_excerpt",
        "value": "We propose and quantitatively validate a novel framework to learn a mapping from low-to-high resolution endoscopic images. We combine conditional adversarial networks with a spatial attention block to improve the resolution by up to factors of 8x, 10x, 12x, respectively. EndoL2H is generally applicable to any endoscopic capsule system and has the potential to improve diagnosis and better harness computational approaches for polyp detection and characterization. \nOur main contributions are as follows:\n  - **Spatial Attention-based Super Resolution cGAN:** We propose a spatial attention based super-resolution cGAN architecture specifically designed and optimized for capsule endoscopy images.\n  - **High fidelity loss function:** We introduce *EndoL2H* loss which is a weighted hybrid loss function specifically optimized for endoscopic images. It collaboratively combines the strengths of perceptual, content, texture, and pixel-based loss descriptions and improves image quality in terms of pixel values, content, and texture. This combination leads to the maintenance of the image quality even under high scaling factors up to, 10x-12x.\n  - **Qualitative and quantitative study:** We conduct a detailed quantitative analysis to assess the effectiveness of our proposed approach and compare it to alternative state-of-the art approaches.\n \n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9984635760285501,
      "result": {
        "original_header": "Network Architecture",
        "type": "Text_excerpt",
        "value": "**a)** Overall system architecture of EndoL2H super-resolution framework. A low resolution input image is fed to the generator that creates an estimated high resolution counterpart, which is then served to the discriminator. The Markovian discriminator takes tuples of an LR input image and the corresponding HR image (real or generated), and tries to recognize whether the HR image is real or fake. Our generator is U-net with additional SAB layer which is sequentially downsampling tensor by factor of 2 until the latent feature representation and upsampled by the following decoder layers by a factor of 2. We are using convolutional PatchGAN as a classifier which penalize the structure in accordance with the image patch sizes (30x30).  **b)** is the flow diagram of the spatial attention block (SAB) which is selectively focus on clinically more relevant regions and also its output images are presented. **c)** The feature maps of attention U-Net which is the summary of applied filters and their input-output tensor sizes for 8x.The low resolution images are of 128x128 sizes and their size changes before and after each convolution layers are given. As seen, SAB block preserves the tensor sizes. Finally, the tensor size ends up with 1024x1024 for 8x.  \n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9960054129382205,
      "result": {
        "original_header": "Super-resolution results on 8\u00d7enlargement",
        "type": "Text_excerpt",
        "value": "Super-resolution  results  for  EndoL2H,  DBPN,  RCAN  and  SRGAN with cropped and zoomed regions inside the yellow squares on 8\u00d7upscale factor and for the five classes of the dataset, abnormal  classes:  esophagitis  (inflammatory  disease  of  esophagus),  polyps  (abnormal  growth  of  mucous  membrane  of small  and  large  intestine),  ulcerative  colitis  (inflammatory  bowel  disease), z-line (gastroesophageal junction images where the esophagus meetsthe stomach) and for pylorus images, respectively.  \nEach image pair consists of LR images and corresponding SAB output visualised using Grad-CAMmethodology  [50].  The  main  aim  of  SAB  is  to  selectively  put  emphasis  on  more  distinctive  regions  with  unusual  gradientalterations that inherently deserves attention as suspicious areas, in this case for example, regions with polyps, inflammatorytissue  or  normal  tissue  regions  containing  more  texture  and  sharp  edges. \n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8936946698797668,
      "result": {
        "original_header": "The evaluation of image groups in terms of structural similarity",
        "type": "Text_excerpt",
        "value": "**a)** Resulting GMSD maps. Red  color denotes  higher GMSD  values  indicating low  structural similarity with  the  original image  and blue color  represents  low  GMSD  values indicating a high  structural similarity with the original image. \n**b)** Resulting SSIM heat maps. Red  color  denotes  lower SSIM  values  representing a low  structural  similarity  with  the  original  image  and  blue  color  represent  high  SSIM  values representing a high structural similarity with the original image. \n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8686550420164232,
      "result": {
        "original_header": "Reproducibility",
        "type": "Text_excerpt",
        "value": "The GMSD, LPIPS, PSNR and SSIM z-scores of the algorithms EndoL2H, DBPN, RCAN and SRGAN are provided [here](https://drive.google.com/open?id=11AKGdmeBVQTR_mz7rxdBM26W7VQ8Rhso). To see the statistical significance analysis results between EndoL2H and the other algorithms, you can run `significance_analysis.ipynb` on Jupyter Notebook. \n- The pretrained model is saved at `./checkpoints/unet_256/latest_net_G.pth`.  \nTo compare the results of our model with DBPN and RCAN, you can download pretrained models of these two algorithms [here](https://drive.google.com/open?id=14kXMxQ74KulwNMBOJ4Efi587UTNiiaMW).\n \n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/CapsuleEndoscope/EndoL2H/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/psnr_ssim_calculations.ipynb"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/psnr_ssim_calculations.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/significance_analysis.ipynb"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/significance_analysis.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 8
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/CapsuleEndoscope/EndoL2H/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "CapsuleEndoscope/EndoL2H"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "EndoL2H: Deep Super-Resolution for Capsule Endoscopy"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/scripts/conda_deps.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/scripts/install_deps.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/EndoL2H: Deep Super-Resolution for Capsule Endoscopy</h1>\n<p>Code, dataset, and trained models for "
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/EndoL2H: Deep Super-Resolution for Capsule Endoscopy</h1>\n<p>Code, dataset, and trained models for "
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/EndoL2H: Deep Super-Resolution for Capsule Endoscopy</h1>\n<p>Code, dataset, and trained models for "
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/EndoL2H: Deep Super-Resolution for Capsule Endoscopy</h1>\n<p>Code, dataset, and trained models for "
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/EndoL2H: Deep Super-Resolution for Capsule Endoscopy</h1>\n<p>Code, dataset, and trained models for "
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "1. Installation",
        "parent_header": [
          "EndoL2H: Deep Super-Resolution for Capsule Endoscopy",
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": "- Clone this repo:\n\n```bash\ncd ~\ngit clone https://github.com/akgokce/EndoL2H\ncd EndoL2H\n```\n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8013371570019835,
      "result": {
        "original_header": "Network Architecture",
        "type": "Text_excerpt",
        "value": "<p align=\"center\">\n<img src='imgs/architectureOverview.png' width=600/> \n</p>\n \n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8013371570019835,
      "result": {
        "original_header": "Super-resolution results on 8\u00d7enlargement",
        "type": "Text_excerpt",
        "value": "<p align=\"center\">\n  <img src='imgs/SR_performance.png' width=512/> \n</p> \n<p align=\"center\">\n  <img src='imgs/attention_map.png' width=512/> \n</p>\n \n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8013371570019835,
      "result": {
        "original_header": "The evaluation of image groups in terms of structural similarity",
        "type": "Text_excerpt",
        "value": "<p align=\"center\">\n  <img src='imgs/white_GMSD_SSIM.png' width=512/> \n</p>\n \n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8639982176011284,
      "result": {
        "original_header": "Reproducibility",
        "type": "Text_excerpt",
        "value": "You can download our pretrained model [here](https://drive.google.com/open?id=1rAi5i5vTdTwtJkWbz2gHaOXemLjaWNGr). \n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/CapsuleEndoscope/EndoL2H/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "Copyright (c) 2017, Jun-Yan Zhu and Taesung Park\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n--------------------------- LICENSE FOR pix2pix --------------------------------\nBSD License\n\nFor pix2pix software\nCopyright (c) 2016, Phillip Isola and Jun-Yan Zhu\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n----------------------------- LICENSE FOR DCGAN --------------------------------\nBSD License\n\nFor dcgan.torch software\n\nCopyright (c) 2015, Facebook, Inc. All rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\nNeither the name Facebook nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/LICENSE",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License",
        "parent_header": [
          "EndoL2H: Deep Super-Resolution for Capsule Endoscopy"
        ],
        "type": "Text_excerpt",
        "value": "This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details\n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "EndoL2H"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "CapsuleEndoscope"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 158977,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 12024,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 271,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2002.05459"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2002.05459 (2020)](https://arxiv.org/abs/2002.05459). \n\n## Overview\n\nWe propose and quantitatively validate a novel framework to learn a mapping from low-to-high resolution endoscopic images. We combine conditional adversarial networks with a spatial attention block to improve the resolution by up to factors of 8x, 10x, 12x, respectively. EndoL2H is generally applicable to any endoscopic capsule system and has the potential to improve diagnosis and better harness computational approaches for polyp detection and characterization.\n\nOur main contributions are as follows:\n  - **Spatial Attention-based Super Resolution cGAN:** We propose a spatial attention based super-resolution cGAN architecture specifically designed and optimized for capsule endoscopy images.\n  - **High fidelity loss function:** We introduce *EndoL2H* loss which is a weighted hybrid loss function specifically optimized for endoscopic images. It collaboratively combines the strengths of perceptual, content, texture, and pixel-based loss descriptions and improves image quality in terms of pixel values, content, and texture. This combination leads to the maintenance of the image quality even under high scaling factors up to, 10x-12x.\n  - **Qualitative and quantitative study:** We conduct a detailed quantitative analysis to assess the effectiveness of our proposed approach and compare it to alternative state-of-the art approaches.\n\n#### Summary of Our Work \n  \nA conditional GAN combined with spatial attention unit maps low resolution(LR) endoscopic images to diagnostically relevant  high resolution(HR) endoscopic images. Unlike an unconditional GAN, both the generator and discriminator observe the input LR images.\n\n<p align=\"center\">\n<img src='imgs/systemSummary.png' width=384/> \n</p>\n\n#### Network Architecture\n\n**a)** Overall system architecture of EndoL2H super-resolution framework. A low resolution input image is fed to the generator that creates an estimated high resolution counterpart, which is then served to the discriminator. The Markovian discriminator takes tuples of an LR input image and the corresponding HR image (real or generated), and tries to recognize whether the HR image is real or fake. Our generator is U-net with additional SAB layer which is sequentially downsampling tensor by factor of 2 until the latent feature representation and upsampled by the following decoder layers by a factor of 2. We are using convolutional PatchGAN as a classifier which penalize the structure in accordance with the image patch sizes (30x30).  **b)** is the flow diagram of the spatial attention block (SAB) which is selectively focus on clinically more relevant regions and also its output images are presented. **c)** The feature maps of attention U-Net which is the summary of applied filters and their input-output tensor sizes for 8x.The low resolution images are of 128x128 sizes and their size changes before and after each convolution layers are given. As seen, SAB block preserves the tensor sizes. Finally, the tensor size ends up with 1024x1024 for 8x. \n\n<p align=\"center\">\n<img src='imgs/architectureOverview.png' width=600/> \n</p>\n\n## Getting Started\n\n### 1. Installation\n\n- Clone this repo:\n\n```bash\ncd ~\ngit clone https://github.com/akgokce/EndoL2H\ncd EndoL2H\n```\n\n### 2. Prerequisites\n\n- Linux (Tested on Ubuntu 16.04)\n- NVIDIA GPU (Tested on Nvidia P100 using Google Cloud)\n- CUDA, CuDNN\n- Python 3\n- Pytorch>=0.4.0\n- torchvision>=0.2.1\n- dominate>=2.3.1\n- visdom>=0.1.8.3\n- scipy\n\n- Install [PyTorch](http://pytorch.org) and 0.4+ and other dependencies (e.g., torchvision, [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate)).\n  - For pip users, please type the command `pip install -r requirements.txt`.\n  - For Conda users, you can use an installation script `./scripts/conda_deps.sh`. Alternatively, you can create a new Conda environment using `conda env create -f environment.yml`.\n  \n### 3. Code Base Structure\n\nThe code base structure is explained below:\n- **train.py:** Script for image-to-image translation. It works for different models (with option '--model': e.g. pipx2pix, cyclegan, colorization) and various datasets (with option '--dataset_mode': e.g. aligned, unaligned, single, colorization). It creates model, dataset and visualizer given the option. Then, it does training. Use '--continue_train' and '--epoch_count' to resume your previous training. \n- **test.py:** You can use this script to test the model after training. First, it creates model and dataset given the option. Then, it runs interference for --num_test images and save results to an HTML file. Use '--results_dir' to specify the results directory. \n- **networks.py:** It contains PyTorch model definitions for all network.\n- **base_options.py:** It defines options used during both training and test time.\n- **psnr_ssim_calculations.ipynb:** You can use this script to see overall and fold by fold PSNR and SSIM results.\n- **combine_A_and_B.py:** pix2pix training requires paired data. It generates training data in the form of pairs of images {A,B"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "2. Prerequisites",
        "parent_header": [
          "EndoL2H: Deep Super-Resolution for Capsule Endoscopy",
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": "- Linux (Tested on Ubuntu 16.04)\n- NVIDIA GPU (Tested on Nvidia P100 using Google Cloud)\n- CUDA, CuDNN\n- Python 3\n- Pytorch>=0.4.0\n- torchvision>=0.2.1\n- dominate>=2.3.1\n- visdom>=0.1.8.3\n- scipy\n\n- Install [PyTorch](http://pytorch.org) and 0.4+ and other dependencies (e.g., torchvision, [visdom](https://github.com/facebookresearch/visdom) and [dominate](https://github.com/Knio/dominate)).\n  - For pip users, please type the command `pip install -r requirements.txt`.\n  - For Conda users, you can use an installation script `./scripts/conda_deps.sh`. Alternatively, you can create a new Conda environment using `conda env create -f environment.yml`.\n  "
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "run",
    "download",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-06 11:17:05",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 19
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "3. Code Base Structure",
        "parent_header": [
          "EndoL2H: Deep Super-Resolution for Capsule Endoscopy",
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": "The code base structure is explained below:\n- **train.py:** Script for image-to-image translation. It works for different models (with option '--model': e.g. pipx2pix, cyclegan, colorization) and various datasets (with option '--dataset_mode': e.g. aligned, unaligned, single, colorization). It creates model, dataset and visualizer given the option. Then, it does training. Use '--continue_train' and '--epoch_count' to resume your previous training. \n- **test.py:** You can use this script to test the model after training. First, it creates model and dataset given the option. Then, it runs interference for --num_test images and save results to an HTML file. Use '--results_dir' to specify the results directory. \n- **networks.py:** It contains PyTorch model definitions for all network.\n- **base_options.py:** It defines options used during both training and test time.\n- **psnr_ssim_calculations.ipynb:** You can use this script to see overall and fold by fold PSNR and SSIM results.\n- **combine_A_and_B.py:** pix2pix training requires paired data. It generates training data in the form of pairs of images {A,B}, where A and B are two different depictions of the same underlying scene. Corresponding images in a pair {A,B} must be the same size and have the same filename. Once the data is formatted this way, call:\n\n```bash\npython datasets/combine_A_and_B.py --fold_A /path/to/data/A --fold_B /path/to/data/B --fold_AB /path/to/data\n```\n\nThis will combine each pair of images (A,B) into a single image file, ready for training.\n  "
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "4. Dataset",
        "parent_header": [
          "EndoL2H: Deep Super-Resolution for Capsule Endoscopy",
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": "- Our dataset is a part of [The Kvasir Dataset](https://datasets.simula.no/kvasir/\n).\n- The data split we used in training can be downloaded [here](https://drive.google.com/open?id=1gvXF3hcYoJaeSyNWcbwgtsV2_gwHLRDr).\n- After downloading the dataset, to create 5 folds:\n```bash\ncd EndoL2H/datasets\npython 5_fold.py\n```\n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "5. Dataset Organization",
        "parent_header": [
          "EndoL2H: Deep Super-Resolution for Capsule Endoscopy",
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": "Data needs to be arranged in the following format:\n\n```python\nEndoL2H                 # Path to main folder\n\u2514\u2500\u2500 datasets            # Folder of all datasets\n      \u2514\u2500\u2500 dataset_xxx   # Name of a dataset\n            |\n            \u251c\u2500\u2500 A       # High resolution images\n            |   \u251c\u2500\u2500 fold1\n            |   |    \u251c\u2500\u2500train\n            |   |    |   \u251c\u2500\u25001.jpg\n            |   |    |   \u251c\u2500\u25002.jpg\n            |   |    |   \u251c\u2500\u2500 ...\n            |   |    \u251c\u2500\u2500 test\n            |   |    \u2514\u2500\u2500 val\n            |   \u251c\u2500\u2500 fold2       \n            |   |     \n            |   \u251c\u2500\u2500 fold3   \n            |   |     \n            |   \u251c\u2500\u2500 fold4   \n            |   |       \n            |   \u2514\u2500\u2500 fold5\n            |\n            \u2514\u2500\u2500 B       # Low resolution images\n                \u251c\u2500\u2500 fold1\n                |    \u251c\u2500\u2500train\n                |    |   \u251c\u2500\u25001.jpg\n                |    |   \u251c\u2500\u25002.jpg\n                |    |   \u251c\u2500\u2500 ...\n                |    \u251c\u2500\u2500 test\n                |    \u2514\u2500\u2500 val\n                \u251c\u2500\u2500 fold2       \n                |     \n                \u251c\u2500\u2500 fold3   \n                |     \n                \u251c\u2500\u2500 fold4   \n                |       \n                \u2514\u2500\u2500 fold5\n                    \n\u2514\u2500\u2500 checkpoints \n     |\n     \u2514\u2500\u2500 generator_name #e.g. unet256, unet128, resnet_6blocks, resnet_9blocks\n         \u251c\u2500\u2500 web\n         |     \u251c\u2500\u2500 images\n         |     |    \u251c\u2500\u2500 epoch001_fake_B.png\n         |     |    \u251c\u2500\u2500 epoch001_real_A.png\n         |     |    \u251c\u2500\u2500 epoch001_real_B.png\n         |     |    \u251c\u2500\u2500 ...\n         |     \u2514\u2500\u2500 index.html\n         |\n         \u251c\u2500\u2500 latest_net_D.pth\n         \u251c\u2500\u2500 latest_net_G.pth\n         \u251c\u2500\u2500 opt.txt\n         \u2514\u2500\u2500 loss_log.txt\n```\n\n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "6. Training",
        "parent_header": [
          "EndoL2H: Deep Super-Resolution for Capsule Endoscopy",
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": " To train a model:\n\n```bash\npython train.py --dataroot ./datasets/${nameOfDataset} --name unet_256 --model pix2pix --netG unet_256 --dataset_mode aligned --direction BtoA --preprocess none\n```\n\n- To see more intermediate results, check out  `./checkpoints/unet_256/web/index.html`.\n- To view training results and loss plots, run `python -m visdom.server` and click the URL <http://localhost:8097.>\n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "7. Testing",
        "parent_header": [
          "EndoL2H: Deep Super-Resolution for Capsule Endoscopy",
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": "To test the model:\n\n```bash\npython test.py --dataroot ./datasets/${nameOfDataset} --name unet_256 --model pix2pix --netG unet_256 --dataset_mode aligned --direction BtoA --preprocess none\n```\n\n- The test results will be saved to a html file here: `./results/unet_256/test_latest/index.html`.\n"
      },
      "source": "https://raw.githubusercontent.com/CapsuleEndoscope/EndoL2H/master/README.md",
      "technique": "header_analysis"
    }
  ]
}