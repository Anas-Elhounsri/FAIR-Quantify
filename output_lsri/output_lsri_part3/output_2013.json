{
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citation",
        "parent_header": [
          "Advanced topics"
        ],
        "type": "Text_excerpt",
        "value": "If you find Discount useful in your research, please cite our paper:\n\nJohan Nystr\u00f6m-Persson, Gabriel Keeble-Gagn\u00e8re, Niamat Zawad, Compact and evenly distributed k-mer binning for genomic \nsequences, Bioinformatics, 2021;, btab156, https://doi.org/10.1093/bioinformatics/btab156\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "References",
        "parent_header": [
          "Advanced topics"
        ],
        "type": "Text_excerpt",
        "value": "1. Petrillo, U. F., Roscigno, G., Cattaneo, G., & Giancarlo, R. (2017). FASTdoop: A versatile and efficient library for \n   the input of FASTA and FASTQ files for MapReduce Hadoop bioinformatics applications. Bioinformatics, 33(10), \n   1575\u20131577.\n2. Ekim, B.et al.(2020).  A Randomized Parallel Algorithm for Efficiently Finding Near-Optimal Universal Hitting Sets.  \n   In R. Schwartz,  editor, Research  in  Computational  Molecular  Biology,  pages  37\u201353,  Cham.Springer International \n   Publishing \n\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/jtnystrom/Discount"
      },
      "technique": "GitHub_API"
    }
  ],
  "contributing_guidelines": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "Contributions to this project are most welcome, for example in the form of pull requests, bug reports, or feature requests.\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/CONTRIBUTING.md",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Contributing",
        "parent_header": [
          "Advanced topics"
        ],
        "type": "Text_excerpt",
        "value": "Contributions are very welcome, for example in the form of bug reports,\npull requests, or general suggestions.\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-10-09T03:14:21Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-10-01T03:51:34Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Very large scale k-mer counting and analysis on Apache Spark."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9448431635875668,
      "result": {
        "original_header": "Discount",
        "type": "Text_excerpt",
        "value": "Discount is a Spark-based tool for k-mer (genomic sequences of length k) counting and analysis. \nIt is able to analyse large metagenomic-scale datasets while having a small memory footprint. \nIt can be used as a standalone command line tool, but also as a general Spark library, including in interactive \nnotebooks. \nDiscount is highly scalable. It has been tested on the [Serratus](https://serratus.io/) dataset for a total of 5.59 \ntrillion k-mers (5.59 x 10^12) with 1.57 trillion distinct k-mers. \n\nThis software includes [Fastdoop](https://github.com/umbfer/fastdoop) by U.F. Petrillo et al [1].\nWe have also included compact universal hitting sets generated by [PASHA](https://github.com/ekimb/pasha) [2].\n \nFor a detailed background and description, please see \n[our paper on evenly distributed k-mer binning](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btab156/6162158).\n \n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9428508264075967,
      "result": {
        "original_header": "Minimizers",
        "type": "Text_excerpt",
        "value": "Discount counts k-mers by constructing super k-mers (supermers) and shuffling these into bins. Each bin corresponds to \na minimizer, which is the minimal m-mer for some m < k in each k-mer, for some ordering of a minimizer set.\nThe choice of minimizer set and ordering does not affect k-mer counting results, but can have a big effect on performance. \nBy default, Discount will use internal minimizers, which are packaged into the jar from the [resources/PASHA](resources/PASHA) \ndirectory. These are universal hitting sets, available for k >= 19, m=9,10,11. \nBy default, they will be ordered by sampled frequency in the dataset being analysed, prioritising uncommon minimizers \nover common ones. \nTo manually select a minimizer set, it is possible to point Discount to a file containing a set, or to a directory \ncontaining minimizer sets. For example: \nIt is also possible (but less efficient ) to operate without a minimizer set, in which case all m-mers will become \nminimizers.  This can be done using the `--allMinimizers` flag. Currently, this flag is must be used when k < 19 as we \ndo not supply minimizer sets in that range: \n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.974017385397246,
      "result": {
        "original_header": "Performance tuning for large datasets",
        "type": "Text_excerpt",
        "value": "The philosophy of Discount is to achieve performance through a large number of small and evenly sized bins, \nwhich are grouped into a large number of modestly sized Spark partitions. This reduces memory pressure as well as CPU \ntime for many of the algorithms that need to run. For most datasets, the default settings should be fine.\nHowever, for huge datasets or constrained environments, the pointers below may be helpful. \n1. Increase m. For very large datasets, with e.g. more than 10<sup>11</sup> total k-mers, m=11 (or more) may be helpful.\n   This would generate a larger number of bins (which would be smaller) by using a larger universal hitting set.\n   However, if m is too large relative to the dataset, then a slowdown may be expected. As of version 2.3, we have \n   tested up to m=13.   \n2. Increase the number of partitions using the `-p` argument. However, if the number is \n   too large, shuffling will be slower, sometimes dramatically so.\n3. Increase the number of input splits by reducing the maximum split size. This affects the number of tasks in the \n   hashing stage. This can be done in the run scripts, however the same caveat as above applies: the number of tasks \n   should not be too large for the data. \nFor very large datasets, it is helpful to understand where the difficulties come from. For a repetitive dataset, \nusing `--method pregrouped` will have large benefits. On the other hand, for a highly complex dataset with many distinct k-mers, \nincreasing m can help by spreading the k-mers into more bins. For some datasets, it may be\nnecessary to use both of these techniques. \nIn general, it is helpful to monitor CPU usage to make sure that the job is not I/O bound (if it is well configured, \nCPU utilisation should be close to 100% on average). \nTo help with I/O pressure, fast SSDs and/or different partition sizes may help. \n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9607677385351158,
      "result": {
        "original_header": "Compiling Discount",
        "type": "Text_excerpt",
        "value": "To compile the software, the SBT build tool (https://www.scala-sbt.org/) is needed.\nDiscount is by default compiled for Scala 2.12/Spark 3.1. A Scala 2.13 branch is also available. \nAs of version 3.0.0, we compile on JDK 11 by default (but backwards compatible so that the jars can run on JDK 8).\nTo compile on JDK 8, it is necessary to edit `build.sbt` and remove the `--release 8` options. \n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/jtnystrom/discount/tree/master/docs"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "wiki",
        "type": "Url",
        "value": "https://github.com/jtnystrom/Discount/wiki"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/jtnystrom/discount/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/jtnystrom/Discount/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "jtnystrom/Discount"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jtnystrom/discount/master/discount-shell.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jtnystrom/discount/master/discount-aws.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jtnystrom/discount/master/discount-gcloud.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jtnystrom/discount/master/discount.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "type": "Text_excerpt",
        "value": "The easiest way to obtain Discount is to download a pre-built release from the \n[Releases](https://github.com/jtnystrom/Discount/releases) page.\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "K-mer counting",
        "parent_header": [
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "The following command produces a statistical summary of a dataset.\n \n`\n./discount.sh -k 55 /path/to/data.fastq stats\n`\n\nAll example commands shown here accept multiple input files. The FASTQ and FASTA formats are supported,\nand must be uncompressed.\n\nTo submit an equivalent job to AWS EMR, after creating a cluster with id j-ABCDEF1234 and uploading the necessary files\n(the GCloud script `discount-gcloud.sh` works in the same way):\n\n`\n./discount-aws.sh j-ABCDEF1234 -k 55 s3://my-data/path/to/data.fastq stats\n`\n\nAs of version 2.3, minimizer sets for k >=19, m=10,11 are bundled with Discount and do not need to be specified\nexplicitly. Advanced users may wish to override this ([see the section on minimizers](#minimizers))\n\nTo generate a full counts table with k-mer sequences (in many cases larger than the input data),\nthe `count` command may be used:\n\n`\n./discount.sh -k 55 /path/to/data.fastq count -o /path/to/output/dir --sequence\n`\n\nA new directory called `/path/to/output/dir_counts` (based on the location specified with `-o`) will be created for the \noutput.\n\nUsage of upper and lower bounds filtering, histogram generation, normalization of\n k-mer orientation, and other functions, may be seen in the online help:\n\n```\n./discount.sh --help\n./discount.sh count --help\n```\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Chromosomes and very long sequences",
        "parent_header": [
          "Installation",
          "K-mer counting"
        ],
        "type": "Text_excerpt",
        "value": "If the input data contains sequences longer than 1,000,000 bp, you must use the `--maxlen` flag to specify the longest\nexpected single sequence length. However, if the sequences in a FASTA file are very long (for example full chromosomes),\nit is essential to generate a FASTA index (.fai). Various tools can be used to do this, for example with \n[SeqKit](https://github.com/shenwei356/seqkit):\n\n`\nseqkit faidx myChromosomes.fasta\n`\n\nDiscount will detect the presence of the `myChromosomes.fasta.fai` file and read the data efficiently. In this case, \nthe parameter `--maxlen` is not necessary.\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Repetitive or very large datasets",
        "parent_header": [
          "Installation",
          "K-mer counting"
        ],
        "type": "Text_excerpt",
        "value": "As of version 2.3, Discount contains two different counting methods, the \"simple\" method, which was the only method \nprior to this version, and the \"pregrouped\" method, which is essential for data that contains highly repetitive k-mers.\nThe pregrouped method counts each distinct super-mer separately prior to k-mer counting.\nDiscount will try to pick the best method automatically, but we would advise users to do their own experiments. \nIf Spark crashes with an exception about buffers being too large, the pregrouped method may also help. It can be forced \nwith a command such as:\n\n`\n./discount.sh --method pregrouped -k 55 /path/to/data.fastq stats\n`\n\nOr, to force the simple method to be used:\n\n`\n./discount.sh --method simple -k 55 /path/to/data.fastq stats\n`\n\nWhile highly scalable, the pregrouped method may sometimes cause a slowdown overall (by requiring one additional shuffle), \nso it should not be used for datasets that do not need it. See the section on [performance tuning](#performance-tuning-for-large-datasets).\n\nAdditional examples may be found in the [wiki](https://github.com/jtnystrom/Discount/wiki).\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "K-mer indexes",
        "parent_header": [
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "Discount can store a multiset of counted k-mers as an index (k-mer database). Indexes can be combined by various \noperations, inspired by the design of `kmc_tools` in [KMC3](https://github.com/refresh-bio/KMC).\nThey are stored in the Apache Parquet format, allowing for a high degree of compression and efficiency in the cloud.\n\nTo create a new index, the `store` command may be used:\n\n`\ndiscount.sh -k 35 input.fasta store -o index_path\n`\n\nThe directory `index_path` will be created and index files will be written to it (or overwritten if they already existed). \nAlongside it, the files `index_path_minimizers.txt` and `index_path.properties` will record the minimizer ordering and \nsome other parameters of the index. These files should not be manually edited or moved.\n\nBy using the `-i` parameter, an index can be used instead of sequence files as a source of input data. \nFor example, k-mers with minimum count 2 can be obtained from an index and written to a set of fasta files by using the \n`count` command from above in the following way:\n\n`\ndiscount.sh -i index_path --min 2 count -o index_min2\n`\n\nSummary statistics for an index can be obtained with this command:\n\n`\ndiscount.sh -i index_path stats\n`\n\nExcept for `union`, `subtract`, and `intersect`, only one index can be used as an input at once. When a new index is \ncreated (like `index_min2` above) it should always be written in a new location. The same location can not simultaneously \nbe both an input and an output.\n\nIndexes may be combined using binary operations such as `intersect`, `union`, and `subtract`. For example, to create the \nintersection of two indexes using the minimum count from either index:\n\n`\ndiscount.sh -i index1_path intersect -i index2_path -r min -o i1i2_min_path\n`\n\nMultiple indexes may be combined at once with the same rule. For example, to union three indexes at once with the \nmaximum rule:\n\n`\ndiscount.sh -i index1_path union -r max -i index2_path index3_path -o union3_path\n`\n\nThe various rules have the same meaning as in KMC3:\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Intersection",
        "parent_header": [
          "Installation",
          "K-mer indexes"
        ],
        "type": "Text_excerpt",
        "value": "* max: choose the maximum of the count of each k-mer in index 1 and index 2 \n* min: choose the minimum \n* left: choose the count from index 1 \n* right: choose the count from index 2\n* sum: the sum of the counts in index 1 and index 2\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Union",
        "parent_header": [
          "Installation",
          "K-mer indexes"
        ],
        "type": "Text_excerpt",
        "value": "Union supports the same rules as intersection does, except that if the k-mer is present\nin only one index, it is still kept and then the value from that index is used.\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Subtract",
        "parent_header": [
          "Installation",
          "K-mer indexes"
        ],
        "type": "Text_excerpt",
        "value": "* kmers_subtract: k-mers that were present in index 1, but not in index 2, are kept. Counts remain as they were in \nindex 1.\n* counters_subtract: the count in index 2 is subtracted from the count in index 1. \nOnly k-mers with positive values after subtraction are kept.\n\nFor additional guidance, consult the command line help for each command, e.g.:\n\n`\ndiscount.sh intersect --help\n`\n\nMore examples can be found in the [wiki](https://github.com/jtnystrom/Discount/wiki).\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Partitions",
        "parent_header": [
          "Installation",
          "K-mer indexes"
        ],
        "type": "Text_excerpt",
        "value": "For each index, a number of parquet files will be created in the corresponding directory. The number of partitions \ncorresponds to the number of *shuffle partitions* that Spark uses. To set the number of partitions, the `-p` argument \nmay be used. For example, for a very large index, creating 10,000 partitions may be helpful: \n\n`\ndiscount.sh -k 35 -p 10000 input.fasta store -o index_path\n`\n\nThis should be tuned so that the resulting files are neither too large nor too small.\nThe `reindex` command can be used to change the number of partitions of an existing index after \nconstruction.\nAlso see [the section on performance tuning](#performance-tuning-for-large-datasets) below.\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Compatible indexes",
        "parent_header": [
          "Installation",
          "K-mer indexes"
        ],
        "type": "Text_excerpt",
        "value": "The combination operations only work if the indexes being combined have the same values of `k` and `m`, and were created\nusing the same minimizer ordering (called a compatible index).  \nTo help create a compatible index, the `-c` flag is provided for the store operation. For example:\n\n`\ndiscount.sh input2.fasta store -c index1_path -o index2_path\n`\n\nHere, index settings will be copied from `index1_path` and reused, which creates a compatible index when indexing \n`input2.fasta` into index2.\n\nWhen necessary, a pre-existing index can be converted to a different minimizer ordering using the reindex command, for \nexample in the following way:\n\n`\ndiscount.sh -i index1_path reindex -c index2_path -o index3_path\n`\n\nThis will create a new copy of index1 according to the parameters in index2, saving it as index3. After this step, \nindex2 and index3 can be combined. However, this will usually reduce the level of data\ncompression, so it is recommended to avoid reindexing when possible.\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Interactive notebooks and REPL",
        "parent_header": [
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "Discount is well suited for data analysis in interactive notebooks. A demo notebook for [Apache Zeppelin](https://zeppelin.apache.org/) is included in the \n`notebooks/` directory. It has been tested with Zeppelin 0.10.1 and Spark 3.1.2.\n(As of Zeppelin 0.10.1, beware that Spark versions above 3.1.2 are not supported out of the box, so we recommend using that version for notebooks.)\n\nTo try this out, after downloading the Spark distribution, also [download Zeppelin](https://zeppelin.apache.org/).  \n(The smaller \"Netinst\" distribution is sufficient, but an external Spark is necessary.)\nThen, load the notebook itself into Zeppelin through the browser to see example use cases and instructions.\n\nThe API examples from the notebook can also for the most part be used unchanged in the Spark shell (Scala REPL).\nFor example, to intersect k-mers from two sequence files, filtering the k-mer counts of one of them, after starting the shell using `discount-shell.sh`:\n\n```scala\nimport com.jnpersson.discount.spark._\nimplicit val sp = spark\nval discount = new Discount(k = 28)\nval discountRoot = \"/path/to/Discount\"\nval i1 = discount.index(s\"$discountRoot/testData/SRR094926_10k.fasta\")\nval i2 = discount.index(i1, s\"$discountRoot/testData/ERR599052_10k.fastq\")\ni1.intersect(i2.filterMin(2), Rule.Max).showStats()\n```\n\nUsing the tool from the REPL in this way, instead of the command-line, can be an efficient alternative when working with temporary indexes, as they can be \navailable for use in-memory without being stored on disk.\n\nFor both notebook and REPL use, it is recommended to consult the API docs\n([available for the latest release here](https://jtnystrom.github.io/Discount/com/jnpersson/discount/spark/index.html)).\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Tips",
        "parent_header": [
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "* Visiting http://localhost:4040 (if you run a standalone Spark cluster) in a browser will show progress details while\n  Discount is running.\n  \n* If you are running a local standalone Spark (everything in one process) then it is helpful to increase driver memory \nas much as possible (this can be configured in discount.sh). Pointing LOCAL_DIR to a fast drive for temporary data \n  storage is also highly recommended. \n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Generating a universal hitting set",
        "parent_header": [
          "Advanced topics"
        ],
        "type": "Text_excerpt",
        "value": "For optimal performance, compact universal hitting sets (of m-mers) should be used as minimizer sets.\nThey may be generated using the [PASHA](https://github.com/ekimb/pasha) tool.\nPrecomputed sets for many values of k and m may also be downloaded from the \n[PASHA website](http://pasha.csail.mit.edu/).\n(Note that the PASHA authors use the symbols (k, L) instead of (m, k), which we use here. \nTheir k corresponds to minimizer length, which we denote by m.)\n\nA universal set generated for some pair of parameters (k, m) will also work for a larger k. However, the larger the gap, \nthe greater the number of bins generated by Discount and the shorter the superkmers would be. This can negatively impact \nperformance.\n\nComputed sets must be combined with their corresponding decycling set (available at the link above), \nfor example as follows:\n\n`\ncat PASHA11_30.txt decyc11.txt > minimizers_30_11.txt\n`\n\nThis produces a set that is ready for use with Discount.\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9566736710162353,
      "result": {
        "original_header": "Minimizers",
        "type": "Text_excerpt",
        "value": "We provide some additional minimizer sets at\nhttps://jnpsolutions.io/minimizers. You can also generate your own set using PASHA (described below). \n`\n./discount.sh -m 10 --minimizers resources/PASHA/minimizers_55_10.txt -k 55 /path/to/data.fastq stats\n` \nIf you instead supply a directory, the best minimizer set in that directory will be chosen automatically,\nby looking for files with the name minimizers_{k}_{m}.txt: \n`\n./discount.sh -k 17 --allMinimizers /path/to/data.fastq stats\n`\n \n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8064249787523758,
      "result": {
        "original_header": "Evaluation of minimizer orderings",
        "type": "Text_excerpt",
        "value": "Discount can be used to evaluate the bin distributions generated by various minimizer orderings.\nSee [docs/Minimizers.md](docs/Minimizers.md) for details.\n \n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9923584740122178,
      "result": {
        "original_header": "Compiling Discount",
        "type": "Text_excerpt",
        "value": "To compile the software, the SBT build tool (https://www.scala-sbt.org/) is needed.\nDiscount is by default compiled for Scala 2.12/Spark 3.1. A Scala 2.13 branch is also available. \nThe command `sbt assembly` will compile the software and produce the necessary jar file in\ntarget/scala-2.12/Discount-assembly-x.x.x.jar. This will be a \"fat\" jar that also contains some necessary dependencies. \nAs of version 3.0.0, we compile on JDK 11 by default (but backwards compatible so that the jars can run on JDK 8).\nTo compile on JDK 8, it is necessary to edit `build.sbt` and remove the `--release 8` options. \nAPI documentation may be generated using the command `sbt doc`. \n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/jtnystrom/Discount/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "bioinformatics, genomics, kmer-counting, kmer-frequency-count, kmers, scala, spark"
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "GNU General Public License v3.0",
        "spdx_id": "GPL-3.0",
        "type": "License",
        "url": "https://api.github.com/licenses/gpl-3.0",
        "value": "https://api.github.com/licenses/gpl-3.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License and support",
        "parent_header": [
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "For any inquiries, please contact JNP Solutions at [info@jnpsolutions.io](mailto:info@jnpsolutions.io). We will do our\nbest to help. Alternatively, feel free to open issues and/or PRs in the GitHub repo if you find a problem.\n\nDiscount is currently released under a dual GPL/commercial license. For a commercial license, custom development, or \ncommercial support please contact us at the email above.\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "logo": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jtnystrom/discount/master/images/discountZeppelin.png"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Discount"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "jtnystrom"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Scala",
        "size": 240889,
        "type": "Programming_language",
        "value": "Scala"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Java",
        "size": 55500,
        "type": "Programming_language",
        "value": "Java"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 5491,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2023-02-13T01:23:41Z",
        "date_published": "2023-02-13T06:40:54Z",
        "description": "Bugfix for self-union or self-intersection of indexes (in v 3.0.0, this caused a cartesian product).\r\n\r\nAdditional convenience methods for Index, e.g. unionLeft, unionRight, etc.",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v3.0.1",
        "name": "Version 3.0.1",
        "release_id": 92175944,
        "tag": "v3.0.1",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v3.0.1",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/92175944",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/92175944",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v3.0.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2023-01-16T07:00:02Z",
        "date_published": "2023-01-16T06:36:34Z",
        "description": "This version adds indexes (k-mer databases with counted k-mers) and ways to combine these, including intersect, union and subtract. Various rules for intersection and union are available, including max, min, left, right. Most operations that could formerly be done only on raw sequence files can now also be done on indexes with a similar syntax.\r\nIndexes are stored using bucketed parquet files, which gives good efficiency when using the same input data multiple times, as the k-mers do not have to be shuffled again during subsequent use.\r\n\r\nIndexes can be manipulated using the command-line interface as well as the API from notebooks or from the Spark shell.\r\n\r\nSummary of changes in this version:\r\n\r\n* The minimum supported Spark version is now 3.1.0.\r\n* Support for indexes (k-mer databases) written as parquet files. \r\n* Index operations such as union, intersect, subtract, with various combination rules like min, max, sum, left, right.\r\n* Restructured the API to use indexes as much as possible.\r\n*  Several operations were moved to Spark SQL (from handcrafted Scala) for performance and simplicity.\r\n*  Run scripts were renamed and can now detect their location, which makes it easy to symlink them to somewhere in $PATH.\r\n* The new -p flag is now the preferred way to specify the number of partitions.\r\n* Most commands that take input can now read input from an index (using -i) as well as from sequence files.\r\n* K-mer counts are now consistently represented as Int instead of Long in the user API as they were limited to 32-bit signed integers internally.\r\n* Added com.globalmentor's hadoop-bare-naked-local-fs to avoid dependency on winutils.exe on Windows when running tests.\r\n* Various simplifications and speedups.\r\n\r\n",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v3.0.0",
        "name": "Version 3.0.0",
        "release_id": 88810661,
        "tag": "v3.0.0",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v3.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/88810661",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/88810661",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v3.0.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2022-06-06T05:49:49Z",
        "date_published": "2022-06-06T06:00:04Z",
        "description": "Version 2.3.0 greatly increases the maximum data size that can be analysed (we have tested up to 6 TB of input data). As some very minor changes are incompatible, API users may need to manually migrate some code.\r\n\r\n* Pre-grouped mode for handling repetitive or very large data, which can be enabled with `--method pregrouped`. \r\n* Some minimizer sets are now bundled in the Discount jar, which means that many users will not need to supply minimizers manually.\r\n* Improved support for large m (up to 13), which helps subdivide complex data with many distinct k-mers.\r\n* Automatic coalescing of partitions in frequency sampling when appropriate (improves performance).\r\n* Support for `@inputs.txt` syntax to supply a list of input files on the command line.\r\n* More efficient frequency sampling by doing the sampling entirely in Spark SQL, instead of partially on the driver.",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v2.3.0",
        "name": "Version 2.3.0",
        "release_id": 68685814,
        "tag": "v2.3.0",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v2.3.0",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/68685814",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/68685814",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v2.3.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2022-03-10T06:50:31Z",
        "date_published": "2022-03-11T03:27:47Z",
        "description": "This release fixes a data loss bug in the parsing of some fastq files.",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v2.2.1",
        "name": "Version 2.2.1",
        "release_id": 61563254,
        "tag": "v2.2.1",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v2.2.1",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/61563254",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/61563254",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v2.2.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2022-02-08T07:13:19Z",
        "date_published": "2022-02-08T07:16:54Z",
        "description": "* Improved support for very long fasta sequences (e.g. full chromosomes), even for multiple sequences per file.  This is done by relying on an external .fai index, which is now necessary for sequences with unbounded length.\r\n* File input formats can now be mixed (e.g. fastq, fasta, long fasta can be read by the same job).\r\n* k-mer statistics can now optionally be written to an output file using a new argument (not just to standard output as before).\r\n* For convenience, additional PASHA minimizer sets for k >= 19, m=10,11 were added to the distribution.",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v2.2.0",
        "name": "Version 2.2.0",
        "release_id": 58862259,
        "tag": "v2.2.0",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v2.2.0",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/58862259",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/58862259",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v2.2.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2021-10-22T02:54:59Z",
        "date_published": "2021-10-22T03:00:10Z",
        "description": "* Classes were restructured under the com.jnpersson.discount package (instead of simply \"discount\") to comply with normal Java/Scala conventions. This is a breaking change for API users, but should be a simple migration.\r\n* Faster algorithms for read splitting and bitwise encoding.\r\n* Sampling and input parsing has changed into a unified API that is consistent across short reads and long sequences, and that samples long sequences more fairly.\r\n* Foundational work towards preserving the sequence locations of input sequence fragments.\r\n* Additional test cases for different kinds of input data.",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v2.1.0",
        "name": "Version 2.1.0",
        "release_id": 51834698,
        "tag": "v2.1.0",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v2.1.0",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/51834698",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/51834698",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v2.1.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2021-08-30T06:39:01Z",
        "date_published": "2021-08-30T06:40:20Z",
        "description": "This release fixes a bug where long, multiline input sequences were not handled correctly and k-mer counts would occasionally be wrong, along with some other minor improvements.",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v2.0.1",
        "name": "Version 2.0.1",
        "release_id": 48574445,
        "tag": "v2.0.1",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v2.0.1",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/48574445",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/48574445",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v2.0.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2021-08-20T08:48:41Z",
        "date_published": "2021-08-20T09:02:35Z",
        "description": "This version includes the following improvements.\r\n\r\n* Nearly 50% faster counting due to better algorithms, including a version of radix sort from the Fastutil library\r\n* Automatic selection of the most appropriate minimizer set from a directory, by matching with the desired (k, m) values\r\n* Support for interactive notebooks (a Zeppelin example is included) and a restructured API to support this\r\n* Hashed superkmers can now be queried by sequences to find matching k-mers\r\n* Support for lowercase nucleotide letters in input\r\n* Support for user-defined minimizer orderings (-o given)\r\n* Various simplifications and enhancements\r\n",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v2.0.0",
        "name": "Version 2.0.0",
        "release_id": 48167651,
        "tag": "v2.0.0",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v2.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/48167651",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/48167651",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v2.0.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2021-04-24T06:56:00Z",
        "date_published": "2021-04-24T07:12:46Z",
        "description": "Version 1.4.0 has the following improvements:\r\n\r\n* Scala 2.12/Spark 3.1 are now the default versions when compiling.\r\n* Bugfix for incorrect counting when k mod 16 = 0.\r\n* sbt-assembly is now the preferred way to package Discount, including its dependencies (Scallop and Fastdoop) in a \"fat\" jar.\r\n* Additional property-based unit tests using ScalaCheck.\r\n* A minimal demo application (ReadSplitDemo) shows how to use the Discount API without Spark.\r\n* Various simplifications, code cleanups and speedups.",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v1.4.0",
        "name": "Version 1.4.0",
        "release_id": 41936859,
        "tag": "v1.4.0",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v1.4.0",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/41936859",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/41936859",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v1.4.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2021-03-03T09:58:47Z",
        "date_published": "2021-03-04T01:47:25Z",
        "description": "Version 1.3.0 has the following improvements:\r\n\r\n* Improved performance for large m\r\n* Reduced memory usage in the hashing stage\r\n* Fixed a bug that caused Discount to crash on empty inputs\r\n* Improved command line argument validation\r\n* Renamed the output path for count --stats\r\n* Renamed the command line arguments --motif-set and --stats to --minimizers and --buckets, respectively, for improved clarity\r\n\r\n",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v1.3.0",
        "name": "Version 1.3.0",
        "release_id": 39241750,
        "tag": "v1.3.0",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v1.3.0",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/39241750",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/39241750",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v1.3.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2021-01-22T02:11:33Z",
        "date_published": "2021-01-22T02:26:18Z",
        "description": "Version 1.2.0 has the following improvements:\r\n\r\n* Includes PASHA sets for k = 28,55 instead of DOCKS sets for k = 20,50\r\n* Support for random minimizer orderings\r\n* Human-readable minimizer output in per-bucket stats for minimizer analysis\r\n* Additional unit tests\r\n* Bugfixes for motifs at the very start of a k-length window, which were not properly detected during hashing\r\n* Bugfix for handling of EOF in Fastdoop\r\n\r\n",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v1.2.0",
        "name": "Version 1.2.0",
        "release_id": 36746289,
        "tag": "v1.2.0",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v1.2.0",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/36746289",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/36746289",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v1.2.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2020-11-19T04:31:05Z",
        "date_published": "2020-11-19T05:04:55Z",
        "description": "Version 1.1.0 adds:\r\n\r\n* FASTA output by default when writing a counts table (--tsv can be used to get a simple tsv table)\r\n* Normalization of k-mer orientation (forward and reverse complement treated as the same value). This is a little slower than the non-normalized mode, however.\r\n* Configurable input split sizes in the run scripts (instead of hardcoded as before)\r\n* A run script for AWS EMR (experimental)\r\n* Improved command line help and validation of parameters\r\n\r\nPlus various enhancements and bug fixes.\r\n\r\nThis version has been pre-compiled with Scala 2.11.12/Spark 2.4.6, and also with Scala 2.12.12/Spark 3.0.1.\r\n\r\n\r\n\r\n",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v1.1.0",
        "name": "Version 1.1.0",
        "release_id": 34154331,
        "tag": "v1.1.0",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v1.1.0",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/34154331",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/34154331",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v1.1.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jtnystrom",
          "type": "User"
        },
        "date_created": "2020-10-14T23:00:59Z",
        "date_published": "2020-10-14T23:10:22Z",
        "description": "Initial release, compiled with Spark 2.4.6 libraries.",
        "html_url": "https://github.com/jtnystrom/Discount/releases/tag/v1.0.0_spark2.4",
        "name": "Version 1.0.0 (Spark 2.4)",
        "release_id": 32589551,
        "tag": "v1.0.0_spark2.4",
        "tarball_url": "https://api.github.com/repos/jtnystrom/Discount/tarball/v1.0.0_spark2.4",
        "type": "Release",
        "url": "https://api.github.com/repos/jtnystrom/Discount/releases/32589551",
        "value": "https://api.github.com/repos/jtnystrom/Discount/releases/32589551",
        "zipball_url": "https://api.github.com/repos/jtnystrom/Discount/zipball/v1.0.0_spark2.4"
      },
      "technique": "GitHub_API"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Running",
        "parent_header": [
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "Discount can run locally on your laptop, on a cluster, or on cloud platforms that support Spark\n(tested on AWS EMR and Google Cloud Dataproc).\n\nTo run locally, download the Spark distribution (3.1.0 or later) (http://spark.apache.org).\n\nScripts to run Discount are provided for macOS and Linux. To run locally, edit the file `discount.sh` and set the path \nto your unpacked Spark distribution). This will be the script used to run Discount. Other critical settings can also be \nchanged in this file. It is very helpful to point `LOCAL_DIR` to a fast drive, such as an SSD.\n\nTo run on AWS EMR (tested on v6.8.0), please use `discount-aws.sh`. In that case, change the example commands below to\nuse that script instead, and insert your EMR cluster name as an additional first parameter when invoking. To run on \nGoogle Cloud Dataproc (tested on v2.1), please use `discount-gcloud.sh` instead.\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "download",
    "requirements",
    "contact",
    "contributors",
    "faq",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 07:16:48",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 18
      },
      "technique": "GitHub_API"
    }
  ],
  "support": [
    {
      "confidence": 1,
      "result": {
        "original_header": "License and support",
        "parent_header": [
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "For any inquiries, please contact JNP Solutions at [info@jnpsolutions.io](mailto:info@jnpsolutions.io). We will do our\nbest to help. Alternatively, feel free to open issues and/or PRs in the GitHub repo if you find a problem.\n\nDiscount is currently released under a dual GPL/commercial license. For a commercial license, custom development, or \ncommercial support please contact us at the email above.\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Use as a library",
        "parent_header": [
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "You can add Discount as a dependency using the following syntax (SBT):\n\n`\n libraryDependencies += \"com.jnpersson\" %% \"discount\" % \"3.0.1\"\n`\n\nPlease note that Discount is still under heavy development and the API may change slightly even between minor versions.\n"
      },
      "source": "https://raw.githubusercontent.com/jtnystrom/discount/master/README.md",
      "technique": "header_analysis"
    }
  ]
}