{
  "application_domain": [
    {
      "confidence": 0.989992331098081,
      "result": {
        "type": "String",
        "value": "Semantic web"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "format": "cff",
        "type": "File_dump",
        "value": "cff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\ntitle: \"PPIntegrator - PPI Triplification Process\"\nversion: 1.0.0\ndate-released: 2023-07-12\nurl: \"https://github.com/YasCoMa/ppintegrator\"\nauthors:\n- family-names: \"Martins\"\n  given-names: \"Yasmmin\"\n  orcid: \"https://orcid.org/0000-0002-6830-1948\"\n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/CITATION.cff",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Reference",
        "parent_header": [
          "PPIntegrator - PPI Triplification Process"
        ],
        "type": "Text_excerpt",
        "value": "Martins, Y. C., Ziviani, A., Cerqueira e Costa, M. D. O., Cavalcanti, M. C. R., Nicol\u00e1s, M. F., & de Vasconcelos, A. T. R. (2023). PPIntegrator: semantic integrative system for protein\u2013protein interaction and application for host\u2013pathogen datasets. Bioinformatics Advances, 3(1), vbad067.\n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md",
      "technique": "header_analysis"
    }
  ],
  "code_of_conduct": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of experience,\neducation, socio-economic status, nationality, personal appearance, race,\nreligion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at {{ email }}. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n\n[homepage]: https://www.contributor-covenant.org\n\n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/CODE_OF_CONDUCT.md",
      "technique": "file_exploration"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/YasCoMa/ppintegrator"
      },
      "technique": "GitHub_API"
    }
  ],
  "contributing_guidelines": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "## Contributing\n\n[fork]: /fork\n[pr]: /compare\n[style]: https://standardjs.com/\n[code-of-conduct]: CODE_OF_CONDUCT.md\n\nHi there! We're thrilled that you'd like to contribute to this project. Your help is essential for keeping it great.\n\nPlease note that this project is released with a [Contributor Code of Conduct][code-of-conduct]. By participating in this project you agree to abide by its terms.\n\n## Issues and PRs\n\nIf you have suggestions for how this project could be improved, or want to report a bug, open an issue! We'd love all and any contributions. If you have questions, too, we'd love to hear them.\n\nWe'd also love PRs. If you're thinking of a large PR, we advise opening up an issue first to talk about it, though! Look at the links below if you're not sure how to open a PR.\n\n## Submitting a pull request\n\n1. [Fork][fork] and clone the repository.\n2. Configure and install the dependencies: `pip3 install -r requirements.txt`.\n3. Create a new branch: `git checkout -b my-branch-name`.\n4. Make your change, add tests, and make sure the tests still pass.\n5. Push to your fork and [submit a pull request][pr].\n6. Pat your self on the back and wait for your pull request to be reviewed and merged.\n\nHere are a few things you can do that will increase the likelihood of your pull request being accepted:\n\n- Follow the [style guide][style] which is using standard. Any linting errors should be shown when running `npm test`.\n- Write and update tests.\n- Keep your changes as focused as possible. If there are multiple changes you would like to make that are not dependent upon each other, consider submitting them as separate pull requests.\n- Write a [good commit message](http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html).\n\nWork in Progress pull requests are also welcome to get feedback early on, or if there is something blocked you.\n\n## Resources\n\n- [How to Contribute to Open Source](https://opensource.guide/how-to-contribute/)\n- [Using Pull Requests](https://help.github.com/articles/about-pull-requests/)\n- [GitHub Help](https://help.github.com)\n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/CONTRIBUTING.md",
      "technique": "file_exploration"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2022-10-01T19:33:09Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2023-01-18T15:46:32Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Python pipelines to prepare PPI (Protein-Protein Interactions) data from reference databases and describe them semantically using ontologies"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Summary",
        "parent_header": [
          "PPIntegrator - PPI Triplification Process"
        ],
        "type": "Text_excerpt",
        "value": "This pipeline has as major goal provide a tool for protein interactions (PPI) prediction data formalization and standardization using the [OntoPPI](https://link.springer.com/chapter/10.1007/978-3-030-36599-8_23) ontology. This pipeline is splitted in two parts: (i) a part to prepare data from three main sources of PPI data ([HINT](http://hint.yulab.org/), [STRING](https://string-db.org/) and [PredPrin](https://github.com/YasCoMa/PredPrin.git)) and create the standard files to be processed by the next part; (ii) the second part uses the data prepared before to semantically describe using ontologies related to the concepts of this domain. It describes the provenance information of PPI prediction experiments, datasets characteristics, functional annotations of proteins involved in the PPIs, description of the PPI detection methods (also named as evidence) used in the experiment,  and the prediction score obtained by each PPI detection method for the PPIs. This pipeline also execute data fusion to map the same protein pairs from different data sources and, finally, it creates a database of all these information in the [alegro](https://allegrograph.com/) graph triplestore. The figure below illustrates the two parts of this pipeline.\n\n<div style=\"text-align: center\">\n\t<img src=\"ppintegrator_pipeline.png\" alt=\"pipeline\"\n\ttitle=\"PPI data preparation and triplification\" width=\"550px\" />\n</div>\n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9364946787670013,
      "result": {
        "original_header": "PPIntegrator - PPI Triplification Process",
        "type": "Text_excerpt",
        "value": "Python pipelines to prepare PPI (Protein-Protein  Interactions) data from reference databases and describe them semantically using ontologies\n \n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9883901619096258,
      "result": {
        "original_header": "Preparation:",
        "type": "Text_excerpt",
        "value": "* Parameter:\n\t- __-q__ or __--query_option__ <br>\n\t\tUse to indicate which query you want to perform: <br>\n\t\t1 - Get all the different organisms whose interactions are stored in the database<br >\n\t\t2 - Get the interactions that have scientific papers associated and the list of these papers<br >\n\t\t3 - Get a list of the most frequent biological processes annotated for the interactions of Escherichia coli bacteria<br >\n\t\t4 - Get only the interactions belonging to a specific biological process (regulation of transcription, DNA-templated) in Escherichia coli bacteria<br >\n\t\t5 - Get the scores of interactions belonging to a specific biological process (regulation of transcription, DNA-templated) in Escherichia coli bacteria<br >\n\t\t6 - Get a list of the most frequent biological processes annotated for the interactions of human organism<br >\n\t\t7 - Get only the interactions belonging to a specific biological process (positive regulation of transcription by RNA polymerase II) in human organism<br >\n\t\t8 - Get the scores of interactions belonging to a specific biological process (positive regulation of transcription by RNA polymerase II) in human organism \n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/YasCoMa/ppintegrator/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 0
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/YasCoMa/ppintegrator/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "YasCoMa/ppintegrator"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "PPIntegrator - PPI Triplification Process"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/ppintegrator_pipeline.png"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Preparation:",
        "parent_header": [
          "PPIntegrator - PPI Triplification Process",
          "Usage Instructions"
        ],
        "type": "Text_excerpt",
        "value": "1. ````git clone https://github.com/YasCoMa/ppintegrator.git````\n2. ````cd ppintegrator````\n3. `pip3 install -r requirements.txt`\n**Allegrograph is a triple store, which is a database to maintain semantic descriptions. This database's server provides a web application with a user interface to run, edit and manage queries, visualize results and manipulate the data without writing codes other than SPARQL query language. The use of the Allegregraph option is not mandatory, but if you want to export and use it, you have to install the server and the client.**\n4. if you want to use the Allegrograph server option (this triple store has free license up to 5,000,000 triples), install allegrograph server in your machine (configure a user and password): Server - https://franz.com/agraph/support/documentation/current/server-installation.html; Client - https://franz.com/agraph/support/documentation/current/python/install.html\n5. Export the following environment variables to configure Allegrograph server\n\n````\nexport AGRAPH_HOST=127.0.0.1\nexport AGRAPH_PORT=10035\nexport AGRAPH_USER=chosen_user\nexport AGRAPH_PASSWORD=chosen_password\n````\n5. Start allegrograph: ````path/to/allegrograph/bin/agraph-control --config path/to/allegrograph/lib/agraph.cfg start````\n6. Read the file data_requirements.txt to understand which files are needed for the process\n\n### Data preparation (first part) - File ````prepare_data_triplification.py```` :\n* Pipeline parameters:\n\t- __-rt__ or __--running_type__ <br>\n\t\tUse to indicate from which source you want to prepare PPI data, as follows: <br>\n\t\t1 - Prepare data for PredPrin <br>\n\t\t2 - Prepare data for String <br>\n\t\t3 - Prepare data for HINT\n\t- __-fec__ or __--file_experiment_config__ <br>\n\t\tFile with the experiment configuration in json format<br>\n\t\t\n\t\tExamples are in these files (all the metadata are required): params_hint.json, params_predrep_5k.json e params_string.json\n\n\t- __-org__ or __--organism__ <br>\n\t\tPrepare data only for one organism of interest (example: homo_sapiens) <br >\n\n\t\tThis parameter is optional. If you do not specify, it will automatically use the organisms described in the experiment configuration file above\n\n\n* Running modes examples:\n\t1. Running for PPI data generated by PredPrin: <br>\n\t````python3 prepare_data_triplification.py -rt 1 -fec params_predrep_5k.json````\n\n\t2. Running for HINT database: <br>\n\t````python3 prepare_data_triplification.py -rt 3 -fec params_hint.json````\n\n\t3. Running for STRING database: <br>\n\t````python3 prepare_data_triplification.py -rt 2 -fec params_string.json````\n\n\tIn the file ````auxiliar_data_preparation.py```` you can run it for all the examples provided automatically, as follows: <br>\n\t````python3 auxiliar_data_preparation.py````\n\n\n### PPI data triplification (second part) - File ````triplification_ppi_data.py````:\n\n* Pipeline parameters:\n\t- __-rt__ or __--running_type__ <br>\n\t\tUse to indicate which execution step you want to run (it is desirable following the order showed): <br>\n\t\t0 - Generate the descriptions for all the protein interaction steps of an experiment  (run steps 1, 2 and 3) <br >\n\t\t1 - Generate triples just about data provenance <br >\n\t\t2 - Generate triples just for protein functional annotations<br >\n\t\t3 - Generate triples just for the score results of each evidence<br >\n\t\t4 - Execute data fusion<br >\n\t\t5 - Generate descriptions and execute data fusion (run steps 1, 2, 3 and 4)<br >\n\t\t6 - Export to allegrograph server\n\n\t- __-fec__ or __--file_experiment_config__ <br>\n\t\tFile with the experiment configuration in json format<br>\n\t\t\n\t\tExamples are in these files (all the metadata are required): params_hint.json, params_predrep_5k.json e params_string.json\n\n\t- __-fev__ or __--file_evidence_info__ <br>\n\t\tFile with the PPI detection methods information in json format<br>\n\t\t\n\t\tExamples are in these files (all the metadata are required): evidences_information.json, evidences_information_hint.json e evidences_information_string.json\n\n\t- __-fcv__ or __--file_config_evidence__ <br>\n\t\tFile with the experiment and evidence methods files addresses in tsv format<br>\n\t\t\n\t\tExample of this file: config_evidence_file.tsv\n\n* Running modes examples:\n\t1. Running to generate all semantic descriptions for PredPrin: <br>\n\t````python3 triplification_ppi_data.py -rt 0 -fec params_predrep_5k.json -fev evidences_information.json````\n\n\t2. Running to generate only triples of data provenance: <br>\n\t````python3 triplification_ppi_data.py -rt 1 -fec params_hint.json -fev evidences_information_hint.json````\n\n\t3. Running to generate only triples of PPI scores for each evidence: <br>\n\t````python3 triplification_ppi_data.py -rt 3 -fec params_hint.json -fev evidences_information_hint.json````\n\n\t4. Running to generate only triples of protein functional annotations (only PredPrin exports these annotations): <br>\n\t````python3 triplification_ppi_data.py -rt 2 -fec params_predrep_5k.json -fev evidences_information.json````\n\n\t5. Running to generate all semantic descrptions for STRING: <br>\n\t````python3 triplification_ppi_data.py -rt 0 -fec params_string.json -fev evidences_information_string.json````\n    \n    **For the next options (4, 5 and 6), it is mandatory running at least mode 1 and 3 for HINT, STRING and PredPrin**\n    \n\t6. Running to execute data fusion of different sources: <br>\n\t````python3 triplification_ppi_data.py -rt 4 -fcv config_evidence_file.tsv````\n\n\t7. Running to generate all semantic descriptions and execute data fusion of different sources (combines mode 0 and 4): <br>\n\t````python3 triplification_ppi_data.py -rt 5 -fcv config_evidence_file.tsv````\n\n\t8.  Export semantic data to allegrograph server: <br>\n\t````python3 triplification_ppi_data.py -rt 6 -fcv config_evidence_file.tsv````\n\n## Query Scenarios for analysis\nSupposing you ran all the steps showed in the section above, you can run the following options to analyse the data stored alegro graph triple store. <br>\nFile to use for this section: ````query_analysis_ppitriplificator.py```` <br>\n\n* Parameter:\n\t- __-q__ or __--query_option__ <br>\n\t\tUse to indicate which query you want to perform: <br>\n\t\t1 - Get all the different organisms whose interactions are stored in the database<br >\n\t\t2 - Get the interactions that have scientific papers associated and the list of these papers<br >\n\t\t3 - Get a list of the most frequent biological processes annotated for the interactions of Escherichia coli bacteria<br >\n\t\t4 - Get only the interactions belonging to a specific biological process (regulation of transcription, DNA-templated) in Escherichia coli bacteria<br >\n\t\t5 - Get the scores of interactions belonging to a specific biological process (regulation of transcription, DNA-templated) in Escherichia coli bacteria<br >\n\t\t6 - Get a list of the most frequent biological processes annotated for the interactions of human organism<br >\n\t\t7 - Get only the interactions belonging to a specific biological process (positive regulation of transcription by RNA polymerase II) in human organism<br >\n\t\t8 - Get the scores of interactions belonging to a specific biological process (positive regulation of transcription by RNA polymerase II) in human organism\n\n* Running modes examples:\n\t1. Running queries: <br>\n\t````python3 query_analysis_ppitriplificator.py -q 1 ```` <br>\n\t\tChange number 1 to the respective number of the query you want to perform\n\n## Reference\nMartins, Y. C., Ziviani, A., Cerqueira e Costa, M. D. O., Cavalcanti, M. C. R., Nicol\u00e1s, M. F., & de Vasconcelos, A. T. R. (2023). PPIntegrator: semantic integrative system for protein\u2013protein interaction and application for host\u2013pathogen datasets. Bioinformatics Advances, 3(1), vbad067.\n\n## Bug Report\nPlease, use the [Issues](https://github.com/YasCoMa/ppintegrator/issues) tab to report any bug.\n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Data preparation (first part) - File BASH5*` :",
        "parent_header": [
          "PPIntegrator - PPI Triplification Process",
          "Usage Instructions"
        ],
        "type": "Text_excerpt",
        "value": "# PPIntegrator - PPI Triplification Process\n\nPython pipelines to prepare PPI (Protein-Protein  Interactions) data from reference databases and describe them semantically using ontologies\n\n## Summary\n\nThis pipeline has as major goal provide a tool for protein interactions (PPI) prediction data formalization and standardization using the [OntoPPI](https://link.springer.com/chapter/10.1007/978-3-030-36599-8_23) ontology. This pipeline is splitted in two parts: (i) a part to prepare data from three main sources of PPI data ([HINT](http://hint.yulab.org/), [STRING](https://string-db.org/) and [PredPrin](https://github.com/YasCoMa/PredPrin.git)) and create the standard files to be processed by the next part; (ii) the second part uses the data prepared before to semantically describe using ontologies related to the concepts of this domain. It describes the provenance information of PPI prediction experiments, datasets characteristics, functional annotations of proteins involved in the PPIs, description of the PPI detection methods (also named as evidence) used in the experiment,  and the prediction score obtained by each PPI detection method for the PPIs. This pipeline also execute data fusion to map the same protein pairs from different data sources and, finally, it creates a database of all these information in the [alegro](https://allegrograph.com/) graph triplestore. The figure below illustrates the two parts of this pipeline.\n\n<div style=\"text-align: center\">\n\t<img src=\"ppintegrator_pipeline.png\" alt=\"pipeline\"\n\ttitle=\"PPI data preparation and triplification\" width=\"550px\" />\n</div>\n\n## Requirements:\n* Python packages needed:\n\t- pip3 install numpy\n\t- pip3 install rdflib\n\t- pip3 install uuid\n\t- pip3 install SPARQLWrapper\n\t- alegro graph tools (pip3 install agraph-python) <br > \n\t\tGo to this [site](https://franz.com/agraph/support/documentation/current/python/install.html) for the installation tutorial\n\n## Usage Instructions\n### Preparation:\n1. ````git clone https://github.com/YasCoMa/ppintegrator.git````\n2. ````cd ppintegrator````\n3. `pip3 install -r requirements.txt`\n**Allegrograph is a triple store, which is a database to maintain semantic descriptions. This database's server provides a web application with a user interface to run, edit and manage queries, visualize results and manipulate the data without writing codes other than SPARQL query language. The use of the Allegregraph option is not mandatory, but if you want to export and use it, you have to install the server and the client.**\n4. if you want to use the Allegrograph server option (this triple store has free license up to 5,000,000 triples), install allegrograph server in your machine (configure a user and password): Server - https://franz.com/agraph/support/documentation/current/server-installation.html; Client - https://franz.com/agraph/support/documentation/current/python/install.html\n5. Export the following environment variables to configure Allegrograph server\n\n````\nexport AGRAPH_HOST=127.0.0.1\nexport AGRAPH_PORT=10035\nexport AGRAPH_USER=chosen_user\nexport AGRAPH_PASSWORD=chosen_password\n````\n5. Start allegrograph: ````path/to/allegrograph/bin/agraph-control --config path/to/allegrograph/lib/agraph.cfg start````\n6. Read the file data_requirements.txt to understand which files are needed for the process\n\n### Data preparation (first part) - File ````prepare_data_triplification.py```` :\n* Pipeline parameters:\n\t- __-rt__ or __--running_type__ <br>\n\t\tUse to indicate from which source you want to prepare PPI data, as follows: <br>\n\t\t1 - Prepare data for PredPrin <br>\n\t\t2 - Prepare data for String <br>\n\t\t3 - Prepare data for HINT\n\t- __-fec__ or __--file_experiment_config__ <br>\n\t\tFile with the experiment configuration in json format<br>\n\t\t\n\t\tExamples are in these files (all the metadata are required): params_hint.json, params_predrep_5k.json e params_string.json\n\n\t- __-org__ or __--organism__ <br>\n\t\tPrepare data only for one organism of interest (example: homo_sapiens) <br >\n\n\t\tThis parameter is optional. If you do not specify, it will automatically use the organisms described in the experiment configuration file above\n\n\n* Running modes examples:\n\t1. Running for PPI data generated by PredPrin: <br>\n\t````python3 prepare_data_triplification.py -rt 1 -fec params_predrep_5k.json````\n\n\t2. Running for HINT database: <br>\n\t````python3 prepare_data_triplification.py -rt 3 -fec params_hint.json````\n\n\t3. Running for STRING database: <br>\n\t````python3 prepare_data_triplification.py -rt 2 -fec params_string.json````\n\n\tIn the file ````auxiliar_data_preparation.py```` you can run it for all the examples provided automatically, as follows: <br>\n\t````python3 auxiliar_data_preparation.py````\n\n\n### PPI data triplification (second part) - File ````triplification_ppi_data.py````:\n\n* Pipeline parameters:\n\t- __-rt__ or __--running_type__ <br>\n\t\tUse to indicate which execution step you want to run (it is desirable following the order showed): <br>\n\t\t0 - Generate the descriptions for all the protein interaction steps of an experiment  (run steps 1, 2 and 3) <br >\n\t\t1 - Generate triples just about data provenance <br >\n\t\t2 - Generate triples just for protein functional annotations<br >\n\t\t3 - Generate triples just for the score results of each evidence<br >\n\t\t4 - Execute data fusion<br >\n\t\t5 - Generate descriptions and execute data fusion (run steps 1, 2, 3 and 4)<br >\n\t\t6 - Export to allegrograph server\n\n\t- __-fec__ or __--file_experiment_config__ <br>\n\t\tFile with the experiment configuration in json format<br>\n\t\t\n\t\tExamples are in these files (all the metadata are required): params_hint.json, params_predrep_5k.json e params_string.json\n\n\t- __-fev__ or __--file_evidence_info__ <br>\n\t\tFile with the PPI detection methods information in json format<br>\n\t\t\n\t\tExamples are in these files (all the metadata are required): evidences_information.json, evidences_information_hint.json e evidences_information_string.json\n\n\t- __-fcv__ or __--file_config_evidence__ <br>\n\t\tFile with the experiment and evidence methods files addresses in tsv format<br>\n\t\t\n\t\tExample of this file: config_evidence_file.tsv\n\n* Running modes examples:\n\t1. Running to generate all semantic descriptions for PredPrin: <br>\n\t````python3 triplification_ppi_data.py -rt 0 -fec params_predrep_5k.json -fev evidences_information.json````\n\n\t2. Running to generate only triples of data provenance: <br>\n\t````python3 triplification_ppi_data.py -rt 1 -fec params_hint.json -fev evidences_information_hint.json````\n\n\t3. Running to generate only triples of PPI scores for each evidence: <br>\n\t````python3 triplification_ppi_data.py -rt 3 -fec params_hint.json -fev evidences_information_hint.json````\n\n\t4. Running to generate only triples of protein functional annotations (only PredPrin exports these annotations): <br>\n\t````python3 triplification_ppi_data.py -rt 2 -fec params_predrep_5k.json -fev evidences_information.json````\n\n\t5. Running to generate all semantic descrptions for STRING: <br>\n\t````python3 triplification_ppi_data.py -rt 0 -fec params_string.json -fev evidences_information_string.json````\n    \n    **For the next options (4, 5 and 6), it is mandatory running at least mode 1 and 3 for HINT, STRING and PredPrin**\n    \n\t6. Running to execute data fusion of different sources: <br>\n\t````python3 triplification_ppi_data.py -rt 4 -fcv config_evidence_file.tsv````\n\n\t7. Running to generate all semantic descriptions and execute data fusion of different sources (combines mode 0 and 4): <br>\n\t````python3 triplification_ppi_data.py -rt 5 -fcv config_evidence_file.tsv````\n\n\t8.  Export semantic data to allegrograph server: <br>\n\t````python3 triplification_ppi_data.py -rt 6 -fcv config_evidence_file.tsv````\n\n## Query Scenarios for analysis\nSupposing you ran all the steps showed in the section above, you can run the following options to analyse the data stored alegro graph triple store. <br>\nFile to use for this section: ````query_analysis_ppitriplificator.py```` <br>\n\n* Parameter:\n\t- __-q__ or __--query_option__ <br>\n\t\tUse to indicate which query you want to perform: <br>\n\t\t1 - Get all the different organisms whose interactions are stored in the database<br >\n\t\t2 - Get the interactions that have scientific papers associated and the list of these papers<br >\n\t\t3 - Get a list of the most frequent biological processes annotated for the interactions of Escherichia coli bacteria<br >\n\t\t4 - Get only the interactions belonging to a specific biological process (regulation of transcription, DNA-templated) in Escherichia coli bacteria<br >\n\t\t5 - Get the scores of interactions belonging to a specific biological process (regulation of transcription, DNA-templated) in Escherichia coli bacteria<br >\n\t\t6 - Get a list of the most frequent biological processes annotated for the interactions of human organism<br >\n\t\t7 - Get only the interactions belonging to a specific biological process (positive regulation of transcription by RNA polymerase II) in human organism<br >\n\t\t8 - Get the scores of interactions belonging to a specific biological process (positive regulation of transcription by RNA polymerase II) in human organism\n\n* Running modes examples:\n\t1. Running queries: <br>\n\t````python3 query_analysis_ppitriplificator.py -q 1 ```` <br>\n\t\tChange number 1 to the respective number of the query you want to perform\n\n## Reference\nMartins, Y. C., Ziviani, A., Cerqueira e Costa, M. D. O., Cavalcanti, M. C. R., Nicol\u00e1s, M. F., & de Vasconcelos, A. T. R. (2023). PPIntegrator: semantic integrative system for protein\u2013protein interaction and application for host\u2013pathogen datasets. Bioinformatics Advances, 3(1), vbad067.\n\n## Bug Report\nPlease, use the [Issues](https://github.com/YasCoMa/ppintegrator/issues) tab to report any bug.\n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9852566305660354,
      "result": {
        "original_header": "Query Scenarios for analysis",
        "type": "Text_excerpt",
        "value": "Supposing you ran all the steps showed in the section above, you can run the following options to analyse the data stored alegro graph triple store. <br>\nFile to use for this section: ````query_analysis_ppitriplificator.py```` <br> \n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/YasCoMa/ppintegrator/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "Copyright 2023 Yasmmin Martins\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/LICENSE.md",
      "technique": "file_exploration"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "ppintegrator"
      },
      "technique": "GitHub_API"
    }
  ],
  "ontologies": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/ontoppi.ttl"
      },
      "technique": "file_exploration"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "YasCoMa"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 107715,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md"
      },
      "technique": "file_exploration"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Requirements:",
        "parent_header": [
          "PPIntegrator - PPI Triplification Process"
        ],
        "type": "Text_excerpt",
        "value": "* Python packages needed:\n\t- pip3 install numpy\n\t- pip3 install rdflib\n\t- pip3 install uuid\n\t- pip3 install SPARQLWrapper\n\t- alegro graph tools (pip3 install agraph-python) <br > \n\t\tGo to this [site](https://franz.com/agraph/support/documentation/current/python/install.html) for the installation tutorial\n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "download",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-04 00:51:31",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 0
      },
      "technique": "GitHub_API"
    }
  ],
  "support": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Bug Report",
        "parent_header": [
          "PPIntegrator - PPI Triplification Process"
        ],
        "type": "Text_excerpt",
        "value": "Please, use the [Issues](https://github.com/YasCoMa/ppintegrator/issues) tab to report any bug.\n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md",
      "technique": "header_analysis"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "PPI data triplification (second part) - File BASH11*`:",
        "parent_header": [
          "PPIntegrator - PPI Triplification Process",
          "Usage Instructions"
        ],
        "type": "Text_excerpt",
        "value": "# PPIntegrator - PPI Triplification Process\n\nPython pipelines to prepare PPI (Protein-Protein  Interactions) data from reference databases and describe them semantically using ontologies\n\n## Summary\n\nThis pipeline has as major goal provide a tool for protein interactions (PPI) prediction data formalization and standardization using the [OntoPPI](https://link.springer.com/chapter/10.1007/978-3-030-36599-8_23) ontology. This pipeline is splitted in two parts: (i) a part to prepare data from three main sources of PPI data ([HINT](http://hint.yulab.org/), [STRING](https://string-db.org/) and [PredPrin](https://github.com/YasCoMa/PredPrin.git)) and create the standard files to be processed by the next part; (ii) the second part uses the data prepared before to semantically describe using ontologies related to the concepts of this domain. It describes the provenance information of PPI prediction experiments, datasets characteristics, functional annotations of proteins involved in the PPIs, description of the PPI detection methods (also named as evidence) used in the experiment,  and the prediction score obtained by each PPI detection method for the PPIs. This pipeline also execute data fusion to map the same protein pairs from different data sources and, finally, it creates a database of all these information in the [alegro](https://allegrograph.com/) graph triplestore. The figure below illustrates the two parts of this pipeline.\n\n<div style=\"text-align: center\">\n\t<img src=\"ppintegrator_pipeline.png\" alt=\"pipeline\"\n\ttitle=\"PPI data preparation and triplification\" width=\"550px\" />\n</div>\n\n## Requirements:\n* Python packages needed:\n\t- pip3 install numpy\n\t- pip3 install rdflib\n\t- pip3 install uuid\n\t- pip3 install SPARQLWrapper\n\t- alegro graph tools (pip3 install agraph-python) <br > \n\t\tGo to this [site](https://franz.com/agraph/support/documentation/current/python/install.html) for the installation tutorial\n\n## Usage Instructions\n### Preparation:\n1. ````git clone https://github.com/YasCoMa/ppintegrator.git````\n2. ````cd ppintegrator````\n3. `pip3 install -r requirements.txt`\n**Allegrograph is a triple store, which is a database to maintain semantic descriptions. This database's server provides a web application with a user interface to run, edit and manage queries, visualize results and manipulate the data without writing codes other than SPARQL query language. The use of the Allegregraph option is not mandatory, but if you want to export and use it, you have to install the server and the client.**\n4. if you want to use the Allegrograph server option (this triple store has free license up to 5,000,000 triples), install allegrograph server in your machine (configure a user and password): Server - https://franz.com/agraph/support/documentation/current/server-installation.html; Client - https://franz.com/agraph/support/documentation/current/python/install.html\n5. Export the following environment variables to configure Allegrograph server\n\n````\nexport AGRAPH_HOST=127.0.0.1\nexport AGRAPH_PORT=10035\nexport AGRAPH_USER=chosen_user\nexport AGRAPH_PASSWORD=chosen_password\n````\n5. Start allegrograph: ````path/to/allegrograph/bin/agraph-control --config path/to/allegrograph/lib/agraph.cfg start````\n6. Read the file data_requirements.txt to understand which files are needed for the process\n\n### Data preparation (first part) - File ````prepare_data_triplification.py```` :\n* Pipeline parameters:\n\t- __-rt__ or __--running_type__ <br>\n\t\tUse to indicate from which source you want to prepare PPI data, as follows: <br>\n\t\t1 - Prepare data for PredPrin <br>\n\t\t2 - Prepare data for String <br>\n\t\t3 - Prepare data for HINT\n\t- __-fec__ or __--file_experiment_config__ <br>\n\t\tFile with the experiment configuration in json format<br>\n\t\t\n\t\tExamples are in these files (all the metadata are required): params_hint.json, params_predrep_5k.json e params_string.json\n\n\t- __-org__ or __--organism__ <br>\n\t\tPrepare data only for one organism of interest (example: homo_sapiens) <br >\n\n\t\tThis parameter is optional. If you do not specify, it will automatically use the organisms described in the experiment configuration file above\n\n\n* Running modes examples:\n\t1. Running for PPI data generated by PredPrin: <br>\n\t````python3 prepare_data_triplification.py -rt 1 -fec params_predrep_5k.json````\n\n\t2. Running for HINT database: <br>\n\t````python3 prepare_data_triplification.py -rt 3 -fec params_hint.json````\n\n\t3. Running for STRING database: <br>\n\t````python3 prepare_data_triplification.py -rt 2 -fec params_string.json````\n\n\tIn the file ````auxiliar_data_preparation.py```` you can run it for all the examples provided automatically, as follows: <br>\n\t````python3 auxiliar_data_preparation.py````\n\n\n### PPI data triplification (second part) - File ````triplification_ppi_data.py````:\n\n* Pipeline parameters:\n\t- __-rt__ or __--running_type__ <br>\n\t\tUse to indicate which execution step you want to run (it is desirable following the order showed): <br>\n\t\t0 - Generate the descriptions for all the protein interaction steps of an experiment  (run steps 1, 2 and 3) <br >\n\t\t1 - Generate triples just about data provenance <br >\n\t\t2 - Generate triples just for protein functional annotations<br >\n\t\t3 - Generate triples just for the score results of each evidence<br >\n\t\t4 - Execute data fusion<br >\n\t\t5 - Generate descriptions and execute data fusion (run steps 1, 2, 3 and 4)<br >\n\t\t6 - Export to allegrograph server\n\n\t- __-fec__ or __--file_experiment_config__ <br>\n\t\tFile with the experiment configuration in json format<br>\n\t\t\n\t\tExamples are in these files (all the metadata are required): params_hint.json, params_predrep_5k.json e params_string.json\n\n\t- __-fev__ or __--file_evidence_info__ <br>\n\t\tFile with the PPI detection methods information in json format<br>\n\t\t\n\t\tExamples are in these files (all the metadata are required): evidences_information.json, evidences_information_hint.json e evidences_information_string.json\n\n\t- __-fcv__ or __--file_config_evidence__ <br>\n\t\tFile with the experiment and evidence methods files addresses in tsv format<br>\n\t\t\n\t\tExample of this file: config_evidence_file.tsv\n\n* Running modes examples:\n\t1. Running to generate all semantic descriptions for PredPrin: <br>\n\t````python3 triplification_ppi_data.py -rt 0 -fec params_predrep_5k.json -fev evidences_information.json````\n\n\t2. Running to generate only triples of data provenance: <br>\n\t````python3 triplification_ppi_data.py -rt 1 -fec params_hint.json -fev evidences_information_hint.json````\n\n\t3. Running to generate only triples of PPI scores for each evidence: <br>\n\t````python3 triplification_ppi_data.py -rt 3 -fec params_hint.json -fev evidences_information_hint.json````\n\n\t4. Running to generate only triples of protein functional annotations (only PredPrin exports these annotations): <br>\n\t````python3 triplification_ppi_data.py -rt 2 -fec params_predrep_5k.json -fev evidences_information.json````\n\n\t5. Running to generate all semantic descrptions for STRING: <br>\n\t````python3 triplification_ppi_data.py -rt 0 -fec params_string.json -fev evidences_information_string.json````\n    \n    **For the next options (4, 5 and 6), it is mandatory running at least mode 1 and 3 for HINT, STRING and PredPrin**\n    \n\t6. Running to execute data fusion of different sources: <br>\n\t````python3 triplification_ppi_data.py -rt 4 -fcv config_evidence_file.tsv````\n\n\t7. Running to generate all semantic descriptions and execute data fusion of different sources (combines mode 0 and 4): <br>\n\t````python3 triplification_ppi_data.py -rt 5 -fcv config_evidence_file.tsv````\n\n\t8.  Export semantic data to allegrograph server: <br>\n\t````python3 triplification_ppi_data.py -rt 6 -fcv config_evidence_file.tsv````\n"
      },
      "source": "https://raw.githubusercontent.com/YasCoMa/ppintegrator/master/readme.md",
      "technique": "header_analysis"
    }
  ]
}