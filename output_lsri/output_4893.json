{
  "application_domain": [
    {
      "confidence": 20.39,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "References to dependencies",
        "parent_header": [
          "PARC"
        ],
        "type": "Text_excerpt",
        "value": "- Leiden (pip install leidenalg) (V.A. Traag, 2019 doi.org/10.1038/s41598-019-41695-z)\n- hsnwlib Malkov, Yu A., and D. A. Yashunin. \"Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small   World graphs.\" TPAMI, preprint: https://arxiv.org/abs/1603.09320\n- igraph (igraph.org/python/)\n"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Citing",
        "parent_header": [
          "PARC"
        ],
        "type": "Text_excerpt",
        "value": "If you find this code useful in your work, please consider citing this paper [PARC:ultrafast and accurate clustering of phenotypic data of millions of single cells](https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/btaa042/5714737)\n"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/ShobiStassen/PARC"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-07-02T19:33:24Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-05-28T02:21:53Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 0.9885205722370659,
      "result": {
        "original_header": "PARC",
        "type": "Text_excerpt",
        "value": "PARC, \u201cphenotyping by accelerated refined community-partitioning\u201d - is a fast, automated, combinatorial  graph-based clustering approach that integrates hierarchical graph construction (HNSW) and data-driven graph-pruning with the new Leiden community-detection algorithm. [PARC:ultrafast and accurate clustering of phenotypic data of millions of single cells](https://academic.oup.com/bioinformatics/article/36/9/2778/5714737). \nCheck out **[Readthedocs](https://parc.readthedocs.io/en/latest/index.html)** for An [installation guide](https://parc.readthedocs.io/en/latest/Installation.html), [examples](https://parc.readthedocs.io/en/latest/Examples.html) on different data and [tutorials](https://parc.readthedocs.io/en/latest/Notebook-covid19.html).   \n:eight_spoked_asterisk: **PARC** forms the clustering basis for our new Trajectory Inference (TI) method **VIA** available on [Github](https://github.com/ShobiStassen/VIA) or [readthedocs](https://parc.readthedocs.io/en/latest/index.html). VIA is a single-cell TI method that offers topology construction and visualization, pseudotimes, automated prediction of terminal cell fates and temporal gene dynamics along detected lineages.\n**VIA can also be used to topologically visualize the graph-based connectivity of clusters found by PARC in a non-TI context.** \n"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9734727483662923,
      "result": {
        "original_header": "Parameter tuning",
        "type": "Text_excerpt",
        "value": "For a more detailed explanation of the impact of tuning key parameters please see the Supplementary Analysis in our paper.\n[PARC Supplementary Analysis](https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/PAP/10.1093_bioinformatics_btaa042/1/btaa042_supplementary-data.pdf?Expires=1583098421&Signature=R1gJB7MebQjg7t9Mp-MoaHdubyyke4OcoevEK5817el27onwA7TlU-~u7Ug1nOUFND2C8cTnwBle7uSHikx7BJ~SOAo6xUeniePrCIzQBi96MvtoL674C8Pd47a4SAcHqrA2R1XMLnhkv6M8RV0eWS-4fnTPnp~lnrGWV5~mdrvImwtqKkOyEVeHyt1Iajeb1W8Msuh0I2y6QXlLDU9mhuwBvJyQ5bV8sD9C-NbdlLZugc4LMqngbr5BX7AYNJxvhVZMSKKl4aMnIf4uMv4aWjFBYXTGwlIKCjurM2GcHK~i~yzpi-1BMYreyMYnyuYHi05I9~aLJfHo~Qd3Ux2VVQ__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA) \n| Input Parameter | Description |\n| ---------- |----------|\n| `data` | (numpy.ndarray) n_samples x n_features |\n| `true_label` | (numpy.ndarray) (optional)|\n| `dist_std_local` |  (optional, default = 2) local pruning threshold: the number of standard deviations above the mean minkowski distance between neighbors of a given node. the higher the parameter, the more edges are retained|\n| `jac_std_global` |  (optional, default = 'median') global level  graph pruning. This threshold can also be set as the number of standard deviations below the network's mean-jaccard-weighted edges. 0.1-1 provide reasonable pruning. higher value means less pruning. e.g. a value of 0.15 means all edges that are above mean(edgeweight)-0.15*std(edge-weights) are retained. We find both 0.15 and 'median' to yield good results resulting in pruning away ~ 50-60% edges |\n| `dist_std_local` |  (optional, default = 2) local pruning threshold: the number of standard deviations above the mean minkowski distance between neighbors of a given node. higher value means less pruning|\n| `random_seed` |  (optional, default = 42) The random seed to pass to Leiden|\n| `resolution_parameter` |  (optional, default = 1) Uses ModuliartyVP and RBConfigurationVertexPartition|\n| `jac_weighted_edges` |  (optional, default = True) Uses Jaccard weighted pruned graph as input to community detection. For very large datasets set this to False to observe a speed-up with negligble impact on accuracy | \n| Attributes | Description |\n| ---------- |----------|\n| `labels` | (list) length n_samples of corresponding cluster labels |\n| `f1_mean` | (list) f1 score (not weighted by population). For details see supplementary section of [paper](https://doi.org/10.1101/765628) |\n| `stats_df` | (DataFrame) stores parameter values and performance metrics |\n \n"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "https://parc.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/ShobiStassen/PARC/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/Covid19_Parc.ipynb"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/Covid19_Parc.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/docs/Notebook-covid19.ipynb"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/docs/Notebook-covid19.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 11
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/ShobiStassen/PARC/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "ShobiStassen/PARC"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "PARC"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/Images/Covid_matrixplot.png"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/Images/Covid_hnsw_umap.png"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/Images/70K_Lung_github_overview.png"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "parent_header": [
          "PARC",
          "Developers"
        ],
        "type": "Text_excerpt",
        "value": "**Using pip**\n```\nconda create --name ParcEnv pip\npip install parc // tested on linux\n```\n**Cloning repository and running setup.py** (ensure dependencies are installed)\n```\ngit clone https://github.com/ShobiStassen/PARC.git\npython3 setup.py install // cd into the directory of the cloned PARC folder containing setup.py and issue this command\n```\n\n**install dependencies separately if needed (linux)**\nIf the pip install doesn't work, it usually suffices to first install all the requirements (using pip) and subsequently install parc (also using pip)\n```\npip install igraph, leidenalg, hnswlib, umap-learn\npip install parc\n```\n\n**Windows installation**\nOnce you have Visual Studio installed it should be smooth sailing (sometimes requires a restart after intalling VS). It might be easier to install dependences using either pip or conda -c conda-forge install. If this doesn't work then you might need to consider using binaries to install igraph and leidenalg.\n\npython-igraph: download the python36 Windows Binaries by [Gohlke](http://www.lfd.uci.edu/~gohlke/pythonlibs)\nleidenalg: depends on python-igraph. download [windows binary](https://pypi.org/project/leidenalg/#files) available for python3.6 only\n\n```\nconda create --name parcEnv python=3.6 pip\npip install igraph #(or install python_igraph-0.7.1.post6-cp36-cp36m-win_amd64.whl )\npip install leidenalg #(or install leidenalg-0.7.0-cp36-cp36m-win_amd64.whl)\npip install hnswlib\npip install parc\n```"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Example Usage 1. (small test sets) - IRIS and Digits dataset from sklearn",
        "parent_header": [
          "PARC"
        ],
        "type": "Text_excerpt",
        "value": "```\nimport parc\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\n// load sample IRIS data\n//data (n_obs x k_dim, 150x4)\niris = datasets.load_iris()\nX = iris.data\ny=iris.target\n\nplt.scatter(X[:,0],X[:,1], c = y) // colored by 'ground truth'\nplt.show()\n\nParc1 = parc.PARC(X,true_label=y) // instantiate PARC\n//Parc1 = parc.PARC(X) // when no 'true labels' are available\nParc1.run_PARC() // run the clustering\nparc_labels = Parc1.labels\n# View scatterplot colored by PARC labels\nplt.scatter(X[:, 0], X[:, 1], c=parc_labels, cmap='rainbow')\nplt.show()\n\n# Run umap on the HNSW knngraph already built in PARC (more time and memory efficient for large datasets)\n// Parc1.knn_struct = p1.make_knn_struct() // if you choose to visualize before running PARC clustering. then you need to include this line\ngraph = Parc1.knngraph_full()\nX_umap = Parc1.run_umap_hnsw(X, graph)\nplt.scatter(X_umap[:, 0], X_umap[:, 1], c=Parc1.labels)\nplt.show()\n\n\n\n\n\n// load sample digits data\ndigits = datasets.load_digits()\nX = digits.data // (n_obs x k_dim, 1797x64)\ny = digits.target\nParc2 = parc.PARC(X,true_label=y, jac_std_global='median') // 'median' is default pruning level\nParc2.run_PARC()\nparc_labels = Parc2.labels\n\n```"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9760581844815507,
      "result": {
        "original_header": "Testing",
        "type": "Text_excerpt",
        "value": "To run the test suite for the package:\n```\ncd PARC\npytest tests/\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/ShobiStassen/PARC/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "MIT License\n\nCopyright (c) 2019 ShobiStassen\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/LICENSE",
      "technique": "file_exploration"
    }
  ],
  "logo": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/Images/10X_PBMC_PARC_andGround.png"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "PARC"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "ShobiStassen"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 834778,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 35550,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1603.09320\n- igraph (igraph.org/python/"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "ShobiStassen",
          "type": "User"
        },
        "date_created": "2019-09-24T05:50:32Z",
        "date_published": "2019-09-24T06:12:45Z",
        "html_url": "https://github.com/ShobiStassen/PARC/releases/tag/v0.33",
        "release_id": 20200742,
        "tag": "v0.33",
        "tarball_url": "https://api.github.com/repos/ShobiStassen/PARC/tarball/v0.33",
        "type": "Release",
        "url": "https://api.github.com/repos/ShobiStassen/PARC/releases/20200742",
        "value": "https://api.github.com/repos/ShobiStassen/PARC/releases/20200742",
        "zipball_url": "https://api.github.com/repos/ShobiStassen/PARC/zipball/v0.33"
      },
      "technique": "GitHub_API"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "References to dependencies",
        "parent_header": [
          "PARC"
        ],
        "type": "Text_excerpt",
        "value": "- Leiden (pip install leidenalg) (V.A. Traag, 2019 doi.org/10.1038/s41598-019-41695-z)\n- hsnwlib Malkov, Yu A., and D. A. Yashunin. \"Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small   World graphs.\" TPAMI, preprint: https://arxiv.org/abs/1603.09320\n- igraph (igraph.org/python/)\n"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "download",
    "contact",
    "contributors",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-06 18:00:45",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 41
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Example Usage on Covid-19 scRNA-seq data",
        "parent_header": [
          "PARC"
        ],
        "type": "Text_excerpt",
        "value": "Check out the [Jupyter Notebook](https://parc.readthedocs.io/en/latest/Notebook-covid19.html) for how to pre-process and PARC cluster the new Covid-19 BALF dataset by [Liao et. al 2020](https://www.nature.com/articles/s41591-020-0901-9).\nWe also show how to integrate UMAP with HNSW such that the embedding in UMAP is constructed using the HNSW graph built in PARC, enabling a very fast and memory efficient viusalization (particularly noticeable when n_cells > 1 Million)\n"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "PARC Cluster-level average gene expression",
        "parent_header": [
          "PARC",
          "Example Usage on Covid-19 scRNA-seq data"
        ],
        "type": "Text_excerpt",
        "value": "![](https://github.com/ShobiStassen/PARC/blob/master/Images/Covid_matrixplot.png)\n"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "PARC visualizes cells by integrating UMAP embedding on the HNSW graph",
        "parent_header": [
          "PARC",
          "Example Usage on Covid-19 scRNA-seq data"
        ],
        "type": "Text_excerpt",
        "value": "![](https://github.com/ShobiStassen/PARC/blob/master/Images/Covid_hnsw_umap.png)\n"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Example Usage 1. (small test sets) - IRIS and Digits dataset from sklearn",
        "parent_header": [
          "PARC"
        ],
        "type": "Text_excerpt",
        "value": "```\nimport parc\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\n// load sample IRIS data\n//data (n_obs x k_dim, 150x4)\niris = datasets.load_iris()\nX = iris.data\ny=iris.target\n\nplt.scatter(X[:,0],X[:,1], c = y) // colored by 'ground truth'\nplt.show()\n\nParc1 = parc.PARC(X,true_label=y) // instantiate PARC\n//Parc1 = parc.PARC(X) // when no 'true labels' are available\nParc1.run_PARC() // run the clustering\nparc_labels = Parc1.labels\n# View scatterplot colored by PARC labels\nplt.scatter(X[:, 0], X[:, 1], c=parc_labels, cmap='rainbow')\nplt.show()\n\n# Run umap on the HNSW knngraph already built in PARC (more time and memory efficient for large datasets)\n// Parc1.knn_struct = p1.make_knn_struct() // if you choose to visualize before running PARC clustering. then you need to include this line\ngraph = Parc1.knngraph_full()\nX_umap = Parc1.run_umap_hnsw(X, graph)\nplt.scatter(X_umap[:, 0], X_umap[:, 1], c=Parc1.labels)\nplt.show()\n\n\n\n\n\n// load sample digits data\ndigits = datasets.load_digits()\nX = digits.data // (n_obs x k_dim, 1797x64)\ny = digits.target\nParc2 = parc.PARC(X,true_label=y, jac_std_global='median') // 'median' is default pruning level\nParc2.run_PARC()\nparc_labels = Parc2.labels\n\n```"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Example Usage 2. (mid-scale scRNA-seq): 10X PBMC (Zheng et al., 2017)",
        "parent_header": [
          "PARC"
        ],
        "type": "Text_excerpt",
        "value": "[pre-processed datafile](https://drive.google.com/file/d/1H4gOZ09haP_VPCwsYxZt4vf3hJ1GZj3b/view?usp=sharing)\n\n[annotations](https://github.com/ShobiStassen/PARC/blob/master/Datasets/zheng17_annotations.txt)\n\n```\nimport parc\nimport csv\nimport numpy as np\nimport pandas as pd\n\n## load data (50 PCs of filtered gene matrix pre-processed as per Zheng et al. 2017)\n\nX = csv.reader(open(\"./pca50_pbmc68k.txt\", 'rt'),delimiter = \",\")\nX = np.array(list(X)) // (n_obs x k_dim, 68579 x 50)\nX = X.astype(\"float\")\n// OR with pandas as: X = pd.read_csv(\"'./pca50_pbmc68k.txt\", header=None).values.astype(\"float\")\n\ny = [] // annotations\nwith open('./zheng17_annotations.txt', 'rt') as f:\n    for line in f: y.append(line.strip().replace('\\\"', ''))\n// OR with pandas as: y =  list(pd.read_csv('./data/zheng17_annotations.txt', header=None)[0])   \n\n// setting small_pop to 50 cleans up some of the smaller clusters, but can also be left at the default 10\nparc1 = parc.PARC(X,true_label=y,jac_std_global=0.15, random_seed =1, small_pop = 50) // instantiate PARC\nparc1.run_PARC() // run the clustering\nparc_labels = parc1.labels\n```\n![](Images/10X_PBMC_PARC_andGround.png) tsne plot of annotations and PARC clustering\n"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Example Usage 3. 10X PBMC (Zheng et al., 2017) integrating Scanpy pipeline",
        "parent_header": [
          "PARC"
        ],
        "type": "Text_excerpt",
        "value": "[raw datafile 68K pbmc from github page](https://github.com/10XGenomics/single-cell-3prime-paper/tree/master/pbmc68k_analysis)\n\n[10X compressed file \"filtered genes\"](http://cf.10xgenomics.com/samples/cell-exp/1.1.0/fresh_68k_pbmc_donor_a/fresh_68k_pbmc_donor_a_filtered_gene_bc_matrices.tar.gz)\n\n```\npip install scanpy\n```\n\n```\nimport scanpy.api as sc\nimport pandas as pd\n//load data\npath = './data/filtered_matrices_mex/hg19/'\nadata = sc.read(path + 'matrix.mtx', cache=True).T  # transpose the data\nadata.var_names = pd.read_csv(path + 'genes.tsv', header=None, sep='\\t')[1]\nadata.obs_names = pd.read_csv(path + 'barcodes.tsv', header=None)[0]\n\n// annotations as per correlation with pure samples\nannotations = list(pd.read_csv('./data/zheng17_annotations.txt', header=None)[0])\nadata.obs['annotations'] = pd.Categorical(annotations)\n\n//pre-process as per Zheng et al., and take first 50 PCs for analysis\nsc.pp.recipe_zheng17(adata)\nsc.tl.pca(adata, n_comps=50)\n// setting small_pop to 50 cleans up some of the smaller clusters, but can also be left at the default 10\nparc1 = parc.PARC(adata.obsm['X_pca'], true_label = annotations, jac_std_global=0.15, random_seed =1, small_pop = 50)  \nparc1.run_PARC() // run the clustering\nparc_labels = parc1.labels\nadata.obs[\"PARC\"] = pd.Categorical(parc_labels)\n\n//visualize\nsc.settings.n_jobs=4\nsc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)\nsc.tl.umap(adata)\nsc.pl.umap(adata, color='annotations')\nsc.pl.umap(adata, color='PARC')\n```"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Example Usage 4. Large-scale (70K subset and 1.1M cells) Lung Cancer cells (multi-ATOM imaging cytometry based features)",
        "parent_header": [
          "PARC"
        ],
        "type": "Text_excerpt",
        "value": "[normalized image-based feature matrix 70K cells](https://drive.google.com/open?id=1LeFjxGlaoaZN9sh0nuuMFBK0bvxPiaUz)\n\n[Lung Cancer cells annotation 70K cells](https://drive.google.com/open?id=1iwXQkdwEwplhZ1v0jYWnu2CHziOt_D9C)\n\n[Lung Cancer Digital Spike Test of n=100 H1975 cells on N281604 ](https://drive.google.com/open?id=1kWtx3j1ixua4nQt1HFHlwzCHnOr7gvKm)\n\n[1.1M cell features and annotations](https://data.mendeley.com/datasets/nnbfwjvmvw/draft?a=dae895d4-25cd-4bdf-b3e4-57dd31c11e37)\n\n```\nimport parc\nimport pandas as pd\n\n// load data: digital mix of 7 cell lines from 7 sets of pure samples (1.1M cells)\nX = pd.read_csv(\"'./LungData.txt\", header=None).values.astype(\"float\")\ny = list(pd.read_csv('./LungData_annotations.txt', header=None)[0]) // list of cell-type annotations\n\n// run PARC on 1.1M cells\n// jac_weighted_edges can be set to false which provides an unweighted graph to leiden and offers some speedup\nparc1 = parc.PARC(X, true_label=y,jac_weighted_edges = False)\nparc1.run_PARC() // run the clustering\nparc_labels = parc1.labels\n\n// run PARC on H1975 spiked cells\nparc2 = parc.PARC(X, true_label=y, jac_std_global = 0.15, jac_weighted_edges = False) // 0.15 corresponds to pruning ~60% edges and can be effective for rarer populations than the default 'median'\nparc2.run_PARC() // run the clustering\nparc_labels_rare = parc2.labels\n\n```\n![](Images/70K_Lung_github_overview.png) tsne plot of annotations and PARC clustering, heatmap of features\n"
      },
      "source": "https://raw.githubusercontent.com/ShobiStassen/PARC/master/README.md",
      "technique": "header_analysis"
    }
  ]
}