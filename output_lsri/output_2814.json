{
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citation",
        "parent_header": [
          "virID"
        ],
        "type": "Text_excerpt",
        "value": "Please cite https://doi.org/10.1182/bloodadvances.2019001260\n\nThis pipeline is nothing without the excellent programs virID makes use of. Please cite these programs in addition to virID if you use virID in any publication! I'll list them below, in no particular order:  \nSPAdes  \nseqtk  \nBWA  \nSamtools  \nbbtools  \nBlast+  \nDIAMOND  \nETE3  \nAnytree\n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/jnoms/virID"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-10-04T15:40:35Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-07-08T04:17:28Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Viral Identification and Discovery - A viral characterization pipeline built in Nextflow."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Description",
        "parent_header": [
          "virID"
        ],
        "type": "Text_excerpt",
        "value": "The purpose of virID is to assemble and classify microorganisms present in next-generation sequencing data. This pipeline can be run in \"assembly mode\" or \"read mode\", as determined by the assembly_pipeline and reads_pipeline parameters. The main difference is whether contigs are generated with SPAdes, or if the reads are queried without assembly. The pipeline steps are listed below, but note that many steps will run in parallel.  \n\n**Assembly Mode:**  \n1) Input fastqs are split into a paired file containing interleaved paired reads, and an unpaired file containing unpaired reads. Thus, each sample should be added to this pipeline as a single fastq containing both paired and unpaired reads.  \n2) Reads are assembled with the SPAdes assembler - SPAdes, metaSPAdes, or rnaSPAdes can be specified.  \n3) bwa mem is used to map reads back to contigs.  \n4) The DIAMOND and megablast aligners are used for taxonomic assignment of assembled contigs.  \n5) DIAMOND and megablast output files are translated to a taxonomic output following last-common-ancestor (LCA) calculation for each query contig.  \n6) Contigs are queried with megablast against a nonredundant database of common cloning vectors. Contigs that are assigned to these sequences are flagged.  \n6) DIAMOND and megablast taxonomic outputs and contig count information are merged to a comprehensive taxonomic output, and unassigned contigs are flagged. Counts outputs, one for each DIAMOND and megablast, are generated which display the counts at every taxonomic level. There is a section below that describes the output files in more detail.  \n\n**Read Mode:**  \n1) Input fastqs are split into a paired file containing interleaved paired reads, and an unpaired file containing unpaired reads. Thus, each sample should be added to this pipeline as a single fastq containing both paired and unpaired reads.  \n2) Reads are converted to .fasta format.  \n3) Each read is queried by megablast and DIAMOND.\n4) The megablast and DIAMOND output files are translated to a taxonomic output following last-common-ancestor (LCA) calculation for each query contig.  \n5) A counts output file that lists the number of reads assigned to each taxon at every level is generated from the translated megablast and translated DIAMOND output files. These outfiles are described in depth later in this readme.  \n6) Reads are queried with megablast against a nonredundant database of common cloning vectors. Reads that are assigned to these sequences are marked in an output file.  \n\nWhile this pipeline is organized in Nextflow, every process is capabale of being used and tested independently of Nextflow as a bash or python script. Execute any python script (with -h) or bash script (without arguments) for detailed instructions on how to run them. The conda environment with all dependencies can be installed with `conda env create -f resouces/virID_environment.yml`.  \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Description of output files",
        "parent_header": [
          "virID"
        ],
        "type": "Text_excerpt",
        "value": "1. Each process will copy its outfiles to params.out_dir. You can disable this setting my removing the `publishDir` line from each module file.  \n2. The `our_dir/results` dir will contain the main pipeline outputs:  \n\n**sampleID_merged.tsv:** This is a tab-delimited file containing assignment information for each contig. Each contig takes up two lines, where one line details the BLAST assignment information and one details the DIAMOND assignment information. Columns from `seq_title` to `length` are a comma-delimited list detailing information from all DIAMOND or blast hits that were considered in the LCA calculation. Starting at `LCA_taxonID`, the taxonomic information is that of the LCA of these hits.  \n\nColumn names include:  \n`query_ID:`               Contig name  \n`seq_title:`              Title of hits (comma-delimited list)  \n`seq_ID:`                 Acession IDs of hits (comma-delimited list)  \n`taxonID:`                taxonID of hits (comma-delimited list)  \n`evalue:`                 Evalues of hists (comma-delimited list)  \n`bitscore:`               Bitscores of hits (comma-delimited list)  \n`pident:`                 Percent identity of each hit (comma-delimited list)  \n`length:`                 Length of the alignment for each hit (comma-delimited list)  \n`LCA_taxonID:`            The taxonID of the LCA of each hit.  \n`superkingdom:`           LCA superkingdom  \n`kingdom:`                LCA kingdom  \n`phylum:`                 LCA phylum  \n`class:`                  LCA class  \n`order:`                  LCA order  \n`family:`                 LCA family  \n`genus:`                  LCA genus  \n`species:`                LCA species  \n`strain:`                 LCA strain  \n`read_count:`             Number of input reads that map to the contig.  \n`average_fold:`           Average fold coverage of the input reads on the contig.  \n`covered_percent:`        Percentage of the contig covered by input reads.  \n`potential_contaminant:`  1 if assigned to the contaminant vector database, 0 otherwise. (Column excluded if no contaminants found).  \n\n**sampleID_blast_counts.tsv and ${sampleID}_diamond_counts.tsv:** Here, the read_count for each contig is distributed to each taxonomic level of the LCA of that contig. For example, if a contig has an LCA of sk__superkingdom/k__kingdom/f__polyomaviridae and has a read_count of 10, 10 reads are assigned to each f__polyomaviridae, k__kingdom, and sk__superkingdom. These outputs detail the results based on the blast or DIAMOND assignments, respectively. **Note that if a contig is assigned by megablast to the vector contaminate database the counts from that contig will NOT be included in this output.** The columns include:  \n`taxonID`      The taxonID  \n`lineage`      The name of each taxon in the lineage of the taxonID.  \n`superkingdom` The superkingdom of the taxonID  \n`taxon`        The name of the taxon  \n`level`        The level of the taxon (i.e. kingdom, or family, etc)  \n`count`        The total number of reads assigned to that taxon.  \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9086336563604585,
      "result": {
        "original_header": "Quickstart",
        "type": "Text_excerpt",
        "value": "----\nThe only software requirements for running this pipeline are the [Conda](https://docs.conda.io/en/latest/miniconda.html) package manager and [Nextflow](https://www.nextflow.io/) version 19.10.0 or higher. When the pipeline is first initiated, Nextflow will create a conda virtual environment containing all required additional software, and all processess will run in this in virtual environment. The only other things you need to take care of are making the DIAMOND and megablast databases, as well as making any changes to the executor depending on the cluster infrastructure you plan to run this pipeline on. There are more detailed descriptions of these steps below. \n2. Prepare DIAMOND and megablast databases.  \n3. Configure input parameters in `nextflow.config`. \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9683882018865845,
      "result": {
        "original_header": "Configure executor and resources",
        "type": "Text_excerpt",
        "value": "**Executor:** The Nextflow executor, explained [here](https://www.nextflow.io/docs/latest/executor.html), dictates how Nextflow will run each process. virID is currently set up to use a SLURM cluster, but you can easily change this by altering the executor in `nextflow.config`. Nextflow takes care of all cluster submissions and automatically parallelizes everything. If you are using a different cluster infrastructure, change the \"executor\" value from 'slurm' to the appropriate infrastructure. In addition, each nextflow module (in `bin/modules/`) contains a `beforeScript` line that dictates code to run prior to running the module. Here, I have added `module load gcc conda2`, which loads the GCC compiler and the conda package manager in my slurm cluster. If this is not relevant to you, remove this code. \n**Resources:** I have set up virID with *dynamic resources!* Each process will request a different amount of resources depending on the size of the input files. In addition, upon failure the process will restart with increased resources, and will restart a up to three times (configurable with the `maxRetries` setting in `nextflow.config`).\n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9870184253075396,
      "result": {
        "original_header": "Inputs and parameters",
        "type": "Text_excerpt",
        "value": "In general, all input values and parameters for this script must be entered in the `nextflow.config` file. Most of the parameters are already filled out for you, but you may want to change some of them.  \n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9922226850050084,
      "result": {
        "original_header": "Input and output",
        "type": "Text_excerpt",
        "value": "**params.out_dir:** Desired output directory. `\"output\"`  \n**params.reads:** A glob detailing the location of reads to be processed through this pipeline. NOTE, input reads for one sample should be in one fastq. This script will process the fastq into an interleaved (paired) fastq and a separate unpaired fastq. *There should be a single fastq for each input sample.* A value of `\"raw_reads/*fastq\"` is an appropriate input for this parameter, and will select as input all fastq files in the directory `raw_reads`. Make sure to wrap in quotes! `\"raw_data/*fastq\"`  \n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8158659369946776,
      "result": {
        "original_header": "Conda",
        "type": "Text_excerpt",
        "value": "**params.conda_env_location:** Location you want the conda virtual environment to be saved to. Change this to somewhere convenient for you. It lets you avoid downloading the conda environment multiple times.  \n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9081917298374915,
      "result": {
        "original_header": "SPAdes",
        "type": "Text_excerpt",
        "value": "**params.spades_type:** Options are 'meta', 'rna', and 'regular'. This specifies whether to use metaspades.py, rnaspades.py, or spades.py. `\"meta\"`  \n**params.temp_dir:** This specifies the location of the SPAdes temporary directory. `\"temp\"`  \n**params.spades_min_length:** The minimum contig length that is output from the SPAdes process. Contigs below this length will be filtered out. `300`  \n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9827514037242114,
      "result": {
        "original_header": "DIAMOND",
        "type": "Text_excerpt",
        "value": "**params.diamond_database:** Path to the diamond database. Wrap all paths in quotes!   \n**params.diamond_evalue:** The maximum evalue of a match to be reported as an alignment by DIAMOND. In general I am a fan of setting this at 10, which is quite high. Lower values, such as 0.001, are more stringent and have fewer false positives. `\"10\"`  \n**params.diamond_outfmt:** This dictates the output format from DIAMOND. This is fairly flexible, but generally staxids and bitscore are required. I recommend leaving it at default, though you can add to it no problem. `\"6 qseqid stitle sseqid staxids evalue bitscore pident length\"`  \n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9964228965051186,
      "result": {
        "original_header": "BLAST",
        "type": "Text_excerpt",
        "value": "**params.blast_database:** Path to the blast nt database. Wrap all paths in quotes!   \n**params.blast_evalue:** The maximum evalue of a match to be reported as an alignment by blast. In general I am a fan of setting this at 10, which is quite high. Lower values, such as 0.001, are more stringent and have fewer false positives. `10`    \n**params.blast_type:** This controls the 'task' switch of blast, and specified whether to run 'megablast', 'dc-megablast', or 'blastn'. I recommend 'megablast', else it will be quite slow. `megablast`  \n**params.blast_outfmt:** This dictates the output format from BLAST. This is fairly flexible, but generally staxid and bitscore are required. I recommend leaving it at default, though you can add to it no problem. `\"6 qseqid stitle sseqid staxid evalue bitscore pident length\"`  \n**params.blast_log_file:** This is a file that will contain log information from the blast bash script. `\"blast.log\"`  \n**params.blast_max_hsphs:** This is the maximum number of alignments per query-subject pair. Recommend setting at 1. `1`  \n**params.blast_max_targets:** This is the maximum number of separate hits that are allowed for each query. `30`  \n**params.blast_restrict_to_taxids:** This parameter lets you limited the blast search to a specified taxonID. This causes an extreme speedup, and is helpful for testing this pipeline. Not compatible with params.blast_ignore_taxids. `\"no\"`  \n**params.blast_ignore_taxids:** This parameter lets you ignore all hits of a particular taxonID. `\"no\"`  \n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9823799975445923,
      "result": {
        "original_header": "BLAST/DIAMOND conversion",
        "type": "Text_excerpt",
        "value": "**params.within_percent_of_top_score:** When finding the LCA of all matches for a given query sequence, this details how close to the maximum bitscore a match must be to be considered in the LCA classification. If this is set at 1, for example, all potential alignments within 1 percent of the highest bitscore for a query sequence will be considered in the LCA classification. **NOTE**: This is limited intrinsically by the DIAMOND -top parameter, which is set at 1. Thus, DIAMOND will only output assignments within 1% of the top bitscore anyway. I will add a switch to change the DIAMOND -top parameter in a future release. `1`  \n**params.taxid_blacklist:** Path to a file containing taxonIDs to be blacklisted. I have included a file in this github repository. Assignments containing one of these taxonIDs will be discarded before LCA calculation. `\"$VID/resources/2019-08-09_blacklist.tsv\"`  \n**params.diamond_readable_colnames:** These are the more-readable column names that will be reported in the output from DIAMOND. If you change the outfmt, change this line accordingly. `\"query_ID seq_title seq_ID taxonID evalue bitscore pident length\"`  \n**params.blast_readable_colnames:** These are the more-readable column names that will be reported in the output from BLAST. If you change the outfmt, change this line accordingly. `\"query_ID seq_title seq_ID taxonID evalue bitscore pident length\"`  \n**params.taxonomy_column:** This details which of the colnames contains the taxonID. `\"taxonID\"`  \n**params.score_column:** This details which of the colnames should be used for calculating the top score. I use bitscore, but you could technically set this as pident or length or evalue to sort by one of those parameters instead. `\"bitscore\"`  \n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9737834306821643,
      "result": {
        "original_header": "Contaminant blast search",
        "type": "Text_excerpt",
        "value": "**params.blast_contaminant_database:** Path to the contaminate database. This database is included at resources/vector_contaminant_database, but you need to specify the path here!   \n**params.blast_contaminant_evalue:** Matches with evalues above this will not be reported. I recommend setting this to be pretty stringent. `0.001`  \n**params.blast_contaminant_outfmt:** Outformat. The only thing that really is critical is keeping qseqid as the first column. `\"6 qseqid stitle sseqid staxid evalue bitscore pident length\"`  \n**params.blast_contaminant_log_file:** Path to the log file to be used for this step. `\"blast_contaminant.log\"`  \n**params.blast_contaminant_type:** The blast type. I recommend megablast, because that is sensitive enough for this. `megablast`  \n**params.blast_contaminant_max_hsphs:** Total number of alignments between each query-subject pair. `1`  \n**params.blast_contaminant_max_targets:** Total number of alignments. `1`  \n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/jnoms/virID/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 3
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/jnoms/virID/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "jnoms/virID"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "virID"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/bash/jellyfish.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/bash/run_diamond.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/bash/map_contigs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/bash/get_reads_containing_kmers.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/bash/SPAdes.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/bash/run_BLASTN.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Prepare databases",
        "parent_header": [
          "virID"
        ],
        "type": "Text_excerpt",
        "value": "This workflow requires you to generate two databases: 1) A DIAMOND database for search, 2) A nucleotide blast database for search. You should activate the pipeline conda environment with `conda env create -f resouces/virID_environment.yml` so that you are making the DIAMOND and blast databases with the same version of DIAMOND and blast used by this pipeline.\n\nA DIAMOND database can be generated by following the DIAMOND [manual](https://github.com/bbuchfink/diamond/raw/master/diamond_manual.pdf). I recommend generating a database using the RefSeq nonredundant protein fastas present at [ftp://ftp.ncbi.nlm.nih.gov/refseq/release/]. When making the DIAMOND database with `diamond makedb` you must use the `--taxonmap` and `--taxonnodes` switches.  \n```\ndiamond makedb \\\n--in <FASTA> \\\n-d <path to output database> \\\n--taxonmap <path to prot.accession2taxid.gz from ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/accession2taxid/prot.accession2taxid.gz> \\\n--taxonnodes <path to nodes.dmp from ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdmp.zip.\n```\n\nFor blast seach, you must download the blast v5 nucleotide database using the same version of blast+ used in this script.  \n`update_blastdb.pl --blastdb_version 5 nt_v5 --decompress`  \nThere is sometimes a bug where this database will not work - you may have to delete the last line of nt_v5.nal that specifies a volume of the blast database that is not actually present.  \n\nIn addition to the blast and DIAMOND databases you need to generate, a nucleotide contaminant blast database is included with this pipeline. This database was generated from [Univec_ core](https://www.ncbi.nlm.nih.gov/tools/vecscreen/univec/?), which is a nonredundant list of common laboratory cloning vectors. I have also added some additional vectors to this database. **Note: You need to specify the path to this database in nextflow.config.**  \n\nFinally, the get_LCA.py script uses [ETE3](http://etetoolkit.org/docs/latest/tutorial/tutorial_ncbitaxonomy.html) to look up taxonomy information from a downloaded taxonomy database. When this script is first executed, it will automatically download a ~300MB taxonomy database to `~/.etetoolkit/taxa.sqlite`. One thing to note is that if your home directory is somewhere with slow I/O, you should make a symbolic link to somewhere faster prior to downloading the database, i.e. run `ln -s /faster/directory/etetoolkit ~/.etetoolkit` prior to running get_LCA.py. In my case, I make a symbolic link from my home directory to my clusters scratch directory. For reference, get_LCA.py script should run in about 40s on a BLAST output file of 500K lines.\n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9762734841841882,
      "result": {
        "original_header": "Quickstart",
        "type": "Text_excerpt",
        "value": "----\nThe only software requirements for running this pipeline are the [Conda](https://docs.conda.io/en/latest/miniconda.html) package manager and [Nextflow](https://www.nextflow.io/) version 19.10.0 or higher. When the pipeline is first initiated, Nextflow will create a conda virtual environment containing all required additional software, and all processess will run in this in virtual environment. The only other things you need to take care of are making the DIAMOND and megablast databases, as well as making any changes to the executor depending on the cluster infrastructure you plan to run this pipeline on. There are more detailed descriptions of these steps below. \n1. Download Nextflow. The following script will download the Nextflow executible in your current directly. Put this somewhere in your PATH.  \n`curl -s https://get.nextflow.io | bash`   \n4. Change executor in `nextflow.config` if not using a SLURM cluster. \n5. `nextflow run virID.nf --reads \"path/to/reads/*fastq\" --out_dir \"path/to/output/dir\"`. If you need to restart the pipeline for any reason, add the `-resume` switch and any finished processess won't be rerun! \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9999127988780682,
      "result": {
        "original_header": "Configure executor and resources",
        "type": "Text_excerpt",
        "value": "**Executor:** The Nextflow executor, explained [here](https://www.nextflow.io/docs/latest/executor.html), dictates how Nextflow will run each process. virID is currently set up to use a SLURM cluster, but you can easily change this by altering the executor in `nextflow.config`. Nextflow takes care of all cluster submissions and automatically parallelizes everything. If you are using a different cluster infrastructure, change the \"executor\" value from 'slurm' to the appropriate infrastructure. In addition, each nextflow module (in `bin/modules/`) contains a `beforeScript` line that dictates code to run prior to running the module. Here, I have added `module load gcc conda2`, which loads the GCC compiler and the conda package manager in my slurm cluster. If this is not relevant to you, remove this code. \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9996143661804053,
      "result": {
        "original_header": "Specify workflow strategy",
        "type": "Text_excerpt",
        "value": "**params.assembly_pipeline:** Options are \"T\" or \"F\". If marked \"T\", the pipeline will run in assembly mode. NOTE - only params.assembly_pipeline OR params.reads_pipeline may be marked \"T\". `\"T\"`  \n**params.reads_pipeline:** Options are \"T\" or \"F\". If marked \"T\", the pipeline will run in read mode without assembly. **NOTE - only params.assembly_pipeline OR params.reads_pipeline may be marked \"T\".** `\"T\"`  \n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9921052107328389,
      "result": {
        "original_header": "Reads pipeline specific parameters",
        "type": "Text_excerpt",
        "value": "**params.reads_pipeline_no_diamond:** Options are \"T\" or \"F\". If marked \"T\", and using the reads pipeline, diamond will not be run - only blast. `\"F\"`  \n**params.reads_pipeline_no_blast:** Options are \"T\" or \"F\". If marked \"T\", and using the reads pipeline, blast will not be run - only diamond. `\"F\"`  \n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.999999987644003,
      "result": {
        "original_header": "Conda",
        "type": "Text_excerpt",
        "value": "**params.conda_env_location:** Location you want the conda virtual environment to be saved to. Change this to somewhere convenient for you. It lets you avoid downloading the conda environment multiple times.  \n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9909636861695851,
      "result": {
        "original_header": "SPAdes",
        "type": "Text_excerpt",
        "value": "**params.spades_type:** Options are 'meta', 'rna', and 'regular'. This specifies whether to use metaspades.py, rnaspades.py, or spades.py. `\"meta\"`  \n**params.temp_dir:** This specifies the location of the SPAdes temporary directory. `\"temp\"`  \n**params.spades_min_length:** The minimum contig length that is output from the SPAdes process. Contigs below this length will be filtered out. `300`  \n \n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/jnoms/virID/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "MIT License\n\nCopyright (c) 2019 jnoms\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/LICENSE",
      "technique": "file_exploration"
    }
  ],
  "logo": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master//resources/images/2019_10_06_virID_workflow.jpg"
      },
      "source": "https://raw.githubusercontent.com/jnoms/virID/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "virID"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "jnoms"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 108435,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 28326,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Nextflow",
        "size": 25320,
        "type": "Programming_language",
        "value": "Nextflow"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jnoms",
          "type": "User"
        },
        "date_created": "2019-10-23T19:28:51Z",
        "date_published": "2019-10-23T19:37:30Z",
        "description": "This is the current version of virID as of 2019-10-23. This is a major update, and incorporates great developments in Nextflow DSL2 syntax. Major changes compared to v1.0.0 include:\r\n1) Automatic querying against a contaminant database.\r\n2) Built-in conda virtual environment creation, meaning all dependencies (besides nextflow and conda) will be automatically installed for you.\r\n3) Capability to run this pipeline in either assembly or read mode.",
        "html_url": "https://github.com/jnoms/virID/releases/tag/v1.1.0",
        "name": "virID v1.1.0",
        "release_id": 20927578,
        "tag": "v1.1.0",
        "tarball_url": "https://api.github.com/repos/jnoms/virID/tarball/v1.1.0",
        "type": "Release",
        "url": "https://api.github.com/repos/jnoms/virID/releases/20927578",
        "value": "https://api.github.com/repos/jnoms/virID/releases/20927578",
        "zipball_url": "https://api.github.com/repos/jnoms/virID/zipball/v1.1.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "jnoms",
          "type": "User"
        },
        "date_created": "2019-10-04T15:57:10Z",
        "date_published": "2019-10-04T16:21:14Z",
        "html_url": "https://github.com/jnoms/virID/releases/tag/v1.0.0",
        "name": "Initial stable release",
        "release_id": 20479305,
        "tag": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/jnoms/virID/tarball/v1.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/jnoms/virID/releases/20479305",
        "value": "https://api.github.com/repos/jnoms/virID/releases/20479305",
        "zipball_url": "https://api.github.com/repos/jnoms/virID/zipball/v1.0.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 10:24:23",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 11
      },
      "technique": "GitHub_API"
    }
  ],
  "workflows": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/virID.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/modules/spades_assembly.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/modules/generate_output.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/modules/blast_to_LCA.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/modules/process_read_pairs.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/modules/get_counts.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/modules/bwa_mem.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/modules/blast.nf"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jnoms/virID/master/bin/modules/diamond.nf"
      },
      "technique": "file_exploration"
    }
  ]
}