{
  "application_domain": [
    {
      "confidence": 74.77,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citation:",
        "parent_header": [
          "DeepAPS"
        ],
        "type": "Text_excerpt",
        "value": "Nye J, Zingaretti LM, P\u00e9rez-Enciso, M Estimating conformational traits in daity cattle with DeepAPS: a two-step Deep Learning Automated Phenotyping and Segmentation approach. Submitted to Frontiers in Genetics\n\nAssessing conformation features in an accurate and rapid manner remains a challenge in the dairy industry. While recent\ndevelopments in computer vision has greatly improved automated background removal, these methods have not been fully\ntranslated to biological studies. Here, we present a composite method (DeepAPS) that combines two readily available algorithms, Mask R-RNN (https://github.com/matterport/Mask_RCNN) and Kanezaki's (https://github.com/kanezaki/pytorch-unsupervised-segmentation) in\norder to create a precise mask for an animal image. This method performs accurately when compared with manual classification\nof proportion of coat color with an adjusted R2 = 0.926. Using the output mask, we are able to automatically extract useful\nphenotypic information for fourteen additional morphological features. Using pedigree and image information from a web catalog\n(www.semex.com), we estimated high heritabilities (ranging from h2 = 0.18 \u2013 0.82), indicating that meaningful biological\ninformation has been extracted automatically from imaging data. This method can be applied to other datasets and requires only\na minimal number of image annotations (~50) to train this partially supervised machine-learning approach. DeepAPS allows for the\nrapid and accurate quantification of multiple phenotypic measurements while minimizing study cost.\n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/lauzingaretti/DeepAPS"
      },
      "technique": "GitHub_API"
    }
  ],
  "contact": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Contact:",
        "parent_header": [
          "DeepAPS"
        ],
        "type": "Text_excerpt",
        "value": "m.lau.zingaretti@gmail.com\n\n\n\n* We strongly recommend to use a conda environment to run this code.\n\nHow you should proceed? \n\nOpen bash and type: \n\n\n```console\nlaura@localhost:~$ conda create --name cows python=3.6 -y\nlaura@localhost:~$ conda activate cows \nlaura@localhost:~$ conda install -c anaconda ipykernel\nlaura@localhost:~$ python -m ipykernel install --user --name=cows\n```\n\nJust check your Jupyter Notebook, to see the shining cows and chose this as notebook.\n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-04-11T20:06:48Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2023-03-14T06:26:16Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "a tool for automatic Phenotyping  Segmentation "
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.8385623047512722,
      "result": {
        "original_header": "Estimating conformational traits in dairy cattle: a deep learning approach",
        "type": "Text_excerpt",
        "value": "Code by Laura Zingaretti with contributions from Jessica Nye. \n \n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9273766345903913,
      "result": {
        "original_header": "3. Load pre-trained weights from coco database (containing 80 categories)",
        "type": "Text_excerpt",
        "value": "    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n      _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n      _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n      _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n      _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n      _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n      np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n    Using TensorFlow backend. \n    \n    Configurations:\n    BACKBONE                       resnet101\n    BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n    BATCH_SIZE                     1\n    BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n    COMPUTE_BACKBONE_SHAPE         None\n    DETECTION_MAX_INSTANCES        100\n    DETECTION_MIN_CONFIDENCE       0.7\n    DETECTION_NMS_THRESHOLD        0.3\n    FPN_CLASSIF_FC_LAYERS_SIZE     1024\n    GPU_COUNT                      1\n    GRADIENT_CLIP_NORM             5.0\n    IMAGES_PER_GPU                 1\n    IMAGE_CHANNEL_COUNT            3\n    IMAGE_MAX_DIM                  1024\n    IMAGE_META_SIZE                93\n    IMAGE_MIN_DIM                  800\n    IMAGE_MIN_SCALE                0\n    IMAGE_RESIZE_MODE              square\n    IMAGE_SHAPE                    [1024 1024    3]\n    LEARNING_MOMENTUM              0.9\n    LEARNING_RATE                  0.001\n    LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n    MASK_POOL_SIZE                 14\n    MASK_SHAPE                     [28, 28]\n    MAX_GT_INSTANCES               100\n    MEAN_PIXEL                     [123.7 116.8 103.9]\n    MINI_MASK_SHAPE                (56, 56)\n    NAME                           coco\n    NUM_CLASSES                    81\n    POOL_SIZE                      7\n    POST_NMS_ROIS_INFERENCE        1000\n    POST_NMS_ROIS_TRAINING         2000\n    PRE_NMS_LIMIT                  6000\n    ROI_POSITIVE_RATIO             0.33\n    RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n    RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n    RPN_ANCHOR_STRIDE              1\n    RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n    RPN_NMS_THRESHOLD              0.7\n    RPN_TRAIN_ANCHORS_PER_IMAGE    256\n    STEPS_PER_EPOCH                1000\n    TOP_DOWN_PYRAMID_SIZE          256\n    TRAIN_BN                       False\n    TRAIN_ROIS_PER_IMAGE           200\n    USE_MINI_MASK                  True\n    USE_RPN_ROIS                   True\n    VALIDATION_STEPS               50\n    WEIGHT_DECAY                   0.0001\n    \n    \n    WARNING:tensorflow:From /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Colocations handled automatically by placer.\n    WARNING:tensorflow:From /home/laura/Mask_RCNN/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Use tf.cast instead. \n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8466215368392784,
      "result": {
        "original_header": "7. Image segmentation",
        "type": "Text_excerpt",
        "value": "```python\n# a regresentation of the percentage of background/ foreground \nplt.imshow(bar) \n``` \n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9511015185710512,
      "result": {
        "original_header": "Calculating black color percentage",
        "type": "Text_excerpt",
        "value": "\n```python\nA_colo = np.round(np.sum(clt.cluster_centers_, axis = 1),decimals = 0)\nindex = int(np.argwhere(A_colo == 255))\npercent1 = list(percent1)\n## Remove masked portion of image\npercent1.pop(index)\nsuma = np.sum(percent1)\nporcent = []\n\n## Calculate proportion of color clusters\nfor i in  percent1:\n    porcent.append(100*i/suma)\n\nprint(porcent) # 60 black ~40 white\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8645057508237971,
      "result": {
        "original_header": "8.Transform final mask into 2-D black and white image",
        "type": "Text_excerpt",
        "value": "\n```python\ngray = cv2.cvtColor(im_target_rgb, cv2.COLOR_BGR2GRAY)\ngray = cv2.GaussianBlur(gray, (7, 7), 0)\n\n## Extract the outline of the object\nedged = cv2.Canny(gray, 50, 100)\nedged = cv2.dilate(edged, None, iterations = 1)\nedged = cv2.erode(edged, None, iterations = 1)\n\n## Clean image - values should be optimized\nprocessed = morphology.remove_small_objects(edged.astype(bool), min_size = 300, connectivity = 1000).astype(int)\n\n## transform the image from a T/F matrix to a B/W image\nout = processed*255\n\n## calcualte size of image\nheight, width = out.shape\ntotal_pixels = out.size\n\n## calculate number of pixels in the perimeter\noutline = cv2.countNonZero(out)\ntotal_perimeter = outline / total_pixels\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download": [
    {
      "confidence": 1,
      "result": {
        "original_header": "2. Download Mask_RCNN",
        "parent_header": [
          "DeepAPS"
        ],
        "type": "Text_excerpt",
        "value": "go to https://github.com/matterport/Mask_RCNN and download repo \n\n\n\n```python\n!git clone https://github.com/matterport/Mask_RCNN.git\n```\n\n    Cloning into 'Mask_RCNN'...\n    remote: Enumerating objects: 956, done.\u001b[K\n    remote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\u001b[K\n    Receiving objects: 100% (956/956), 111.82 MiB | 18.65 MiB/s, done.\n    Resolving deltas: 100% (568/568), done.\n\n\n\n```python\n#check the path \n#print('\\n'.join(sys.path))\n```\n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/lauzingaretti/deepaps/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 6
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/lauzingaretti/DeepAPS/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "lauzingaretti/DeepAPS"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "DeepAPS"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/output_5_1.png"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/output_18_1.png"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/output_22_1.png"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/output_26_1.png"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/output_31_1.png"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/output_32_1.png"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/output_33_1.png"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/output_34_1.png"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/output_60_1.png"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "1. Installing dependences",
        "parent_header": [
          "DeepAPS"
        ],
        "type": "Text_excerpt",
        "value": "Note that you can do this step from bash using conda utilities. \n\nTo install dependences from bash, just type: \n\n```console\nlaura@localhost:~$ conda activate cows\nlaura@localhost:~$ conda install -c anaconda pytorch-gpu tensorflow-gpu=1.13.1\nlaura@localhost:~$ conda install -c conda-forge keras=2.1 matplotlib python-utils\nlaura@localhost:~$ conda install -c anaconda pandas\nlaura@localhost:~$ conda install pillow\nlaura@localhost:~$ conda install -c anaconda scikit-image\nlaura@localhost:~$ conda install -c intel scikit-learn\nlaura@localhost:~$ conda install -c auto python-utils\nlaura@localhost:~$ conda install -c conda-forge imgaug\nlaura@localhost:~$ pip install cython\nlaura@localhost:~$ pip install pycocotools\nlaura@localhost:~$ pip install utils\n```\n\n*Warning*: Note that these instructions run the code using CUDA-GPU toolkit. Please install tensorflow-cpu and torch-cpu if you don't have a nvidia graphic card. \n\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport cv2\nimport sys\nimport os\nimport numpy as np\nimport torch.nn.init\nimport utils\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport skimage.io\nfrom skimage import morphology\nfrom scipy import ndimage\nfrom skimage import morphology\nfrom skimage.feature import peak_local_max\n```\n\n\n```python\n# Load image\nOrig=cv2.imread(\"/home/laura/images/FR2238143895.jpg\")\n```\n\n\n```python\n# Show image\nplt.imshow(Orig)\n```\n\n\n\n\n    <matplotlib.image.AxesImage at 0x7f2c0e848eb8>\n\n\n\n\n![png](output_5_1.png)\n\n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9999994515102175,
      "result": {
        "original_header": "3. Load pre-trained weights from coco database (containing 80 categories)",
        "type": "Text_excerpt",
        "value": "- Note that the following instruction works in linux/mac to download the .h5 file automatically.\n- If you are in a windows machine, please go to the link and downoad the dataset by hand. \n- We save mask_rcnn_coco.h5 in /home/laura/Mask_RCNN/samples/coco/mask_rcnn_coco.h5 folder, which should be the COCO_MODEL_PATH \n\n```python\n!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n``` \n    --2020-04-11 17:51:09--  https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n    Resolving github.com (github.com)... 140.82.118.4\n    Connecting to github.com (github.com)|140.82.118.4|:443... connected.\n    HTTP request sent, awaiting response... 302 Found\n    Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200411%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200411T155110Z&X-Amz-Expires=300&X-Amz-Signature=1417db0979de70322e32361342271b0ea9f1e3ad819a62a113ef0391474b7a38&X-Amz-SignedHeaders=host&actor_id=0&repo_id=107595270&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream [following]\n    --2020-04-11 17:51:10--  https://github-production-release-asset-2e65be.s3.amazonaws.com/107595270/872d3234-d21f-11e7-9a51-7b4bc8075835?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200411%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200411T155110Z&X-Amz-Expires=300&X-Amz-Signature=1417db0979de70322e32361342271b0ea9f1e3ad819a62a113ef0391474b7a38&X-Amz-SignedHeaders=host&actor_id=0&repo_id=107595270&response-content-disposition=attachment%3B%20filename%3Dmask_rcnn_coco.h5&response-content-type=application%2Foctet-stream\n    Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.42.44\n    Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.42.44|:443... connected.\n    HTTP request sent, awaiting response... 200 OK\n    Length: 257557808 (246M) [application/octet-stream]\n    Saving to: 'mask_rcnn_coco.h5'\n    \n    100%[======================================>] 257,557,808 1.17MB/s   in 64s    \n    \n    2020-04-11 17:52:15 (3.81 MB/s) - 'mask_rcnn_coco.h5' saved [257557808/257557808]\n     \n```python\n## Add repo to sys path \nsys.path.append(os.path.join(\"/home/laura/Mask_RCNN/\"))  # To find local version of the library\nimport mrcnn.model as modellib\n\n## Import MS-COCO from Lin et al., 2014\n## Change for custom training set\nsys.path.append(os.path.join(\"/home/laura/Mask_RCNN/samples/coco/\"))  # To find local version\n#this is to find coco.py\n\n## Directory to save logs and trained model\nMODEL_DIR = os.path.join(\"/home/laura/Mask_RCNN/logs\")\n\n## Local path to trained weights file\nCOCO_MODEL_PATH = os.path.join(\"/home/laura/Mask_RCNN/samples/coco/mask_rcnn_coco.h5\")\n\n\n\n``` \n    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n      _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n      _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n      _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n      _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n      _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n      np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n    Using TensorFlow backend. \n    \n    Configurations:\n    BACKBONE                       resnet101\n    BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n    BATCH_SIZE                     1\n    BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n    COMPUTE_BACKBONE_SHAPE         None\n    DETECTION_MAX_INSTANCES        100\n    DETECTION_MIN_CONFIDENCE       0.7\n    DETECTION_NMS_THRESHOLD        0.3\n    FPN_CLASSIF_FC_LAYERS_SIZE     1024\n    GPU_COUNT                      1\n    GRADIENT_CLIP_NORM             5.0\n    IMAGES_PER_GPU                 1\n    IMAGE_CHANNEL_COUNT            3\n    IMAGE_MAX_DIM                  1024\n    IMAGE_META_SIZE                93\n    IMAGE_MIN_DIM                  800\n    IMAGE_MIN_SCALE                0\n    IMAGE_RESIZE_MODE              square\n    IMAGE_SHAPE                    [1024 1024    3]\n    LEARNING_MOMENTUM              0.9\n    LEARNING_RATE                  0.001\n    LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n    MASK_POOL_SIZE                 14\n    MASK_SHAPE                     [28, 28]\n    MAX_GT_INSTANCES               100\n    MEAN_PIXEL                     [123.7 116.8 103.9]\n    MINI_MASK_SHAPE                (56, 56)\n    NAME                           coco\n    NUM_CLASSES                    81\n    POOL_SIZE                      7\n    POST_NMS_ROIS_INFERENCE        1000\n    POST_NMS_ROIS_TRAINING         2000\n    PRE_NMS_LIMIT                  6000\n    ROI_POSITIVE_RATIO             0.33\n    RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n    RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n    RPN_ANCHOR_STRIDE              1\n    RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n    RPN_NMS_THRESHOLD              0.7\n    RPN_TRAIN_ANCHORS_PER_IMAGE    256\n    STEPS_PER_EPOCH                1000\n    TOP_DOWN_PYRAMID_SIZE          256\n    TRAIN_BN                       False\n    TRAIN_ROIS_PER_IMAGE           200\n    USE_MINI_MASK                  True\n    USE_RPN_ROIS                   True\n    VALIDATION_STEPS               50\n    WEIGHT_DECAY                   0.0001\n    \n    \n    WARNING:tensorflow:From /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Colocations handled automatically by placer.\n    WARNING:tensorflow:From /home/laura/Mask_RCNN/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n    Instructions for updating:\n    Use tf.cast instead. \n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9998945369235955,
      "result": {
        "original_header": "3. Call functions.py from code folder",
        "type": "Text_excerpt",
        "value": "```python\nsys.path.append(os.path.join(\"/home/laura/code/\"))  # To find local version of the library\nfrom functions import *\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8668457433137593,
      "result": {
        "original_header": "6. Cleaning image",
        "type": "Text_excerpt",
        "value": "\n```python\n#Applying mediaBlur filtering to clean the output \n# median smoothing \nim_target_rgb = cv2.medianBlur(im_target_rgb, 7)\nplt.imshow(im_target_rgb)\n#write final mask to file \n#cv2.imwrite(Outdir + \"/Final_Mask_\" + file_name, cv2.medianBlur(im_target_rgb, 5))\n\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8167601612264033,
      "result": {
        "original_header": "7. Image segmentation",
        "type": "Text_excerpt",
        "value": "\n```python\n## Apply mask\nimga = np.zeros([Seg.shape[0],Seg.shape[1],3], np.uint8)\n## Color applied can be changed to be any color Grey = (200, 200, 200), Black = (0, 0, 0), White = (255, 255, 255)\nimga[:] = (255, 0, 0)\nm1 = np.all(im_target_rgb ==  central, axis = -1)\nimga[m1] = im[m1]\ndif = imga-im\n\nimage = imga.reshape((imga.shape[0] * imga.shape[1], 3))\nclt = KMeans(n_clusters = 3)\nclt.fit(image)\nhist1 = centroid_histogram(clt)\nbar1,percent1 = plot_colors(hist1, clt.cluster_centers_)\n\n``` \n```python\n# mask in black, object minus background\nplt.imshow(dif) \n``` \n```python\n# the original image into a homogeneous background \nplt.imshow(imga[:,:,::-1]) \n#cv2.imwrite( Outdir + \"/applied_Mask_\" + file_name, imga[:,:,::-1] )\n\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8617354079651861,
      "result": {
        "original_header": "Add point of morphological measures to image",
        "type": "Text_excerpt",
        "value": "\n```python\n\nZ=cv2.circle(im_target_rgb,(backleg_x[bl_index],backleg_y[bl_index]), 5, (255,255,255), -1)\nZ=cv2.circle(Z,(frontleg_x[fl_index],frontleg_y[fl_index]), 5, (255,255,255), -1)\nZ=cv2.circle(Z,(rear_x[r_index],rear_y[r_index]), 5, (255,255,255), -1)\nZ=cv2.circle(Z,(head_x[ear_index],head_y[ear_index]), 5, (255,255,255), -1)\nZ=cv2.circle(Z,(belly_x[tip_index],belly_y[tip_index]), 5, (255,255,255), -1)\nZ=cv2.circle(Z,(bleg_x[bl_top_index],bleg_y[bl_top_index]), 5, (255,255,255), -1)\nZ=cv2.circle(Z,(fleg_x[fl_top_index],fleg_y[fl_top_index]), 5, (255,255,255), -1)\n\nplt.imshow(Z)\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/lauzingaretti/DeepAPS/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "DeepAPS"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "lauzingaretti"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 1760,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "1. Installing dependences",
        "parent_header": [
          "DeepAPS"
        ],
        "type": "Text_excerpt",
        "value": "Note that you can do this step from bash using conda utilities. \n\nTo install dependences from bash, just type: \n\n```console\nlaura@localhost:~$ conda activate cows\nlaura@localhost:~$ conda install -c anaconda pytorch-gpu tensorflow-gpu=1.13.1\nlaura@localhost:~$ conda install -c conda-forge keras=2.1 matplotlib python-utils\nlaura@localhost:~$ conda install -c anaconda pandas\nlaura@localhost:~$ conda install pillow\nlaura@localhost:~$ conda install -c anaconda scikit-image\nlaura@localhost:~$ conda install -c intel scikit-learn\nlaura@localhost:~$ conda install -c auto python-utils\nlaura@localhost:~$ conda install -c conda-forge imgaug\nlaura@localhost:~$ pip install cython\nlaura@localhost:~$ pip install pycocotools\nlaura@localhost:~$ pip install utils\n```\n\n*Warning*: Note that these instructions run the code using CUDA-GPU toolkit. Please install tensorflow-cpu and torch-cpu if you don't have a nvidia graphic card. \n\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport cv2\nimport sys\nimport os\nimport numpy as np\nimport torch.nn.init\nimport utils\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport skimage.io\nfrom skimage import morphology\nfrom scipy import ndimage\nfrom skimage import morphology\nfrom skimage.feature import peak_local_max\n```\n\n\n```python\n# Load image\nOrig=cv2.imread(\"/home/laura/images/FR2238143895.jpg\")\n```\n\n\n```python\n# Show image\nplt.imshow(Orig)\n```\n\n\n\n\n    <matplotlib.image.AxesImage at 0x7f2c0e848eb8>\n\n\n\n\n![png](output_5_1.png)\n\n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "4. Run Mask-RCNN to object detection",
        "parent_header": [
          "DeepAPS"
        ],
        "type": "Text_excerpt",
        "value": "\n```python\nimage = skimage.io.imread('/home/laura/images/FR2238143895.jpg')\n```\n\n\n```python\n## Run Object detection using coco\nresults = model_Coco.detect([image], verbose = 1)\n\n## Visualize results\nr = results[0]\n\n## Select only class id you are interested in (we have chosen cow n=20, to change object the 20 in the next two lines should be changed)\nif (20 in r['class_ids']):\n    w = r['class_ids'].tolist().index(20)\n    im = image[r['rois'][w].item(0):r['rois'][w].item(2), r['rois'][w].item(1):r['rois'][w].item(3),]\n\n    ## Select only region of interest for desired object\n    A = np.transpose(r['masks'], [0,1,2])\n    A = A.astype(int)\n    A = A[r['rois'][w].item(0):r['rois'][w].item(2), r['rois'][w].item(1):r['rois'][w].item(3),0]\n\n```\n\n    Processing 1 images\n    image                    shape: (749, 1000, 3)        min:    0.00000  max:  255.00000  uint8\n    molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  151.10000  float64\n    image_metas              shape: (1, 93)               min:    0.00000  max: 1024.00000  float64\n    anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n\n\n\n```python\n# im contains the box (RoI) with cow\n# note most of the background is out\nplt.imshow(im)\n```\n\n\n\n\n    <matplotlib.image.AxesImage at 0x7f2bb42f7ba8>\n\n\n\n\n![png](output_18_1.png)\n\n\n\n```python\n# CUDA is STRONGLY recommended\nuse_cuda = False #when cuda is not available\n#use_cuda = torch.cuda.is_available()\n```\n\n\n```python\ndata = torch.from_numpy( np.array([im.transpose( (2, 0, 1) ).astype('float32')/255.]))\nif use_cuda:\n    data=data.cuda()\n```\n\n\n```python\n## Unsupervised refinement adapted from Kanezaki, 2018 https://github.com/kanezaki/pytorch-unsupervised-segmentation\n## SLIC from Achanta et al., 2012\nnConv=3 #you can choose more or less conv\nlr= 0.2 # lr you can change this parameter \nmaxIter=200 #maximum number of iteractions \nminLabels=3 #minimun number of cluster\nvisualize=True\nlabels = A\nlabels = labels.reshape(im.shape[0]*im.shape[1])\nu_labels = np.unique(A)\nl_inds = []\n\nfor i in range(len(u_labels)):\n    l_inds.append( np.where( labels == u_labels[ i ] )[ 0 ] )\n    \n    \n    torch.cuda.empty_cache()\n    model = MyNet( data.size(1) )\n    if use_cuda:\n        model.cuda()\n        for i in range(nConv-1):\n            model.conv2[i].cuda()\n            model.bn2[i].cuda()\n    model.train()\n       \n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr = lr, momentum = 0.9)\n    label_colours = np.random.randint(255,size = (100,3))\n        \n    for batch_idx in range(maxIter):\n        optimizer.zero_grad()\n        output = model( data )[ 0 ]\n        output = output.permute( 1, 2, 0 ).contiguous().view( -1, 100 )\n        ignore, target = torch.max( output, 1 )\n        im_target = target.data.cpu().numpy()\n        nLabels = len(np.unique(im_target))\n        if visualize:\n            im_target_rgb = np.array([label_colours[ c % 100 ] for c in im_target])\n            im_target_rgb = im_target_rgb.reshape( im.shape ).astype( np.uint8 )\n        for i in range(len(l_inds)):\n            labels_per_sp = im_target[ l_inds[ i ] ]\n            u_labels_per_sp = np.unique( labels_per_sp )\n            hist = np.zeros( len(u_labels_per_sp) )\n            for j in range(len(hist)):\n                hist[ j ] = len( np.where( labels_per_sp == u_labels_per_sp[ j ] )[ 0 ] )\n            im_target[l_inds[ i ]] = u_labels_per_sp[ np.argmax( hist ) ]\n        target = torch.from_numpy( im_target )\n        if use_cuda:\n            target = target.cuda()\n        target = Variable( target ,volatile=True)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n\n        if nLabels <= minLabels:\n            print (\"nLabels\", nLabels, \"reached minLabels\", minLabels, \".\")\n            break\n            \n        ## Save output image\n        if not visualize:\n            output = model( data )[ 0 ]\n            output = output.permute( 1, 2, 0 ).contiguous().view( -1, 100 )\n            ignore, target = torch.max( output, 1 )\n            im_target = target.data.cpu().numpy()\n            im_target_rgb = np.array([label_colours[ c % 100 ] for c in im_target])\n            im_target_rgb = im_target_rgb.reshape( im.shape ).astype( np.uint8 )\n        #cv2.imwrite(Outdir + \"raw_Mask_\" + file_name, im_target_rgb)\n\n```\n\n    /usr/anaconda3/envs/DeepAPS/lib/python3.6/site-packages/ipykernel_launcher.py:50: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n\n\n    nLabels 2 reached minLabels 3 .\n\n\n\n```python\n# show output\nplt.imshow(im_target_rgb)\n```\n\n\n\n\n    <matplotlib.image.AxesImage at 0x7f2bb425a3c8>\n\n\n\n\n![png](output_22_1.png)\n\n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "contributors",
    "documentation",
    "license",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 12:23:33",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 5
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "9. Example of feature extraction",
        "parent_header": [
          "DeepAPS"
        ],
        "type": "Text_excerpt",
        "value": "The following codes contain some examples about morphological features extraction from images \n\n\n```python\nheight #see height\n```\n\n\n\n\n    397\n\n\n\n\n```python\nwidth #see body width\n```\n\n\n\n\n    527\n\n\n\n\n```python\n#total number of pixels \ntotal_pixels\n```\n\n\n\n\n    209219\n\n\n\n\n```python\ntotal_perimeter  ### proportion of pixels in the perimeter \n```\n\n\n\n\n    0.011714997203886836\n\n\n\n\n```python\n## Export the outline of the object\n## cv2.imwrite(Outdir + \"object_outline_\" + file_name, out)\ny, x = np.nonzero(out)     \n```\n\n\n```python\n## Example of feature extraction - bottom of back leg\nbackleg_x = []\nbackleg_y = []\nfor i in range(len(y)):\n    if y[i] > max(y)- height/5:\n        if x[i] < min(x) + width/5:\n            backleg_x.append(x[i])\n            backleg_y.append(y[i])\nif backleg_x:\n    bl_index = np.argmax(backleg_y)\n\n```\n\n\n```python\n## Example of feature extraction - bottom of front leg\nfrontleg_x = []\nfrontleg_y = []\nfor i in range(len(y)):\n    if y[i] > max(y) - height/5:\n        if x[i] > max(x) - width/2:\n            frontleg_x.append(x[i])\n            frontleg_y.append(y[i])\nif frontleg_x:\n    fl_index = np.argmax(frontleg_y)\n           \n```\n\n\n```python\n## Example of distance calculation\nif frontleg_x and backleg_x:\n    len_gait = np.sqrt(((frontleg_x[fl_index] - backleg_x[bl_index])**2)+((frontleg_y[fl_index] - backleg_y[bl_index])**2))\n\n```\n\n\n```python\nlen_gait ## distance \n```\n\n\n\n\n    318.8306760648981\n\n\n\n\n```python\n(cenx, ceny) = (im.shape[1]/2, im.shape[0]/2) \n\ncentral=im_target_rgb[int(round(ceny)),int(round(cenx)),:]\n\n\n# show our color bar\nimga = np.zeros([Seg.shape[0],Seg.shape[1],3], np.uint8)\nimga[:] = (200, 200, 200)\n\nm1=np.all(im_target_rgb ==  central, axis=-1)\n\nimga[m1]=im[m1]\ndif= imga-im\n\n```\n\n\n```python\n### find local maxima\ncoordinates = peak_local_max(imga, min_distance=20)\n\n### transform image into 2-D black and white image\ngray = cv2.cvtColor(imga, cv2.COLOR_BGR2GRAY)\ngray = cv2.GaussianBlur(gray, (7, 7), 0)\n\n### only keep the outline of the cow\nedged = cv2.Canny(gray, 50, 100)\nedged = cv2.dilate(edged, None, iterations=1)\nedged = cv2.erode(edged, None, iterations=1)\n\n### clean the islands from the image\nprocessed = morphology.remove_small_objects(edged.astype(bool), min_size=300, connectivity=1000).astype(int)\n\n### transform the image from a T/F matrix to a B/W image\nout = processed*255\n\n### calcualte size of image\nheight, width = out.shape\ntotal_pixels = out.size\n\n### calculate number of pixels in the perimeter\t\noutline = cv2.countNonZero(out)\ntotal_perimeter = outline / total_pixels\n```\n\n\n```python\ny, x = np.nonzero(out)\n### find bottom of back leg\nbackleg_x=[]\nbackleg_y=[]\nfor i in range(len(y)):\n    if y[i] > max(y)-height/5:\n        if x[i] < min(x)+width/5:\n            backleg_x.append(x[i])\n            backleg_y.append(y[i])\n    if backleg_x:\n        bl_index=np.argmax(backleg_y)\n\n### find bottom of front leg\nfrontleg_x=[]\nfrontleg_y=[]\nfor i in range(len(y)):\n    if y[i] > max(y)-height/5:\n        if x[i] > max(x)-width/2:\n            frontleg_x.append(x[i])\n            frontleg_y.append(y[i])\n    if frontleg_x:\n        fl_index=np.argmax(frontleg_y)\n\n\n```\n\n\n```python\n### find rear\nrear_x=[]\nrear_y=[]\nfor i in range(len(y)):\n    if y[i] < height/4:\n        if x[i] < min(x) + width/11:\n            rear_x.append(x[i])\n            rear_y.append(y[i])\n    if rear_x:\n        r_index=np.argmin(rear_x)\n```\n\n\n```python\n### find middle of the stomach\nbelly_x=[]\nbelly_y=[]\nfor i in range(len(y)):\n    if height-height/4 > y[i] > height/2:\n        if width/4 < x[i] < width/2:\n            belly_x.append(x[i])\n            belly_y.append(y[i])\n    if belly_x:\n        tip_index=np.argmax(belly_y)\n\n### find back\nmback_y=[]\nfor i in range(len(y)):\n    if x[i] == belly_x[tip_index]:\n        mback_y.append(y[i])\n    if mback_y:\n        mb_index=np.argmin(mback_y)\n\n### find top of back legs\nbleg_x=[]\nbleg_y=[]\nfor i in range(len(y)):\n    if y[i] > height/2:\n        if belly_x:\n            if belly_x[tip_index] > x[i] > belly_x[tip_index]-width/5:\n                bleg_x.append(x[i])\n                bleg_y.append(y[i])\n        if bleg_x:\n            bl_top_index=np.argmin(bleg_y)\n\n### find back\nbback_y=[]\nfor i in range(len(y)):\n    if x[i] == bleg_x[bl_top_index]:\n        bback_y.append(y[i])\n        if bback_y:\n            bb_index=np.argmin(bback_y)\n\n            ### find top of front legs\nfleg_x=[]\nfleg_y=[]\nfor i in range(len(y)):\n    if y[i] > height/2:\n        if belly_x and frontleg_x:\n            if belly_x[tip_index] < x[i] < belly_x[tip_index]+width/3:\n                if x[i] < frontleg_x[fl_index]:\n                    fleg_x.append(x[i])\n                    fleg_y.append(y[i])\n    if fleg_x:\n        fl_top_index=np.argmin(fleg_y)\n\n### find back\nfback_y=[]\nfor i in range(len(y)):\n    if x[i] == fleg_x[fl_top_index]:\n        fback_y.append(y[i])\n        if fback_y:\n            fb_index=np.argmin(fback_y)\nhead_x=[]\nhead_y=[]\n\nfor i in range(len(y)):\n    if y[i] < min(y)+height/4:\n        if x[i] > max(x)-width/3:\n            head_x.append(x[i])\n            head_y.append(y[i])\n    if head_x:\n        ear_index=np.argmin(head_y)\n```\n\n\n```python\n### find neck from small head\nShead_x=[]\nShead_y=[]\nfor i in range(len(y)):\n    if y[i] < min(y)+height/4:\n        if x[i] > max(x)-width/3:\n            Shead_x.append(x[i])\n            Shead_y.append(y[i])\n        if Shead_x:\n            Sbneck_index=np.argmin(Shead_x)\n            Sfneck_index=np.argmax(Shead_y)\n\n```\n\n\n```python\n### distance between legs\nif frontleg_x and backleg_x:\n    len_gait = np.sqrt(((frontleg_x[fl_index] - backleg_x[bl_index])**2)+((frontleg_y[fl_index] - backleg_y[bl_index])**2))\n```\n\n\n```python\nlen_gait\n```\n\n\n\n\n    470.7706447942565\n\n\n\n\n```python\n### area of body triangle between rear, middle stomach, and back of neck\nif Shead_x and belly_x and rear_x:\n    tail_tip = np.sqrt(((belly_x[tip_index] - rear_x[r_index])**2)+((belly_y[tip_index] - rear_y[r_index])**2))\n    neck_tip = np.sqrt(((belly_x[tip_index] - Shead_x[Sbneck_index])**2)+((belly_y[tip_index] - Shead_y[Sbneck_index])**2))\n    neck_tail = np.sqrt(((rear_x[r_index] - Shead_x[Sbneck_index])**2)+((rear_y[r_index] - Shead_y[Sbneck_index])**2))\n    perim = 0.5*(tail_tip + neck_tip + neck_tail)\n    cow_triangle = np.sqrt(perim * (perim - tail_tip) * (perim - neck_tip) * (perim - neck_tail))\n\n### area of body polygon between rear, middle stomach, front of neck, and back of neck\ncow_poly = np.abs((((belly_x[tip_index]*Shead_y[Sfneck_index])-(belly_y[tip_index]*Shead_x[Sfneck_index]))+((Shead_x[Sfneck_index]*Shead_y[Sbneck_index])-(Shead_y[Sfneck_index]*Shead_x[Sbneck_index]))+((Shead_x[Sbneck_index]*rear_y[r_index])-(Shead_y[Sbneck_index]*rear_x[r_index]))+((rear_x[r_index]*belly_y[tip_index])-(rear_y[r_index]*belly_x[tip_index])))/2)\n```\n\n\n```python\n# Perimeter\nperim\n```\n\n\n\n\n    707.1557276217562\n\n\n"
      },
      "source": "https://raw.githubusercontent.com/lauzingaretti/deepaps/master/README.md",
      "technique": "header_analysis"
    }
  ]
}