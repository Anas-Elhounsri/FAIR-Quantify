{
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "References",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>"
        ],
        "type": "Text_excerpt",
        "value": "[1] Zhuang, Juntang, et al. \"Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE.\" arXiv preprint arXiv:2006.02493 (2020). [[arxiv]](https://arxiv.org/abs/2006.02493) <br/>\n\nPlease cite our paper if you find this repository useful:\n```\n@article{zhuang2020adaptive,\n  title={Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE},\n  author={Zhuang, Juntang and Dvornek, Nicha and Li, Xiaoxiao and Tatikonda, \n  Sekhar and Papademetris, Xenophon and Duncan, James},\n  journal={ICML},\n  year={2020}\n}\n```\n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Zhuang, Juntang and Dvornek, Nicha and Li, Xiaoxiao and Tatikonda, \nSekhar and Papademetris, Xenophon and Duncan, James",
        "format": "bibtex",
        "title": "Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE",
        "type": "Text_excerpt",
        "value": "@article{zhuang2020adaptive,\n    year = {2020},\n    journal = {ICML},\n    author = {Zhuang, Juntang and Dvornek, Nicha and Li, Xiaoxiao and Tatikonda, \nSekhar and Papademetris, Xenophon and Duncan, James},\n    title = {Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE},\n}"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/juntang-zhuang/torch_ACA"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-06-03T18:35:35Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-02-25T08:14:23Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "repo for paper: Adaptive Checkpoint Adjoint (ACA) method for gradient estimation in neural ODE"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9985849066900416,
      "result": {
        "original_header": "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
        "type": "Text_excerpt",
        "value": "- This library provides ordinary differential equation (ODE) solvers implemented in PyTorch as proposed in [[1]](https://arxiv.org/abs/2006.02493), and can be plugged into exisiting neural network models. <br/>\n- Compared with ```torchdiffeq``` implementation, ACA uses a trajectory checkpoint strategy to guarantee numerical accuracy in reverse-mode trajectory, hence is more accurate in gradient estimation. <br/>\n- To our knowledge, ACA is the first adaptive solver to enable Neural-ODE model to outperform a ResNet model on benchmark such as Cifar classification, which also supports adaptive-stepsize and error estimation as most widely used softwares. <br/>\n- ACA supports conventional parametric ODE models. <br/>\n- ACA supports multi-GPU trainng and higher order derivative (e.g. add gradient penalty to the loss function).\n \n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/juntang-zhuang/torch-ACA/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "faq": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Three-body problem",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "Examples"
        ],
        "type": "Text_excerpt",
        "value": "Please run ```python three_body_problem.py ```. <br/>\nThe problem is: given trajectories of three stars, how to estimate their masses and predict their future trajectory.<br/>\n[Watch the videos in folder ```figures```](https://www.youtube.com/playlist?list=PL7KkG3n9bER4ODAMzAKzfXIaF0ndUxK-N)\n[![Alt text](./figures/three_body.png)](https://www.youtube.com/playlist?list=PL7KkG3n9bER4ODAMzAKzfXIaF0ndUxK-N)\n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 6
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/juntang-zhuang/torch_ACA/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "juntang-zhuang/torch_ACA"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Update: ACA is now integrated into TorchDiffEqPack (https://jzkay12.github.io/TorchDiffEqPack/) with more flexiable API to support tuple-output ODEs, memory efficient asynchronous leapfrog integrator (MALI) and more examples."
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/./figures/three_body.png"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/./figures/results.png"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "regular_expression"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/juntang-zhuang/torch_ACA/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "torch_ACA"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "juntang-zhuang"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 165943,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2006.02493 (2020). [[arxiv]](https://arxiv.org/abs/2006.02493) <br/>\n\nPlease cite our paper if you find this repository useful:\n```\n@article{zhuang2020adaptive,\n  title={Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2006.02493"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "regular_expression"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Dependencies",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>"
        ],
        "type": "Text_excerpt",
        "value": "- PyTorch 1.0 (Will test on other versions later)\n- tensorboardX\n- Python 3\n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "installation",
    "acknowledgement",
    "run",
    "download",
    "contact",
    "contributors",
    "documentation",
    "license",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 05:36:15",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 55
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "How to use",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>"
        ],
        "type": "Text_excerpt",
        "value": "```\noptions = {}\noptions.update({'method':args.method})\noptions.update({'h': args.h})\noptions.update({'t0': args.t0})\noptions.update({'t1': args.t1})\noptions.update({'rtol': args.rtol})\noptions.update({'atol': args.atol})\noptions.update({'print_neval': args.print_neval})\noptions.update({'neval_max': args.neval_max})\noptions.update({'t_eval': [t0, t0 + (t1-t0)/10, ...  ,t1]})\n```"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Note:",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "- t_eval should be a list of float type, specifying which time points for the solution to evaluate\n- x is a PyTorch tensor\n\n- Mode 1: directly get evaluated results\n```\nout = odesolve(odefunc, x, options)\n```\n\n- Mode 2: get the solver then evaluate it\n```\nsolver = odesolve(odefunc, x, options, return_solver = True)\nout = solver.integrate(x, t_eval = option['t_eval'])\n```\n\n- Mode 3: evaluate at different time points (new_t_eval) with dense mode\n```\noptions.update({'dense_output': True})\nsolver = odesolve(odefunc, x, options, return_solver = True)\nout = solver.integrate(x, t_eval = option['t_eval'])\nout2 = solver.evaluate_dense_mode(t_eval = new_t_eval)  \n```\nsolver.integrate performs integration and update dense states, solver.evaluate_dense_mode only evaluates without updating dense states\n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Parameters",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "See https://github.com/juntang-zhuang/torch_ACA/blob/master/torch_ACA/odesolver/adaptive_grid_solver.py for a full list of parameters\n- ```method```: which ode solver. Fixed stepsize solvers include ['Euler','RK2','RK4'], adaptive stepsize solvers include ['RK12','RK23','RK45','Dopri5'] ('RK12' is also called 'HeunEuler' in the literature).\n- ```h```: initial stepsize. h must be specified for fixed stepsize solvers, and can be set as None (or not parsed) for adaptive solvers.\n- ```t0, t1```: start and end time. t1 can be either smaller or larger than t0.\n- ```rtol, atol```: relative and absolute error tolerance for adaptive solvers. \n- ```print_neval```: bool type, print number of evaluations of the function.\n- ```neval_max```: the maximum number of function evaluation, typically set as a large number (e.g. 500,000). If this number is reached, the ODE is stiff.\n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "End-time fast mode <br/>",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "How to use",
          "Train with different modes"
        ],
        "type": "Text_excerpt",
        "value": "```cifar_classification/train.py``` uses the solver defined in ```torch_ACA/odesolver_mem/ode_solver_endtime.py```, this mode only support integration from start time t0 to end time t1, and output a tensor for time t1.\n```\nfrom torch_ACA import odesolve_endtime as odesolve\nout = odesolve(odefunc, x, options)\n```\n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "End-time memory-efficient mode <br/>",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "How to use",
          "Train with different modes"
        ],
        "type": "Text_excerpt",
        "value": "```cifar_classification/train_mem.py``` uses the solver defined in ```torch_ACA/odesolver_mem/adjoint_mem.py```, this mode only support integration from start time t0 to end time t1, and output a tensor for time t1. Furtheremore, this mode uses O(Nf + Nt) memory, which is more memory-efficient than normal mode, but the running time is longer.\n```\nfrom torch_ACA import odesolve_adjoint as odesolve\nout = odesolve(odefunc, x, options)\n```\n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Multiple evaluation time-points mode <br/>",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "How to use",
          "Train with different modes"
        ],
        "type": "Text_excerpt",
        "value": "```cifar_classification/train_multieval.py``` uses the solver defined in ```torch_ACA/odesolver/ode_solver.py```, this mode supports extracting outputs from multiple time points between t0 and t1. \n\n- Case1: t_eval contains one evaluation time points\n```\nfrom torch_ACA import odesolve\noptions.update({'t_eval': [args.t1]})\nout = odesolve(odefunc, x, options)\n```\n- Case2:  t_eval contains multiple time points\n```\nfrom torch_ACA import odesolve\noptions.update({'t_eval': [a1, a2, a3, ... an]})\nout = odesolve(odefunc, x, options)\nout1, out2, ... outn = out[0,...], out[1,...], ... out[n-1,...]\n```\n\n- Note for multiple evaluation time-points mode: <br/>\n```\n  (1) Evaluation time 't_eval' must be specified in a list. \n        e.g.  t_eval = [a1, a2, a3 ..., an]  \n        where t0 < a1 < a2 < ... t1, or t1 < a1 < a2 < ... < t0 \n  (2) Suppose 'z' is of shape 'AxBxCx...', then the output is of shape 'nxAxBxCx...', \n        while in the end-time mode the output is of shape 'AxBxCx...'\n  (3) Both multiple time-points mode and end-time fast mode support higher order derivatives \n        (e.g. add gradient penalty in the loss function)\n        (e.g. loss.backward(retain_graph=True, create_graph=True); a = param.grad; \n              b = torch.sum(a); b.backward() )\n  (4) Adaptive stepsize solver is recommended in multi evaluation time-points mode\n```\n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Gradient w.r.t. start time t0 and end time t1 when f is non-autonomous",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "How to use",
          "Train with different modes"
        ],
        "type": "Text_excerpt",
        "value": "- In case we need to take derivate w.r.t. time, we can add one more dimension to the output of ```f```, s.t. ```\\frac{dt}{dt}=1```. Basically, view time as a separate dimension, whose derivate is a constant 1. \n- Set the start time t0 as a ```Variable``` or a ```Parameter``` in PyTorch, then you can get its derivative.\n- Gradient w.r.t t1 is somehow trivial, you can simply calculate it as <br/>\n```\\frac{dL}{dt1} = \\frac{dL}{dz(t1)} \\frac{dz(t1)}{dt1} = \\frac{dL}{dz(t1)} f(t1,z(t1), \\theta)```\n- If we want to get ```\\frac{dL}{dt}``` where ```t0<t<t1```, there are two cases: <br/>\n  -  (1) We explicitly take out the hidden states ```z(t)```, and the loss function contains ```z(t)``` explcitly, then ```\\frac{dL}{dz(t)}``` can be easily solved with standard backpropagation, and ```\\frac{dL}{dt}=\\frac{dL}{dz(t)} \\frac{dz(t)}{dt} = \\frac{dL}{dz(t)}f(t,z(t),\\theta)```. <br/>\n  - (2) Loss function does not contain ```z(t)``` explcitly, this situation is slightly tricky, and we need to solve a separate adjoint equation. The coding to achieve this is somehow messy, and currently not included in this repo.\n  \n#### The type of returned result from ```f``` should be a single tensor\n- This repository currently only supports ``` \\frac{dz}{dt} = f(t,z) ``` where ```z``` is a tensor (other data types such as tuple are not supported). <br/>\n- If you are using a function ```f``` which produces many output tensors or ```z``` is a list of tensors, you can concatenate them into a single tensor within definition of ```f```.\n\n\n## Examples\n### Three-body problem\nPlease run ```python three_body_problem.py ```. <br/>\nThe problem is: given trajectories of three stars, how to estimate their masses and predict their future trajectory.<br/>\n[Watch the videos in folder ```figures```](https://www.youtube.com/playlist?list=PL7KkG3n9bER4ODAMzAKzfXIaF0ndUxK-N)\n[![Alt text](./figures/three_body.png)](https://www.youtube.com/playlist?list=PL7KkG3n9bER4ODAMzAKzfXIaF0ndUxK-N)\n\n### Image classification on Cifar\nA ResNet18 is modified into its corresponding ODE model, and achieve ~5% errorate (vs 10% by adjoint method and naive method).\nCode is in folder ```cifar_classification```\n#### How to train\n```\npython train.py\n```\n#### Visualize the training and validation curve with \n```\ntensorboard --logdir cifar_classification/resnet/resnet_RK12_lr_0.1_h_None\n```\n#### Test with different solvers WITHOUT re-training\nTest a pre-trained model using different ode solvers, you can specify solver type, stepsize, toelrance and other parameters in ```test.py```\n```\npython test.py --method Euler --h 0.2 --resume 'pre-trained weights path'\npython test.py --method RK23 --rtol 1e-3 --atol 1e-3 --resume 'pre-trained weights path'\n...\n```\n\n### Results\n<img src=\"./figures/results.png\">\n\n### Updates\n- Added dense state for ODE solvers, by adding ```options.update({'dense_output': True})```. In this mode, the solver can be called the same way as a function, without re-integrating. <br/>\n  ```\n  from torch_ACA.odesolver import odesolve\n  options.update({'dense_output': True})\n  solver = odesolve(func, initial_condition, options=options, return_solver=True)#, time_points=t_list)\n  out = solver.integrate(initial_condition, t_eval=options['t_eval'])\n  out_tmp = solver.evaluate_dense_mode(t_eval=options['t_eval'])\n  print(torch.sum((out_tmp - out)**2))\n  ```\n## References\n[1] Zhuang, Juntang, et al. \"Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE.\" arXiv preprint arXiv:2006.02493 (2020). [[arxiv]](https://arxiv.org/abs/2006.02493) <br/>\n\nPlease cite our paper if you find this repository useful:\n```\n@article{zhuang2020adaptive,\n  title={Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE},\n  author={Zhuang, Juntang and Dvornek, Nicha and Li, Xiaoxiao and Tatikonda, \n  Sekhar and Papademetris, Xenophon and Duncan, James},\n  journal={ICML},\n  year={2020}\n}\n```\n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "The type of returned result from BASH23* should be a single tensor",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "How to use",
          "Train with different modes"
        ],
        "type": "Text_excerpt",
        "value": "# Update: ACA is now integrated into TorchDiffEqPack (https://jzkay12.github.io/TorchDiffEqPack/) with more flexiable API to support tuple-output ODEs, memory efficient asynchronous leapfrog integrator (MALI) and more examples.\n\n# PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver [[arxiv]](https://arxiv.org/abs/2006.02493)[[slides]](https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing)\n- This library provides ordinary differential equation (ODE) solvers implemented in PyTorch as proposed in [[1]](https://arxiv.org/abs/2006.02493), and can be plugged into exisiting neural network models. <br/>\n- Compared with ```torchdiffeq``` implementation, ACA uses a trajectory checkpoint strategy to guarantee numerical accuracy in reverse-mode trajectory, hence is more accurate in gradient estimation. <br/>\n- To our knowledge, ACA is the first adaptive solver to enable Neural-ODE model to outperform a ResNet model on benchmark such as Cifar classification, which also supports adaptive-stepsize and error estimation as most widely used softwares. <br/>\n- ACA supports conventional parametric ODE models. <br/>\n- ACA supports multi-GPU trainng and higher order derivative (e.g. add gradient penalty to the loss function).\n\n## Dependencies\n- PyTorch 1.0 (Will test on other versions later)\n- tensorboardX\n- Python 3\n\n## How to use\n```\noptions = {}\noptions.update({'method':args.method})\noptions.update({'h': args.h})\noptions.update({'t0': args.t0})\noptions.update({'t1': args.t1})\noptions.update({'rtol': args.rtol})\noptions.update({'atol': args.atol})\noptions.update({'print_neval': args.print_neval})\noptions.update({'neval_max': args.neval_max})\noptions.update({'t_eval': [t0, t0 + (t1-t0)/10, ...  ,t1]})\n```\n### Note:\n- t_eval should be a list of float type, specifying which time points for the solution to evaluate\n- x is a PyTorch tensor\n\n- Mode 1: directly get evaluated results\n```\nout = odesolve(odefunc, x, options)\n```\n\n- Mode 2: get the solver then evaluate it\n```\nsolver = odesolve(odefunc, x, options, return_solver = True)\nout = solver.integrate(x, t_eval = option['t_eval'])\n```\n\n- Mode 3: evaluate at different time points (new_t_eval) with dense mode\n```\noptions.update({'dense_output': True})\nsolver = odesolve(odefunc, x, options, return_solver = True)\nout = solver.integrate(x, t_eval = option['t_eval'])\nout2 = solver.evaluate_dense_mode(t_eval = new_t_eval)  \n```\nsolver.integrate performs integration and update dense states, solver.evaluate_dense_mode only evaluates without updating dense states\n\n### Parameters\nSee https://github.com/juntang-zhuang/torch_ACA/blob/master/torch_ACA/odesolver/adaptive_grid_solver.py for a full list of parameters\n- ```method```: which ode solver. Fixed stepsize solvers include ['Euler','RK2','RK4'], adaptive stepsize solvers include ['RK12','RK23','RK45','Dopri5'] ('RK12' is also called 'HeunEuler' in the literature).\n- ```h```: initial stepsize. h must be specified for fixed stepsize solvers, and can be set as None (or not parsed) for adaptive solvers.\n- ```t0, t1```: start and end time. t1 can be either smaller or larger than t0.\n- ```rtol, atol```: relative and absolute error tolerance for adaptive solvers. \n- ```print_neval```: bool type, print number of evaluations of the function.\n- ```neval_max```: the maximum number of function evaluation, typically set as a large number (e.g. 500,000). If this number is reached, the ODE is stiff.\n\n### Train with different modes\n#### End-time fast mode <br/>\n```cifar_classification/train.py``` uses the solver defined in ```torch_ACA/odesolver_mem/ode_solver_endtime.py```, this mode only support integration from start time t0 to end time t1, and output a tensor for time t1.\n```\nfrom torch_ACA import odesolve_endtime as odesolve\nout = odesolve(odefunc, x, options)\n```\n\n#### End-time memory-efficient mode <br/>\n```cifar_classification/train_mem.py``` uses the solver defined in ```torch_ACA/odesolver_mem/adjoint_mem.py```, this mode only support integration from start time t0 to end time t1, and output a tensor for time t1. Furtheremore, this mode uses O(Nf + Nt) memory, which is more memory-efficient than normal mode, but the running time is longer.\n```\nfrom torch_ACA import odesolve_adjoint as odesolve\nout = odesolve(odefunc, x, options)\n```\n\n#### Multiple evaluation time-points mode <br/>\n```cifar_classification/train_multieval.py``` uses the solver defined in ```torch_ACA/odesolver/ode_solver.py```, this mode supports extracting outputs from multiple time points between t0 and t1. \n\n- Case1: t_eval contains one evaluation time points\n```\nfrom torch_ACA import odesolve\noptions.update({'t_eval': [args.t1]})\nout = odesolve(odefunc, x, options)\n```\n- Case2:  t_eval contains multiple time points\n```\nfrom torch_ACA import odesolve\noptions.update({'t_eval': [a1, a2, a3, ... an]})\nout = odesolve(odefunc, x, options)\nout1, out2, ... outn = out[0,...], out[1,...], ... out[n-1,...]\n```\n\n- Note for multiple evaluation time-points mode: <br/>\n```\n  (1) Evaluation time 't_eval' must be specified in a list. \n        e.g.  t_eval = [a1, a2, a3 ..., an]  \n        where t0 < a1 < a2 < ... t1, or t1 < a1 < a2 < ... < t0 \n  (2) Suppose 'z' is of shape 'AxBxCx...', then the output is of shape 'nxAxBxCx...', \n        while in the end-time mode the output is of shape 'AxBxCx...'\n  (3) Both multiple time-points mode and end-time fast mode support higher order derivatives \n        (e.g. add gradient penalty in the loss function)\n        (e.g. loss.backward(retain_graph=True, create_graph=True); a = param.grad; \n              b = torch.sum(a); b.backward() )\n  (4) Adaptive stepsize solver is recommended in multi evaluation time-points mode\n```\n\n#### Gradient w.r.t. start time t0 and end time t1 when f is non-autonomous\n- In case we need to take derivate w.r.t. time, we can add one more dimension to the output of ```f```, s.t. ```\\frac{dt}{dt}=1```. Basically, view time as a separate dimension, whose derivate is a constant 1. \n- Set the start time t0 as a ```Variable``` or a ```Parameter``` in PyTorch, then you can get its derivative.\n- Gradient w.r.t t1 is somehow trivial, you can simply calculate it as <br/>\n```\\frac{dL}{dt1} = \\frac{dL}{dz(t1)} \\frac{dz(t1)}{dt1} = \\frac{dL}{dz(t1)} f(t1,z(t1), \\theta)```\n- If we want to get ```\\frac{dL}{dt}``` where ```t0<t<t1```, there are two cases: <br/>\n  -  (1) We explicitly take out the hidden states ```z(t)```, and the loss function contains ```z(t)``` explcitly, then ```\\frac{dL}{dz(t)}``` can be easily solved with standard backpropagation, and ```\\frac{dL}{dt}=\\frac{dL}{dz(t)} \\frac{dz(t)}{dt} = \\frac{dL}{dz(t)}f(t,z(t),\\theta)```. <br/>\n  - (2) Loss function does not contain ```z(t)``` explcitly, this situation is slightly tricky, and we need to solve a separate adjoint equation. The coding to achieve this is somehow messy, and currently not included in this repo.\n  \n#### The type of returned result from ```f``` should be a single tensor\n- This repository currently only supports ``` \\frac{dz}{dt} = f(t,z) ``` where ```z``` is a tensor (other data types such as tuple are not supported). <br/>\n- If you are using a function ```f``` which produces many output tensors or ```z``` is a list of tensors, you can concatenate them into a single tensor within definition of ```f```.\n\n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Image classification on Cifar",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "Examples"
        ],
        "type": "Text_excerpt",
        "value": "A ResNet18 is modified into its corresponding ODE model, and achieve ~5% errorate (vs 10% by adjoint method and naive method).\nCode is in folder ```cifar_classification```"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "How to train",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "Examples",
          "Image classification on Cifar"
        ],
        "type": "Text_excerpt",
        "value": "```\npython train.py\n```"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Visualize the training and validation curve with",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "Examples",
          "Image classification on Cifar"
        ],
        "type": "Text_excerpt",
        "value": "```\ntensorboard --logdir cifar_classification/resnet/resnet_RK12_lr_0.1_h_None\n```"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Test with different solvers WITHOUT re-training",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "Examples",
          "Image classification on Cifar"
        ],
        "type": "Text_excerpt",
        "value": "Test a pre-trained model using different ode solvers, you can specify solver type, stepsize, toelrance and other parameters in ```test.py```\n```\npython test.py --method Euler --h 0.2 --resume 'pre-trained weights path'\npython test.py --method RK23 --rtol 1e-3 --atol 1e-3 --resume 'pre-trained weights path'\n...\n```\n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Results",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "Examples"
        ],
        "type": "Text_excerpt",
        "value": "<img src=\"./figures/results.png\">\n"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Updates",
        "parent_header": [
          "PyTorch implementation of \"Adaptive Checkpoint Adjoint\" (ACA) for an accurate and differentiable ODE solver <a href=\"https://arxiv.org/abs/2006.02493\">[arxiv]</a><a href=\"https://docs.google.com/presentation/d/1SHSJUJpof5_KbgTGyDfkoso25dGhmICU70yFFs_3dAw/edit?usp=sharing\">[slides]</a>",
          "Examples"
        ],
        "type": "Text_excerpt",
        "value": "- Added dense state for ODE solvers, by adding ```options.update({'dense_output': True})```. In this mode, the solver can be called the same way as a function, without re-integrating. <br/>\n  ```\n  from torch_ACA.odesolver import odesolve\n  options.update({'dense_output': True})\n  solver = odesolve(func, initial_condition, options=options, return_solver=True)#, time_points=t_list)\n  out = solver.integrate(initial_condition, t_eval=options['t_eval'])\n  out_tmp = solver.evaluate_dense_mode(t_eval=options['t_eval'])\n  print(torch.sum((out_tmp - out)**2))\n  ```"
      },
      "source": "https://raw.githubusercontent.com/juntang-zhuang/torch-ACA/dense_state2/readme.md",
      "technique": "header_analysis"
    }
  ]
}