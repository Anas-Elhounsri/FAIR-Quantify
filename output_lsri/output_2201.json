{
  "application_domain": [
    {
      "confidence": 21.55,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citation: <a id=\"citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"><span class=\"octicon octicon-link\"></span></a>",
        "parent_header": [
          "<a href=\"https://arxiv.org/abs/2008.00230\">RGB-D Salient Object Detection: A Survey</a>",
          "------"
        ],
        "type": "Text_excerpt",
        "value": "If you find our survey paper and evaluation code are useful, please cite the following paper:\n\n\t@article{zhou2020rgbd,\n  \t\ttitle={RGB-D Salient Object Detection: A Survey},\n  \t\tauthor={Zhou, Tao and Fan, Deng-Ping and Cheng, Ming-Ming and Shen, Jianbing and Shao, Ling},\n  \t\tjournal={Computational Visual Media},\n  \t\tpages={1--33},\n  \t\tyear={2021},\n  \t\tpublisher={Springer}\n\t}\n\t\n\t\n\n\n\n"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Zhou, Tao and Fan, Deng-Ping and Cheng, Ming-Ming and Shen, Jianbing and Shao, Ling",
        "format": "bibtex",
        "title": "RGB-D Salient Object Detection: A Survey",
        "type": "Text_excerpt",
        "value": "@article{zhou2020rgbd,\n    publisher = {Springer},\n    year = {2021},\n    pages = {1--33},\n    journal = {Computational Visual Media},\n    author = {Zhou, Tao and Fan, Deng-Ping and Cheng, Ming-Ming and Shen, Jianbing and Shao, Ling},\n    title = {RGB-D Salient Object Detection: A Survey},\n}"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/taozh2017/RGBD-SODsurvey"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-06-15T08:28:21Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-11T13:48:05Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "RGB-D Salient Object Detection: A Survey"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9068677504817853,
      "result": {
        "original_header": "<a href=\"https://arxiv.org/abs/2008.00230\">RGB-D Salient Object Detection: A Survey</a>",
        "type": "Text_excerpt",
        "value": "This is a survey to review related RGB-D SOD models along with benchmark datasets, and provide a comprehensive evaluation for these models. We also collect related review papers for SOD and light field SOD models. If you have papers to recommend or any suggestions, please feel free to contact us. \n![alt text](./figures/Fig0.jpg)\n*Fig.0: A brief chronology of RGB-D based SOD. The first early RGB-D based SOD work was the [DM](https://link.springer.com/content/pdf/10.1007/978-3-642-33709-3_8.pdf) model, proposed in 2012. Deep learning\ntechniques have been widely applied to RGB-D based SOD since 2017. More details can be found in our [paper](https://arxiv.org/pdf/2008.00230.pdf).* \n"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8736754485730367,
      "result": {
        "original_header": "Related Reviews and Surveys to SOD:  <a id=\"survey\" class=\"anchor\" href=\"#survey\" aria-hidden=\"true\"><span class=\"octicon octicon-link\"></span></a>",
        "type": "Text_excerpt",
        "value": "**No.** | **Year** | **Pub.** | **Title** | **Links** \n:-: | :-: | :-: | :-  | :-: \n01 | 2015 | IEEE TIP   |  Salient object detection: A benchmark | [Paper](https://arxiv.org/pdf/1501.02741.pdf)/Project\n02 | 2018 | IEEE TCSVT |  Review of visual saliency detection with comprehensive information | [Paper](https://arxiv.org/pdf/1803.03391.pdf)/Project\n03 | 2018 | ACM TIST   |  A review of co-saliency detection algorithms: Fundamentals, applications, and challenges | [Paper](https://arxiv.org/pdf/1604.07090.pdf)/Project\n04 | 2018 | IEEE TSP   |  Advanced deep-learning techniques for salient and category-specific object detection: A survey| [Paper](https://ieeexplore.ieee.org/document/8253582)/Project\n05 | 2018 | IJCV       |  Attentive systems: A survey | [Paper](https://link.springer.com/article/10.1007/s11263-017-1042-6)/Project\n06 | 2018 | ECCV       |  Salient Objects in Clutter: Bringing Salient Object Detection to the Foreground | [Paper](http://mftp.mmcheng.net/Papers/18ECCV-SOCBenchmark.pdf)/[Project](https://github.com/DengPingFan/SODBenchmark/)\n07 | 2019 | CVM        |  Salient object detection: A survey | [Paper](https://link.springer.com/content/pdf/10.1007/s41095-019-0149-9.pdf)/Project\n08 | 2019 | IEEE TNNLS |  Object detection with deep learning: A review | [Paper](https://arxiv.org/pdf/1807.05511.pdf)/Project\n09 | 2020 | arXiv      |  Light Field Salient Object Detection: A Review and Benchmark | [Paper](https://arxiv.org/pdf/2010.04968.pdf)/[Project](https://github.com/kerenfu/LFSOD-Survey) \n10 | 2021 | IEEE TPAMI      |  Salient Object Detection in the Deep Learning Era: An In-Depth Survey | [Paper](https://arxiv.org/pdf/1904.09146.pdf)/[Project](https://github.com/wenguanwang/SODsurvey)  \n"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9053731929825195,
      "result": {
        "original_header": "RGB-D SOD Models:  <a id=\"RGBDmodels\" class=\"anchor\" href=\"#RGBDmodels\" aria-hidden=\"true\"><span class=\"octicon octicon-link\"></span></a>",
        "type": "Text_excerpt",
        "value": ":fire::fire::fire:Update (in 2023-07-26) \n**No.** | **Year** | **Model** |**Pub.** | **Title** | **Links** \n:-: | :-: | :-: | :-  | :-  | :-: \n:fire: 219 | 2023 |XMSNet| ACM MM | Object Segmentation by Mining Cross-Modal Semantics | [Paper](https://arxiv.org/pdf/2305.10469.pdf)/[Project](https://github.com/Zongwei97/XMSNet)\n:fire: 218 | 2023 |PopNet| ICCV | Source-free Depth for Object Pop-out | [Paper](https://arxiv.org/pdf/2212.05370.pdf)/[Project](https://github.com/Zongwei97/PopNet)\n:fire: 217 | 2023 |FCFNet| IEEE TSCVT | Feature Calibrating and Fusing Network for RGB-D Salient Object Detection | [Paper](https://ieeexplore.ieee.org/abstract/document/10185946)/Project\n:fire: 216 | 2023 |AirSOD| IEEE TSCVT | AirSOD: A Lightweight Network for RGB-D Salient Object Detection | [Paper](https://ieeexplore.ieee.org/abstract/document/10184101)/Project\n:fire: 215 | 2023 |CATNet| IEEE TMM | CATNet: A Cascaded and Aggregated Transformer Network For RGB-D Salient Object Detection | [Paper](https://ieeexplore.ieee.org/abstract/document/10179145)/Project\n214 | 2023 |--| arXiv | Hierarchical Cross-modal Transformer for RGB-D Salient Object Detection  | [Paper](https://arxiv.org/pdf/2302.08052.pdf)/Project \n213| 2023 |HiDAnet| IEEE TIP | HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness  | [Paper](https://ieeexplore.ieee.org/document/10091765)/[Project](https://github.com/Zongwei97/HIDANet/)\n212 | 2023 |AFNet| Neurocomputing | Adaptive fusion network for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0925231222015090)/[Project](https://github.com/clelouch/AFNet)\n211 | 2023 |EGA-Net| Information Sciences | EGA-Net: Edge feature enhancement and global information attention network for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0020025523000324)/[Project](https://github.com/guanyuzong/EGA-Net)\n210 | 2022 |RFNet| 3DV | Robust RGB-D Fusion for Saliency Detection | [Paper](https://ieeexplore.ieee.org/document/10044460)/[Project](https://github.com/Zongwei97/RFnet)\n209 | 2022 |--|IEEE TIP| Improving RGB-D Salient Object Detection via Modality-Aware Decoder   | [Paper](https://ieeexplore.ieee.org/abstract/document/9894275)/[Project](https://github.com/MengkeSong/MaD)\n208 | 2022 |--| ICIP | Multi-Modal Transformer for RGB-D Salient Object Detection  | [Paper](https://ieeexplore.ieee.org/abstract/document/9898069)/Project\n207 | 2022 |--| AI | A cascaded refined rgb-d salient object detection network based on the attention mechanism  | [Paper](https://link.springer.com/article/10.1007/s10489-022-04186-9)/Project\n206 | 2022 |EFGNet| DSP | EFGNet: Encoder steered multi-modality feature guidance network for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S105120042200392X)/Project\n205 | 2022 |--| ICIP | Dynamic Selection Network For Rgb-D Salient Object Detection  | [Paper](https://ieeexplore.ieee.org/abstract/document/9897821)/Project\n204 | 2022 |--| IEEE SPL | Cross-stage Multi-scale Interaction Network for RGB-D Salient Object Detection | [Paper](https://ieeexplore.ieee.org/abstract/document/9956739)/Project\n203 | 2022 |GCENet| JVCIR | GCENet: Global contextual exploration network for RGB-D salient object detection | [Paper](https://www.sciencedirect.com/science/article/pii/S1047320322002000)/Project\n202 | 2022 |SiamRIR|ACCV | Multi-scale Residual Interaction for RGB-D Salient Object Detection  | [Paper](https://openaccess.thecvf.com/content/ACCV2022/papers/Hu_Multi-scale_Residual_Interaction_for_RGB-D_Salient_Object_Detection_ACCV_2022_paper.pdf)/Project\n201 | 2022 |SAFNet| ACCV | Scale Adaptive Fusion Network for RGB-D Salient Object Detection  | [Paper](https://openaccess.thecvf.com/content/ACCV2022/papers/Kong_Scale_Adaptive_Fusion_Network_for_RGB-D_Salient_Object_Detection_ACCV_2022_paper.pdf)/Project\n200 | 2022 |TBINet|ACCV| Three-Stage Bidirectional Interaction Network for Efficient RGB-D Salient Object Detection  | [Paper](https://openaccess.thecvf.com/content/ACCV2022/papers/Wang_Three-Stage_Bidirectional_Interaction_Network_for_Efficient_RGB-D_Salient_Object_Detection_ACCV_2022_paper.pdf)/[Project](https://github.com/AWORKERINKIKIMORE/TBINet)\n199 | 2022 |--|Neurocomputing| Few-shot learning-based RGB-D salient object detection: A case study  | [Paper](https://www.sciencedirect.com/science/article/pii/S0925231222011055)/Project\n198 | 2022 |--|ACM MM | Depth-inspired Label Mining for Unsupervised RGB-D Salient Object Detection  | [Paper](https://dl.acm.org/doi/pdf/10.1145/3503161.3548037)/Project\n197 | 2022 |RD3D|IEEE TNNLS | 3-D Convolutional Neural Networks for RGB-D Salient Object Detection and Beyond  | [Paper](https://ieeexplore.ieee.org/abstract/document/9889257)/[Project](https://github.com/QianChen98/RD3D)\n196 | 2022 |--| IEEE TIP | Middle-Level Feature Fusion for Lightweight RGB-D Salient Object Detection  | [Paper](https://ieeexplore.ieee.org/abstract/document/9923611)/Project\n195 | 2022 |--| Neurocomputing | Depth-aware inverted refinement network for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0925231222014126)/Project\n194 | 2022 |DCMNet| ESA | DCMNet: Discriminant and cross-modality network for RGB-D salient object detection   | [Paper](https://www.sciencedirect.com/science/article/pii/S0957417422020656)/Project\n193 | 2022 |HINet| PR | Cross-modal hierarchical interaction network for RGB-D salient object detection   | [Paper](https://www.sciencedirect.com/science/article/pii/S0031320322006732)/[Project](https://github.com/RanwanWu/HINet)\n192 | 2022 |CIR-Net| IEEE TIP | CIR-Net: Cross-modality interaction and refinement for RGB-D salient object detection   | [Paper](https://ieeexplore.ieee.org/abstract/document/9930882)/[arXiv](https://arxiv.org/pdf/2210.02843.pdf)/[Project](https://rmcong.github.io/proj_CIRNet.html)\n191 | 2022 |MVSalNet | ECCV | MVSalNet:Multi-View Augmentation for RGB-D Salient Object Detection  | [Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136890268.pdf)/[Project](https://github.com/Heart-eartH/MVSalNet)\n190 | 2022 |SPSN     | ECCV | SPSN: Superpixel Prototype Sampling Network for RGB-D Salient Object Detection  | [Paper](https://arxiv.org/pdf/2207.07898.pdf)/[Project](https://github.com/Hydragon516/SPSN)\n189 | 2022 |RLLNet     | SCIS | RLLNet: a lightweight remaking learning network for saliency redetection on RGB-D images  | [Paper](https://link.springer.com/article/10.1007/s11432-020-3337-9)/Project\n188 | 2022 |-| IEEE TIP | Learning Implicit Class Knowledge for RGB-D Co-Salient Object Detection with Transformers  | [Paper](https://ieeexplore.ieee.org/abstract/document/9810116)/Project\n187 | 2022 |-| MTAP | A benchmark dataset and baseline model for co-salient object detection within RGB-D images | [Paper](https://link.springer.com/article/10.1007/s11042-021-11555-y)/Project\n186 | 2022 |SA-DPNet| PR | SA-DPNet: Structure-aware dual pyramid network for salient object detection | [Paper](https://www.sciencedirect.com/science/article/pii/S0031320322001054)/Project\n185 | 2022 |-| JPCS | RGBD salient object detection based on depth feature enhancement  | [Paper](https://iopscience.iop.org/article/10.1088/1742-6596/2181/1/012008/meta)/Project\n184 | 2022 |-| JCSC | Bifurcation Fusion Network for RGB-D Salient Object Detection  | [Paper](https://www.worldscientific.com/doi/abs/10.1142/S0218126622502152)/Project\n183 | 2022 |-| arXiv | Dynamic Message Propagation Network for RGB-D Salient Object Detection   | [Paper](https://arxiv.org/pdf/2206.09552.pdf)/Project\n182 | 2022 |-| IEEE TMM | Depth-induced Gap-reducing Network for RGB-D Salient Object Detection: An Interaction, Guidance and Refinement Approach   | [Paper](https://ieeexplore.ieee.org/abstract/document/9769984)/Project\n181 | 2022 |-| arXiv | Dual Swin-Transformer based Mutual Interactive Network for RGB-D Salient Object Detection   | [Paper](https://arxiv.org/pdf/2206.03105.pdf)/Project\n180 | 2022 |MoADNet| IEEE TCSVT | MoADNet: Mobile Asymmetric Dual-Stream Networks for Real-Time and Lightweight RGB-D Salient Object Detection  | [Paper](https://ieeexplore.ieee.org/abstract/document/9789193)/Project\n179 | 2022 |A2TPNet| Electronics | A2TPNet: Alternate Steered Attention and Trapezoidal Pyramid Fusion Network for RGB-D Salient Object Detection  | [Paper](https://www.mdpi.com/2079-9292/11/13/1968)/Project\n178 | 2022 |--| NPL | Depth Enhanced Cross-Modal Cascaded Network for RGB-D Salient Object Detection  | [Paper](https://link.springer.com/article/10.1007/s11063-022-10886-7)/Project\n177 | 2022 |--| KBS | Boosting RGB-D salient object detection with adaptively cooperative dynamic fusion network  | [Paper](https://www.sciencedirect.com/science/article/pii/S0950705122005998)/Project\n176 | 2022 |C2DFNet| IEEE TMM | C2DFNet: Criss-Cross Dynamic Filter Network for RGB-D Salient Object Detection  | [Paper](https://ieeexplore.ieee.org/abstract/document/9813422)/[Project](https://github.com/OIPLab-DUT/C2DFNet)\n175 | 2022 |MEANet| Neurocomputing | MEANet: Multi-modal edge-aware network for light field salient object detection  | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231222003502)/[Project](https://github.com/jiangyao-scu/MEANet)\n174 | 2022 |--| arXiv | Depth-Cooperated Trimodal Network for Video Salient Object Detection   | [Paper](https://arxiv.org/abs/2202.06060)/Project\n173 | 2022 |DFTR| arXiv | DFTR: Depth-supervised Fusion Transformer for Salient Object Detection  | [Paper](https://arxiv.org/abs/2203.06429)/Project\n172 | 2022 |--| JP: CS | Multi-level interactions for RGB-D object detection | [Paper](https://iopscience.iop.org/article/10.1088/1742-6596/2181/1/012003/meta)/Project\n171 | 2022 |--| AAAI |  Self-Supervised Pretraining for RGB-D Salient Object Detection | [Paper](https://www.aaai.org/AAAI22Papers/AAAI-4882.ZhaoX.pdf)/[Project](https://github.com/Xiaoqi-Zhao-DLUT/SSLSOD)\n170 | 2022 |GroupTransNet| arXiv | GroupTransNet: Group Transformer Network for RGB-D Salient Object Detection | [Paper](https://arxiv.org/abs/2203.10785)/Project\n169 | 2022 |GroupTransNet| arXiv | GroupTransNet: Group Transformer Network for RGB-D Salient Object Detection | [Paper](https://arxiv.org/abs/2203.10785)/Project\n168 | 2022 |BGRDNet| MTAP | BGRDNet: RGB-D salient object detection with a bidirectional gated recurrent decoding network  | [Paper](https://link.springer.com/article/10.1007/s11042-022-12799-y)/Project\n167 | 2022 |LIANet| IEEE Access| LIANet: Layer Interactive Attention Network for RGB-D Salient Object Detectionn  | [Paper](https://ieeexplore.ieee.org/abstract/document/9729233)/Project\n166 | 2022 |AGRFNet| SP:IC| AGRFNet: Two-stage cross-modal and multi-level attention gated recurrent fusion network for RGB-D saliency detection  | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0923596522000261)/Project\n165 | 2022 |--| IEEE TIP| Weakly Supervised RGB-D Salient Object Detection With Prediction Consistency Training and Active Scribble Boosting  | [Paper](https://ieeexplore.ieee.org/abstract/document/9720104)/Project\n164 | 2022 |--| PR| Discriminative unimodal feature selection and fusion for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0031320321005392)/Project\n163 | 2022 |MIA_DPD| Neurocomputing| Multi-modal interactive attention and dual progressive decoding network for RGB-D/T salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0925231222002971)/[Project](https://github.com/Liangyh18/MIA_DPD)\n162 | 2022 |--| PR| Discriminative unimodal feature selection and fusion for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0031320321005392)/Project\n161 | 2022 |--| ESA| Aggregate interactive learning for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0957417422001051)/Project\n160 | 2022 |FCMNet| Neurocomputing| FCMNet: Frequency-aware cross-modality attention networks for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0925231222003848)/Project\n159 | 2022 |--| PR| Encoder Deep Interleaved Network with Multi-scale Aggregation for RGB-D Salient Object Detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0031320322001479)/Project\n158 | 2022 |--| ICLR| Promoting Saliency From Depth: Deep Unsupervised RGB-D Saliency Detection  | [Paper](https://openreview.net/pdf?id=BZnnMbt0pW)/Project\n157 | 2022 |FANet| SP:IC| FANet: Feature aggregation network for RGBD saliency detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0923596521002964)/Project\n156 | 2022 |DCMF | IEEE TIP| Learning Discriminative Cross-Modality Features for RGB-D Saliency Detection  | [Paper](https://ieeexplore.ieee.org/abstract/document/9678058)/[Project](https://github.com/fereenwong/DCMF)\n155 | 2022 |DS-Net | IEEE TIP| Boosting RGB-D Saliency Detection by Leveraging Unlabeled RGB Images | [Paper](https://ieeexplore.ieee.org/abstract/document/9673131)/Project\n154 | 2022 |CFIDNet | NCA| CFIDNet: cascaded feature interaction decoder for RGB-D salient object detection | [Paper](https://link.springer.com/article/10.1007/s00521-021-06845-3)/[Project](https://github.com/clelouch/CFIDNet)\n153 | 2022 |-- | IVC| Double cross-modality progressively guided network for RGB-D salient object detection | [Paper](https://www.sciencedirect.com/science/article/pii/S0262885621002560)/Project\n152 | 2022 |-- | PR|Discriminative unimodal feature selection and fusion for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0031320321005392)/Project\n151 | 2022 |BPGNet | IEEE TCSVT |Bi-directional Progressive Guidance Network for RGB-D Salient Object Detection    | [Paper](https://ieeexplore.ieee.org/abstract/document/9686679)/Project\n150 | 2022 |-- | ESA |Aggregate interactive learning for RGB-D salient object detection   | [Paper](https://www.sciencedirect.com/science/article/pii/S0957417422001051)/Project\n149 | 2021 |MobileSal | IEEE TPAMI | MobileSal: Extremely Efficient RGB-D Salient Object Detection     | [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9647954)/[Project](https://github.com/yuhuan-wu/mobilesal)\n148 | 2021 |PGFNet | IEEE TNNLS |RGB-D Point Cloud Registration Based on Salient Object Detection   | [Paper](https://ieeexplore.ieee.org/abstract/document/9350205)/Project\n147 | 2021 |M2RNet | arXiv |M2RNet: Multi-modal and Multi-scale Refined Network for RGB-D Salient Object Detection  | [Paper](https://arxiv.org/abs/2109.07922)/Project\n146 | 2021 |MGSNet | 3DV |Modality-Guided Subnetwork for Salient Object Detection | [Paper](https://ieeexplore.ieee.org/document/9665910)/[Project](https://github.com/Zongwei97/MGSnet)\n145 | 2021 |-- | TVC |Guided residual network for RGB-D salient object detection with efficient depth feature learning | [Paper](https://link.springer.com/article/10.1007/s00371-021-02106-5)/Project\n144 | 2021 |-- | Neurocomputing |A cross-modal edge-guided salient object detection for RGB-D image | [Paper](https://www.sciencedirect.com/science/article/pii/S0925231221007244)/Project\n143 | 2021 |-- | CEI |A deep multimodal feature learning network for RGB-D salient object detection | [Paper](https://www.sciencedirect.com/science/article/pii/S004579062100029X)/Project\n142 | 2021 |PGFNet | IEEE Access |Progressive Guided Fusion Network With Multi-Modal and Multi-Scale Attention for RGB-D Salient Object Detection  | [Paper](https://ieeexplore.ieee.org/abstract/document/9606676)/Project\n141 | 2021 |-- | TVC |Multi-level progressive parallel attention guided salient object detection for RGB-D images  | [Paper](https://link.springer.com/article/10.1007/s00371-020-01821-9)/Project\n140 | 2021 |-- | EAAI | Multi-scale iterative refinement network for RGB-D salient object detection     | [Paper](https://www.sciencedirect.com/science/article/pii/S0952197621003213)/Project\n139 | 2021 |-- | IEEE TMM | Employing Bilinear Fusion and Saliency Prior Information for RGB-D Salient Object Detection     | [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9392336)/Project\n138 | 2021 |-- | PR | Context-aware network for RGB-D salient object detection    | [Paper](https://www.sciencedirect.com/science/article/pii/S0031320320304337)/Project\n137 | 2021 |DSNet | IEEE TIP | Dynamic Selective Network for RGB-D Salient Object Detection    | [Paper](https://ieeexplore.ieee.org/abstract/document/9605221)/Project\n136 | 2021 |ACFNet | arXiv | ACFNet: Adaptively-Cooperative Fusion Network for RGB-D Salient Object Detection   | [Paper](https://arxiv.org/abs/2109.04627)/Project\n135 | 2021 |AFI-Net | CIN | AFI-Net:\u2009Attention-Guided Feature Integration Network for RGBD Saliency Detection  | [Paper](https://www.hindawi.com/journals/cin/2021/8861446/)/Project\n134 | 2021 |JSM | NIPS | Joint Semantic Mining for Weakly Supervised RGB-D Salient Object Detection  | [Paper](https://papers.nips.cc/paper/2021/file/642e92efb79421734881b53e1e1b18b6-Paper.pdf)/[Project](https://github.com/jiwei0921/JSM)\n133 | 2021 |PGFNet | IEEE Access | Progressive Guided Fusion Network With Multi-Modal and Multi-Scale Attention for RGB-D Salient Object Detection  | [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9606676)/Project\n132 | 2021 |-- | PR | Discriminative unimodal feature selection and fusion for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0031320321005392)/Project\n131 | 2021 |UTA | IEEE TIP| RGB-D Salient Object Detection With Ubiquitous Target Awareness  | [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9529069)/Project\n130 | 2021 |SP-Net  | ICCV| Specificity-preserving RGB-D Saliency Detection  | [Paper](https://arxiv.org/pdf/2108.08162.pdf)/[Project](https://github.com/taozh2017/SPNet)\n129 | 2021 |--      | ICCV| RGB-D Saliency Detection via Cascaded Mutual Information Minimization  | Paper/[Project](https://github.com/JingZhang617/cascaded_rgbd_sod)\n128 | 2021 |TMFNet   | IEEE TETCI| TMFNet: Three-Input Multilevel Fusion Network for Detecting Salient Objects in RGB-D Images  | [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9512550)/Project\n127 | 2021 |--   | SP| Multiscale multilevel context and multimodal fusion for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0165168420303091)/Project\n126 | 2021 |--   | IEEE TMM |Employing Bilinear Fusion and Saliency Prior Information for RGB-D Salient Object Detection  | [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9392336)/Project\n125 | 2021 |CAN  | PR        |Context-aware network for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0031320320304337)/Project\n124 | 2021 |--   | Neurocomputing |Rethinking feature aggregation for deep RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0925231220316921)/Project\n123 | 2021 |--   | Neurocomputing |Circular Complement Network for RGB-D Salient Object Detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0925231221005944)/Project\n122 | 2021 | LSF   | IJCV          | CNN-based RGB-D Salient Object Detection: Learn, Select and Fuse | [Paper](https://arxiv.org/pdf/1909.09309.pdf)/Project\n121 | 2021 |TriTransNet  | ACM MM | TriTransNet RGB-D Salient Object Detection with a Triplet Transformer Embedding Network  | Paper/[Project](https://github.com/liuzywen/TriTransNet)\n120 | 2021 |CDINet  | ACM MM | Cross-modality Discrepant Interaction Network for RGB-D Salient Object Detection  | Paper/Project\n119 | 2021 |DFM-Net  | ACM MM | Depth Quality-Inspired Feature Manipulation for Efficient RGB-D Salient Object Detection  | [Paper](https://arxiv.org/pdf/2107.01779.pdf)/[Project](https://github.com/zwbx/DFM-Net)\n118 | 2021 |VST  | arXiv | Visual Saliency Transformer   | [Paper](https://arxiv.org/pdf/2104.12099.pdf)/Project\n117 | 2021 |--  | arXiv | Progressive Multi-scale Fusion Network for RGB-D Salient Object Detection  | [Paper](https://arxiv.org/pdf/2106.03941.pdf)/Project\n116 | 2021 |--  | arXiv | Dynamic Knowledge Distillation with A Single Stream Structure for RGB-D Salient Object Detection  | [Paper](https://arxiv.org/pdf/2106.09517.pdf)/Project\n115 | 2021 |MRINet  | SPL | MRINet: Multilevel Reverse-Context Interactive-Fusion Network for Detecting Salient Objects in RGB-D Images  | [Paper](https://ieeexplore.ieee.org/abstract/document/9466383)/Project\n114 | 2021 |DSA^2F  | CVPR | Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion  | [Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_Deep_RGB-D_Saliency_Detection_With_Depth-Sensitive_Attention_and_Automatic_Multi-Modal_CVPR_2021_paper.pdf)/[Project](https://github.com/sunpeng1996/DSA2F)\n113 | 2021 |DCF  | CVPR | Calibrated RGB-D Salient Object Detection  | [Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Ji_Calibrated_RGB-D_Salient_Object_Detection_CVPR_2021_paper.pdf)/[Project](https://github.com/jiwei0921/DCF)\n112 | 2021 |BTS-Net  | ICME| BTS-Net: Bi-directional Transfer-and-Selection Network for RGB-D Salient Object Detection  | Paper/[Project](https://github.com/zwbx/BTS-Net)\n111 | 2021 |CDNet  | IEEE TIP| CDNet: Complementary Depth Network for RGB-D Salient Object Detection  | [Paper](https://ieeexplore.ieee.org/abstract/document/9366409)/[Project](https://github.com/blanclist/CDNet)\n110 | 2021 |CCAFNet  | IEEE TMM | CCAFNet: Crossflow and Cross-scale Adaptive Fusion Network for Detecting Salient Objects in RGB-D Images  | [Paper](https://ieeexplore.ieee.org/abstract/document/9424966)/Project\n109 | 2021 |ShuffeNet  | arXiv | Middle-level Fusion for Lightweight RGB-D Salient Object Detection  | [Paper](https://arxiv.org/pdf/2104.11543.pdf)/Project\n108 | 2021 |--  | The Visual Computer | Guided residual network for RGB-D salient object detection with efficient depth feature learning  | [Paper](https://link.springer.com/article/10.1007/s00371-021-02106-5)/Project\n107 | 2021 |--  | IEEE TIP | Hierarchical Alternate Interaction Network for RGB-D Salient Object Detection  | [Paper](https://ieeexplore.ieee.org/abstract/document/9371407)/[Project](https://github.com/MathLee/HAINet)\n106 | 2021 |AFLNet  | SP: IC  | AFLNet: Adversarial focal loss network for RGB-D salient object detection  | [Paper](https://www.sciencedirect.com/science/article/pii/S0923596521000497)/Project\n105 | 2021 |--   | arXiv   | Self-Supervised Representation Learning for RGB-D Salient Object Detection | [Paper](https://arxiv.org/pdf/2101.12482.pdf)/Project\n104 | 2021 |--   | Sensors | Saliency Detection with Bilateral Absorbing Markov Chain Guided by Depth Information | [Paper](https://www.mdpi.com/1424-8220/21/3/838)/Project\n103 | 2021 |BPA-Net    | DSP   | Boundary-aware pyramid attention network for detecting salient objects in RGB-D images | [Paper](https://www.sciencedirect.com/science/article/pii/S1051200421000142)/Project\n102 | 2021 |MobileSal    | arXiv   | MobileSal: Extremely Efficient RGB-D Salient Object Detection | [Paper](https://arxiv.org/abs/2012.13095)/Project\n101 | 2021 |--    | The Visual Computer    | A robust RGBD saliency method with improved probabilistic contrast and the global reference surface | [Paper](https://link.springer.com/article/10.1007/s00371-020-02050-w)/Project\n100 | 2021 |RD3D    | AAAI   | RGB-D Salient Object Detection via 3D Convolutional Neural | Paper/[Project](https://github.com/PPOLYpubki/RD3D)\n99 | 2021 |WGI-Net    | CVM   | WGI-Net: A weighted group integration network for RGB-D salient object detection | [Paper](https://link.springer.com/article/10.1007/s41095-020-0200-x)/Project\n98 | 2021 |JL-DCF   | IEEE TPAMI  | Siamese Network for RGB-D Salient Object Detection and Beyond | [Paper](https://arxiv.org/pdf/2008.12134.pdf)/[Project](https://github.com/kerenfu/JLDCF)\n97 | 2020 |CRACE    | arXiv   | A Unified Structure for Efficient RGB and RGB-D Salient Object Detection | [Paper](https://arxiv.org/pdf/2012.00437.pdf)/Project\n96 | 2020 |EF-Net   | PR      | EF-Net: A novel enhancement and fusion network for RGB-D saliency detection | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0031320320305434)/Project\n95 | 2020 |SMAC   | arXiv (CVPR extension)   | Learning Selective Mutual Attention and Contrast for RGB-D Saliency Detection| [Paper](https://arxiv.org/pdf/2010.05537.pdf)/[Project](https://github.com/nnizhang/SMAC)\n94 | 2020 |DCMF   | IEEE TIP   | RGBD Salient Object Detection via Disentangled Cross-Modal Fusion| [Paper](https://ieeexplore.ieee.org/document/9165931)/[Project](https://github.com/haochen593/Disen_Fuse_TIP2020)\n93 | 2020 |MMNet  | ACM MM  | MMNet: Multi-Stage and Multi-Scale Fusion Network for RGB-D Salient Object Detection | [Paper](https://dl.acm.org/doi/pdf/10.1145/3394171.3413523)/Project\n92 | 2020 |DASNet  | ACM MM        | Is depth really necessary for salient object detection? | [Paper](https://arxiv.org/pdf/2006.00269.pdf)/[Project](http://cvteam.net/projects/2020/DASNet/)\n91 | 2020 |FRDT    | ACM MM        | Feature Reintegration over Differential Treatment: A Top-down and Adaptive Fusion Network for RGB-D Salient Object Detection | [Paper](https://dl.acm.org/doi/pdf/10.1145/3394171.3413969)/[Project](https://github.com/jack-admiral/ACM-MM-FRDT)\n90 | 2020 | HANet   | Appl. Sci.       | Hybrid\u2010Attention Network for RGB\u2010D Salient Object Detection | Paper/Project\n89 | 2020 | DQSD   | IEEE TIP        | Depth Quality Aware Salient Object Detection | [Paper](https://arxiv.org/pdf/2008.04159.pdf)/[Project](https://github.com/qdu1995/DQSD)\n88 | 2020 | DQAM   | arXiv         | Knowing Depth Quality In Advance: A Depth Quality Assessment Method For RGB-D Salient Object Detection | [Paper](https://arxiv.org/pdf/2008.04157.pdf)/Project\n87 | 2020 | DRLF   | IEEE TIP      | Data-Level Recombination and Lightweight Fusion Scheme for RGB-D Salient Object Detection | [Paper](http://probb268dca.pic5.ysjianzhan.cn/upload/TIP20_WXH_q02i.pdf)/[Project](https://github.com/XueHaoWang-Beijing/DRLF)\n86 | 2020 | MCINet | arXiv         | MCINet: Multi-level Cross-modal Interaction Network for RGB-D Salient Object Detection | [Paper](https://arxiv.org/pdf/2007.14352.pdf)/Project\n85 | 2020 | PGAR   | ECCV          | Progressively Guided Alternate Refinement Network for RGB-D Salient Object Detection | [Paper](https://arxiv.org/pdf/2008.07064.pdf)/[Project](https://github.com/ShuhanChen/PGAR_ECCV20)\n84 | 2020 | ATSA   | ECCV          | Asymmetric Two-Stream Architecture for Accurate RGB-D Saliency Detection\t| [Paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730375.pdf)/[Project](https://github.com/sxfduter/ATSA)\n83 | 2020 | BBS-Net| ECCV          | BBS-Net: RGB-D Salient Object Detection with a Bifurcated Backbone Strategy Network | [Paper](https://arxiv.org/pdf/2007.02713.pdf)/[Project](https://github.com/zyjwuyan/BBS-Net)\n82 | 2020 | CoNet  | ECCV          | Accurate RGB-D Salient Object Detection via Collaborative Learning | [Paper](https://arxiv.org/pdf/2007.11782.pdf)/[Project](https://github.com/jiwei0921/CoNet)\n81 | 2020 | DANet  | ECCV          | A Single Stream Network for Robust and Real-time RGB-D Salient Object Detection | [Paper](https://arxiv.org/pdf/2007.06811.pdf)/[Project](https://github.com/Xiaoqi-Zhao-DLUT/DANet-RGBD-Saliency)\n80 | 2020 | CMMS   | ECCV          | RGB-D salient object detection with cross-modality modulation and selection | [Paper](https://arxiv.org/pdf/2007.07051.pdf)/[Project](https://github.com/Li-Chongyi/cmMS-ECCV20)\n79 | 2020 | CAS-GNN| ECCV          | Cascade graph neural networks for RGB-D salient object detection | [Paper](https://arxiv.org/pdf/2008.03087.pdf)/[Project](https://github.com/LA30/Cas-Gnn)\n78 | 2020 | HDFNet | ECCV          | Hierarchical Dynamic Filtering Network for RGB-D Salient Object Detection | [Paper](https://arxiv.org/pdf/2007.06227.pdf)/[Project](https://github.com/lartpang/HDFNet)\n77 | 2020 | CMWNet | ECCV          | Cross-modal weighting network for RGB-D salient object detection | [Paper](https://arxiv.org/pdf/2007.04901.pdf)/[Project](https://github.com/MathLee/CMWNet)\n76 | 2020 | UC-Net | CVPR          | UC-Net: Uncertainty Inspired RGB-D Saliency Detection via Conditional Variational Autoencoders | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_UC-Net_Uncertainty_Inspired_RGB-D_Saliency_Detection_via_Conditional_Variational_Autoencoders_CVPR_2020_paper.pdf)/[Project](https://github.com/JingZhang617/UCNet)\n75 | 2020 | S2MA   | CVPR          | Learning selective self-mutual attention for RGB-D saliency detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Learning_Selective_Self-Mutual_Attention_for_RGB-D_Saliency_Detection_CVPR_2020_paper.pdf)/[Project](https://github.com/nnizhang/S2MA)\n74 | 2020 | SSF    | CVPR          | Select, supplement and focus for RGB-D saliency detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Select_Supplement_and_Focus_for_RGB-D_Saliency_Detection_CVPR_2020_paper.pdf)/[Project](https://github.com/OIPLab-DUT/CVPR_SSF-RGBD)\n73 | 2020 | A2dele | CVPR          | A2dele: Adaptive and Attentive Depth Distiller for Efficient RGB-D Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Piao_A2dele_Adaptive_and_Attentive_Depth_Distiller_for_Efficient_RGB-D_Salient_CVPR_2020_paper.pdf)/[Project](https://github.com/OIPLab-DUT/CVPR2020-A2dele)\n72 | 2020 | JL-DCF | CVPR          | JL-DCF: Joint learning and densely-cooperative fusion framework for RGB-D salient object detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Fu_JL-DCF_Joint_Learning_and_Densely-Cooperative_Fusion_Framework_for_RGB-D_Salient_CVPR_2020_paper.pdf)/[Project](https://github.com/kerenfu/JLDCF)\n71 | 2020 | D3Net  |IEEE TNNLS     | Rethinking RGB-D salient object detection: models, datasets, and large-scale benchmarks | [Paper](https://arxiv.org/pdf/1907.06781.pdf)/[Project](https://github.com/DengPingFan/D3NetBenchmark)\n70 | 2020 | RGBS   |MTAP           | Salient object detection for RGB-D images by generative adversarial network | [Paper](https://link.springer.com/article/10.1007/s11042-020-09188-8)/Project\n69 | 2020 | GFNet   |IEEE SPL       | GFNet: Gate fusion network with res2net for detecting salient objects in RGB-D images | [Paper](https://ieeexplore.ieee.org/document/9090350)/Project\n68 | 2020 | SDF   | IEEE TIP       | Improved saliency detection in RGB-D images using two-phase depth estimation and selective deep fusion | [Paper](https://ieeexplore.ieee.org/document/8976428)/Project\n67 | 2020 | ICNet | IEEE TIP       | ICNet: Information Conversion Network for RGB-D Based Salient Object Detection| [Paper](https://ieeexplore.ieee.org/document/9024241)/[Project](https://github.com/MathLee/ICNet-for-RGBD-SOD)\n66 | 2020 |Triple-Net | IEEE SPL   | Triple-complementary network for RGB-D salient object detection| [Paper](https://ieeexplore.ieee.org/document/9076277)/Project\n65 | 2020 |ASIF-Net | IEEE TCYB    | ASIF-Net: Attention steered interweave fusion network for RGB-D salient object detection| [Paper](https://ieeexplore.ieee.org/document/8998588)/[Project](https://github.com/Li-Chongyi/ASIF-Net)\n64 | 2020 |BiANet | IEEE TIP       | Bilateral Attention Network for RGB-D Salient Object Detection | [Paper](https://arxiv.org/pdf/2004.14582.pdf)/Project\n63 | 2020 |PGHF   | IEEE Access    | Multi-modal weights sharing and hierarchical feature fusion for rgbd salient object detection | [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8981965)/Project\n62 | 2020 |cmSalGAN | IEEE TMM     | cmSalGAN: RGB-D Salient Object Detection with Cross-View Generative Adversarial Networks | [Paper](https://arxiv.org/pdf/1912.10280.pdf)/Project\n61 | 2020 | CoCNN | PR             | CoCNN: RGB-D deep fusion for stereoscopic salient object detection | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0031320320301321)/Project\n60 | 2020 | GFNet | Neurocomputing | A cross-modal adaptive gated fusion generative adversarial network for RGB-D salient object detection | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231220300904)/Project\n59 | 2020 | AttNet| IVC            | Attention-guided RGBD saliency detection using appearance information | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0262885620300202)/Project\n58 | 2020 | SSDP   |arXiv          | Synergistic saliency and depth prediction for RGB-D saliency detection | [Paper](https://arxiv.org/pdf/2007.01711.pdf)/Project\n57 | 2020 |DPANet | arXiv          | DPANet: Depth Potentiality-Aware Gated Attention Network for RGB-D Salient Object Detection | [Paper](https://arxiv.org/pdf/2003.08608.pdf)/Project\n56 | 2019 | DSD   | JVCIR          | Depth-aware saliency detection using convolutional neural networks | [Paper](https://www.sciencedirect.com/science/article/pii/S104732031930118X)/Project\n55 | 2019 | DMRA  | ICCV           | Depth-induced Multi-scale Recurrent Attention Network for Saliency Detection | [Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Piao_Depth-Induced_Multi-Scale_Recurrent_Attention_Network_for_Saliency_Detection_ICCV_2019_paper.pdf)/[Project](https://github.com/jiwei0921/DMRA)\n54 | 2019 | CPFP  | CVPR           | Contrast Prior and Fluid Pyramid Integration for RGBD Salient Object Detection | [Paper](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhao_Contrast_Prior_and_Fluid_Pyramid_Integration_for_RGBD_Salient_Object_CVPR_2019_paper.pdf)/[Project](https://github.com/JXingZhao/ContrastPrior)\n53 | 2019 | EPM   | IEEE Access    | Co-saliency detection for rgbd images based on effective propagation mechanism | [Paper](https://ieeexplore.ieee.org/document/8849990)/Project\n52 | 2019 | AFNet | IEEE Access    | Adaptive Fusion for RGB-D Salient Object Detection | [Paper](https://arxiv.org/pdf/1901.01369.pdf)/[Project](https://github.com/Lucia-Ningning/Adaptive_Fusion_RGBD_Saliency_Detection)\n51 | 2019 | DGT   | IEEE TCYB      | Going from RGB to RGBD saliency: A depth-guided transformation model | [Paper](https://www.researchgate.net/publication/335360400_Going_From_RGB_to_RGBD_Saliency_A_Depth-Guided_Transformation_Model)/[Project](https://rmcong.github.io/proj_RGBD_sal_DTM_tcyb.html)\n50 | 2019 | DCMF  | IEEE TCYB      | Discriminative cross-modal transfer learning and densely cross-level feedback fusion for RGB-D salient object detection | [Paper](https://ieeexplore.ieee.org/document/8820129)/Project\n49 | 2019 | TANet | IEEE TIP       | Three-stream attention-aware network for RGB-D salient object detection | [Paper](https://ieeexplore.ieee.org/document/8603756)/Project\n48 | 2019 | MMCI  | PR             | Multi-modal fusion network with multi-scale multi-path and cross-modal interactions| [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0031320318303054)/Project\n47 | 2019 | PDNet | ICME           | Prior-model guided depth-enhanced network for salient object detection| [Paper](https://arxiv.org/pdf/1803.08636.pdf)/[Project](https://github.com/cai199626/PDNet)\n46 | 2019 | CAFM  | IEEE TSMC      | Global and Local-Contrast Guides Content-Aware Fusion for RGB-D Saliency Prediction | [Paper](https://ieeexplore.ieee.org/document/8941002)/Project\n45 | 2019 | DIL   | MTAP           | Salient object segmentation based on depth-aware image layering | [Paper](https://link.springer.com/article/10.1007/s11042-018-6736-4)/Project\n44 | 2019 | TSRN  | ICIP           | Two-stream refinement network for RGB-D saliency detection | [Paper](https://ieeexplore.ieee.org/document/8803653)/Project\n43 | 2019 | MLF   | SPL            | RGB-D salient object detection by a CNN with multiple layers fusion | [Paper](https://ieeexplore.ieee.org/document/8638984)/Project\n42 | 2019 | SSRC  | Neurocomputing | Salient object detection for RGB-D image by single stream recurrent convolution neural network | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231219309403)/Project\n41 | 2018 | CDB   | Neurocomputing | Stereoscopic saliency model using contrast and depth-guided-background prior | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231217317034)/Project\n40 | 2018 | ACCF  | IROS           | Attention-Aware Cross-Modal Cross-Level Fusion Network for RGB-D Salient Object Detection | [Paper](https://ieeexplore.ieee.org/document/8594373)/Project\n39 | 2018 | SCDL  | ICDSP          | Rgbd salient object detection using spatially coherent deep learning framework | [Paper](https://ieeexplore.ieee.org/document/8631584)/Project\n38 | 2018 | PCF   | CVPR           | Progressively complementarityaware fusion network for RGB-D salient object detection | [Paper](https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Progressively_Complementarity-Aware_Fusion_CVPR_2018_paper.pdf)/[Project](https://github.com/haochen593/PCA-Fuse_RGBD_CVPR18)\n37 | 2018 | CTMF  | IEEE TCYB      | CNNs-based RGB-D saliency detection via cross-view transfer and multiview fusion | [Paper](https://ieeexplore.ieee.org/document/8091125)/[Project](https://github.com/haochen593/CTMF)\n36 | 2018 | ICS   | IEEE TIP       | Co-saliency detection for RGBD images based on multi-constraint feature matching and cross label propagation | [Paper](https://arxiv.org/pdf/1710.05172.pdf)/Project\n35 | 2018 | HSCS  | IEEE TMM       | HSCS: Hierarchical sparsity based co-saliencydetection for RGBD images | [Paper](https://arxiv.org/pdf/1811.06679.pdf)/[Project](https://github.com/rmcong/Results-for-2018TMM-HSCS)\n34 | 2017 | ISC   | SIVP           | An integration of bottom-up and top-down salient cueson rgb-d data: saliency from objectness versus non-objectness | [Paper](https://arxiv.org/pdf/1807.01532.pdf)/Project\n33 | 2017 | MCLP  | IEEE TCYB      | An iterative co-saliency framework for RGBD images | [Paper](https://arxiv.org/pdf/1711.01371.pdf)/Project\n32 | 2017 | DF    | IEEE TIP       | RGBD Salient Object Detection via Deep Fusion | [Paper](https://arxiv.org/pdf/1607.03333.pdf)/[Project](https://pan.baidu.com/s/1Y-PqAjuH9xREBjfl7H45HA)\n31 | 2017 | MDSF  | IEEE TIP       | Depth-Aware Salient Object Detection and Segmentation via Multiscale Discriminative Saliency Fusion and Bootstrap Learning | [Paper](https://ieeexplore.ieee.org/document/7938352)/[Project](https://github.com/ivpshu/Depth-aware-salient-object-detection-and-segmentation-via-multiscale-discriminative-saliency-fusion-)\n30 | 2017 | MFF   | IEEE SPL       | RGB-D saliency object detection via minimum barrier distance transformand saliency fusion | [Paper](https://wanganzhi.github.io/papers/SPL17.pdf)/Project\n29 | 2017 | TPF   | ICCVW          | A Three-Pathway Psychobiological Framework of Salient Object Detection Using Stereoscopic Technology | [Paper](https://ieeexplore.ieee.org/document/8265566)/Project\n28 | 2017 | CDCP  | ICCVW          | An innovative salient object detection using center-dark channel prior | [Paper](https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w22/Zhu_An_Innovative_Salient_ICCV_2017_paper.pdf)/[Project](https://github.com/ChunbiaoZhu/ACVR2017)\n27 | 2017 | BED   | ICCVW          | Learning RGB-D Salient Object Detection using background enclosure, depth contrast, and top-down features | [Paper](https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w40/Shigematsu_Learning_RGB-D_Salient_ICCV_2017_paper.pdf)/[Project](https://github.com/sshige/rgbd-saliency)\n26 | 2017 | MFLN  | ICCVS          | RGB-D Saliency Detection by Multi-stream Late Fusion Network | [Paper](https://link.springer.com/chapter/10.1007/978-3-319-68345-4_41)/Project\n25 | 2017 | M3Net | IROS           | M3Net: Multi-scale multi-path multi-modal fusion network and example application to RGB-D salient object detection | [Paper](https://ieeexplore.ieee.org/abstract/document/8206370)/Project\n24 | 2017 | HOSO  | DICTA          | HOSO: Histogram of Surface Orientation for RGB-D Salient Object Detection | [Paper](https://ieeexplore.ieee.org/document/8227440)/Project\n23 | 2016 | GM    | ACCV           | Visual Saliency detection for RGB-D images with generative mode | [Paper](https://link.springer.com/chapter/10.1007/978-3-319-54193-8_2)/Project\n22 | 2016 | DSF   | ICASSP         | Depth-aware saliency detection using discriminative saliency fusion | [Paper](https://ieeexplore.ieee.org/document/7471952)/Project\n21 | 2016 | DCI   | ICASSP         | Saliency analysis based on depth contrast increased | [Paper](http://sites.nlsde.buaa.edu.cn/~shenghao/Download/publications/2016/9.Saliency%20analysis%20based%20on%20depth%20contrast%20increased.pdf)/Project\n20 | 2016 | BF    | ICPR           | RGB-D saliency detection under Bayesian framework | [Paper](https://ieeexplore.ieee.org/document/7899911)/Project\n19 | 2016 | DCMC  | IEEE SPL       | Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion  | [Paper](https://ieeexplore.ieee.org/document/7457641)/[Project](https://github.com/rmcong/Code-for-DCMC-method)\n18 | 2016 | SE    | ICME           | Salient object detection for RGB-D image via saliency evolution | [Paper](https://ieeexplore.ieee.org/document/7552907)/Project\n17 | 2016 | LBE   | CVPR           | Local Background Enclosure for RGB-D Salient Object Detection| [Paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S10-09.pdf)/[Project](http://users.cecs.anu.edu.au/~u4673113/lbe.html)\n16 | 2016 | PRC   | IEEE Access    | Improving RGBD Saliency Detection Using Progressive Region Classification and Saliency Fusion| [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7762806)/Project\n15 | 2015 | SF    | CAC            | Selective features for RGB-D saliency | [Paper](https://ieeexplore.ieee.org/document/7382554)/Project\n14 | 2015 | MGMR  | ICIP           | RGB-D saliency detection via mutual guided manifold ranking | [Paper](https://ieeexplore.ieee.org/document/7350882)/Project\n13 | 2015 | SRD   | ICRA           | Salient Regions Detection for Indoor Robots using RGB-D Data | [Paper](http://www.cogsys.cs.uni-tuebingen.de/publikationen/2015/Jiang_ICRA15.pdf)/Project\n12 | 2015 | DIC   | TVC            | Depth incorporating with color improves salient object detection | [Paper](https://link.springer.com/article/10.1007/s00371-014-1059-6)/Project\n11 | 2015 | SFP   | ICIMCS         | Salient object detection in RGB-D image based on saliency fusion and propagation | [Paper](https://dl.acm.org/doi/10.1145/2808492.2808551)/Project\n10 | 2015 | GP    | CVPRW          | Exploiting global priors for RGB-D saliency detection | [Paper](https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W14/papers/Ren_Exploiting_Global_Priors_2015_CVPR_paper.pdf)/[Project](https://github.com/JianqiangRen/Global_Priors_RGBD_Saliency_Detection)\n09 | 2014 | ACSD  | ICIP           | Depth saliency based on anisotropic center-surround difference | [Paper](https://projet.liris.cnrs.fr/imagine/pub/proceedings/ICIP-2014/Papers/1569913831.pdf)/[Project](https://github.com/HzFu/DES_code)\n08 | 2014 | DESM  | ICIMCS         | Depth Enhanced Saliency Detection Method | [Paper](http://dpfan.net/wp-content/uploads/DES_dataset_ICIMCS14.pdf)/Project\n07 | 2014 | LHM   | ECCV           | RGBD Salient Object Detection: A Benchmark and Algorithms | [Paper](http://dpfan.net/wp-content/uploads/NLPR_dataset_ECCV14.pdf)/[Project](https://sites.google.com/site/rgbdsaliency/code)\n06 | 2014 | SRDS  | ICDSP          | Salient region detection for stereoscopic images | [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6900706)/Project \n05 | 2013 | SOS   | Neurocomputing | Depth really Matters: Improving Visual Salient Region Detection with Depth | [Paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231213002981)/Project \n04 | 2013 | RC    | BMVC           | Depth really Matters: Improving Visual Salient Region Detection with Depth | [Paper](http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/ConferencePapers/2013/cv_deepth-really.pdf)/Project \n03 | 2013 | LS    | BMVC           | An In Depth View of Saliency                                     | [Paper](http://www.cs.utah.edu/~thermans/papers/ciptadi-bmvc2013.pdf)/Project \n02 | 2012 | RCM   | ICCSE          | Depth combined saliency detection based on region contrast model | [Paper](https://ieeexplore.ieee.org/document/6295184)/Project \n01 | 2012 | DM    | ECCV           | Depth matters: Influence of depth cues on visual saliency        | [Paper](https://link.springer.com/content/pdf/10.1007/978-3-642-33709-3_8.pdf)/Project  \n"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9706694857739767,
      "result": {
        "original_header": "LF Datasets: <a id=\"LFdatasets\" class=\"anchor\" href=\"#LFdatasets\" aria-hidden=\"true\"><span class=\"octicon octicon-link\"></span></a>",
        "type": "Text_excerpt",
        "value": "**No.** |**Dataset** | **Year** | **Pub.** |**Size** | **Description** | **Download**\n:-: | :-: | :-: | :-  | :-  | :-:| :-: \n1   | [**LFSD**](https://sites.duke.edu/nianyi/files/2020/06/Li_Saliency_Detection_on_2014_CVPR_paper.pdf)   |2014 |CVPR   | 100  | It contains 60 indoor and 40 outdoor scenes, and most scenes consist of only one salient object | [link](https://sites.duke.edu/nianyi/publication/saliency-detection-on-light-field/)\n2   | [**HFUT**](http://www.linliang.net/wp-content/uploads/2017/07/ACMTOM_Saliency.pdf)                 |2017 |ACM TOMM   | 255  | Most scenes contain multipleobjects that appear within different locations and scales under complex background clutter | [link](https://github.com/pencilzhang/HFUT-Lytro-dataset)\n3   | [**HFUT**](http://www.linliang.net/wp-content/uploads/2017/07/ACMTOM_Saliency.pdf)                 |2017 |ACM TOMM   | 255  | Most scenes contain multipleobjects that appear within different locations and scales under complex background clutter | [link](https://github.com/pencilzhang/HFUT-Lytro-dataset)\n4   | [**DUTLF-FS**](https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Deep_Learning_for_Light_Field_Saliency_Detection_ICCV_2019_paper.pdf)             |2019 |ICCV       | 1465 | It contains several challenges, including lower contrast between salient objects and cluttered background, multiple disconnected salient objects, and dark or strong light conditions | [link](https://github.com/OIPLab-DUT/ICCV2019_Deeplightfield_Saliency)\n5   | [**DUTLF-MV**](https://www.ijcai.org/Proceedings/2019/0127.pdf)                                    |2019 |IJCAI      | 1580 | Each light field consists of multi-view images and a corresponding ground truth | [link](https://github.com/TuesdayT/IJCAI2019-Deep-Light-Field-Driven-Saliency-Detection-from-A-Single-View)\n6   | [**Lytro Illum**](https://arxiv.org/pdf/1906.08331.pdf)                                            |2020 |IEEE TIP   | 640  | It includes several challenging factors, e.g., inconsistent illumi?nation conditions, and small salient objects existing in a similar or cluttered background | [link](https://github.com/pencilzhang/MAC-light-field-saliency-net) \n"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9700220090169118,
      "result": {
        "original_header": "Overall Evaluation: <a id=\"overallevaluation\" class=\"anchor\" href=\"#overallevaluation\" aria-hidden=\"true\"><span class=\"octicon octicon-link\"></span></a>",
        "type": "Text_excerpt",
        "value": "![alt text](./figures/Fig_overall.jpg)\n*Fig.1: A comprehensive evaluation for 24 representative RGB-D based SOD models, including [LHM](http://dpfan.net/wp-content/uploads/NLPR_dataset_ECCV14.pdf), [ACSD](http://dpfan.net/wp-content/uploads/DES_dataset_ICIMCS14.pdf), [DESM](http://dpfan.net/wp-content/uploads/DES_dataset_ICIMCS14.pdf), \n[GP](https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2015/W14/papers/Ren_Exploiting_Global_Priors_2015_CVPR_paper.pdf),\n[LBE](https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S10-09.pdf), \n[DCMC](https://ieeexplore.ieee.org/document/7457641), \n[SE](https://ieeexplore.ieee.org/document/7552907), \n[CDCP](https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w22/Zhu_An_Innovative_Salient_ICCV_2017_paper.pdf), \n[CDB](https://www.sciencedirect.com/science/article/abs/pii/S0925231217317034), \n[DF](https://arxiv.org/pdf/1607.03333.pdf), \n[PCF](https://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Progressively_Complementarity-Aware_Fusion_CVPR_2018_paper.pdf), \n[CTMF](https://ieeexplore.ieee.org/document/8091125), \n[CPFP](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhao_Contrast_Prior_and_Fluid_Pyramid_Integration_for_RGBD_Salient_Object_CVPR_2019_paper.pdf), \n[TANet](https://ieeexplore.ieee.org/document/8603756), \n[AFNet](https://arxiv.org/pdf/1901.01369.pdf), \n[MMCI](https://www.sciencedirect.com/science/article/abs/pii/S0031320318303054), \n[DMRA](https://openaccess.thecvf.com/content_ICCV_2019/papers/Piao_Depth-Induced_Multi-Scale_Recurrent_Attention_Network_for_Saliency_Detection_ICCV_2019_paper.pdf), \n[D3Net](https://arxiv.org/pdf/1907.06781.pdf), \n[SSF](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Select_Supplement_and_Focus_for_RGB-D_Saliency_Detection_CVPR_2020_paper.pdf), \n[A2dele](https://openaccess.thecvf.com/content_CVPR_2020/papers/Piao_A2dele_Adaptive_and_Attentive_Depth_Distiller_for_Efficient_RGB-D_Salient_CVPR_2020_paper.pdf), \n[S2MA](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Learning_Selective_Self-Mutual_Attention_for_RGB-D_Saliency_Detection_CVPR_2020_paper.pdf), \n[ICNet](https://ieeexplore.ieee.org/document/9024241), \n[JL-DCF](https://openaccess.thecvf.com/content_CVPR_2020/papers/Fu_JL-DCF_Joint_Learning_and_Densely-Cooperative_Fusion_Framework_for_RGB-D_Salient_CVPR_2020_paper.pdf), and \n[UC-Net](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_UC-Net_Uncertainty_Inspired_RGB-D_Saliency_Detection_via_Conditional_Variational_Autoencoders_CVPR_2020_paper.pdf). \nWe obtain the terms of $S_{\\alpha}$ and MAE values for the 24 models on five datasets (i.e., [STERE](http://dpfan.net/wp-content/uploads/STERE_dataset_CVPR12.pdf), \n[NLPR](http://dpfan.net/wp-content/uploads/NLPR_dataset_ECCV14.pdf), \n[LFSD](http://dpfan.net/wp-content/uploads/LFSD_dataset_CVPR14.pdf), \n[DES](http://dpfan.net/wp-content/uploads/DES_dataset_ICIMCS14.pdf), and [SIP](http://dpfan.net/wp-content/uploads/SIP_dataset_TNNLS20.pdf)\n). We report the mean values of $S_{\\alpha}$ and MAE across the five datasets. Note that these better models are shown in the upper left corner (\\ie, with a larger $S_{\\alpha}$ and smaller MAE).* \n1. We have computed values of different evaluation metrics for each image of each models and save as '***.mat', and the results can be downloaded from Google Drive or [Baidu Drive](https://pan.baidu.com/s/1kGRoErBvEzYY3t4pRxUSrA)(extraction code: urra).\n2. Please unzip the downloaded file 'Sal_Det_Results_24_Models.zip' and put it into the file 'results';\n3. To run 'run_overall_evaluation.m' (plot Fig.1 ) \n\n![alt text](./figures/Fig_PR.jpg)\n*Fig.2: PR curves for 24 RGB-D based models on [STERE](http://dpfan.net/wp-content/uploads/STERE_dataset_CVPR12.pdf),\n[NLPR](http://dpfan.net/wp-content/uploads/NLPR_dataset_ECCV14.pdf), \n[LFSD](http://dpfan.net/wp-content/uploads/LFSD_dataset_CVPR14.pdf), \n[DES](http://dpfan.net/wp-content/uploads/DES_dataset_ICIMCS14.pdf), \n[SIP](http://dpfan.net/wp-content/uploads/SIP_dataset_TNNLS20.pdf), \n[GIT](http://www.bmva.org/bmvc/2013/Papers/paper0112/abstract0112.pdf), \n[SSD](http://dpfan.net/wp-content/uploads/SSD_dataset_ICCVW17.pdf), and \n[NJUD](http://dpfan.net/wp-content/uploads/NJU2K_dataset_ICIP14.pdf) datasets.* \n![alt text](./figures/Fig_F_curve.jpg)\n*Fig.3:  F-measures under different thresholds for 24 RGB-D based models on [STERE](http://dpfan.net/wp-content/uploads/STERE_dataset_CVPR12.pdf),\n[NLPR](http://dpfan.net/wp-content/uploads/NLPR_dataset_ECCV14.pdf), \n[LFSD](http://dpfan.net/wp-content/uploads/LFSD_dataset_CVPR14.pdf), \n[DES](http://dpfan.net/wp-content/uploads/DES_dataset_ICIMCS14.pdf), \n[SIP](http://dpfan.net/wp-content/uploads/SIP_dataset_TNNLS20.pdf), \n[GIT](http://www.bmva.org/bmvc/2013/Papers/paper0112/abstract0112.pdf), \n[SSD](http://dpfan.net/wp-content/uploads/SSD_dataset_ICCVW17.pdf), and \n[NJUD](http://dpfan.net/wp-content/uploads/NJU2K_dataset_ICIP14.pdf) datasets.* \n"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/taozh2017/RGBD-SODsurvey/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 31
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/taozh2017/RGBD-SODsurvey/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "taozh2017/RGBD-SODsurvey"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "RGB-D Salient Object Detection: A Survey"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/./figures/Fig0.jpg"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/./figures/Fig_overall.jpg"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/./figures/Fig_PR.jpg"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/./figures/Fig_F_curve.jpg"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8423110048012955,
      "result": {
        "original_header": "Overall Evaluation: <a id=\"overallevaluation\" class=\"anchor\" href=\"#overallevaluation\" aria-hidden=\"true\"><span class=\"octicon octicon-link\"></span></a>",
        "type": "Text_excerpt",
        "value": "======================= **run plot code** ===========================\n1. To run 'run_plot_curves.m' (plot Fig.2 and Fig.3) \n"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/taozh2017/RGBD-SODsurvey/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "benchmark, light-field, light-field-sod, rgb-d, saliency, salient-object-detection, sod, survey"
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "RGBD-SODsurvey"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "taozh2017"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "MATLAB",
        "size": 9018,
        "type": "Programming_language",
        "value": "MATLAB"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1811.06679.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2006.00269.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2106.03941.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1906.08331.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2008.00230"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2108.08162.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2008.00230.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1501.02741.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2012.13095"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2008.03087.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1807.05511.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2007.01711.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2206.09552.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2104.11543.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2007.11782.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2007.04901.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2004.14582.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2302.08052.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1912.10280.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2010.05537.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1710.05172.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2109.04627"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1803.08636.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2007.07051.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2206.03105.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2203.10785"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2008.04157.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2104.12099.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2007.14352.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2109.07922"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2107.01779.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2207.07898.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2008.04159.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2007.06227.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2203.06429"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2007.02713.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2101.12482.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1907.06781.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1904.09146.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2003.08608.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2305.10469.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2008.12134.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2010.04968.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2008.07064.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1807.01532.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2210.02843.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1607.03333.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2007.06811.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1901.01369.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2212.05370.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2106.09517.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1909.09309.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1711.01371.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1604.07090.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1803.03391.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2202.06060"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2012.00437.pdf"
      },
      "source": "https://raw.githubusercontent.com/taozh2017/RGBD-SODsurvey/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "somef_missing_categories": [
    "installation",
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 08:08:59",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 341
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "non-software"
      },
      "technique": "software_type_heuristics"
    }
  ]
}