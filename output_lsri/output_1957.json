{
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/HALFpipe/HALFpipe"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-06-30T13:27:55Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-21T16:12:02Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "ENIGMA HALFpipe is a user-friendly software that facilitates reproducible analysis of fMRI data"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9406118357768907,
      "result": {
        "type": "Text_excerpt",
        "value": "``HALFpipe`` is a user-friendly software that facilitates reproducible\nanalysis of fMRI data, including preprocessing, single-subject, and\ngroup analysis. It provides state-of-the-art preprocessing using\n`fmriprep <https://fmriprep.readthedocs.io/>`__, but removes the\nnecessity to convert data to the `BIDS\n<https://bids-specification.readthedocs.io/en/stable/>`__ format. Common\nresting-state and task-based fMRI features can then be calculated on the\nfly. \n   Subscribe to our `mailing list <https://mailman.charite.de/mailman/listinfo/halfpipe-announcements>`_ to stay up to date with new developments and releases. \n   If you encounter issues, please see the `troubleshooting\n   <#troubleshooting>`__ section of this document. \n   Some sections of this document are marked as outdated. While we are\n   working on updating them, the `paper <https://doi.org/hmts>`__\n   and the `analysis manual\n   <https://docs.google.com/document/d/108-XBIuwtJziRVVdOQv73MRgtK78wfc-NnVu-jSc9oI/edit#heading=h.3y6rt7h7o483>`__\n   should be able to answer most questions. \n*******************\n Table of Contents\n******************* \n   -  `Files <#files>`__\n   -  `Features <#features>`__\n   -  `Models <#models>`__ \n``HALFpipe`` is distributed as a container, meaning that all required\nsoftware comes bundled in a monolithic file, the container. This allows\nfor easy installation on new systems, and makes data analysis more\nreproducible, because software versions are guaranteed to be the same\nfor all users.\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8005869720648086,
      "result": {
        "original_header": "Container platform",
        "type": "Text_excerpt",
        "value": "If not, we recommend using the latest version of\\ `Singularity\n<https://sylabs.io>`__. However, it can be somewhat cumbersome to\ninstall, as it needs to be built from source. \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9794110148014431,
      "result": {
        "original_header": "Files",
        "type": "Text_excerpt",
        "value": "To run preprocessing, at least a T1-weighted structural image and a BOLD\nimage file is required. Preprocessing and data analysis proceeds\nautomatically. However, to be able to run automatically, data files need\nto be input in a way suitable for automation. \nFor this kind of automation, ``HALFpipe`` needs to know the\nrelationships between files, such as which files belong to the same\nsubject. However, even though it would be obvious for a human, a program\ncannot easily assign a file name to a subject, and this will be true as\nlong as there are differences in naming between different researchers or\nlabs. One researcher may name the same file ``subject_01_rest.nii.gz``\nand another ``subject_01/scan_rest.nii.gz``. \nIn ``HALFpipe``, we solve this issue by inputting file names in a\nspecific way. For example, instead of ``subject_01/scan_rest.nii.gz``,\n``HALFpipe`` expects you to input ``{subject}/scan_rest.nii.gz``.\n``HALFpipe`` can then match all files on disk that match this naming\nschema, and extract the subject ID ``subject_01``. Using the extracted\nsubject ID, other files can now be matched to this image. If all input\nfiles are available in BIDS format, then this step can be skipped.\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9584011149564985,
      "result": {
        "original_header": ". `Specify working directory` All intermediate and outputs of",
        "type": "Text_excerpt",
        "value": "   ``HALFpipe`` will be placed in the working directory. Keep in mind to\n   choose a location with sufficient free disk space, as intermediates\n   can be multiple gigabytes in size for each subject.\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8847780785362335,
      "result": {
        "original_header": ". `Is the data available in BIDS format?`",
        "type": "Text_excerpt",
        "value": "      #. ``Specify anatomical/structural data`` ``Specify the path of\n         the T1-weighted image files`` \n      #. ``Specify functional data`` ``Specify the path of the BOLD\n         image files`` \n      #. ``Check repetition time values`` / ``Specify repetition time in\n         seconds`` \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.883646035385097,
      "result": {
        "original_header": ". `Specify field maps?` If the data was imported from a BIDS",
        "type": "Text_excerpt",
        "value": "            #. ``Specify the path of the blip-up blip-down EPI image\n               files`` \n            #. ``Specify the path of the magnitude image files``\n            #. ``Specify the path of the phase/phase difference image\n               files``\n            #. ``Specify echo time difference in seconds`` \n      #. ``Specify effective echo spacing for the functional data in\n         seconds`` \n      #. ``Specify phase encoding direction for the functional data`` \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8459744447897786,
      "result": {
        "original_header": "Features",
        "type": "Text_excerpt",
        "value": "Features are analyses that are carried out on the preprocessed data, in\nother words, first-level analyses.\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9060980787736087,
      "result": {
        "original_header": ". `Specify first-level features?`",
        "type": "Text_excerpt",
        "value": "            -  ``BIDS TSV`` A tab-separated table with named columns\n               ``trial_type`` (condition), ``onset`` and ``duration``.\n               While BIDS supports defining additional columns,\n               ``HALFpipe`` will currently ignore these \n            #. ``Apply a temporal filter to the design matrix?`` A\n               separate temporal filter can be specified for the design\n               matrix. In contrast, the temporal filtering of the input\n               image and any confound regressors added to the design\n               matrix is specified in 10. In general, the two settings\n               should match \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8363127530878416,
      "result": {
        "original_header": ". `Output a preprocessed image?`",
        "type": "Text_excerpt",
        "value": "            #. ``Specify the type of temporal filter`` \n               -  ``Gaussian-weighted``\n               -  ``Frequency-based`` \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9211096684500889,
      "result": {
        "original_header": ". As soon as you finish specifying all your data, features and models",
        "type": "Text_excerpt",
        "value": "   in the user interface, ``HALFpipe`` will now generate everything\n   needed to run on the cluster. For hundreds of subjects, this can take\n   up to a few hours.\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9364491832881291,
      "result": {
        "original_header": "Subject-level features",
        "type": "Text_excerpt",
        "value": "-  |  For task-based, seed-based connectivity and dual regression\n      features, ``HALFpipe`` outputs the statistical maps for the\n      effect, the variance, the degrees of freedom of the variance and\n      the z-statistic. In FSL, the effect and variance are also called\n      ``cope`` and ``varcope``\n   |  ``derivatives/halfpipe/sub-.../func/..._stat-effect_statmap.nii.gz``\n   |  ``derivatives/halfpipe/sub-.../func/..._stat-variance_statmap.nii.gz``\n   |  ``derivatives/halfpipe/sub-.../func/..._stat-dof_statmap.nii.gz``\n   |  ``derivatives/halfpipe/sub-.../func/..._stat-z_statmap.nii.gz``\n   |  The design and contrast matrix used for the final model will be\n      outputted alongside the statistical maps\n   |  ``derivatives/halfpipe/sub-.../func/sub-..._task-..._feature-..._desc-design_matrix.tsv``\n   |  ``derivatives/halfpipe/sub-.../func/sub-..._task-..._feature-..._desc-contrast_matrix.tsv`` \n-  |  ReHo and fALFF are not calculated based on a linear model. As\n      such, only one statistical map of the z-scaled values will be\n      output\n   |  ``derivatives/halfpipe/sub-.../func/..._alff.nii.gz``\n   |  ``derivatives/halfpipe/sub-.../func/..._falff.nii.gz``\n   |  ``derivatives/halfpipe/sub-.../func/..._reho.nii.gz`` \n-  For every feature, a ``.json`` file containing a summary of the\n   preprocessing \n-  |  settings, and a list of the raw data files that were used for the\n      analysis (``RawSources``)\n   |  ``derivatives/halfpipe/sub-.../func/....json`` \n-  |  For every feature, the corresponding brain mask is output beside\n      the statistical maps. Masks do not differ between different\n      features calculated, they are only copied out repeatedly for\n      convenience\n   |  ``derivatives/halfpipe/sub-.../func/...desc-brain_mask.nii.gz`` \n-  |  Atlas-based connectivity outputs the time series and the full\n      covariance and correlation matrices as text files\n   |  ``derivatives/halfpipe/sub-.../func/..._timeseries.txt``\n   |  ``derivatives/halfpipe/sub-.../func/..._desc-covariance_matrix.txt``\n   |  ``derivatives/halfpipe/sub-.../func/..._desc-correlation_matrix.txt``\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9450384200793557,
      "result": {
        "original_header": "Preprocessed images",
        "type": "Text_excerpt",
        "value": "-  |  Just like for features\n   |  ``derivatives/halfpipe/sub-.../func/..._bold.json`` \n-  |  Just like for features\n   |  ``derivatives/halfpipe/sub-.../func/sub-..._task-..._setting-..._desc-brain_mask.nii.gz`` \n-  |  Filtered confounds time series, where all filters that are applied\n      to the BOLD image are applied to the regressors as well. Note that\n      this means that when grand mean scaling is active, confounds time\n      series are also scaled, meaning that values such as ``framewise\n      displacement`` can not be interpreted in terms of their original\n      units anymore.\n   |  ``derivatives/halfpipe/sub-.../func/sub-..._task-..._setting-..._desc-confounds_regressors.tsv``\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9567991255937891,
      "result": {
        "original_header": "Group-level",
        "type": "Text_excerpt",
        "value": "   -  Usually, the last line of these text files contains the error\n      message. Please read this carefully, as may allow you to\n      understand the error \n   -  For example, consider the following error message: ``ValueError:\n      shape (64, 64, 33) for image 1 not compatible with first image\n      shape (64, 64, 34) with axis == None`` This error message may seem\n      cryptic at first. However, looking at the message more closely, it\n      suggests that two input images have different, incompatible\n      dimensions. In this case, ``HALFpipe`` correctly recognized this\n      issue, and there is no need for concern. The images in question\n      will simply be excluded from preprocessing and/or analysis \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8455497492685904,
      "result": {
        "original_header": "Control command line logging",
        "type": "Text_excerpt",
        "value": "By default, only errors and warnings will be output to the command line.\nThis makes it easier to see when something goes wrong, because there is\nless output. However, if you want to be able to inspect what is being\nrun, you can add the ``--verbose`` flag to the end of the command used\nto call ``HALFpipe``. \nVerbose logs are always written to the ``log.txt`` file in the working\ndirectory, so going back and inspecting this log is always possible,\neven if the ``--verbose`` flag was not specified. \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9467411347643445,
      "result": {
        "original_header": "Automatically remove unneeded files",
        "type": "Text_excerpt",
        "value": "``HALFpipe`` saves intermediate files for each pipeline step. This\nspeeds up re-running with different settings, or resuming after a job\nafter it was cancelled. The intermediate file are saved by the `nipype\n<https://nipype.readthedocs.io/>`__ workflow engine, which is what\n``HALFpipe`` uses internally. ``nipype`` saves the intermediate files in\nthe ``nipype`` folder in the working directory. \nIn environments with limited disk capacity, this can be problematic. To\nlimit disk usage, ``HALFpipe`` can delete intermediate files as soon as\nthey are not needed anymore. This behavior is controlled with the\n``--keep`` flag. \nThe default option ``--keep some`` keeps all intermediate files from\nfMRIPrep and MELODIC, which would take the longest to re-run. We believe\nthis is a good tradeoff between disk space and computer time. ``--keep\nall`` turns of all deletion of intermediate files. ``--keep none``\ndeletes as much as possible, meaning that the smallest amount possible\nof disk space will be used.\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9269167256390716,
      "result": {
        "original_header": "Configure nipype",
        "type": "Text_excerpt",
        "value": "``HALFpipe`` chooses sensible defaults for all of these values.\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "original_header": ". Request an interactive job. Refer to your cluster\u2019s documentation for",
        "type": "Text_excerpt",
        "value": "   how to do this\n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "format": "wiki",
        "type": "Url",
        "value": "https://github.com/HALFpipe/HALFpipe/wiki"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "regular_expression"
    }
  ],
  "download": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Download",
        "type": "Text_excerpt",
        "value": "========\n\nThe second step is to download the ``HALFpipe`` to your computer. This\nrequires approximately 5 gigabytes of storage.\n\n.. list-table::\n   :header-rows: 1\n\n   -  -  Container platform\n      -  Version\n      -  Installation\n\n   -  -  Singularity\n      -  3.x\n      -  https://download.fmri.science/singularity/halfpipe-latest.sif\n\n   -  -  Singularity\n      -  2.x\n      -  https://download.fmri.science/singularity/halfpipe-latest.simg\n\n   -  -  Docker\n      -  ..\n      -  ``docker pull halfpipe/halfpipe:latest``\n\n``Singularity`` version ``3.x`` creates a container image file called\n``HALFpipe_{version}.sif`` in the directory where you run the ``pull``\ncommand. For ``Singularity`` version ``2.x`` the file is named\n``halfpipe-halfpipe-master-latest.simg``. Whenever you want to use the\ncontainer, you need pass ``Singularity`` the path to this file.\n\n   **NOTE:** ``Singularity`` may store a copy of the container in its\n   cache directory. The cache directory is located by default in your\n   home directory at ``~/.singularity``. If you need to save disk space\n   in your home directory, you can safely delete the cache directory\n   after downloading, i.e. by running ``rm -rf ~/.singularity``.\n   Alternatively, you could move the cache directory somewhere with more\n   free disk space using a symlink. This way, files will automatically\n   be stored there in the future. For example, if you have a lot of free\n   disk space in ``/mnt/storage``, then you could first run ``mv\n   ~/.singularity /mnt/storage`` to move the cache directory, and then\n   ``ln -s /mnt/storage/.singularity ~/.singularity`` to create the\n   symlink.\n\n``Docker`` will store the container in its storage base directory, so it\ndoes not matter from which directory you run the ``pull`` command.\n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "header_analysis"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/HALFpipe/HALFpipe/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "faq": [
    {
      "confidence": 1,
      "result": {
        "original_header": ". Request an interactive job. Refer to your cluster\u2019s documentation for",
        "type": "Text_excerpt",
        "value": "   how to do this\n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": ". |  In the interactive job, run the `HALFpipe` user interface, but",
        "type": "Text_excerpt",
        "value": "      add the flag ``--use-cluster`` to the end of the command.\n   |  For example, ``singularity run --containall --bind /:/ext\n      halfpipe-halfpipe-latest.sif --use-cluster``\n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "header_analysis"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 12
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "HALFpipe/HALFpipe"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "regular_expression"
    }
  ],
  "has_build_file": [
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/Dockerfile",
      "technique": "file_exploration"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/install-requirements.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/recipes/rmath/build.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "installation": [
    {
      "confidence": 0.9817429471220814,
      "result": {
        "type": "Text_excerpt",
        "value": ".. image:: https://github.com/HALFpipe/HALFpipe/workflows/build/badge.svg\n   :target: https://github.com/HALFpipe/HALFpipe/actions?query=workflow%3A%22build%22 \n.. image:: https://github.com/HALFpipe/HALFpipe/workflows/continuous%20integration/badge.svg\n   :target: https://github.com/HALFpipe/HALFpipe/actions?query=workflow%3A%22continuous+integration%22 \n.. image:: https://codecov.io/gh/HALFpipe/HALFpipe/branch/main/graph/badge.svg\n   :target: https://codecov.io/gh/HALFpipe/HALFpipe \n`HALFpipe` relies on tools from well-established neuroimaging software\npackages, either directly or through our dependencies, including `ANTs\n<https://antspy.readthedocs.io/>`__, `FreeSurfer\n<https://surfer.nmr.mgh.harvard.edu/>`__,  `FSL <http://fsl.fmrib.ox.ac.uk/>`__\nand `nipype <https://nipype.readthedocs.io/>`__. We strongly urge users to\nacknowledge these tools when publishing results obtained with HALFpipe. \n   If you encounter issues, please see the `troubleshooting\n   <#troubleshooting>`__ section of this document. \n.. raw:: html \n   -  `Container platform <#container-platform>`__\n   -  `Download <#download>`__\n   -  `Running <#running>`__ \n-  `Command line flags <#command-line-flags>`__ \n   -  `Control command line logging <#control-command-line-logging>`__\n   -  `Automatically remove unneeded files\n      <#automatically-remove-unneeded-files>`__\n   -  `Adjust nipype <#adjust-nipype>`__\n   -  `Choose which parts to run or to skip\n      <#choose-which-parts-to-run-or-to-skip>`__\n   -  `Working directory <#working-directory>`__\n   -  `Data file system root <#data-file-system-root>`__ \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9984607124260224,
      "result": {
        "original_header": "Container platform",
        "type": "Text_excerpt",
        "value": "The first step is to install one of the supported container platforms.\nIf you\u2019re using a high-performance computing cluster, more often than\nnot `Singularity <https://sylabs.io>`__ will already be available. \nIf not, we recommend using the latest version of\\ `Singularity\n<https://sylabs.io>`__. However, it can be somewhat cumbersome to\ninstall, as it needs to be built from source. \nThe `NeuroDebian <https://neuro.debian.net/>`__ package repository\nprovides an older version of `Singularity\n<https://sylabs.io/guides/2.6/user-guide/>`__ for `some\n<https://neuro.debian.net/pkgs/singularity-container.html>`__ Linux\ndistributions. \nIf you are running ``mac OS``, then you should be able to run the\ncontainer with ``Docker Desktop``. \nIf you are running Windows, you can also try running with ``Docker\nDesktop``, but we have not done any compatibility testing yet, so issues\nmay occur, for example with respect to file systems. \n   -  -  Container platform\n      -  Version\n      -  Installation \n   -  -  Singularity\n      -  3.x\n      -  https://sylabs.io/guides/3.8/user-guide/quick_start.html \n   -  -  Singularity\n      -  2.x\n      -  ``sudo apt install singularity-container`` \n   -  -  Docker\n      -  ..\n      -  See https://docs.docker.com/engine/install/\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8916917986857535,
      "result": {
        "original_header": ". `Is the data available in BIDS format?`",
        "type": "Text_excerpt",
        "value": "      #. ``Specify the path of the BIDS directory`` \n         -  ``Yes`` Loop back to 2\n         -  ``No`` Continue\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.965422569848166,
      "result": {
        "original_header": ". `Specify field maps?` If the data was imported from a BIDS",
        "type": "Text_excerpt",
        "value": "   directory, this step will be omitted. \n            #. ``Specify the path of the magnitude image files``\n            #. ``Specify the path of the phase/phase difference image\n               files``\n            #. ``Specify echo time difference in seconds`` \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8803770570711356,
      "result": {
        "original_header": ". `Specify first-level features?`",
        "type": "Text_excerpt",
        "value": "            #. ``Specify feature name``\n            #. ``Specify images to use``\n            #. ``Specify the event file type`` \n            #. ``Specify the path of the event files`` \n            #. ``Specify contrasts`` \n               #. ``Specify contrast name`` \n               #. ``Specify contrast values`` \n                  -  ``Yes`` Loop back to 1\n                  -  ``No`` Continue \n            #. ``Specify feature name`` \n            #. ``Specify binary seed mask file(s)`` \n            #. ``Specify feature name``\n            #. ``Specify images to use``\n            #. TODO \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8352866422037711,
      "result": {
        "original_header": ". `Add another first-level feature?`",
        "type": "Text_excerpt",
        "value": "   -  ``Yes`` Loop back to 1\n   -  ``No`` Continue\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8988860160590562,
      "result": {
        "original_header": ". `Output a preprocessed image?`",
        "type": "Text_excerpt",
        "value": "      #. ``Specify setting name`` \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9574102373800235,
      "result": {
        "original_header": ". When `HALFpipe` exits, edit the generated submit script",
        "type": "Text_excerpt",
        "value": "   ``submit.slurm.sh`` according to your cluster\u2019s documentation and\n   then run it. This submit script will calculate everything except\n   group statistics.\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9643729236346448,
      "result": {
        "original_header": "Group-level",
        "type": "Text_excerpt",
        "value": "-  If an error occurs, this will be output to the command line and\n   simultaneously to the ``err.txt`` file in the working directory \n   -  In some cases, the cause of the error can be a bug in the\n      ``HALFpipe`` code. Please check that no similar issue has been\n      reported `here on GitHub\n      <https://github.com/HALFpipe/HALFpipe/issues>`__. In this case,\n      please submit an `issue\n      <https://github.com/HALFpipe/HALFpipe/issues/new/choose>`__. \n********************\n Command line flags\n********************\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9346601589349963,
      "result": {
        "original_header": "Control command line logging",
        "type": "Text_excerpt",
        "value": ".. code:: bash \nSpecifying the flag ``--debug`` will print additional, fine-grained\nmessages. It will also automatically start the `Python Debugger\n<https://docs.python.org/3/library/pdb.html>`__ when an error occurs.\nYou should only use ``--debug`` if you know what you\u2019re doing.\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9184993628743423,
      "result": {
        "original_header": "Configure nipype",
        "type": "Text_excerpt",
        "value": "   --nipype-<omp-nthreads|memory-gb|n-procs|run-plugin> \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9694914305050799,
      "result": {
        "original_header": ". The `spec-ui` stage is where you specify things in the user",
        "type": "Text_excerpt",
        "value": "   interface. It creates the ``spec.json`` file that contains all the\n   information needed to run ``HALFpipe``. To only run this stage, use\n   the option ``--only-spec-ui``. To skip this stage, use the option\n   ``--skip-spec-ui``\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9922219488864928,
      "result": {
        "original_header": "Data file system root",
        "type": "Text_excerpt",
        "value": "For questions or support, please submit an `issue\n<https://github.com/HALFpipe/HALFpipe/issues/new/choose>`__ or contact\nus via e-mail at enigma@charite.de.\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8275918421108757,
      "result": {
        "type": "Text_excerpt",
        "value": "   -  `Container platform <#container-platform>`__\n   -  `Download <#download>`__\n   -  `Running <#running>`__ \n-  `Command line flags <#command-line-flags>`__ \n   -  `Control command line logging <#control-command-line-logging>`__\n   -  `Automatically remove unneeded files\n      <#automatically-remove-unneeded-files>`__\n   -  `Adjust nipype <#adjust-nipype>`__\n   -  `Choose which parts to run or to skip\n      <#choose-which-parts-to-run-or-to-skip>`__\n   -  `Working directory <#working-directory>`__\n   -  `Data file system root <#data-file-system-root>`__ \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8111740009521102,
      "result": {
        "original_header": "Group-level",
        "type": "Text_excerpt",
        "value": "********************\n Command line flags\n********************\n \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8079212049053366,
      "result": {
        "original_header": "Configure nipype",
        "type": "Text_excerpt",
        "value": "   --nipype-<omp-nthreads|memory-gb|n-procs|run-plugin> \n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "bids, fmri, meta-analysis, neuroimaging, pipeline, reproducibility"
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "HALFpipe"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "HALFpipe"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 1122125,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 3251,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Dockerfile",
        "size": 3193,
        "type": "Programming_language",
        "value": "Dockerfile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Nix",
        "size": 1813,
        "type": "Programming_language",
        "value": "Nix"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst"
      },
      "technique": "file_exploration"
    }
  ],
  "related_documentation": [
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "https://fmriprep.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "https://bids-specification.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "https://antspy.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "https://nipype.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "regular_expression"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2022-03-22T14:43:43Z",
        "date_published": "2022-03-22T14:44:15Z",
        "description": "Bug fixes\r\n---------\r\n\r\n- Fix issue with BOLD to T1w registration (#230, #238, #239)\r\n- Also detect `exclude.json` files that are placed in the `reports/` folder (#228)\r\n- Improve error message when the FreeSurfer license file is missing (#231)\r\n- Fix a rare calculation error for `fd_mean` and related image quality metrics (#237, #241)\r\n- Fix various warning messages (#247)\r\n- Fix performance issue when collecting inputs for group statistics ()\r\n- Fix a user interface issue where the option `Start over after models` was missing (#259, #260)\r\n- Fix an issue where `sub-` prefixes were not recognized correctly when filtering inputs for group statistics (#264)\r\n- Fix an issue when writing mixed data type columns to the text files in the `reports/` folder (#274)\r\n- Fix warnings for missing quality check information (#276)\r\n- Fix errors when aggregating subjects with different numbers of scans during group statistics (#280)\r\n- Fix error when fMRIPrep skips a BOLD file (#285)\r\n\r\nMaintenance\r\n-----------\r\n\r\n- Bump `indexed_gzip` (#240)\r\n- Bump `nipype` after bug fix (#255)\r\n- Bump `fmriprep` after bug fix (#262)\r\n- Upgrade to Python 3.10, clean up code and add more unit tests (#269)\r\n- Make continuous integration tests run faster (#282, #284)\r\n- Add type checking and linting to continuous integration (#285)\r\n",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.2.2",
        "name": "1.2.2",
        "release_id": 61884752,
        "tag": "1.2.2",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.2.2",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/61884752",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/61884752",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.2.2"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2021-10-05T07:18:52Z",
        "date_published": "2021-10-05T07:20:31Z",
        "description": "Bug fixes\r\n---------\r\n\r\n- Fix issues that occurred after re-scaling ``fd_perc`` to be percent (#217)\r\n- Catch error when ``NaN`` values occur within the linear algebra code (#215)\r\n- Reduce memory usage when running large workflows by only loading the\r\n  chunks that will be necessary for the current process (#216)\r\n- Improve memory usage prediction for cluster submission scripts (#219)\r\n- Update metadata module with better log messages (#220)",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.2.1",
        "name": "1.2.1",
        "release_id": 50792670,
        "tag": "1.2.1",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.2.1",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/50792670",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/50792670",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.2.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2021-09-19T07:44:24Z",
        "date_published": "2021-09-19T07:45:28Z",
        "description": "New features and enhancements\r\n-----------------------------\r\n\r\n-  Improve the assignment of field maps to functional scans, print\r\n   warnings when detecting an incomplete field map or when a complete\r\n   field map is not recognized by fMRIPrep (#115 and #192)\r\n-  Remove conditions that have no events from the task-based model. This\r\n   is important for designs where the conditions depend on subject\r\n   performance (#90)\r\n-  Output additional images during group mode. Voxel-wise descriptive\r\n   statistics (#142), typical subject-level variance (#148)\r\n-  Divide outputs into subfolders to make navigating the files easier\r\n-  Output metadata to sidecar files, including resolution, field-of-view\r\n   and field map type (#154 and #181)\r\n-  Add an option to skip dummy/non-steady-state scans and modify event\r\n   onsets accordingly (#167, #176, #182 and #187)\r\n-  Improve performance during workflow creation (#192)\r\n\r\nBug fixes\r\n---------\r\n\r\n-  Update ``fMRIPrep`` to fix normalization bug (#51)\r\n-  Improve memory usage prediction. Fixes ``BrokenProcessPool`` and\r\n   ``Killed: 137`` errors (#125, #156 and #157)\r\n-  Refactor ``Dockerfile`` to correctly re-build ``matplotlib`` caches\r\n   (#107)\r\n-  Fix assignment of event files to functional scans. Make sure that the\r\n   assignment is consistent between what is shown in the user interface\r\n   and during workflow creation. Add unit tests (#139)\r\n-  Fix crashes for datasets deviating from the ``BIDS`` specification\r\n   and remove misleading warnings for incompatible and hidden files\r\n-  Fix ``AssertionError`` crash when no group model is specified\r\n-  Rephrase user interface for loading ``.mat`` event files. Do not say\r\n   that the time unit (seconds or scans) is missing, which was\r\n   confusing.\r\n-  Fix various crashes when running on a cluster\r\n-  Fix user interface crash when no categorical variables are defined in\r\n   a spreadsheet\r\n-  Fix loading subject-level results during group model. Get rid of\r\n   ``LoadResult`` nodes, instead use a subclass of ``Node`` (#137)\r\n-  Use slower but more robust least-squares solve for group statistics\r\n   (#141)\r\n-  Fix performance issue during ``t2z_convert`` procedure during group\r\n   statistics (#143, #144 and #145)\r\n-  Remove output from heterogeneity group statistics that was causing\r\n   performance issues (#146)\r\n-  Fix confusing ``EOFError`` message on exit by gracefully stopping\r\n   child processes before exit (#130 and #160)\r\n-  Fix running FreeSurfer with ``run_reconall`` option (#87)\r\n-  Add error message when running on an unsupported file system such as\r\n   ``FAT`` (#102)\r\n-  Fix confusing error message when no features are specified (#147)\r\n-  Re-scale ``fd_perc`` output to percent (#186)\r\n-  Reduce user interface memory usage (#191)\r\n-  Fix automated testing hanging on the logging worker (#192)\r\n\r\nMaintenance\r\n-----------\r\n\r\n-  Update Python to version 3.8\r\n-  Update ``templateflow``, ``pybids``, ``nibabel``\r\n-  Pin ``dipy`` version due to incompatibility with ``nipype``\r\n-  Pin ``indexed_gzip`` version due to incompatibility of newer version\r\n   with some files (#85)\r\n-  Add new Singularity container build workflow (#97 and #138)\r\n-  Improve documentation to suggest running Singularity with\r\n   ``--containall`` instead of ``--no-home --cleanenv``\r\n-  Refactor code to use ``defaultdict`` to increase readability\r\n-  Add more type hints\r\n-  Rename main branch from ``master`` to ``main``\r\n-  Add ``pre-commit`` and ``pip-tools`` to better manage dependencies\r\n-  Install as many dependencies as possible via ``conda`` and the rest\r\n   via ``pip`` (#164)\r\n-  Refactor workflow code to allow handling of surface-based functional\r\n   images (#161)\r\n-  In-progress refactor ``model`` package into ``schema`` package. Use\r\n   ``dataclasses`` for better integration with type checkers (#173, #174\r\n   and #178)\r\n\r\n",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.2.0",
        "name": "1.2.0",
        "release_id": 49809057,
        "tag": "1.2.0",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.2.0",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/49809057",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/49809057",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.2.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2021-09-06T11:55:35Z",
        "date_published": "2021-09-05T16:44:19Z",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.2.0rc2",
        "release_id": 49034052,
        "tag": "1.2.0rc2",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.2.0rc2",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/49034052",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/49034052",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.2.0rc2"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2021-08-06T12:30:50Z",
        "date_published": "2021-08-06T12:31:12Z",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.2.0rc1",
        "release_id": 47408572,
        "tag": "1.2.0rc1",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.2.0rc1",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/47408572",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/47408572",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.2.0rc1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2021-05-13T12:23:49Z",
        "date_published": "2021-05-13T12:32:35Z",
        "description": "### Enhancements\r\n\r\n- Add user interface checks for slice timing so that errors in configuration can be detected before running\r\n- Reduce memory usage\r\n\r\n### Bug fixes\r\n\r\n- Fix using curly brackets in a tag regex, for example `/data/{subject:[0-9]{5}}.nii.gz`\r\n- Fix disabling the high pass filter for task-based feature extraction\r\n- Fix performance issue with importing large BIDS datasets that contain field maps\r\n- Fix matplotlib error (#107)\r\n- Fix performance issue for large datasets (#105)",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.1.1",
        "name": "1.1.1",
        "release_id": 42899727,
        "tag": "1.1.1",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.1.1",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/42899727",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/42899727",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.1.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2021-04-19T10:04:12Z",
        "date_published": "2021-04-19T10:05:35Z",
        "description": "With many thanks to @jstaph for contributions\r\n\r\n### New features and enhancements\r\n\r\n- Create high-performance computing cluster submission scripts for Torque/PBS\r\n  and SGE cluster as well (#71)\r\n- Calculate additional statistics such as heterogeneity\r\n  (<https://doi.org/fzx69f>) and a test that data is\r\n  missing-completely-at-random via logistic regression (#67)\r\n- Always enable ICA-AROMA even when its outputs are not required for feature\r\n  extraction so that its report image is always available for quality assessment\r\n  (#75)\r\n- Support loading presets or plugins that may make it easier to do harmonized\r\n  analyses across many sites (#8)\r\n- Support adding derivatives of the HRF to task-based GLM design matrices\r\n- Support detecting the amount of available memory when running as a cluster\r\n  job, or when running as a container with a memory limit such as when using\r\n  Docker on Mac\r\n\r\n### Maintenance\r\n\r\n- Add type hints to code. This allows a type checker like `pyright` to suggest\r\n  possible error sources ahead of time, making programming more efficient\r\n- Add `openpyxl` and `xlsxwriter` dependencies to support reading/writing Excel\r\n  XLSX files\r\n- Update `numpy`, `scipy` and `nilearn` versions\r\n- Add additional automated tests\r\n\r\n### Bug fixes\r\n\r\n- Fix importing slice timing information from a file after going back to the\r\n  prompt via undo (#55)\r\n- Fix a warning when loading task event timings from a MAT-file.\r\n  NiftiheaderLoader tried to load metadata for it like it would for a NIfTI file\r\n  (#56)\r\n- Fix `numpy` array broadcasting error when loading data from 3D NIfTI files\r\n  that have been somehow marked as being four-dimensional\r\n- Fix misunderstanding of the output value `resels` of FSL's `smoothest`\r\n  command. The value refers to the size of a resel, not the number of them in\r\n  the image. The helper function `_critical_z` now taked this into account now.\r\n  (nipy/nipype#3316)\r\n- Fix naming of output files in `derivatives/halfpipe` and `grouplevel` folder\r\n  so that capitalization is consistent with original IDs and names (#57)\r\n- Fix the summary display after `BIDS` import to show the number of \"subjects\"\r\n  and not the number of \"subs\"\r\n- Fix getting the required metadata fields for an image type by implementing a\r\n  helper function\r\n- Fix outputting source files for the quality check web app (#62)\r\n- Fix assigning field maps to specific functional images, which is done by a\r\n  mapping between field map taks and functional image tags. The mapping is\r\n  automatically inferred for BIDS datasets and manually specified otherwise\r\n  (#66)\r\n- Force re-calculation of `nipype` workflows after `HALFpipe` update so that\r\n  changes from the new version are applied in existing working directories as\r\n  well\r\n- Do not fail task-based feature extraction if no events are available for a\r\n  particular condition for a particular subject (#58)\r\n- Force using a recent version of the `indexed_gzip` dependency to avoid error\r\n  (#85)\r\n- Improve loading delimited data in `loadspreadsheet` function\r\n- Fix slice timing calculation in user interface",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.1.0",
        "name": "1.1.0",
        "release_id": 41634105,
        "tag": "1.1.0",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.1.0",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/41634105",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/41634105",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.1.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2021-04-13T15:30:01Z",
        "date_published": "2021-04-13T15:34:12Z",
        "description": "- Performance improvements for large datasets\r\n- Improve running on SGE and Torque/PBS clusters\r\n- Bug fixes\r\n\r\n- Does not fail even when events are missing for a condition for a participant\r\n- Check missing-completely-at-random assumption via logistic regression at group level\r\n",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.1.0rc1",
        "name": "1.1.0rc1",
        "release_id": 41363091,
        "tag": "1.1.0rc1",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.1.0rc1",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/41363091",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/41363091",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.1.0rc1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2021-01-27T14:48:26Z",
        "date_published": "2021-01-27T14:52:03Z",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.0.1",
        "name": "1.0.1",
        "release_id": 36975423,
        "tag": "1.0.1",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.0.1",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/36975423",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/36975423",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.0.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2021-01-19T14:29:25Z",
        "date_published": "2021-01-19T14:30:40Z",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.0.0",
        "name": "1.0.0",
        "release_id": 36602006,
        "tag": "1.0.0",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/36602006",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/36602006",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.0.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2020-12-08T20:34:45Z",
        "date_published": "2020-12-11T14:48:06Z",
        "description": "## Enhancements\r\n- Run group models with listwise deletion so that missing brain coverage in one\r\n  subject does not lead to a missing voxel in the group statistic. This is not\r\n  possible to do with FSL `flameo`, but we still wanted to use the FLAME\r\n  algorithm ([Woolrich et al. 2004](https://doi.org/10.1016/j.neuroimage.2003.12.023)).\r\n  As such, I re-implemented the algorithm to adaptively adjust the design matrix\r\n  depending on brain coverage.\r\n- Add automated testing. Any future code changes need to pass all automated\r\n  tests before they can be uploaded to the master branch (and thus be\r\n  available for download). The tests take around two hours to complete\r\n  and include a full run of Halfpipe for one subject.\r\n- Increase run speed by running all tasks in parallel as opposed to only most.\r\n  Previously, the code would run all tasks related to copying and organizing data\r\n  on the main thread. This is a convention introduced by `nipype`. It is based\r\n  on the assumption that the main thread may run on the head node of a cluster\r\n  and submit all tasks as jobs to the cluster. To prevent quick tasks from\r\n  clogging the cluster queue, they are run on the head node. However, as we\r\n  do not use `nipype` that way, we can improve performance by getting rid of\r\n  this behavior.\r\n- Improve debug output to include variable names when an error occurs.\r\n- Improve `--watchdog` option to include memory usage information.\r\n\r\n## Maintenance\r\n- Bump `pybids`, `fmriprep`, `smriprep`, `niworkflows`, `nipype` and\r\n  `templateflow` versions.\r\n\r\n## Bug fixes\r\n- Fix design matrix specification with numeric subject names and leading zeros.\r\n- Fix design matrix specification of F-contrasts.\r\n- Fix selecting subjects by group for numeric group names.\r\n- Fix an error with seed connectivity when excluding a seed due to missing\r\n  brain coverage (#19).\r\n- Force output file names to be BIDS compatible and improve their naming.\r\n- Stop `fmriprep` from creating a `work` folder in the Halfpipe working directory.",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.0.0b6",
        "name": "1.0.0 Beta 6",
        "release_id": 34994726,
        "tag": "1.0.0b6",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.0.0b6",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/34994726",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/34994726",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.0.0b6"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2020-10-29T10:01:21Z",
        "date_published": "2020-10-29T10:03:52Z",
        "description": "## Enhancements\r\n\r\n- Implement continuous integration that runs automated tests of any\r\n  changes in code. This means that, if implemented correctly, bugs that\r\n  are fixed once can be covered by these tests so that they are not\r\n  accidentally introduced again further down the line. This approach is \r\n  called regression testing.\r\n- Add codecov plugin to monitor the percentage of code that is covered by\r\n  automated tests. Halfpipe is currently at 2%, which is very low, but\r\n  this will improve over time as we write more testing code.\r\n- Improve granularity of the `--keep` automatic intermediate file deletion\r\n  so that more files are deleted, and add automated tests to verify the\r\n  correctness of file deletion decisions.\r\n- Add `--nipype-resource-monitor` command line option to monitor memory\r\n  usage of the workflow and thus diagnose memory issues\r\n- Re-implement logging code to run in a separate process, reducing the\r\n  burden on the main process. This works by passing a Python\r\n  `multiprocessing.Queue` to all nipype worker processes, so that all\r\n  workers put log messages into the queue using a\r\n  `logging.handlers.QueueHandler`.\r\n  I then implemented a listener that would read from this queue and route\r\n  the log messages to the appropriate log files and the terminal standard\r\n  output.\r\n  I first implemented the listener with `threading`. Threading is a simple\r\n  way to circumvent I/O delays slowing down the main code. With threading,\r\n  the Python interpreter switches between the logging and main threads\r\n  regularly. As a result, when the logging thread waits for the operating\r\n  system to write to disk or to acquire a file lock, the main thread can\r\n  do work in the meantime, and vice versa.\r\n  Very much unexpectedly, this code led to segmentation faults in\r\n  Python. To better diagnose these errors, I refactored the logging\r\n  thread to a separate process, because I thought there may be some\r\n  kind of problem with threading.\r\n  Through this work, I discovered that I was using a different\r\n  `multiprocessing` context for instantiating the logging queue and the\r\n  nipype workers, which caused the segmentation faults. Even though it\r\n  is now unnecessary, I decided to keep the refactored code with\r\n  logging in a separate process, because there are no downsides and I\r\n  had already put the work in.\r\n- Re-phrase some logging messages for improved clarity.\r\n- Refactor command line argument parser and dispatch code to a\r\n  separate module to increase code clarity and readability.\r\n- Refactor spreadsheet loading code to new parse module.\r\n- Print warnings when encountering invalid NIfTI file headers.\r\n- Avoid unnecessary re-runs of preprocessing steps by naming workflows\r\n  using hashes instead of counts. This way adding/removing features\r\n  and settings from the spec.json can be more efficient if\r\n  intermediate results are kept.\r\n- Refactor `--watchdog` code\r\n- Refactor workflow code to use the new collect_boldfiles function to\r\n  decide which functional images to pre-process and which to exclude\r\n  from processing.\r\n  The collect_boldfiles function implements new rules to resolve\r\n  duplicate files. If multiple functional images with the same tags are found,\r\n  for example identical subject name, task and run number, only one will\r\n  be included. Ideally, users would delete such duplicate files before\r\n  running Halfpipe, but we also do not want Halfpipe to fail in these\r\n  cases.\r\n  Two heuristic rules are used: 1) Use the longer functional image.\r\n  Usually, the shorter image will be a scan that was aborted due to\r\n  technical issues and had to be repeated. 2) If both images have the\r\n  same number of volumes, the one with the alphabetically last file\r\n  name will be used.\r\n\r\n## Maintenance\r\n\r\n- Apply pylint code style rules.\r\n- Refactor automated tests to use pytest fixtures.\r\n\r\n## Bug fixes\r\n\r\n- Log all warning messages but reduce the severity level of warnings\r\n  that are known to be benign.\r\n- Fix custom interfaces MaskCoverage, MergeMask, and others based on\r\n  the Transformer class to not discard the NIfTI header when\r\n  outputting the transformed images\r\n- Fix execution stalling when the logger is unable to acquire a lock\r\n  on the log file. Use the `flufl.lock` package for hard link-based file\r\n  locking, which is more robust on distributed file systems and NFS.\r\n  Add a fallback to regular `fcntl`-based locking if that fails, and\r\n  another fallback to circumvent log file locking entirely, so that\r\n  logs will always be written out no matter what (#10).\r\n- Fix accidentally passing T1w images to fmriprep that don\u2019t have\r\n  corresponding functional images.\r\n- Fix merging multiple exclude.json files when quality control is done\r\n  collaboratively.\r\n- Fix displaying a warning for README and dataset_description.json\r\n  files in BIDS datasets.\r\n- Fix parsing phase encoding direction from user interface to not only\r\n  parse the axis  but also the direction. Before, there was no\r\n  difference between selecting anterior-to-posterior and\r\n  posterior-to-anterior, which is incorrect.\r\n- Fix loading repetition time coded in milliseconds or microseconds\r\n  from NIfTI files (#13).\r\n- Fix error when trying to load repetition time from 3D NIfTI file\r\n  (#12).\r\n- Fix spreadsheet loading with UTF-16 file encoding (#3).\r\n- Fix how missing values are displayed in the user interface when\r\n  checking metadata.\r\n- Fix unnecessary inconsistent setting warnings in the user interface.\r\n",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.0.0b5",
        "name": "1.0.0 Beta 5",
        "release_id": 33207774,
        "tag": "1.0.0b5",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.0.0b5",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/33207774",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/33207774",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.0.0b5"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2020-10-01T17:53:43Z",
        "date_published": "2020-10-01T17:58:31Z",
        "description": "Thank you all so much for beta-testing and providing detailed feedback for Beta 3. For Beta 4, we have fixed the following issues and have made some minor improvements:\r\n\r\n- ENH: Add adaptive memory requirement for the submit script generated by `--use-cluster`\r\n- ENH: Output the proportion of seeds and atlas region that is covered by the brain mask to the sidecar JSON file as key `Coverage`\r\n- ENH: Add option to exclude seeds and atlas regions that do not meet a user-specified `Coverage` threshold\r\n- ENH: More detailed display of missing metadata in user interface\r\n- ENH: More robust handling of NIfTI headers <br><br>\r\n- MAINT: Update `fmriprep` to latest release 20.2.0\r\n- MAINT: Update `setup.cfg` with latest `pandas`, `smriprep`, `mriqc` and `niworkflows`\r\n- MAINT: Update `Dockerfile` and `Singularity` recipes to use the latest version of `fmriprep`<br><br>\r\n- FIX: Fix an error that occurred when first level design matrices are sometimes passed to the higher level model code alongside the actual statistics\r\n- FIX: Missing sidecar JSON file for atlas-based connectivity features\r\n- FIX: Allow reading of spreadsheets that contain byte-order marks (#3)\r\n- FIX: Incorrect file name for execgraphs file was generated or the submit script generated by `--use-cluster`\r\n- FIX: Misleading warning for inconsistencies between NIfTI header `slice_duration` and repetition time\r\n- FIX: Ignore additional misleading warnings\r\n- FIX: Incorrect regular expression to select aCompCor columns from confounds\r\n- FIX: Detect all exclude.json files in workdir\r\n- FIX: Replace existing derivatives if nipype outputs have been overwritten ",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.0.0b4",
        "name": "1.0.0 Beta 4",
        "release_id": 31983709,
        "tag": "1.0.0b4",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.0.0b4",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/31983709",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/31983709",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.0.0b4"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2020-09-14T17:27:32Z",
        "date_published": "2020-09-14T17:42:47Z",
        "description": "For Beta 3, we are fixing the bugs you reported and improving parts of the user interface. For more information, please see the detailed changelog below. \r\n\r\n\r\n- ENH: Implement listwise deletion for missing values in linear model via the new filter type `missing`\r\n- ENH: Allow the per-variable specification of missing value strategy for linear models, either listwise deletion (default) or mean substitution\r\n- ENH: Add validators for metadata\r\n- ENH: Allow slice timing to be specified by selecting the slice order from a menu\r\n- ENH: Add option `Add another feature` when using a working directory with existing `spec.json`\r\n- ENH: Add minimum region coverage option for atlas-based connectivity <br><br>\r\n- MAINT: Update `setup.cfg` with latest `nipype`, `fmriprep`, `smriprep` and `niworkflows` versions <br><br>\r\n- FIX: Do not crash when `MergeColumns` `row_index` is empty\r\n- FIX: Remove invalid fields from result in `AggregateResultdicts`\r\n- FIX: Show slice timing option for BIDS datasets\r\n- FIX: Correctly store manually specified slice timing in the `spec.json` for BIDS datasets\r\n- FIX: Build `nitime` dependency from source to avoid build error\r\n- FIX: Do not crash when confounds contain `n/a` values in `init_confounds_regression_wf`\r\n- FIX: Adapt code to new `fmriprep` and `niworkflows` versions\r\n- FIX: Correct capitalization in fixed effects aggregate model names\r\n- FIX: Do not show group model option for atlas-based connectivity features\r\n- FIX: Rename output files so that `contrast` from task-based features becomes `taskcontrast` to avoid conflict with the contrast names in group-level models\r\n- FIX: Catch input file errors in report viewer so that it doesn\u2019t crash\r\n- FIX: Improve naming of group level design matrix TSV files",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.0.0b3",
        "name": "1.0.0 Beta 3",
        "release_id": 31275463,
        "tag": "1.0.0b3",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.0.0b3",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/31275463",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/31275463",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.0.0b3"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2020-09-08T07:35:04Z",
        "date_published": "2020-09-14T17:14:32Z",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.0.0b2",
        "name": "1.0.0 Beta 2",
        "release_id": 31274126,
        "tag": "1.0.0b2",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.0.0b2",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/31274126",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/31274126",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.0.0b2"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "HippocampusGirl",
          "type": "User"
        },
        "date_created": "2020-07-17T09:32:11Z",
        "date_published": "2020-09-14T17:15:27Z",
        "html_url": "https://github.com/HALFpipe/HALFpipe/releases/tag/1.0.0b1",
        "name": "1.0.0 Beta 1",
        "release_id": 31274173,
        "tag": "1.0.0b1",
        "tarball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/tarball/1.0.0b1",
        "type": "Release",
        "url": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/31274173",
        "value": "https://api.github.com/repos/HALFpipe/HALFpipe/releases/31274173",
        "zipball_url": "https://api.github.com/repos/HALFpipe/HALFpipe/zipball/1.0.0b1"
      },
      "technique": "GitHub_API"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Running",
        "type": "Text_excerpt",
        "value": "=======\n\nThe third step is to run the downloaded container. You may need to\nreplace ``halfpipe-halfpipe-latest.simg`` with the actual path and\nfilename where ``Singularity`` downloaded your container.\n\n.. list-table::\n   :header-rows: 1\n\n   -  -  Container platform\n      -  Command\n\n   -  -  Singularity\n      -  ``singularity run --containall --bind /:/ext\n         halfpipe-halfpipe-latest.simg``\n\n   -  -  Docker\n      -  ``docker run --interactive --tty --volume /:/ext\n         halfpipe/halfpipe``\n\nYou should now see the user interface.\n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Background",
        "parent_header": [
          "Running"
        ],
        "type": "Text_excerpt",
        "value": "----------\n\nContainers are by default isolated from the host computer. This adds\nsecurity, but also means that the container cannot access the data it\nneeds for analysis. ``HALFpipe`` expects all inputs (e.g., image files\nand spreadsheets) and outputs (the working directory) to be places in\nthe path\\ ``/ext`` (see also ```--fs-root``\n<#data-file-system-root---fs-root>`__). Using the option ``--bind\n/:/ext``, we instruct ``Singularity`` to map all of the host file system\n(``/``) to that path (``/ext``). You can also run ``HALFpipe`` and only\nmap only part of the host file system, but keep in mind that any\ndirectories that are not mapped will not be visible later.\n\n``Singularity`` passes the host shell environment to the container by\ndefault. This means that in some cases, the host computer\u2019s\nconfiguration can interfere with the software. To avoid this, we need to\npass the option ``--containall``. ``Docker`` does not pass the host\nshell environment by default, so we don\u2019t need to pass an option.\n\n****************\n User interface\n****************\n\n   Outdated\n\nThe user interface asks a series of questions about your data and the\nanalyses you want to run. In each question, you can press ``Control+C``\nto cancel the current question and go back to the previous one.\n``Control+D`` exits the program without saving. Note that these keyboard\nshortcuts are the same on Mac.\n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": ". |  In the interactive job, run the `HALFpipe` user interface, but",
        "type": "Text_excerpt",
        "value": "      add the flag ``--use-cluster`` to the end of the command.\n   |  For example, ``singularity run --containall --bind /:/ext\n      halfpipe-halfpipe-latest.sif --use-cluster``\n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": ". As soon as all processing has been completed, you can run group",
        "type": "Text_excerpt",
        "value": "   statistics. This is usually very fast, so you can do this in an\n   interactive session. Run ``singularity run --containall --bind /:/ext\n   halfpipe-halfpipe-latest.sif --only-model-chunk`` and then select\n   ``Run without modification`` in the user interface.\n\n..\n\n   A common issue with remote work via secure shell is that the\n   connection may break after a few hours. For batch jobs this is not an\n   issue, but for interactive jobs this can be quite frustrating. When\n   the connection is lost, the node you were connected to will\n   automatically quit all programs you were running. To prevent this,\n   you can run interactive jobs within ``screen`` or ``tmux`` (whichever\n   is available). These commands allow you to open sessions in the\n   terminal that will continue running in the background even when you\n   close or disconnect. Here\u2019s a quick overview of how to use the\n   commands (more in-depth documentation is available for example at\n   http://www.dayid.org/comp/tm.html).\n\n   #. Open a new screen/tmux session on the head node by running either\n      ``screen`` or ``tmux``\n\n   #. Request an interactive job from within the session, for example\n      with ``srun --pty bash -i``\n\n   #. Run the command that you want to run\n\n   #. Detach from the screen/tmux session, meaning disconnecting with\n      the ability to re-connect later For screen, this is done by first\n      pressing ``Control+a``, then letting go, and then pressing ``d``\n      on the keyboard. For tmux, it\u2019s ``Control+b`` instead of\n      ``Control+a``. Note that this is always ``Control``, even if\n      you\u2019re on a mac.\n\n   #. Close your connection to the head node with ``Control+d``.\n      ``screen``/``tmux`` will remain running in the background\n\n   #. Later, connect again to the head node. Run ``screen -r`` or ``tmux\n      attach`` to check back on the interactive job. If everything went\n      well and the command you wanted to run finished, close the\n      interactive job with ``Control+d`` and then the\n      ``screen``/``tmux`` session with ``Control+d`` again. If the\n      command hasn\u2019t finished yet, detach as before and come back later\n\n..\n\n    Are you getting a \"missing dependencies\" error? Some clusters configure singularity with an option called `mount hostfs <https://sylabs.io/guides/3.9/user-guide/bind_paths_and_mounts.html#disabling-system-binds>`_ that will bind all cluster file systems into the container. These file systems may in some cases have paths that conflict with where software is installed in the ``HALFpipe`` container, effectively overwriting that software. You can disable this by adding the option ``--no-mount hostfs`` right after ``singularity run``.\n\n****************\n Quality checks\n****************\n\nPlease see the `manual <https://drive.google.com/file/d/1TMg9MRvBwZO8HB1UJmH0gm4tYaBVnvcQ/view>`_\n\n*********\n Outputs\n*********\n\n   Outdated\n\n-  A visual report page ``reports/index.html``\n\n-  A table with image quality metrics ``reports/reportvals.txt``\n\n-  A table containing the preprocessing status\n   ``reports/reportpreproc.txt``\n\n-  The untouched ``fmriprep`` derivatives. Some files have been omitted\n   to save disk space ``fmriprep`` is very strict about only processing\n   data that is compliant with the BIDS standard. As such, we may need\n   to format subjects names for compliance. For example, an input\n   subject named ``subject_01`` will appear as ``subject01`` in the\n   ``fmriprep`` derivatives. ``derivatives/fmriprep``\n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Choose which parts to run or to skip",
        "type": "Text_excerpt",
        "value": "====================================\n\n   Outdated\n\n.. code:: bash\n\n   --<only|skip>-<spec-ui|workflow|run|model-chunk>\n\nA ``HALFpipe`` run is divided internally into three stages, spec-ui,\nworkflow, and run.\n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": ". The `run` stage loads the",
        "type": "Text_excerpt",
        "value": "   ``execgraph.{n_chunks}_chunks.{uuid}.pickle.xz`` file generated in\n   the previous step and runs it. This file usually contains two chunks,\n   one for the subject level preprocessing and feature extraction\n   (\u201csubject level chunk\u201d), and one for group statistics (\u201cmodel\n   chunk\u201d). To run a specific chunk, you can use the flags\n   ``--only-chunk-index ...`` and ``--only-model-chunk``.\n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "requirements",
    "contact",
    "contributors",
    "support",
    "identifier",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 07:01:34",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 74
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": ". The `workflow` stage is where `HALFpipe` uses the `spec.json`",
        "type": "Text_excerpt",
        "value": "   data to search for all the files that match what was input in the\n   user interface. It then generates a ``nipype`` workflow for\n   preprocessing, feature extraction and group models. ``nipype`` then\n   validates the workflow and prepares it for execution. This usually\n   takes a couple of minutes and cannot be parallelized. For hundreds of\n   subjects, this may even take a few hours. This stage has the\n   corresponding option ``--only-workflow`` and ``--skip-workflow``.\n\n-  This stage saves several intermediate files. These are named\n   ``workflow.{uuid}.pickle.xz``, ``execgraph.{uuid}.pickle.xz`` and\n   ``execgraph.{n_chunks}_chunks.{uuid}.pickle.xz``. The ``uuid`` in the\n   file name is a unique identifier generated from the ``spec.json``\n   file and the input files. It is re-calculated every time we run this\n   stage. The uuid algorithm produces a different output if there are\n   any changes (such as when new input files for new subjects become\n   available, or the ``spec.json`` is changed, for example to add a new\n   feature or group model). Otherwise, the ``uuid`` stays the same.\n   Therefore, if a workflow file with the calculated ``uuid`` already\n   exists, then we do not need to run this stage. We can simple reuse\n   the workflow from the existing file, and save some time.\n\n-  In this stage, we can also decide to split the execution into chunks.\n   The flag ``--subject-chunks`` creates one chunk per subject. The flag\n   ``--use-cluster`` automatically activates ``--subject-chunks``. The\n   flag ``--n-chunks`` allows the user to specify a specific number of\n   chunks. This is useful if the execution should be spread over a set\n   number of computers. In addition to these, a model chunk is\n   generated.\n"
      },
      "source": "https://raw.githubusercontent.com/HALFpipe/HALFpipe/main/README.rst",
      "technique": "header_analysis"
    }
  ]
}