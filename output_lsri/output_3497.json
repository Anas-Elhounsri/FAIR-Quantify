{
  "application_domain": [
    {
      "confidence": 32.32,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citation",
        "type": "Text_excerpt",
        "value": "If you find MATNet useful for your research, please consider citing the following papers:\n```\n@inproceedings{zhou2020motion,\n  title={Motion-Attentive Transition for Zero-Shot Video Object Segmentation},\n  author={Zhou, Tianfei and Wang, Shunzhou and Zhou, Yi and Yao, Yazhou and Li, Jianwu and Shao, Ling},\n  booktitle={Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI)},\n  year={2020},\n  pages={13066--13073}\n}\n\n@article{zhou2020matnet,\n  title={MATNet: Motion-Attentive Transition Network for Zero-Shot Video Object Segmentation},\n  author={Zhou, Tianfei and Li, Jianwu and Wang, Shunzhou and Tao, Ran and Shen, Jianbing},\n  journal={IEEE Transactions on Image Processing},\n  volume={29},\n  pages={8326-8338},\n  year={2020}\n}\n\n@inproceedings{zhou2021unsupervised,\n  author = {Zhou, Tianfei and Li, Jianwu and Li, Xueyi and Shao, Ling},\n  title = {Target-Aware Object Discovery and Association for Unsupervised Video Multi-Object Segmentation},\n  booktitle = {CVPR},\n  year = {2021}\n}\n```\n\n\n"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Zhou, Tianfei and Wang, Shunzhou and Zhou, Yi and Yao, Yazhou and Li, Jianwu and Shao, Ling",
        "format": "bibtex",
        "title": "Motion-Attentive Transition for Zero-Shot Video Object Segmentation",
        "type": "Text_excerpt",
        "value": "@inproceedings{zhou2020motion,\n    pages = {13066--13073},\n    year = {2020},\n    booktitle = {Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI)},\n    author = {Zhou, Tianfei and Wang, Shunzhou and Zhou, Yi and Yao, Yazhou and Li, Jianwu and Shao, Ling},\n    title = {Motion-Attentive Transition for Zero-Shot Video Object Segmentation},\n}"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Zhou, Tianfei and Li, Jianwu and Wang, Shunzhou and Tao, Ran and Shen, Jianbing",
        "format": "bibtex",
        "title": "MATNet: Motion-Attentive Transition Network for Zero-Shot Video Object Segmentation",
        "type": "Text_excerpt",
        "value": "@article{zhou2020matnet,\n    year = {2020},\n    pages = {8326-8338},\n    volume = {29},\n    journal = {IEEE Transactions on Image Processing},\n    author = {Zhou, Tianfei and Li, Jianwu and Wang, Shunzhou and Tao, Ran and Shen, Jianbing},\n    title = {MATNet: Motion-Attentive Transition Network for Zero-Shot Video Object Segmentation},\n}"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Zhou, Tianfei and Li, Jianwu and Li, Xueyi and Shao, Ling",
        "format": "bibtex",
        "title": "Target-Aware Object Discovery and Association for Unsupervised Video Multi-Object Segmentation",
        "type": "Text_excerpt",
        "value": "@inproceedings{zhou2021unsupervised,\n    year = {2021},\n    booktitle = {CVPR},\n    title = {Target-Aware Object Discovery and Association for Unsupervised Video Multi-Object Segmentation},\n    author = {Zhou, Tianfei and Li, Jianwu and Li, Xueyi and Shao, Ling},\n}"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/tfzhou/MATNet"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-11-17T09:36:27Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-17T14:58:33Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Motion-Attentive Transition for Zero-Shot Video Object Segmentation (AAAI2020&TIP2021)"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9320955875152719,
      "result": {
        "original_header": "Motion-Attentive Transition for Zero-Shot Video Object Segmentation",
        "type": "Text_excerpt",
        "value": "> UPDATES:<br>\n> - [2021/04/17] Our MATNet achieves state-of-the-art results (__64.2__ in terms of _Mean J_) on the [MoCA](https://drive.google.com/drive/folders/1x-owzr9Voz65NQghrN_H1LEYDaaQP5n1?usp=sharing) dataset in \"Self-supervised Video Object Segmentation by Motion Grouping\" by Charig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman, Weidi Xie. Thanks [Charig Yang](https://charigyang.github.io/) for providing the segmentation results [Google Drive](https://drive.google.com/drive/folders/1x-owzr9Voz65NQghrN_H1LEYDaaQP5n1?usp=sharing).\n> - [2020/06/15] Update results for DAVIS-17 test-dev set!\n> - [2020/03/04] Update results for DAVIS-17 validation set!\n> - [2019/11/17] Codes released! \nThis is a PyTorch implementation of our MATNet for unsupervised video object segmentation. \n"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9605544732093498,
      "result": {
        "original_header": "Segmentation Results",
        "type": "Text_excerpt",
        "value": "1. The segmentation results on DAVIS-16 and Youtube-objects can be downloaded from [Google Drive](https://drive.google.com/file/d/1d23TGBtrr11g8KFAStwewTyxLq2nX4PT/view?usp=sharing).\n2. The segmentation results on DAVIS-17 __val__ can be downloaded from [Google Drive](https://drive.google.com/open?id=1GTqjWc7tktw92tBNKln2eFmb9WzdcVrz). We achieved __58.6__ in terms of _Mean J&F_.\n3. The segmentation results on DAVIS-17 __test-dev__ can be downloaded from [Google Drive](https://drive.google.com/file/d/1Ood-rr0d4YRFSrGGh6yVpYvOvE_h0tVK/view?usp=sharing). We achieved __59.8__ in terms of _Mean J&F_. The method also achieved the second place in DAVIS-20 unsupervised object segmentation challenge. Please refer to [paper](https://davischallenge.org/challenge2020/papers/DAVIS-Unsupervised-Challenge-2nd-Team.pdf) for more details of our challenge solution.\n \n"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Download Datasets",
        "parent_header": [
          "Train"
        ],
        "type": "Text_excerpt",
        "value": "In the paper, we use the following two public available dataset for training. Here are some steps to prepare the data:\n- [DAVIS-17](https://davischallenge.org/davis2017/code.html): we use all the data in the train subset of DAVIS-16. \n    However, please download DAVIS-17 to fit the code. It will automatically choose the subset of DAVIS-16 for training. \n- [YoutubeVOS-2018](https://youtube-vos.org/dataset/): we sample the training data every 10 frames in YoutubeVOS-2018. We use the dataset version with 6fps rather than 30fps.\n- Create soft links:\n\n    ```cd data; ln -s your/davis17/path DAVIS2017; ln -s your/youtubevos/path YouTubeVOS_2018;```\n    "
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/tfzhou/MATNet/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 20
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/tfzhou/MATNet/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "tfzhou/MATNet"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Prepare Edge Annotations",
        "parent_header": [
          "Train"
        ],
        "type": "Text_excerpt",
        "value": "I have provided some matlab scripts to generate edge annotations from mask. Please run ```data/run_davis2017.m``` \nand ```data/run_youtube.m```.\n"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Prepare HED Results",
        "parent_header": [
          "Train"
        ],
        "type": "Text_excerpt",
        "value": "I have provided the pytorch codes to generate HED results for the two datasets (see ```3rdparty/pytorch-hed```).\nPlease run ```run_davis.py``` and ```run_youtube.py```. \n\nThe codes are borrowed from https://github.com/sniklaus/pytorch-hed. \n"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Prepare Optical Flow",
        "parent_header": [
          "Train"
        ],
        "type": "Text_excerpt",
        "value": "I have provided the pytorch codes to generate optical flow results for the two datasets (see ```3rdparty/pytorch-pwc```).\nPlease run ```run_davis_flow.py``` and ```run_youtubevos_flow.py```. \n\nThe codes are borrowed from https://github.com/sniklaus/pytorch-pwc. \nPlease follow the [setup](https://github.com/sniklaus/pytorch-pwc#setup) section to install ```cupy```. \n\n`warning: Total size of optical flow results of Youtube-VOS is more than 30GB.`\n\n### Train\nOnce all data is prepared, please run ```python train_MATNet.py``` for training.\n"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9999999849266602,
      "result": {
        "original_header": "Clone",
        "type": "Text_excerpt",
        "value": "```git clone --recursive https://github.com/tfzhou/MATNet.git```\n \n"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8857762475948185,
      "result": {
        "original_header": "Test",
        "type": "Text_excerpt",
        "value": "1. Run ```python test_MATNet.py``` to obtain the saliency results on DAVIS-16 val set.\n2. Run BASH2* for binary segmentation results. \n"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8472294450011667,
      "result": {
        "original_header": "Test",
        "type": "Text_excerpt",
        "value": "1. Run ```python test_MATNet.py``` to obtain the saliency results on DAVIS-16 val set.\n2. Run BASH2* for binary segmentation results. \n"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/tfzhou/MATNet/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "aaai, attention-mechanism, davis-challenge, deep-learning, multi-object, optical-flow, segmentation, two-stream, video-object-segmentation"
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "MATNet"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "tfzhou"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 108610,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "MATLAB",
        "size": 3576,
        "type": "Programming_language",
        "value": "MATLAB"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2003.04253"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Prerequisites",
        "type": "Text_excerpt",
        "value": "The training and testing experiments are conducted using PyTorch 1.0.1 with a single GeForce RTX 2080Ti GPU with 11GB Memory.\n- [PyTorch 1.0.1](https://github.com/pytorch/pytorch)\n                   \nOther minor Python modules can be installed by running\n\n```bash\npip install -r requirements.txt\n```\n"
      },
      "source": "https://raw.githubusercontent.com/tfzhou/MATNet/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 12:58:04",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 191
      },
      "technique": "GitHub_API"
    }
  ]
}