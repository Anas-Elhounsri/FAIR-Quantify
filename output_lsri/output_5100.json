{
  "application_domain": [
    {
      "confidence": 27.6,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/MIC-DKFZ/TractSeg"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2017-11-08T12:23:08Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-10-04T00:39:39Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Automatic White Matter Bundle Segmentation"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9494347418840947,
      "result": {
        "original_header": "TractSeg",
        "type": "Text_excerpt",
        "value": "Tool for fast and accurate white matter bundle segmentation from Diffusion MRI. It can create \nbundle segmentations, segmentations of the endregions of bundles and Tract Orientation Maps (TOMs). Moreover, it can \ndo tracking on the TOMs creating bundle-specific tractogram and do Tractometry analysis on those. \nThe tool works very well for data similar to the Human Connectome Project. For other MRI datasets it works well for all\nbundles except for the Commissure Anterior (CA) and the Fornix (FX) which are [incomplete sometimes](#small-bundles-like-the-ca-and-fx-are-incomplete). \nTractSeg is the code for the following papers. Please cite the papers if you use it. \n* Tract Segmentation:   \n[TractSeg - Fast and accurate white matter tract segmentation](https://doi.org/10.1016/j.neuroimage.2018.07.070) ([free arxiv version](https://arxiv.org/abs/1805.07103))\n[NeuroImage 2018]\n* Tract Orientation Mapping (TOM):   \n[Tract orientation mapping for bundle-specific tractography](https://arxiv.org/abs/1806.05580)\n[MICCAI 2018]\n* Tracking on TOMs:  \n[Combined tract segmentation and orientation mapping for bundle-specific tractography](https://www.sciencedirect.com/science/article/pii/S136184151930101X)\n[Medical Image Analysis 2019]\n* Tractometry:  \n[Multiparametric mapping of white matter microstructure in catatonia](https://www.nature.com/articles/s41386-020-0691-2) ([free preprint](resources/Wasserthal2020_Multiparametric_mapping_of_white_matter.pdf))\n[Nature Neuropsychopharmacology 2020] \n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9484056059845999,
      "result": {
        "original_header": "Docker",
        "type": "Text_excerpt",
        "value": "You can also directly use TractSeg via Docker (contains all prerequisites).\n```\nsudo docker run -v /absolute/path/to/my/data/directory:/data \\\n-t wasserth/tractseg_container:master TractSeg -i /data/my_diffusion_file.nii.gz -o /data --raw_diffusion_input\n```\nOn OSX you might have to increase the Docker memory limit from the default of 2GB to something\nlike 7GB.\n \n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9312686338664564,
      "result": {
        "original_header": "Train your own model",
        "type": "Text_excerpt",
        "value": "TractSeg uses a pretrained model. You can also train your own model on your own data, however TractSeg is not \noptimised to make this convenient, as most people will use the pretrained model. The following \nguide is quite short and you might have problems following every step. Contact the author if\nyou need help training your own model. \n1. Use your own data or download the data from [Zenodo](https://doi.org/10.5281/zenodo.1088277)\n2. If you have streamlines you have to transform them to binary masks. \nYou can use [this](https://github.com/MIC-DKFZ/TractSeg/blob/master/resources/utility_scripts/trk_2_binary.py) \nscript for that. \n(If you want to train a TOM model you have to create peak maps from the streamlines. This can be done \nby using the MITK Diffusion miniapp `MitkFiberDirectionExtraction`. See the following two issues for more \ndetails: [82](https://github.com/MIC-DKFZ/TractSeg/issues/82), \n[92](https://github.com/MIC-DKFZ/TractSeg/issues/92))\n3. Install TractSeg from local source code:\n    ```\n    git clone https://github.com/MIC-DKFZ/TractSeg.git\n    pip install -e TractSeg\n    ```\n4. Install BatchGenerators: \n    BASH2*  \n5. The folder structure of your training data should be the following:\n    BASH3*  \n6. Preprocess the data using `tractseg/data/preprocessing.py` to remove all non-brain area (crop to brain \nbounding box). Adapt the data pathes in `tractseg/data/preprocessing.py` to fit your data (look for `#todo: adapt` \ninside of the file.)\n7. Adapt the file `tractseg/config/custom/My_custom_experiment.py`.\n8. Create a file `~/.tractseg/config.txt`. This contains the path to your data directory `working_dir=XXX`, e.g.\n`working_dir=custom_path`.\n9. Adapt `tractseg.data.dataset_specific_utils.get_bundle_names()` with the bundles you use in your reference data.\n10. Adapt `tractseg.data.dataset_specific_utils.get_labels_filename()` with the names of your label files.\n11. Adapt `tractseg.data.Subjects` with the list of your subject IDs.\n12. Run `ExpRunner --config My_custom_experiment` \n13. `custom_path/hcp_exp/My_custom_experiment` contains the results \n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9968172958222302,
      "result": {
        "original_header": "Copyright",
        "type": "Text_excerpt",
        "value": "Copyright \u00a9 German Cancer Research Center (DKFZ), Division of Medical Image Computing (MIC).\nPlease make sure that your usage of this code is in compliance with the [code license](LICENSE). \n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "http://mrtrix.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "regular_expression"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/MIC-DKFZ/TractSeg/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/examples/.ipynb_checkpoints/plot_tractometry_results-checkpoint.ipynb"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/examples/.ipynb_checkpoints/plot_tractometry_results-checkpoint.ipynb",
      "technique": "file_exploration"
    }
  ],
  "faq": [
    {
      "confidence": 1,
      "result": {
        "original_header": "My output segmentation does not look like any bundle at all!",
        "parent_header": [
          "TractSeg",
          "FAQ"
        ],
        "type": "Text_excerpt",
        "value": "Make sure your input image is in MNI space. Even if the input image is in MNI space the Mrtrix peaks might still be flipped. \nIn those cases you should view the peaks in `mrview` and make sure they have the proper \norientation. Otherwise you might have to flip the sign along the x, y or z axis using the following command: \n```\nflip_peaks -i my_peaks.nii.gz -o my_peaks_flip_y.nii.gz -a y\n```\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "My data has anisotropic spacing. Does it still work?",
        "parent_header": [
          "TractSeg",
          "FAQ"
        ],
        "type": "Text_excerpt",
        "value": "If the spacing is only slightly anisotropic (e.g. 1.9mm x 1.9mm x 2mm) it still works fine. If the spacing is heavily \nanisotropic (e.g. 1mm x 1mm x 2mm) results will be a lot better if you resample your image to isotropic spacing first.\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Small bundles like the CA and FX are incomplete",
        "parent_header": [
          "TractSeg",
          "FAQ"
        ],
        "type": "Text_excerpt",
        "value": "You can use the following options to improve your results:  \n`--super_resolution` The input image is upsampled to 1.25mm resolution (the resolution TractSeg was trained on) and \nfinally downsampled back to the original resolution. Using `--super_resolution` will output the image at 1.25mm. \nEspecially if image resolution is low parts of the CA can get lost during downsampling.\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Can I save the pretrained weights at a different location?",
        "parent_header": [
          "TractSeg",
          "FAQ"
        ],
        "type": "Text_excerpt",
        "value": "Per default the pretrained weights will be downloaded to and loaded from `~/.tractseg/`. You can change this directory\nby adding `weights_dir=/absolute/path_to_where/you_want_it` to `~/.tractseg/config.txt` in a new line (if the file does \nnot exist yet you have to create it).  \nNormally the pretrained weights will automatically be downloaded in the background right when they are needed. In some\ncases you might want to download all of them at once. To do so you can simply run `download_all_pretrained_weights` and \nthe weights will be download to `~/.tractseg/` or the location you specified in `~/.tractseg/config.txt`.\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 76
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/MIC-DKFZ/TractSeg/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "MIC-DKFZ/TractSeg"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "TractSeg"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/tests.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/tractseg/experiments/run_multiple.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/resources/utility_scripts/dwi_preprocessing.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/resources/Pipeline_img_v2.png"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Install",
        "parent_header": [
          "TractSeg"
        ],
        "type": "Text_excerpt",
        "value": "TractSeg only runs on Linux and OSX. It works with Python >= 3.6.\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Install Prerequisites",
        "parent_header": [
          "TractSeg",
          "Install"
        ],
        "type": "Text_excerpt",
        "value": "* [Pytorch](http://pytorch.org/)\n* [Mrtrix 3](http://mrtrix.readthedocs.io/en/latest/installation/linux_install.html) (>= 3.0 RC3)\n* [FSL](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation) (if you already have a brain mask and do not use the \noption `--preprocess` this is not needed)\n* xvfb (`apt-get install xvfb`) (only needed if you use `plot_tractometry_results`)\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Install TractSeg",
        "parent_header": [
          "TractSeg",
          "Install"
        ],
        "type": "Text_excerpt",
        "value": "Latest stable version:\n```\npip install TractSeg\n```\n> NOTE: See [CHANGELOG](CHANGELOG.md) for (breaking) changes of each version  \n\nIf you want to use Cython for `Tracking` (speedup 2x) then install via \n```\npip install https://github.com/MIC-DKFZ/TractSeg/archive/cython_tracking.zip\n```\n\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Did I install the prerequisites correctly?",
        "parent_header": [
          "TractSeg",
          "FAQ"
        ],
        "type": "Text_excerpt",
        "value": "You can check if you installed Mrtrix correctly if you can run the following command on your terminal:\n`dwi2response -help`\n\nYou can check if you installed FSL correctly if you can run the following command on your terminal: \n`bet -help`\n\nTractSeg uses these commands so they have to be available.\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "How can I install the latest master branch?",
        "parent_header": [
          "TractSeg",
          "FAQ"
        ],
        "type": "Text_excerpt",
        "value": "```\npip install https://github.com/MIC-DKFZ/TractSeg/archive/master.zip\n```\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9985023834876925,
      "result": {
        "original_header": "Docker",
        "type": "Text_excerpt",
        "value": "You can also directly use TractSeg via Docker (contains all prerequisites).\n```\nsudo docker run -v /absolute/path/to/my/data/directory:/data \\\n-t wasserth/tractseg_container:master TractSeg -i /data/my_diffusion_file.nii.gz -o /data --raw_diffusion_input\n```\nOn OSX you might have to increase the Docker memory limit from the default of 2GB to something\nlike 7GB.\n \n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9999989178172144,
      "result": {
        "original_header": "Train your own model",
        "type": "Text_excerpt",
        "value": "1. Use your own data or download the data from [Zenodo](https://doi.org/10.5281/zenodo.1088277)\n2. If you have streamlines you have to transform them to binary masks. \nYou can use [this](https://github.com/MIC-DKFZ/TractSeg/blob/master/resources/utility_scripts/trk_2_binary.py) \nscript for that. \n(If you want to train a TOM model you have to create peak maps from the streamlines. This can be done \nby using the MITK Diffusion miniapp `MitkFiberDirectionExtraction`. See the following two issues for more \ndetails: [82](https://github.com/MIC-DKFZ/TractSeg/issues/82), \n[92](https://github.com/MIC-DKFZ/TractSeg/issues/92))\n3. Install TractSeg from local source code:\n    ```\n    git clone https://github.com/MIC-DKFZ/TractSeg.git\n    pip install -e TractSeg\n    ```\n4. Install BatchGenerators: \n    BASH2*  \n5. The folder structure of your training data should be the following:\n    BASH3*  \n6. Preprocess the data using `tractseg/data/preprocessing.py` to remove all non-brain area (crop to brain \nbounding box). Adapt the data pathes in `tractseg/data/preprocessing.py` to fit your data (look for `#todo: adapt` \ninside of the file.)\n7. Adapt the file `tractseg/config/custom/My_custom_experiment.py`.\n8. Create a file `~/.tractseg/config.txt`. This contains the path to your data directory `working_dir=XXX`, e.g.\n`working_dir=custom_path`.\n9. Adapt `tractseg.data.dataset_specific_utils.get_bundle_names()` with the bundles you use in your reference data.\n10. Adapt `tractseg.data.dataset_specific_utils.get_labels_filename()` with the names of your label files.\n11. Adapt `tractseg.data.Subjects` with the list of your subject IDs.\n12. Run `ExpRunner --config My_custom_experiment` \n13. `custom_path/hcp_exp/My_custom_experiment` contains the results \n\n## Docker\nTo build a docker container with all dependencies run the following command in project root:\n```\nsudo docker build -t tractseg_container -f Dockerfile_CPU .\n```\n> NOTE: With docker the option `--plot3D` of the command `plot_tractometry_results` might not work.\n \n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/MIC-DKFZ/TractSeg/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Apache License 2.0",
        "spdx_id": "Apache-2.0",
        "type": "License",
        "url": "https://api.github.com/licenses/apache-2.0",
        "value": "https://api.github.com/licenses/apache-2.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright {yyyy} {name of copyright owner}\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License."
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/LICENSE",
      "technique": "file_exploration"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "TractSeg"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "MIC-DKFZ"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 526095,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 11658,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1806.05580"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1805.07103"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1506.02142"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "regular_expression"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Install Prerequisites",
        "parent_header": [
          "TractSeg",
          "Install"
        ],
        "type": "Text_excerpt",
        "value": "* [Pytorch](http://pytorch.org/)\n* [Mrtrix 3](http://mrtrix.readthedocs.io/en/latest/installation/linux_install.html) (>= 3.0 RC3)\n* [FSL](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FslInstallation) (if you already have a brain mask and do not use the \noption `--preprocess` this is not needed)\n* xvfb (`apt-get install xvfb`) (only needed if you use `plot_tractometry_results`)\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Did I install the prerequisites correctly?",
        "parent_header": [
          "TractSeg",
          "FAQ"
        ],
        "type": "Text_excerpt",
        "value": "You can check if you installed Mrtrix correctly if you can run the following command on your terminal:\n`dwi2response -help`\n\nYou can check if you installed FSL correctly if you can run the following command on your terminal: \n`bet -help`\n\nTractSeg uses these commands so they have to be available.\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "contact",
    "contributors",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-06 18:47:29",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 225
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Simple example:",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "To segment the bundles on a Diffusion Nifti image run the following command. (Diffusion.bvals and Diffusion.bvecs have to be in the same directory\nas the input image.)\n(You can use the example image provided in this repository under `examples`.)  \n```\nTractSeg -i Diffusion.nii.gz --raw_diffusion_input\n```\nThis will create a folder `tractseg_ouput` inside of the same directory as your input file with one binary segmentation nifti image\nfor each bundle.\n \n> NOTE: If results look bad you probably have to align your images to have the same orientation as MNI space. Moreover it should have isotropic spacing. See [here](#aligning-image-to-mni-space) for more details.\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Custom input and output path:",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "```\nTractSeg -i my/path/my_diffusion_image.nii.gz\n         -o my/output/directory\n         --bvals my/other/path/my.bvals\n         --bvecs yet/another/path/my.bvec\n         --raw_diffusion_input\n```\nUse `--help` to see all options.\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Use existing peaks",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "To avoid generating the MRtrix CSD peaks every time you `run TractSeg you can also provide them directly by skipping the \noption `--raw_diffusion_input`.\n```\nTractSeg -i my/path/my_mrtrix_csd_peaks.nii.gz\n```\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Segment bundle start and end regions",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "Get segmentations of the regions were the bundles start and end (helpful for filtering fibers that do not run\nfrom start until end).\n```\nTractSeg -i peaks.nii.gz --output_type endings_segmentation\n```\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Create Tract Orientation Maps (TOMs)",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "For each bundle create a Tract Orientation Map ([Wasserthal et al., Tract orientation mapping for bundle-specific tractography](https://arxiv.org/abs/1806.05580)). \nThis gives you one peak per voxel telling you the main orientation of the respective bundle at this voxel. \nCan be used for bundle-specific tracking later on.\n```\nTractSeg -i peaks.nii.gz --output_type TOM\n```\n> NOTE: `--output_type tract_segmentation` and `endings_segmentation` has to be run first. Same input and output \ndirectories have to be used for all commands. \n\nPeaks and streamlines can be visualized using for example [MITK Diffusion](http://mitk.org/wiki/DiffusionImaging#Downloads).\n> NOTE: Peaks have to be flipped along the z-axis to be displayed correctly in MITK.  \n\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Create bundle-specific tractograms",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "Tracks on TOMs and only keeps fibers not leaving the bundle mask and starting and ending in the endpoint regions.\n```\nTractSeg -i peaks.nii.gz --output_type tract_segmentation\nTractSeg -i peaks.nii.gz --output_type endings_segmentation\nTractSeg -i peaks.nii.gz --output_type TOM \nTracking -i peaks.nii.gz\n```\n\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Use bedpostX peaks instead of CSD peaks",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "TractSeg also works with bedpostX as input. You have to pass `dyads1.nii.gz` as input and TractSeg will automatically\n find all the other necessary bedpostX files (`dyads2_thr0.05.nii.gz` & `dyads3_thr0.05.nii.gz`. `mean_f1-3samples` \n will be used for scaling the peaks.). This only works if you did not change the default bedpostX file naming.\n```\nTractSeg -i dyads1.nii.gz\n```\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Show uncertainty map",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "Create map showing where the method is uncertain about its segmentation (uses monte carlo dropout: https://arxiv.org/abs/1506.02142)\n```\nTractSeg -i peaks.nii.gz --uncertainty\n```\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Perform Tractometry",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "See [Documentation of Tractometry](resources/Tractometry_documentation.md).\n\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Extended Tutorial",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "[Best pratices for standard usecases](resources/Tutorial.md).  \n(You can also check out the [tutorial from OHBM 2019](https://colab.research.google.com/github/brainhack101/IntroDL/blob/master/notebooks/2019/Wasserthal/TractSegTutorial.ipynb)\n[OUTDATED].)\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Track subset of bundles",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "You can specify to only track a subset of bundles.\n```\nTracking -i peaks.nii.gz --bundles CST_right,CA,IFO_right\n```\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Bundle names",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "The following list shows the index of each extracted bundle in the output file (if using `--single_output_file`).\n```\n0: AF_left         (Arcuate fascicle)\n1: AF_right\n2: ATR_left        (Anterior Thalamic Radiation)\n3: ATR_right\n4: CA              (Commissure Anterior)\n5: CC_1            (Rostrum)\n6: CC_2            (Genu)\n7: CC_3            (Rostral body (Premotor))\n8: CC_4            (Anterior midbody (Primary Motor))\n9: CC_5            (Posterior midbody (Primary Somatosensory))\n10: CC_6           (Isthmus)\n11: CC_7           (Splenium)\n12: CG_left        (Cingulum left)\n13: CG_right   \n14: CST_left       (Corticospinal tract)\n15: CST_right \n16: MLF_left       (Middle longitudinal fascicle)\n17: MLF_right\n18: FPT_left       (Fronto-pontine tract)\n19: FPT_right \n20: FX_left        (Fornix)\n21: FX_right\n22: ICP_left       (Inferior cerebellar peduncle)\n23: ICP_right \n24: IFO_left       (Inferior occipito-frontal fascicle) \n25: IFO_right\n26: ILF_left       (Inferior longitudinal fascicle) \n27: ILF_right \n28: MCP            (Middle cerebellar peduncle)\n29: OR_left        (Optic radiation) \n30: OR_right\n31: POPT_left      (Parieto\u2010occipital pontine)\n32: POPT_right \n33: SCP_left       (Superior cerebellar peduncle)\n34: SCP_right \n35: SLF_I_left     (Superior longitudinal fascicle I)\n36: SLF_I_right \n37: SLF_II_left    (Superior longitudinal fascicle II)\n38: SLF_II_right\n39: SLF_III_left   (Superior longitudinal fascicle III)\n40: SLF_III_right \n41: STR_left       (Superior Thalamic Radiation)\n42: STR_right \n43: UF_left        (Uncinate fascicle) \n44: UF_right \n45: CC             (Corpus Callosum - all)\n46: T_PREF_left    (Thalamo-prefrontal)\n47: T_PREF_right \n48: T_PREM_left    (Thalamo-premotor)\n49: T_PREM_right \n50: T_PREC_left    (Thalamo-precentral)\n51: T_PREC_right \n52: T_POSTC_left   (Thalamo-postcentral)\n53: T_POSTC_right \n54: T_PAR_left     (Thalamo-parietal)\n55: T_PAR_right \n56: T_OCC_left     (Thalamo-occipital)\n57: T_OCC_right \n58: ST_FO_left     (Striato-fronto-orbital)\n59: ST_FO_right \n60: ST_PREF_left   (Striato-prefrontal)\n61: ST_PREF_right \n62: ST_PREM_left   (Striato-premotor)\n63: ST_PREM_right \n64: ST_PREC_left   (Striato-precentral)\n65: ST_PREC_right \n66: ST_POSTC_left  (Striato-postcentral)\n67: ST_POSTC_right\n68: ST_PAR_left    (Striato-parietal)\n69: ST_PAR_right \n70: ST_OCC_left    (Striato-occipital)\n71: ST_OCC_right\n```\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Use different tract definitions",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "TractSeg was also trained on the bundles provided by [XTRACT](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/XTRACT). \nThese bundles are slightly differently defined. They tend to be more specific but therefore also a bit \nless complete. Depending on your application this might be of interest for you. Using TractSeg instead of XTRACT\nhas the advantage of being at least 10x faster. Please cite XTRACT if you use this. \n> NOTE: This is only supported for output type `tract_segmentation` and `dm_regression`.\n```\nTractSeg -i peaks.nii.gz --tract_definition xtract\n```\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Use python interface",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "```python\nimport nibabel as nib\nimport numpy as np\nfrom tractseg.python_api import run_tractseg\npeaks = nib.load(\"tests/reference_files/peaks.nii.gz\").get_fdata()\nsegmentation = run_tractseg(peaks)\n```\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Different tracking types",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "You can use different types of tracking:\n\n* \"Probabilistic\" tracking on TOM peaks [**default**].    \n`Tracking -i peaks.nii.gz`  \nProbabilistic means that at each step a small random factor will be added to the direction given by the TOM peaks.\nIf not doing this on low resolution data it sometimes gets difficult finding fibers running from start to end and\ncovering the whole bundle.\n\n* Probabilistic tracking on original FODs.\n`Tracking -i WM_FODs.nii.gz --track_FODs iFOD2`  \nIs calling Mrtrix iFOD2 tracking internally. Does not use TOM peaks but the original FODs. The results will get \nfiltered by the bundle mask and have to start and end in the endings masks.\n\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Tracking formats",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "You can use the option `--tracking_format` to define the file format of the streamline files.\n`tck` is the most stable tracking format and recommended. `trk` might get displayed differently in different viewers.  \n> NOTE: When calling `Tractometry` and `plot_tractometry_results` you have to set the same tracking format as was used in `Tracking`. \n\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Aligning image to MNI space",
        "parent_header": [
          "TractSeg",
          "How to use"
        ],
        "type": "Text_excerpt",
        "value": "For best results the input image must have the same \"orientation\" as the Human Connectome Project data (MNI space) \n(LEFT must be on the same side as LEFT of the HCP data) and have isotropic spacing. \nIf the image orientation and the gradient orientation of your data is the same as in `examples/Diffusion.nii.gz`\nyou are fine. Otherwise you should use `fslreorient2std` or rigidly register your image to MNI space (the brains\ndo not have to be perfectly aligned but must have the same LEFT/RIGHT orientation).\nYou can use the following FSL commands to rigidly register your image to MNI space (uses \nthe FA to calculate the transformation as this is more stable):\n```shell\ncalc_FA -i Diffusion.nii.gz -o FA.nii.gz --bvals Diffusion.bvals --bvecs Diffusion.bvecs \\\n--brain_mask nodif_brain_mask.nii.gz\n\nflirt -ref tractseg/tractseg/resources/MNI_FA_template.nii.gz -in FA.nii.gz \\\n-out FA_MNI.nii.gz -omat FA_2_MNI.mat -dof 6 -cost mutualinfo -searchcost mutualinfo\n\nflirt -ref tractseg/tractseg/resources/MNI_FA_template.nii.gz -in Diffusion.nii.gz \\\n-out Diffusion_MNI.nii.gz -applyxfm -init FA_2_MNI.mat -dof 6\ncp Diffusion.bvals Diffusion_MNI.bvals\nrotate_bvecs -i Diffusion.bvecs -t FA_2_MNI.mat -o Diffusion_MNI.bvecs\n```\nTo enforce isotropic spacing you can replace `-applyxfm` by `-applyisoxfm <your_spacing>`.\n\nTo move the results back to subject space you can use the following commands:\n```shell\nconvert_xfm -omat MNI_2_FA.mat -inverse FA_2_MNI.mat  # invert transformation\n\nflirt -ref FA.nii.gz -in my_bundle.nii.gz -out my_bundle_subject_space.nii.gz \\\n-applyxfm -init MNI_2_FA.mat -dof 6 -interp spline  # for TOM maps you have to use the command vecreg\n\nfslmaths my_bundle_subject_space.nii.gz -thr 0.5 -bin my_bundle_subject_space.nii.gz  # float to binary \n```\n\nThe option `--preprocess` will automatically rigidly register the input image to MNI space, run TractSeg and then \nconvert the output back to subject space. This only works for `tract_segmentation` and `endings_segmentation`. For `TOMs` and trackings you have to register you data manually to MNI space:\n```shell\n# in first step --raw_diffusion_input has to be used together with --preprocess\nTractSeg -i Diffusion.nii.gz -o tractseg_output --output_type tract_segmentation --raw_diffusion_input --preprocess\n# -o has to be set in all following step\nTractSeg -i tractseg_output/peaks.nii.gz -o tractseg_output --output_type endings_segmentation --preprocess\n```\n> NOTE: `--preprocess` does not work if you are using the option `--csd_type csd_msmt_5tt`, \nbecause the T1 image will not automatically be registered to MNI space\n\nIf you are not familiar with preprocessing of DWI images (e.g. correcting for artifacts) you can have a look at \n[this](resources/utility_scripts/dwi_preprocessing.sh) preprocessing script.\n\n"
      },
      "source": "https://raw.githubusercontent.com/MIC-DKFZ/TractSeg/master/Readme.md",
      "technique": "header_analysis"
    }
  ]
}