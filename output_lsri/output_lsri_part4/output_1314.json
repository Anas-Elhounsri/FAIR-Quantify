{
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Reference",
        "parent_header": [
          "HadoopCNV"
        ],
        "type": "Text_excerpt",
        "value": "Yang H, Chen G, Lima L, Fang H, Jimenez L, Li M, Lyon GJ, He M, Wang K. [HadoopCNV: A dynamic programming imputation algorithm to detect copy number variants from sequencing data](http://biorxiv.org/content/early/2017/04/05/124339), doi: https://doi.org/10.1101/124339, 2017\n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/WGLab/HadoopCNV"
      },
      "technique": "GitHub_API"
    }
  ],
  "contributing_guidelines": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "## Contributing to source codes and documentation\n\nPlease fork the [GitHub repository](https://github.com/WGLab/HadoopCNV), modify it, and submit a pull request to us. We will incorporate the change promptly after review.\n\n\n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/docs/misc/contributing.md",
      "technique": "file_exploration"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2015-03-23T16:33:22Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-08-05T09:44:17Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "HadoopCNV is a MapReduce-based copy number variation caller for genome sequencing data"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Introduction",
        "parent_header": [
          "HadoopCNV"
        ],
        "type": "Text_excerpt",
        "value": "HadoopCNV (internal codename: PennCNV3) is a Java implementation of MapReduce-based copy number variation caller for next-generation whole-genome sequencing data.\n\nIn addition to single nucleotide variants (SNVs) and small insertions or deletions (INDELs), whole-genome sequencing (WGS) data may also be used to identify large-scale alterations, such as copy number variations (CNVs) and other types of structural variants (SVs).  Existing CNV detection methods mostly rely on read depth or paired end distance or the combination thereof.  Additionally, resolving small regions in WGS samples with deep coverage can be very time consuming due to massive I/O cost. To facilitate the CNV detection from WGS data, we developed HadoopCNV, a Dynamic Programming Imputation based algorithm, which infers detects aberration events such as copy number changes through information encoded in both allelic and overall read depth.  Our implementation is built on the Hadoop MapReduce paradigm, enabling parallel multiple processors at multiple nodes to efficiently process separate genomic regions in tandem. Extensive benchmarking studies on real and simulated data demonstrated that HadoopCNV has a comparable or better performance than leading CNV callers for WGS data. Additionally, HadoopCNV on a 32-node cluster requires only 1.6 hours for a human genome with 30X coverage, making rapid analysis on thousands of genomes feasible.\n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9296486786030717,
      "result": {
        "original_header": "1) **VCF**:",
        "type": "Text_excerpt",
        "value": "Should be in your local hard drive. You need to check GATK or Samtools or any small variant calling tool for your sample.\n \n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9864221636631227,
      "result": {
        "original_header": "2) **BAM**:",
        "type": "Text_excerpt",
        "value": "Should be in HDFS. This is the alignment file for your sequencing sample.\n \n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8882847634621879,
      "result": {
        "original_header": "Preprocessing",
        "type": "Text_excerpt",
        "value": "For example, if your file is called 'NA12878.chrom\\*.ILLUMINA.bwa.CEU.high_coverage.20100311.bam', you should change 'hg19.bam.extra' to 'NA12878.chrom.ILLUMINA.bwa.CEU.high_coverage.20100311.bam.extra', and change 'BAM_FILE' parameter in your config.txt to 'NA12878.chrom\\*.ILLUMINA.bwa.CEU.high_coverage.20100311.bam.\\*'. Then this additional file is automatically included. \n2) For hg38 or other genomes, we provided a python script **preprocessBAM.py** to generate this baseline BAM file from FASTA files. \nNOTICE: \\<prefix\\> can be any name you give to the file. 'fasta/chrall.fa.fai' is the fasta index associated with all chromosomes.  \n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/WGLab/HadoopCNV/tree/master/docs"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Documentation",
        "parent_header": [
          "HadoopCNV"
        ],
        "type": "Text_excerpt",
        "value": "A formal documentation for HadoopCNV can be accessed [here](http://hadoopcnv.openbioinformatics.org).\n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "format": "wiki",
        "type": "Url",
        "value": "https://github.com/WGLab/HadoopCNV/wiki"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/WGLab/HadoopCNV/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 2
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/WGLab/HadoopCNV/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "WGLab/HadoopCNV"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "HadoopCNV"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/resource/make_users.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/resource/delete_tmp_data.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/resource/test.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/example/run.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/docs/img/PennCNV3.png"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Setup",
        "parent_header": [
          "HadoopCNV"
        ],
        "type": "Text_excerpt",
        "value": "To setup the environment for HadoopCNV, you need to install Hadoop2.0+ in your computer or cluster first. Please refer to [Initialization](docs/user-guide/initialization.md)\n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "1) Put the BAM file in HDFS and VCF file in your local file system. In **config.txt**, set the correct path for **BAM_FILE** and **VCF_FILE**.",
        "parent_header": [
          "HadoopCNV",
          "Run"
        ],
        "type": "Text_excerpt",
        "value": "NOTICE: Sometimes you have your individual BAM files for each individual chromosome. You can use a '\\*' to include many BAM files.\n\nFor example, my NA12878 BAM file name is in the 'NA12878.chrom\\*.ILLUMINA.bwa.CEU.high_coverage.20100311.bam' format. The '\\*' can refer to any character. Thus there is no need to merge your BAM files first anymore.\n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.8089608285863236,
      "result": {
        "original_header": "Input",
        "type": "Text_excerpt",
        "value": "You need a **VCF** file and a **BAM** file to call CNVs from your sample. \n \n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9379070282142257,
      "result": {
        "original_header": "Preprocessing",
        "type": "Text_excerpt",
        "value": "For example, if your file is called 'NA12878.chrom\\*.ILLUMINA.bwa.CEU.high_coverage.20100311.bam', you should change 'hg19.bam.extra' to 'NA12878.chrom.ILLUMINA.bwa.CEU.high_coverage.20100311.bam.extra', and change 'BAM_FILE' parameter in your config.txt to 'NA12878.chrom\\*.ILLUMINA.bwa.CEU.high_coverage.20100311.bam.\\*'. Then this additional file is automatically included. \n2) For hg38 or other genomes, we provided a python script **preprocessBAM.py** to generate this baseline BAM file from FASTA files. \nThen run this script by `python preprocessBAM.py <prefix> ./fasta`. A SAM file will be generated as '\\<prefix\\>.extra.sam'.\nThen use [Samtools](http://www.htslib.org/) to transform it to BAM file by:  \n`samtools -t fasta/chrall.fa.fai <prefix>.extra.sam > NA12878.chrom.ILLUMINA.bwa.CEU.high_coverage.20100311.bam.extra` \nNOTICE: \\<prefix\\> can be any name you give to the file. 'fasta/chrall.fa.fai' is the fasta index associated with all chromosomes.  \nIf you don't have this file in your './fasta' dir, please first concatenate all fasta files into one: \n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8026094213220734,
      "result": {
        "original_header": "Input",
        "type": "Text_excerpt",
        "value": "You need a **VCF** file and a **BAM** file to call CNVs from your sample. \n \n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8701790441292456,
      "result": {
        "original_header": "Preprocessing",
        "type": "Text_excerpt",
        "value": "2) For hg38 or other genomes, we provided a python script **preprocessBAM.py** to generate this baseline BAM file from FASTA files. \n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/WGLab/HadoopCNV/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "original_header": "License",
        "parent_header": [
          "HadoopCNV"
        ],
        "type": "Text_excerpt",
        "value": "[WGLab MIT License](http://wglab.mit-license.org) is used for HadoopCNV.\n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "HadoopCNV"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "WGLab"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Java",
        "size": 95489,
        "type": "Programming_language",
        "value": "Java"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Perl",
        "size": 7224,
        "type": "Programming_language",
        "value": "Perl"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 3774,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 689,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Run",
        "parent_header": [
          "HadoopCNV"
        ],
        "type": "Text_excerpt",
        "value": "In general, after you compile HadoopCNV succesfully and setup your Hadoop environment, it is very easy to run it:\n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "2) In general, there is no need to change any other parameters in the config.txt file.",
        "parent_header": [
          "HadoopCNV",
          "Run"
        ],
        "type": "Text_excerpt",
        "value": "In case you want to change anything, the only parameters you should play with are 'ABBERATION_PENALTY' and 'TRANSITION_PENALTY'. Increase 'ABBERATION_PENALTY' to make CNV calls more stringent. Increase 'TRANSITION_PENALTY' to make only longer calls to be considered.\n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "3) After all above is done, just run by `./run.sh`",
        "parent_header": [
          "HadoopCNV",
          "Run"
        ],
        "type": "Text_excerpt",
        "value": "This example will generate a CNV inference file called `results.txt`.\n\nThe output of the CNV inference file may look something like\n\n```\n[kaiwang@compute-0-0 results]$ head results.txt \nchr15   20000007        20000099        -0.6997057      2       2       0.0     0.0     0.0     0.0     0.0     0.0\nchr15   20000100        20000199        -0.39941147     2       2       0.0     0.0     0.0     0.0     0.0     0.0\nchr15   20000200        20000299        -0.14916627     2       2       0.0     0.0     0.0     0.0     0.0     0.0\nchr15   20000300        20000399        9.808615E-4     2       2       0.0     0.0     0.0     0.0     0.0     0.0\nchr15   20000400        20000499        0.051029906     2       2       0.0     0.0     0.0     0.0     0.0     0.0\nchr15   20000500        20000599        9.808615E-4     2       2       0.0     0.0     0.0     0.0     0.0     0.0\nchr15   20000600        20000699        0.10107895      2       2       0.0     0.0     0.0     0.0     0.0     0.0\n```\n\nThe columns are interpreted as chromosome name, start base for the bin, end base for the bin, median depth count (normalized to [-1.0,1.0]), HMM state (0=total deletion,1=single deletion,2=copy neutral LOH,3=normal,4=single copy amplification, 5 = double copy amplification), copy number, MSE_BAF_0, MSE_BAF_1, MSE_BAF_2, MSE_BAF_3, MSE_BAF_4, MSE_BAF_5. For more information regarding the last four fields, please refer to our manuscript. Briefly, these are mean squared errors for each of the four states conditional on the BAF model for that state. Lower values signify a better fit for that particular state's hypothesis.\n\nThen run\n```\nperl compile_penncnv3.pl < results.txt > out.cnv\n```\nto generate the final output CNV file, which looks like:\n```\ndeletion\tchr15:21101500-21299999\tCN:1\ndeletion\tchr15:25309100-25337899\tCN:1\ndeletion\tchr15:27340000-27340999\tCN:1\ndeletion\tchr15:28342100-28360899\tCN:1\ndeletion\tchr15:33022100-33520999\tCN:1\ndeletion\tchr15:37597000-37605899\tCN:1\ndeletion\tchr15:40612700-40613899\tCN:1\n\n```\n\nThe first column indicates the type of CNVs (deletion, duplication or loh), the second column is in \"chromosome:start-end\" format, the third column indicates the copy number.\n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "download",
    "requirements",
    "contact",
    "contributors",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-11-04 06:44:54",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 18
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Example",
        "parent_header": [
          "HadoopCNV"
        ],
        "type": "Text_excerpt",
        "value": "After initialization, to run HadoopCNV, please check our example first:\n[Example](docs/user-guide/startup.md)\n"
      },
      "source": "https://raw.githubusercontent.com/WGLab/HadoopCNV/master/README.md",
      "technique": "header_analysis"
    }
  ]
}