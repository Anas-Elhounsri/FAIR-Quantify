{
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/bigdatagenomics/deca"
      },
      "technique": "GitHub_API"
    }
  ],
  "contributing_guidelines": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "How to contribute to ADAM\n=========================\n\nThank you for sharing your code with the ADAM project. We appreciate your contribution!\n\n## Join the mailing list and our IRC channel\n\nIf you're not already on the ADAM developers list, [take a minute to join](http://bigdatagenomics.github.io/mail/).\nIt would be great if you'd introduce yourself to the group but it's not required. You can just\nlet your code do the talking for you if you like.\n\nYou can find us on Freenode IRC in the #adamdev room.\n\n## Check the issue tracker and pull requests\n\nBefore you write too much code, check the [open issues in the ADAM issue tracker](https://github.com/bigdatagenomics/adam/issues?state=open)\nor [open pull requests](https://github.com/bigdatagenomics/adam/pulls) to see if someone else is already working on it. If the issue is\na new one, and you've already written the code to address the issue, simply [submit a pull request](https://help.github.com/articles/creating-a-pull-request).\nIf you haven't written any code but plan to, go ahead and [open a new issue](https://github.com/bigdatagenomics/adam/issues/new) and assign\nit to yourself to socialize your work. When you submit your PR, please reference this pull request so that it will be automatically closed\nwhen the PR is merged.\n\nThe issue tracker has a list of [\"pick me up!\"](https://github.com/bigdatagenomics/adam/issues?labels=pick+me+up%21&page=1&state=open) issues\nthat are good tasks to help you get familiar with ADAM; they don't require an understanding of genomics and are fairly\nlimited in scope.\n\n## Announce your work on the mailing list\n\nShoot us a quick email on the mailing list letting us know what you're working on. There\nwill likely be people on the list who can give you tips about where to find relevant \nsource or alert you to other planned changes that might effect your work.\n\nIf the work you're proposing makes substantive changes to ADAM, you may be asked to attach a design document\nto your issue in the issue tracker. This document should provide a high-level explanation of your design, clearly define the goal\nof the new design and explain the expected effects on performance, APIs, etc. This document is meant to save you time\nas it allows the team a chance to provide feedback on the proposes changes. It's likely we can help you find a way\nto achieve your goals with less work. The document also allows the team to prepare for large changes to the code\nbase. We welcome change but also want to ensure that code quality is kept high.\n\n## Submit your pull request\n\nGithub provides a nice [overview on how to create a pull request](https://help.github.com/articles/creating-a-pull-request).\n\nSome general rules to follow:\n\n* Do your work in [a fork](https://help.github.com/articles/fork-a-repo) of the ADAM repo.\n* Create a branch for each feature/bug in ADAM that you're working on. These branches are often called \"feature\"\nor \"topic\" branches.\n* Use your feature branch in the pull request. Any changes that you push to your feature branch will automatically\nbe shown in the pull request.  If your feature branch is not based off the latest master, you will be asked to rebase\nit before it is merged. This ensures that the commit history is linear, which makes the commit history easier to read.\n* Remember to reference any issues that your pull request fixes in the commit message. This will ensure that the issue\nis automatically closed when the PR is merged.\n* Run the ./scripts/format-source script in order to format the code and ensure correct license headers\n  ```\n  $ ./scripts/format-source\n  ```\n* Please alphabetize your imports. We follow the following approach: first, alphabetize by package name (e.g., `org.apache.spark`\nshould be before `org.bigdatagenomics.adam`). Within a project, \"lower level\" packages should be sorted ahead (e.g.,\n`org.apache.spark.SparkContext` should be before `org.apache.spark.rdd.RDD`). Within a single package, sort alphabetically,\nbut put object implicit imports first (e.g., put `org.apache.spark.SparkContext._` before `org.apache.spark.Logging`, and\n`org.apache.spark.Logging` before `org.apache.spark.SparkContext`).\n* Keep your pull requests as small as possible. Large pull requests are hard to review. Try to break up your changes\ninto self-contained and incremental pull requests, if need be, and reference dependent pull requests, e.g. \"This pull\nrequest builds on request #92. Please review #92 first.\"\n* The first line of commit messages should start by referencing the issue number they fix (i.e., \"[ADAM-307]\" indicates that\nthis commit fixes ADAM issue #307), followed by a short (<80 character) summary, followed by an empty line and then,\noptionally, any details that you want to share about the commit.\n* Include unit tests with your pull request. We love tests and [use Jenkins](https://amplab.cs.berkeley.edu/jenkins/)\nto check every pull request and commit. Just look for files in the ADAM repo that end in \"*Suite.scala\", \ne.g. ADAMContextSuite.scala, to see examples of how to write tests. You might also want to glance at the \n`./scripts/jenkins-test` script for more end-to-end tests.\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/CONTRIBUTING.md",
      "technique": "file_exploration"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2017-08-24T20:13:06Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2022-09-29T06:48:33Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Distributed exome CNV analyzer. Apache 2 licensed."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Introduction",
        "type": "Text_excerpt",
        "value": "DECA is a distributed re-implementation of the [XHMM](https://atgu.mgh.harvard.edu/xhmm/) exome CNV caller using ADAM and Apache Spark.\n\nIf you use DECA please cite:\n\nLinderman MD, Chia D, Wallace F, Nothaft FA. DECA: scalable XHMM exome copy-number variant calling with ADAM and Apache Spark. BMC Bioinformatics. 2019;20:493. [doi:10.1186/s12859-019-3108-7](https://doi.org/10.1186/s12859-019-3108-7).\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "format": "wiki",
        "type": "Url",
        "value": "https://github.com/bigdatagenomics/deca/wiki"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/bigdatagenomics/deca/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/bigdatagenomics/deca/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "bigdatagenomics/deca"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "DECA: Distributed Exome CNV Analyzer"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/scripts/emr_bootstrap.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/scripts/xhmm_discover.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/scripts/xhmm_normalize.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/scripts/release/rollback.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/scripts/release/release.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/docs/build.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "parent_header": [
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": "**Note**: These instructions are shared with other tools that build on [ADAM](https://github.com/bigdatagenomics/adam).\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Building from Source",
        "parent_header": [
          "Getting Started",
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "You will need to have [Maven](http://maven.apache.org/) installed in order to build DECA.\n\n> **Note:** The default configuration is for Hadoop 2.7.3. If building against a different\n> version of Hadoop, please edit the build configuration in the `<properties>` section of\n> the `pom.xml` file.\n\n```dtd\n$ git clone https://github.com/.../deca.git\n$ cd deca\n$ export MAVEN_OPTS=\"-Xmx512m\"\n$ mvn clean package\n```\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Installing Spark",
        "parent_header": [
          "Getting Started",
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "You'll need to have a Spark release on your system and the `$SPARK_HOME` environment variable pointing at it; prebuilt binaries can be downloaded from the\n[Spark website](http://spark.apache.org/downloads.html). DECA has been developed and tested with\n[Spark 2.1.0 built against Hadoop 2.7 with Scala 2.11](http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz), but any more recent Spark distribution should likely work.\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/bigdatagenomics/deca/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License",
        "type": "Text_excerpt",
        "value": "DECA is released under an [Apache 2.0 license](LICENSE.txt).\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "deca"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "bigdatagenomics"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Scala",
        "size": 122602,
        "type": "Programming_language",
        "value": "Scala"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "R",
        "size": 14053,
        "type": "Programming_language",
        "value": "R"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 11846,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Java",
        "size": 2412,
        "type": "Programming_language",
        "value": "Java"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Running DECA in \"stand-alone\" mode on a workstation",
        "parent_header": [
          "Example Usage"
        ],
        "type": "Text_excerpt",
        "value": "A small dataset (30 samples by 300 targets) is distributed as part of the [XHMM tutorial](http://atgu.mgh.harvard.edu/xhmm/tutorial.shtml). \nAn example DECA command to call CNVs from the [pre-computed read-depth matrix and related files](http://atgu.mgh.harvard.edu/xhmm/RUN.zip)\non a 16-core workstation with 128 GB RAM is shown below; a step-by-step listing of the commands to call CNVs in the tutorial data is available [here](TUTORIAL.md). Note that you will need to set the `DECA_JAR` environment variable to point to the jar file created by `mvn package`, set `spark.local.dir` to a suitable temporary directory for your system and likely need to change the executor and driver memory to suitable values for your system. The `exclude_targets.txt` and `DATA.RD.txt` files from the XHMM tutorial data are also distributed as part of the DECA test resources in the `deca-core/src/test/resources/` directory.\n\nFrom within the unzip'd RUN directory, prepare `exclude_targets.txt`:\n\n```\ncat low_complexity_targets.txt extreme_gc_targets.txt | sort -u > exclude_targets.txt\n```\n\nthen run DECA:\n\n```dtd\ndeca-submit \\\n--master local[16] \\\n--driver-class-path $DECA_JAR \\\n--conf spark.local.dir=/path/to/temp/directory \\\n--conf spark.driver.maxResultSize=0 \\\n--conf spark.kryo.registrationRequired=true \\\n--driver-memory 16G \\\n-- normalize_and_discover \\\n-min_some_quality 29.5 \\\n-exclude_targets exclude_targets.txt \\\n-I DATA.RD.txt \\\n-o DECA.gff3\n```\n\nThe resulting [GFF3](https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md) file should contain\n\n```dtd\n22      HG00121 DEL     18898402        18913235        9.167771318038923       .       .       END_TARGET=117;START_TARGET=104;Q_SOME=90;Q_START=8;Q_STOP=4;Q_EXACT=9;Q_NON_DIPLOID=90\n22      HG00113 DUP     17071768        17073440        25.32122306047942       .       .       END_TARGET=11;START_TARGET=4;Q_SOME=99;Q_START=53;Q_STOP=25;Q_EXACT=25;Q_NON_DIPLOID=99\n```\n\nThe `exlude_targets.txt` file is the unique combination of the `extreme_gc_targets.txt` and `low_complexity_targets.txt` \nfiles provided in the tutorial data. The `min_some_quality` parameter is set to 29.5 to mimic XHMM behavior which uses a \ndefault minimum SOME quality of 30 *after* rounding (while DECA applies the filter prior to rounding). Depending on your particular\ncomputing environment, you may need to modify the [spark-submit](https://spark.apache.org/docs/latest/submitting-applications.html) \n[configuration parameters](https://spark.apache.org/docs/latest/configuration.html). `spark.driver.maxResultSize` is set to 0 (unlimited) \nto address errors collecting larger amounts of data to the driver.\n\nThe corresponding xcnv output from XHMM is:\n\n```dtd\nSAMPLE  CNV     INTERVAL        KB      CHR     MID_BP  TARGETS NUM_TARG        Q_EXACT Q_SOME  Q_NON_DIPLOID   Q_START Q_STOP  MEAN_RD MEAN_ORIG_RD\nHG00121 DEL     22:18898402-18913235    14.83   22      18905818        104..117        14      9       90      90      8       4       -2.51   37.99\nHG00113 DUP     22:17071768-17073440    1.67    22      17072604        4..11   8       25      99      99      53      25      4.00    197.73\n```\n\nView a [recording](https://i.imgur.com/gp0D4B1.gifv) of the above installation and CNV calling workflow executed on OSX.\n\nTo call CNVs from the original [BAM files](http://atgu.mgh.harvard.edu/xhmm/EXAMPLE_BAMS.zip) compute the coverage:\n\n```dtd\ndeca-submit \\\n--master local[16] \\\n--driver-class-path $DECA_JAR \\\n--conf spark.local.dir=/path/to/temp/directory \\\n--conf spark.driver.maxResultSize=0 \\\n--conf spark.kryo.registrationRequired=true \\\n--driver-memory 16G \\\n-- coverage \\\n-L EXOME.interval_list \\\n-I *.bam \\\n-o DECA.RD.txt\n```\n\nfollowed by the `normalize_and_discovery` command above (with `DECA.RD.txt` as the input). DECA's coverage calculation is \ndesigned to match the output of the GATK DepthOfCoverage command specified in the XHMM protocol, i.e. count fragment depth with \nzero minimum base quality.\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Running DECA on a YARN cluster",
        "parent_header": [
          "Example Usage"
        ],
        "type": "Text_excerpt",
        "value": "The equivalent example command to call CNVs on a YARN cluster with Spark dynamic allocation would be:\n\n```\ndeca-submit \\\n\t--master yarn \\\n\t--deploy-mode cluster \\\n\t--num-executors 1 \\\n\t--executor-memory 72G \\\n\t--executor-cores 5 \\\n\t--driver-memory 72G \\\n\t--driver-cores 5 \\\n\t--conf spark.driver.maxResultSize=0 \\\n\t--conf spark.yarn.executor.memoryOverhead=4096 \\\n\t--conf spark.yarn.driver.memoryOverhead=4096 \\\n\t--conf spark.kryo.registrationRequired=true \\\n\t--conf spark.hadoop.mapreduce.input.fileinputformat.split.minsize=$(( 8 * 1024 * 1024 )) \\\n\t--conf spark.default.parallelism=10 \\\n\t--conf spark.dynamicAllocation.enabled=true \\\n\t-- normalize_and_discover \\\n\t-min_partitions 10 \\\n\t-exclude_targets \"hdfs://path/to/exclude_targets.txt\" \\\n\t-min_some_quality 29.5 \\\n\t-I \"hdfs://path/to/DATA.RD.txt\" \\\n\t-o \"hdfs://path/to/DECA.gff3\"\n```\n\nNote that many of the parameters above, e.g. driver and executor cores and memory, are specific to a particular cluster \nenvironment and would likely need to be modified for other environments.\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Running DECA on AWS with Elastic MapReduce",
        "parent_header": [
          "Example Usage"
        ],
        "type": "Text_excerpt",
        "value": "DECA can readily be run on Amazon AWS using the Elastic MapReduce (EMR) [Spark configuration](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark.html). Data can be read from and written to S3 using the s3a:// scheme. For example, the 1000 Genomes data are available as a [public dataset](https://aws.amazon.com/1000genomes/) on S3 in the `1000genomes` bucket (i.e. `s3a://1000genomes/...`). S3a is an overlay over the AWS Simple Storage System (S3) cloud data store which is provided by Apache Hadoop.\n\nNote that unlike HDFS, S3 is an eventually-consistent filesystem and so you may encounter problems when trying to read recently written files, such as occurs at the end of the DECA operations when combining sharded files. When writing to S3 use the `-multi_file` option to leave the files sharded for subsequent combination or analysis.\n\nDECA has been tested with emr-5.12.2. Clusters can be created with the command-line tools or the AWS management console. A JSON file [`emr_config.json`](scripts/emr_config.json) is provided in the scripts directory to configure clusters for maximum resource utilization.\n\nA bootstrap script [`emr_bootstrap.sh`](scripts/emr_bootstrap.sh) is provided in the scripts directory for use as a [bootstrap action](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html). The bootstrap script can copy a pre-built JAR onto the cluster (faster) or build DECA directly from GitHub (slower). To use the bootstrap script, copy it to S3 and provide the S3 path as the bootstrap action when creating the cluster. To copy a pre-built JAR onto the cluster provide a s3 path to the DECA CLI jar, e.g. `s3://path/to/deca-cli_2.11-0.2.1-SNAPSHOT.jar`, as the optional argument to bootstrap action. After connecting to the EMR master node via SSH, you can launch DECA as you would on any YARN cluster. For example the following command calls CNVs in the entire 1000 Genomes phase 3 cohort on a cluster of i3.2xlarge nodes.\n\n```\ndeca-submit \\\n    --master yarn \\\n    --deploy-mode cluster \\\n    --num-executors 7 \\\n    --executor-memory 22G \\\n    --executor-cores 4 \\\n    --driver-memory 22G \\\n    --driver-cores 5 \\\n    --conf spark.driver.maxResultSize=0 \\\n    --conf spark.kryo.registrationRequired=true \\\n    --conf spark.dynamicAllocation.enabled=false \\\n    --conf spark.hadoop.mapreduce.input.fileinputformat.split.minsize=$(( 104 * 1024 * 1024 )) \\\n    --conf spark.default.parallelism=28 \\\n    --conf spark.executor.extraJavaOptions=\"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35\" \\\n    -- normalize_and_discover \\\n    -min_partitions 28 \\\n    -exclude_targets \"s3a://path/to/20130108.exome.targets.exclude.txt\" \\\n    -min_some_quality 29.5 \\\n    -print_metrics \\\n    -I \"s3a://path/to/DATA.2535.RD.txt\" \\\n    -o \"s3a://path/to/DATA.2535.RD.gff3\" \\\n    -multi_file\n```\n\nAlternately jobs can be launched as steps on cluster. In this approach, no bootstrap actions are needed; the JAR file can be downloaded directly from S3. The `spark-submit` arguments are:\n\n```\n--class org.bdgenomics.deca.cli.DecaMain\n--conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n--conf spark.kryo.registrator=org.bdgenomics.deca.serialization.DECAKryoRegistrator\n```\n\n(in addition to the arguments shown above) and the application arguments would be\n\n```\nnormalize_and_discover\n-min_partitions 28\n-exclude_targets s3a://path/to/20130108.exome.targets.exclude.txt\n-min_some_quality 29.5\n-print_metrics\n-I s3a://path/to/DATA.2535.RD.txt\n-o s3a://path/to/DATA.2535.RD.gff3\n-multi_file\n```\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Running DECA on Databricks",
        "parent_header": [
          "Example Usage"
        ],
        "type": "Text_excerpt",
        "value": "DECA can readily be run on [Databricks](https://databricks.com) on the Amazon cloud. DECA has been tested on Databricks Light 2.4 as a spark-submit job using the DECA jar fetched from a S3 bucket. As with EMR, data can be read from and written to S3 using the s3a:// scheme. The Databricks cluster was configured to access S3 via [AWS IAM roles](https://docs.databricks.com/administration-guide/cloud-configurations/aws/iam-roles.html#secure-access-to-s3-buckets-using-iam-roles). Note that access to any public buckets, e.g. the 1000genomes bucket, must also be included in the cross account IAM role created according to the above instructions. The same issues with eventual consistency described above also apply when writing data to S3 from the Databricks cluster.\n\nAn example configuration for calling CNVs directly from the original BAM files:\n\n```json\n[\n  \"--class\", \"org.bdgenomics.deca.cli.DecaMain\",\n  \"--conf\", \"spark.serializer=org.apache.spark.serializer.KryoSerializer\",\n  \"--conf\", \"spark.kryo.registrator=org.bdgenomics.deca.serialization.DECAKryoRegistrator\",\n  \"--conf\", \"spark.kryo.registrationRequired=true\",\n  \"--conf\", \"spark.hadoop.fs.s3.impl=com.databricks.s3a.S3AFileSystem\",\n  \"--conf\", \"spark.hadoop.fs.s3a.impl=com.databricks.s3a.S3AFileSystem\",\n  \"--conf\", \"spark.hadoop.fs.s3n.impl=com.databricks.s3a.S3AFileSystem\",\n  \"--conf\", \"spark.hadoop.fs.s3a.canned.acl=BucketOwnerFullControl\",\n  \"--conf\", \"spark.hadoop.fs.s3a.acl.default=BucketOwnerFullControl\",\n  \"--conf\", \"spark.hadoop.mapreduce.input.fileinputformat.split.minsize=536870912\",\n  \"s3://path/to/deca-cli_2.11-0.2.1-SNAPSHOT.jar\",\n  \"cnv\",\n  \"-L\", \"s3a://path/to/20130108.exome.targets.filtered.interval_list\",\n  \"-I\", \"s3a://path/to/1kg.bams.50.list\",\n  \"-l\",\n  \"-o\", \"s3a://path/to/DECA.50.gff3\",\n  \"-multi_file\"\n]\n```\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "download",
    "requirements",
    "contact",
    "contributors",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-11-04 02:36:36",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 3
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Building from Source",
        "parent_header": [
          "Getting Started",
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "You will need to have [Maven](http://maven.apache.org/) installed in order to build DECA.\n\n> **Note:** The default configuration is for Hadoop 2.7.3. If building against a different\n> version of Hadoop, please edit the build configuration in the `<properties>` section of\n> the `pom.xml` file.\n\n```dtd\n$ git clone https://github.com/.../deca.git\n$ cd deca\n$ export MAVEN_OPTS=\"-Xmx512m\"\n$ mvn clean package\n```\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Helpful Scripts",
        "parent_header": [
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": "The `bin/deca-submit` script wraps the `spark-submit` commands to set up and launch DECA.\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Commands",
        "parent_header": [
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": "```dtd\n$ deca-submit\n\nUsage: deca-submit [<spark-args> --] <deca-args> [-version]\n\nChoose one of the following commands:\n\n             normalize : Normalize XHMM read-depth matrix\n              coverage : Generate XHMM read depth matrix from read data\n              discover : Call CNVs from normalized read matrix\nnormalize_and_discover : Normalize XHMM read-depth matrix and discover CNVs\n                   cnv : Discover CNVs from raw read data\n\n```\nYou can learn more about a command, by calling it without arguments or with `--help`, e.g.\n\n```dtd\n$ deca-submit normalize_and_discover --help\n -I VAL                   : The XHMM read depth matrix\n -cnv_rate N              : CNV rate (p). Defaults to 1e-8.\n -exclude_targets STRING  : Path to file of targets (chr:start-end) to be excluded from analysis\n -fixed_pc_toremove INT   : Fixed number of principal components to remove if defined. Defaults to undefined.\n -h (-help, --help, -?)   : Print help\n -initial_k_fraction N    : Set initial k to fraction of max components. Defaults to 0.10.\n -max_sample_mean_RD N    : Maximum sample mean read depth prior to normalization. Defaults to 200.\n -max_sample_sd_RD N      : Maximum sample standard deviation of the read depth prior to normalization. Defaults to 150.\n -max_target_length N     : Maximum target length. Defaults to 10000.\n -max_target_mean_RD N    : Maximum target mean read depth prior to normalization. Defaults to 500.\n -max_target_sd_RD_star N : Maximum target standard deviation of the read depth after normalization. Defaults to 30.\n -mean_target_distance N  : Mean within-CNV target distance (D). Defaults to 70000.\n -mean_targets_cnv N      : Mean targets per CNV (T). Defaults to 6.\n -min_partitions INT      : Desired minimum number of partitions to be created when reading in XHMM matrix\n -min_sample_mean_RD N    : Minimum sample mean read depth prior to normalization. Defaults to 25.\n -min_some_quality N      : Min Q_SOME to discover a CNV. Defaults to 30.0.\n -min_target_length N     : Minimum target length. Defaults to 10.\n -min_target_mean_RD N    : Minimum target mean read depth prior to normalization. Defaults to 10.\n -o VAL                   : Path to write discovered CNVs as GFF3 file\n -print_metrics           : Print metrics to the log on completion\n -save_zscores STRING     : Path to write XHMM normalized, filtered, Z score matrix\n -zscore_threshold N      : Depth Z score threshold (M). Defaults to 3.\n```\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Using native library algebra libraries",
        "parent_header": [
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": "Apache Spark includes the [Netlib-Java](https://github.com/fommil/netlib-java) library for high-performance linear algebra. \nNetlib-Java can invoke optimized BLAS and Lapack system libraries if available; however, many Spark distributions are built \nwithout Netlib-Java system library support. You may be able to use system libraries by including the DECA jar on \nthe Spark driver classpath when running locally, e.g.\n\n```dtd\ndeca-submit --driver-class-path $DECA_JAR ...\n```\nor you may need to rebuild Spark as described in the [Spark MLlib guide](http://spark.apache.org/docs/2.1.0/ml-guide.html). \n\nIf you see the following warning messages in the log file, you have not successfully invoked the system libraries:\n\n```dtd\nWARN  BLAS:61 - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\nWARN  BLAS:61 - Failed to load implementation from: com.github.fommil.neltlib.NativeRefARPACK\n```\n\nTo build DECA with the optimized netlib native code in, you will need to invoke the `native-lgpl` profile when running Maven:\n\n```\nmvn package -P native-lgpl\n```\n\nWe cannot package this code by default, as netlib is licensed under the LGPL and cannot be bundled in Apache 2 licensed code.\n"
      },
      "source": "https://raw.githubusercontent.com/bigdatagenomics/deca/master/README.md",
      "technique": "header_analysis"
    }
  ]
}