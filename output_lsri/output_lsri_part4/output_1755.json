{
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/jade-cheng/ohana"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2016-04-28T14:29:10Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-21T11:53:52Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "mixture classification, constraint optimization, outlier detection, population structure, admixture history, and selection detection."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Introduction",
        "parent_header": [
          "Ohana"
        ],
        "type": "Text_excerpt",
        "value": "`Ohana` is a suite of software for analyzing population structure and admixture history using unsupervised learning methods.  We construct statistical models to infer individual clustering from which we identify outliers for selection analyses.\n\nThis project was directed by [Dr. Rasmus Nielsen](https://nielsen-lab.github.io/research/) at University of California Berkeley.  [Jade Cheng](http://www.jade-cheng.com/) was funded by the Bioinformatics, Computer Science Department at Aarhus University and the Natural History Museum of Denmark at Copenhagen University.\n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "convert",
        "parent_header": [
          "Ohana",
          "Description"
        ],
        "type": "Text_excerpt",
        "value": "To facilitate different stages of the analysis, we provide several conversion subroutines. `ped2dgm` converts genotype observations from the plink format to feed into `qpas`. `bgl2lgm` converts genotype likelihoods from the beagle format to feed into `qpas`. `cov2nwk` first converts a covariance matrix to a distance matrix, then it implements the Neighbor Joining algorithm to approximate the distance matrix into a Newick tree. `nwk2svg` produces a scalar vector graphics representation of the Newick tree.  The output can be viewed with web browsers and modified with graphics editors like Inkscape.  Finally, if a tree-compatible covariance matrix is desired for `selscan`, we have `nwk2cov` to converts a Newick tree to a covariance matrix.\n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "qpas and cpax",
        "parent_header": [
          "Ohana",
          "Description"
        ],
        "type": "Text_excerpt",
        "value": "Under the assumption of Hardy Weinberg Equilibrium, the likelihood of assigning an observed genotype `g` in individual `i` at locus `j` to ancestral component `k` is a function of the allelic frequency `f_kj` of the locus at `k` and the fraction of the genome of the individual `q_ik` that comes from that component.  We thus consider the likelihood of the ancestral component proportions vector `Q` and their vector of allele frequencies `F`.  In particular, if we denote `K` as the number of ancestry components, `I` as the number of individuals, and `J` as the number of polymorphic sites among the `I` individuals, then the log likelihood of observing the genotype is:\n\n    sum_i sum_j {\n      g_ij * ln[sum_k (q_ik * f_kj)] +\n      (2 - g_ij) * ln[sum_k (q_ik * (1 - f_kj))]\n    }\n\nTo estimate `Q` and `F`, we apply a Newton-style optimization method using quadratic programming through the active set algorithm.  `qpas` operates by solving equality-constraint quadratic problems using the Karush-Kuhn-Tucker algorithm, which is a nonlinear programming generalization of the Lagrange multiplier method.  `cpax` operates through complementarity pivoting using Lemke's algorithm.\n\nLeveraging the block structure of the hessian matrices for `Q` and `F`, we decompose the problem into a sequence of small-matrix manipulations rather than managing one large linear system.  This allows us to update `Q`, row after row, and `F`, column after column.  The optimization task, therefore, becomes feasible.\n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "nemeco",
        "parent_header": [
          "Ohana",
          "Description"
        ],
        "type": "Text_excerpt",
        "value": "We model the joint distribution of allele frequencies across all ancestral components as a multivariate Gaussian distribution.  The likelihood denotes the probability of observing the given allele frequencies.  The variances and covariances of the multivariate Gaussian are determined by a product term of two factors.  The first factor is site-specific, and the second factor records the ancestral components' variances and covariances relation.\n\n    P(f_j | C, u_j) ~ N(u_j * (1 - u_j) * C)\n\nWe denote `C` as the covariance matrix to be inferred.  We denote the allele frequency at site `j` as `f_j` and average major allele frequency at site `j` as `u_j`.  We root the covariance matrix to avoid multiple likelihood optima and obtain `C'`.  We formulate the log likelihood analytically as the following:\n\n    -0.5 * sum_j {\n      (K - 1) * ln(2 * pi * c_j) +\n      ln(det(C')) +\n      (1/c_j) * (f_j')^T * (1/C') * f_j'\n    }\n    c_j = u_j * (1 - u_j)\n    f_j' = f_j - f_j0\n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "nemetree",
        "parent_header": [
          "Ohana",
          "Description"
        ],
        "type": "Text_excerpt",
        "value": "`nemetree` is a web service dedicated to delivering clean, accurate, and presentation-ready visualizations of phylogenetic trees. To find an appealing arrangement of a tree, `nemetree` takes inspiration from an electrostatic field and models tree components as like-signed charged particles with nodes constrained by the branches that connect them. `nemetree` then utilizes the Nelder-Mead algorithm to minimize the total potential energy of this system and achieve an optimal tree layout. `nemetree` allows users to specify tree structures in Newick format and adjust the model and rendering configuration through a JSON editor. `nemetree` animates the progression of the optimization and provides a method to pause and resume the process. All rendered trees may be downloaded as SVG.  What you see is what you get at http://www.jade-cheng.com/trees/\n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "selscan",
        "parent_header": [
          "Ohana",
          "Description"
        ],
        "type": "Text_excerpt",
        "value": "We scan for covariance outliers by applying a likelihood model to each locus, similar to the one used genome-wide but with certain scalar factor variations. This creates a nested likelihood model.  Through a likelihood ratio test, it identifies loci in which the variance among components is larger than expected from the genome-wide estimated covariance matrix.\n\nThis program takes two input matrices, the `c matrix` and `c-scale matrix`.  These two matrices provide the minimum and maximum values of the optimization and the interpolation is used to define how to optimize multiple values at the same time using a single parameter.  In this way the same framework can be used for both optimization of additive and multiplicative models.  If `-cs` option is not supplied, by default, the programs uses a `c-scale matrix` that is 10 times of the `c matrix`.\n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9939838987713082,
      "result": {
        "original_header": "About me",
        "type": "Text_excerpt",
        "value": "I immigrated from China to Hawaii, where I met my husband.  We lived in Honolulu for eight years.  During that time, I earned two degrees from the University of Hawai\u02bbi at M\u0101noa.  My MS in CS paved the way to my PhD program in Denmark and later the birth of this project. \n*Ohana* means *extended family* in Hawaiian, and the word seems fitting to represent this project since it couldn't exist without the love and support of the friends, family, and colleagues who joined me on this journey many moons ago.\n \n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9757503151220877,
      "result": {
        "original_header": "Workflow",
        "type": "Text_excerpt",
        "value": "A typical workflow of genetic data analysis using `Ohana` starts with structure inference with `qpas` using either genotype observations or genotype likelihoods.  For genotype observations, we first prepare the data with Plink including the `--recode12 --geno 0.0 --tab` flags.  We then convert the .ped file to a .dgm file, which is used by `qpas`. \nWe approximate the estimated covariance matrix into a component tree using `cov2nwk`, and produce a visualization of the tree using `nwk2svg`.  For a better visualization of the inferred Newick tree, copy the nwk file contents and visit `nemetree` at http://www.jade-cheng.com/trees/ \nWith the inferred component covariances and allele frequencies, we scan for covariance outliers.  The standard output from `selscan` contains four columns, the scalar value when local best likelihood is reached, the local log likelihood obeying global covariances, the local optimal log likelihood, and the likelihood ratio of this locus.  The total number and order of loci match with the input genotype data. \nThe workflow described above is for demonstration purposes only.  For real data analysis, the structure inference using `qpas` would include many fewer loci than a selection scan using `selscan`, which would require multiple millions of loci.  What's shown above, a scan of allele frequencies produced directly from the structure analysis, would be inadequate.\n \n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "format": "wiki",
        "type": "Url",
        "value": "https://github.com/jade-cheng/ohana/wiki"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/jade-cheng/ohana/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 6
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/jade-cheng/ohana/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "jade-cheng/ohana"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Ohana"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "parent_header": [
          "Ohana"
        ],
        "type": "Text_excerpt",
        "value": "The `Ohana` source code is available on [github](https://github.com/jade-cheng/ohana). Building `Ohana` from source requires a UNIX development environment with [Make](https://www.gnu.org/software/make/), a [C++ compiler](https://gcc.gnu.org/), and [BLAS](http://www.netlib.org/blas/)/[LAPACK](http://www.netlib.org/lapack/) libraries.\n\nOn macOS, the BLAS/LAPACK libraries come preinstalled with the operating system through the [Accelerate Framework](https://developer.apple.com/documentation/accelerate), so no prerequisite steps are required beyond installing Xcode, which provides the necessary tools to build software from the terminal. See [Apple&rsquo;s documentation](https://developer.apple.com/xcode/) for more information.\n\nOn Linux, the development tools and libraries may need to be installed explicitly. For example, on Debian-based systems, these packages should be installed:\n\n    # Debian/Ubuntu distributions only:\n    $ sudo apt install git make g++ libblas-dev liblapacke-dev\n\nOnce the development environment is prepared, `Ohana` can be downloaded and built by following these steps:\n\n    $ git clone https://github.com/jade-cheng/ohana\n    $ cd ./ohana\n    $ make\n\nThis creates several programs in the `./bin/*` directory, which are described in later sections of this documentation.\n\n    $ ls ./bin\n    convert\n    cpax\n    filter\n    nemeco\n    neoscan\n    qpas\n    selscan\n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9999870530481136,
      "result": {
        "original_header": "Workflow",
        "type": "Text_excerpt",
        "value": "    iter   duration   log-likelihood   delta-lle\n    0      0.188569   -5.871533e+06\n    1      0.440765   -2.328232e+06    3.543301e+06\n    2      0.474969   -2.286454e+06    4.177773e+04\n    3      0.444536   -2.241289e+06    4.516513e+04\n    4      0.437131   -2.210211e+06    3.107805e+04\n    5      0.431438   -2.186895e+06    2.331641e+04 \n    iter   duration    log-likelihood   delta-lle\n    0      0.381237    -6.925665e+06\n    1      1.647531    -5.092075e+06    1.833590e+06\n    2      1.572898    -5.018263e+06    7.381209e+04\n    3      1.547442    -4.983535e+06    3.472794e+04\n    4      1.533596    -4.952988e+06    3.054679e+04\n    5      1.529023    -4.939365e+06    1.362250e+04 \n    $ nemeco ./g.lgm ./f.matrix -co c.matrix -mi 5\n    iter   duration   delta-lle      log-likelihood\n    1      0.002092   0.000000e+00   -1.693898e+04\n    2      0.000860   2.341308e+04   6.474094e+03\n    3      0.000859   8.106349e+03   1.458044e+04\n    4      0.000857   0.000000e+00   1.458044e+04\n    5      0.000857   0.000000e+00   1.458044e+04 \n    $ nemeco ./g.dgm ./f.matrix -co ./c.matrix -mi 5\n    iter   duration   delta-lle      log-likelihood\n    1      0.001374   0.000000e+00   -8.439035e+03\n    2      0.000561   1.353884e+04   5.099802e+03\n    3      0.000582   5.148315e+03   1.024812e+04\n    4      0.000549   0.000000e+00   1.024812e+04\n    5      0.000557   0.000000e+00   1.024812e+04 \n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8134199018189462,
      "result": {
        "original_header": "Workflow",
        "type": "Text_excerpt",
        "value": "    python ./tools/plot_q.py ./q.matrix ./q-bar-chart.pdf \n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/jade-cheng/ohana/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "original_header": "License",
        "parent_header": [
          "Ohana"
        ],
        "type": "Text_excerpt",
        "value": "Copyright (c) 2015-2020 Jade Cheng<br>\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n* Neither the name of Jade Cheng nor the names of her contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nThis software is provided by the copyright holders and contributors \"as is\" and any express or implied warranties, including, but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. in no event shall Jade Cheng be liable for any direct, indirect, incidental, special, exemplary, or consequential damages (including, but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) arising in any way out of the use of this software, even if advised of the possibility of such damage.\n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "ohana"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "jade-cheng"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "C++",
        "size": 686365,
        "type": "Programming_language",
        "value": "C++"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Makefile",
        "size": 33459,
        "type": "Programming_language",
        "value": "Makefile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 2744,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "usage",
    "faq",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-11-04 08:11:10",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 80
      },
      "technique": "GitHub_API"
    }
  ],
  "support": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Selection study",
        "parent_header": [
          "Ohana"
        ],
        "type": "Text_excerpt",
        "value": "If selection study is the goal, the first step should be to obtain a full genome dataset of high quality.  Then a subset of this data, ~100 Kbp unlinked markers, should be used for the structure analysis with `qpas`.  After that, the `-fq` and `-qi` options should be used for `qpas` to produce admixture-corrected allele frequencies for the full genome dataset.  Finally, it should be possible to proceed to the selection study with full genome allele frequencies.\n\n    $ qpas ./g_subset.lgm -k 4 -qo ./q_subset.matrix -fo ./f_subset.matrix -mi 5\n    :\n    $ nemeco ./g_subset.lgm ./f_subset.matrix -mi 5 -co ./cout.matrix\n    :\n    $ qpas ./g_full.dgm -k 4 -qi ./q_subset.matrix -fo ./f_full.matrix -mi 5 -fq\n    :\n    $ selscan ./g_full.dgm ./f_full.matrix cout.matrix\n    :\n\nFor faster analysis and smaller memory footprints, we recommend performing the last two steps in parallel.  For example, suppose we have the full genome dataset `HGDP.ped`, we have split it into three pieces, each containing 1/3 of the markers, and the files are named `HGDP1.ped`, `HGDP2.ped`, and `HGDP3.ped`.\n\nWe first down-sample the full dataset to perform structure analysis and then infer the component covariances.\n\n    python ./sample-sites.py ./HGDP.dgm 5 ./HGDP_5percent.dgm\n    qpas ./HGDP_5percent.dgm -e 0.0001 -k 7 -qo ./HGDP_5percent_Q.matrix -fo ./HGDP_5percent_F.matrix\n    nemeco ./HGDP_5percent.dgm ./HGDP_5percent_F.matrix -e 0.0 -co ./HGDP_5percent_C.matrix\n\nWe then produce admixture-corrected allele frequencies and perform the selection scan separately, possibly in parallel.\n\n    convert ped2dgm ./HGDP1.ped ./HGDP1.dgm\n    convert ped2dgm ./HGDP2.ped ./HGDP2.dgm\n    convert ped2dgm ./HGDP3.ped ./HGDP3.dgm\n\n    qpas ./HGDP1.dgm -qi ./HGDP_5percent_Q.matrix -fo ./HGDP1_F.matrix -e 0.0001 -fq\n    qpas ./HGDP2.dgm -qi ./HGDP_5percent_Q.matrix -fo ./HGDP2_F.matrix -e 0.0001 -fq\n    qpas ./HGDP3.dgm -qi ./HGDP_5percent_Q.matrix -fo ./HGDP3_F.matrix -e 0.0001 -fq\n\n    selscan ./HGDP1.dgm ./HGDP1_F.matrix ./HGDP_5percent_C.matrix > scan1.txt\n    selscan ./HGDP2.dgm ./HGDP2_F.matrix ./HGDP_5percent_C.matrix > scan2.txt\n    selscan ./HGDP3.dgm ./HGDP3_F.matrix ./HGDP_5percent_C.matrix > scan3.txt\n\n    cat scan*.txt > scan_all.txt\n\nThe file `scan_all.txt` contains selection results for the full dataset with markers following the order defined in HGDP.ped.\n"
      },
      "source": "https://raw.githubusercontent.com/jade-cheng/ohana/master/README.md",
      "technique": "header_analysis"
    }
  ]
}