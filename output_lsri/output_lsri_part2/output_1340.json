{
  "application_domain": [
    {
      "confidence": 30.92,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citation",
        "parent_header": [
          "HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance"
        ],
        "type": "Text_excerpt",
        "value": "If you found this code repo useful, please consider citing the associated publication:\n```\n@article{doughty2022hmd,\n  title = {{HMD}-{EgoPose}: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance},\n  issn = {1861-6429},\n  url = {https://doi.org/10.1007/s11548-022-02688-y},\n  doi = {10.1007/s11548-022-02688-y},\n  journaltitle = {International Journal of Computer Assisted Radiology and Surgery},\n  author = {Doughty, Mitchell and Ghugre, Nilesh R.},\n  date = {2022-06-14},\n}\n```\n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Doughty, Mitchell and Ghugre, Nilesh R.",
        "doi": "10.1007/s11548-022-02688-y",
        "format": "bibtex",
        "title": "{HMD}-{EgoPose}: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance",
        "type": "Text_excerpt",
        "url": "https://doi.org/10.1007/s11548-022-02688-y",
        "value": "@article{doughty2022hmd,\n    date = {2022-06-14},\n    author = {Doughty, Mitchell and Ghugre, Nilesh R.},\n    journaltitle = {International Journal of Computer Assisted Radiology and Surgery},\n    doi = {10.1007/s11548-022-02688-y},\n    url = {https://doi.org/10.1007/s11548-022-02688-y},\n    issn = {1861-6429},\n    title = {{HMD}-{EgoPose}: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance},\n}"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/doughtmw/hmd-ego-pose"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2022-02-23T19:42:43Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-02-16T06:11:09Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "A single-shot learning-based approach to hand and object pose estimation, made available on a head-mounted display."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9687366077782905,
      "result": {
        "original_header": "HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance",
        "type": "Text_excerpt",
        "value": "We present HMD-EgoPose: a deep learning-based framework making hand and rigid object pose estimation available to commercially available optical see-through head-mounted displays (OST-HMDs) using a low-latency streaming approach and a computing workstation. \nThis work contains code adapted from the following repositories:\n- [EfficientPose by ybkscht:](https://github.com/ybkscht/EfficientPose) Tensorflow/Keras implementation with scripts for model structure, data augmentation, and evaluation\n- [Yet-Another-EfficientDet-Pytorch by zylo117:](https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch) PyTorch network implementation for the EfficientNet and EfficientDet backbones\n- [node-dss by bengreenier:](https://github.com/bengreenier/node-dss) Simple signalling server for WebRTC\n \n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9133092452135312,
      "result": {
        "original_header": "Data",
        "type": "Text_excerpt",
        "value": "Download the Synthetic and Real datasets for the Colibri II drill, unzip the folders to a specified destination, and convert data format to prepare for training [link to project site for data access](http://medicalaugmentedreality.org/handobject.html).\n- [Synthetic dataset direct download link](http://medicalaugmentedreality.org/datasets/syn_colibri_v1.zip)\n- [Real dataset direct download link](http://medicalaugmentedreality.org/datasets/real_colibri_v1.zip)\n \n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9380902943290872,
      "result": {
        "original_header": "Format data for training",
        "type": "Text_excerpt",
        "value": "After successful completion of these commands, you should have the following output and directory structure for both the `syn_colibri_v1` and `real_colibri_v1` folders.  \n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8385892138186353,
      "result": {
        "original_header": "Debug",
        "type": "Text_excerpt",
        "value": "- We include an optional sample script `debug.py` to view the ground truth data projected onto input RGB image data and confirm correct data formatting\n- To check the formatting and correctness of training data, run the following sample commands\n```bash\n# View the synthetic dataset fold 0\npython .\\debug.py --dataset syn_colibri --fold 0 \n\n# View the real dataset fold 0\npython .\\debug.py --dataset real_colibri --fold 0 \n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9895625181053399,
      "result": {
        "original_header": "Training",
        "type": "Text_excerpt",
        "value": "Includes the resources for data formatting, loading, training, and export of a custom machine learning framework for rigid surgical drill pose estimation\n \n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9850621359737813,
      "result": {
        "original_header": "Evaluation",
        "type": "Text_excerpt",
        "value": "- For each fold the following metrics are presented:\n  - Tool ADD (in mm) is given by \n  `TranslationErrorMean_in_mm` with standard deviation `TranslationErrorStd_in_mm`\n  - Drill tip error (in mm) is given by `TranslationErrorTipMean_in_mm` with standard deviation `TranslationErrorTipStd_in_mm`\n  - Drill bit direction error (in deg) is given by `RotationErrorMean_in_degree` with standard deviation `RotationErrorStd_in_degree`\n  - Hand ADD (in mm) is given by `TranslationErrorHandMean_in_mm` with standard deviation `TranslationErrorHandStd_in_mm`\n \n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/doughtmw/hmd-ego-pose/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 3
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/doughtmw/hmd-ego-pose/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "doughtmw/hmd-ego-pose"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/fig/hmd-egopose.gif"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/fig/folds.PNG"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/fig/img.PNG"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/fig/results.PNG"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/fig/node-dss.PNG"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/fig/node-dss-unity.PNG"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/fig/sample-prediction-desktop.PNG"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/fig/sample-prediction-hl2.jpg"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/fig/000000-sample.PNG"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/fig/000000-output.PNG"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Project setup",
        "parent_header": [
          "HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance"
        ],
        "type": "Text_excerpt",
        "value": "Folder setup for the project sample.\n\n```\ndatasets/\n... real_colibri_v1/\n... syn_colibri_v1/\n\nformat-labels/\n\npytorch-sandbox/\n... efficientdet/\n... efficientnet/\n... efficientpose/\n... eval/\n... generators/\n... onnx-models/\n... utils/\n... weights/\n\nunity-sandbox/\n... HoloLens2-Machine-Learning-WebRTC/\n... node-dss/\n... OpenCVDNNSandboxNetCore/\n... WebRTCNetCoreSandbox/\n```\n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9769409848295225,
      "result": {
        "original_header": "Format data for training",
        "type": "Text_excerpt",
        "value": "- Navigate to the `format-labels/` directory and create a minimal virtualenv to run the extraction scripts\n- Download [Python for Windows](https://www.python.org/getit/windows/) (tested with Python 3.7.9)\n```bash\npython -m virtualenv -p 3.7 venv\n.\\venv\\Scripts\\activate\n\n# Install requirements\npip install -r requirements.txt\n```\n- After unzipping the downloaded datasets, begin formatting the Synthetic and Real datasets with the following commands (this can take a while)\n```bash\n# Prepare synthetic dataset\npython .\\pkl_to_formatted_txt.py --dataset syn_colibri --in_dir \"C:/Users/Mitch/Downloads/\" --out_dir \"C:/git/public/hmd-ego-pose/datasets/\"\n\n# Prepare real dataset\npython .\\pkl_to_formatted_txt.py --dataset real_colibri --in_dir \"C:/Users/Mitch/Downloads/\" --out_dir \"C:/git/public/hmd-ego-pose/datasets/\"\n```\n \nAfter successful completion of these commands, you should have the following output and directory structure for both the `syn_colibri_v1` and `real_colibri_v1` folders.  \n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9184079670392854,
      "result": {
        "original_header": "Evaluation",
        "type": "Text_excerpt",
        "value": "```bash\n# Best performing weights on synthetic colibri dataset\npython evaluate.py --dataset='syn_colibri' --iter=0 --img_size='256,256' --batch_size=1 --fold=4 --n_workers=6 --weights='train_weights/syn_colibri__fold_3__iter_0__mixed_t_mean_10.29__epo_49.pth'\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8567177790806662,
      "result": {
        "original_header": "Debug",
        "type": "Text_excerpt",
        "value": "- We include an optional sample script `debug.py` to view the ground truth data projected onto input RGB image data and confirm correct data formatting\n- To check the formatting and correctness of training data, run the following sample commands\n```bash\n# View the synthetic dataset fold 0\npython .\\debug.py --dataset syn_colibri --fold 0 \n\n# View the real dataset fold 0\npython .\\debug.py --dataset real_colibri --fold 0 \n```\n \n<img src=\"fig/img.PNG\" height=\"350\" />\n \n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.820453822353467,
      "result": {
        "original_header": "Continue training with existing weights",
        "type": "Text_excerpt",
        "value": "- After training on the synthetic colibri dataset, the best performing training weights can be used for fine-tuning performance on the smaller real colibri dataset\n```bash\n# Fine tune model using best performing weights from the synthetic colibri dataset training on fold 0 of the real colibri dataset\npython main.py --dataset='real_colibri' --iter=0 --img_size='256,256' --batch_size=16 --fold=0 --ckpt=train_weights/syn_colibri__fold_3__iter_0__mixed_t_mean_10.29__epo_49.pth --fine_tune=True\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8771739910866717,
      "result": {
        "original_header": "Evaluation",
        "type": "Text_excerpt",
        "value": "```bash\n# Best performing weights on synthetic colibri dataset\npython evaluate.py --dataset='syn_colibri' --iter=0 --img_size='256,256' --batch_size=1 --fold=4 --n_workers=6 --weights='train_weights/syn_colibri__fold_3__iter_0__mixed_t_mean_10.29__epo_49.pth'\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/doughtmw/hmd-ego-pose/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "deep-learning, hand-pose-estimation, hololens2, object-pose-estimation"
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "MIT License\n\nCopyright (c) 2016 Modest Tree Media Inc\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/unity-sandbox/HoloLens2-Machine-Learning-WebRTC/Assets/MRTK/Core/Utilities/Async/License.md",
      "technique": "file_exploration"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "hmd-ego-pose"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "doughtmw"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "C#",
        "size": 7848708,
        "type": "Programming_language",
        "value": "C#"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 476217,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "ShaderLab",
        "size": 145357,
        "type": "Programming_language",
        "value": "ShaderLab"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "GLSL",
        "size": 23823,
        "type": "Programming_language",
        "value": "GLSL"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Cython",
        "size": 4636,
        "type": "Programming_language",
        "value": "Cython"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "JavaScript",
        "size": 3962,
        "type": "Programming_language",
        "value": "JavaScript"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "C",
        "size": 1302,
        "type": "Programming_language",
        "value": "C"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "HLSL",
        "size": 856,
        "type": "Programming_language",
        "value": "HLSL"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2202.11891"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Run sample",
        "parent_header": [
          "HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance",
          "Training"
        ],
        "type": "Text_excerpt",
        "value": "- Ensure that the correct [CUDA](https://developer.nvidia.com/cuda-toolkit-archive) and [cuDNN](https://developer.nvidia.com/rdp/cudnn-archive) library versions are installed to support [PyTorch 1.8.2 LTS](https://pytorch.org/) (tested with CUDA 11.1)\n- Download [Python for Windows](https://www.python.org/getit/windows/) (tested with Python 3.7.9)\n- In the `pytorch-sandbox/` directory, create the virtual environment setup for training as below\n\n```bash\npython -m virtualenv -p 3.7 venv\n.\\venv\\Scripts\\activate\n\n# For training on NVIDIA 3000 series GPUs (CUDA 11.1 support)\npip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n\n# Install requirements\npip install -r requirements.txt\n\n# Compile the cython modules\npython setup.py build_ext --inplace\n```\n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "About",
        "parent_header": [
          "HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance",
          "Run sample with the HoloLens 2 and a workstation"
        ],
        "type": "Text_excerpt",
        "value": "- Tested with Unity 2019.4 LTS, Visual Studio 2019, and the HoloLens 2 with build 19041.1161 (Windows Holographic, version 20H2 - August 2021 Update) though will likely work fine with newer versions\n  - The 10.0.19041.1161 HoloLens 2 build can be downloaded from the following [link](https://aka.ms/hololens2download/10.0.19041.1161) and installed using the [Advanced Recovery Companion](https://www.microsoft.com/en-ca/p/advanced-recovery-companion/9p74z35sfrs8?rtc=1&activetab=pivot:overviewtab)\n- Input video frames of size `(1, 3, 512, 512)` for online inference `(NCWH)`\n- Pretrained PyTorch weights of the HMD-EgoPose framework were formatted and exported to an ONNX model format for use\n\nTo run the sample on the HoloLens 2, there are four required components: \n1. `HoloLens2-Machine-Learning-WebRTC`: the Unity project which can be deployed on the computing workstation (desktop) or HoloLens 2\n2. `WebRTCNetCoreSandbox`: the WebRTC C# project for receiving video frames and performing network inference on the computing workstation (desktop)\n3. `OpenCVDNNSandboxNetCore`: a sample C# project used for optional model validation against the Python implementation\n4. `node-dss`: the signaling server for WebRTC based communication\n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Node dss",
        "parent_header": [
          "HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance",
          "Run sample with the HoloLens 2 and a workstation"
        ],
        "type": "Text_excerpt",
        "value": "We use [node-dss](https://github.com/bengreenier/node-dss) as a simple signaling server for WebRTC and streaming our video/pose predictions from the HoloLens 2 to the computing workstation.\nTo install, follow the directions below:\n- Install [nodejs](https://nodejs.org) to leverage this service\n- Install dependencies with [npm](http://npmjs.com/) - from the `unity-sandbox/node-dss` open a [Git Bash](https://git-scm.com/downloads) window and run:\n```\nnpm install\nexport DEBUG=dss*\nset DEBUG=dss*\nnpm start\n```\n\n- You should see the Git Bash window output as below:\n\n![](fig/node-dss.PNG)\n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Run sample on a single desktop",
        "parent_header": [
          "HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance",
          "Run sample with the HoloLens 2 and a workstation"
        ],
        "type": "Text_excerpt",
        "value": "- Open `HoloLens2-Machine-Learning-WebRTC` project in Unity, load the `WebcamVideoLocal` scene\n- Navigate to the `NodeDssSignaler` component in the Unity window and adjust the `HttpServerAddress` to the IP of your current machine\n  - For example, `http://xxx.xxx.x.xx:3000/`, while keeping the port as 3000\n  - Leave the `LocalPeerId` and `RemotePeerId` values as is\n\n![](fig/node-dss-unity.PNG)\n\n- On playing the Unity scene, you should be able to see video from a webcam connected to the PC (on the `LocalMedia`\u2192`VideoPlayer` component) and you should see that your program has begun polling in the node dss window as:\n\n```\n2022-06-17T14:33:28.852Z dss:boot online @ 3000\n2022-06-17T14:44:32.776Z dss GET /data/HoloLens2MachineLearningWebRTC 404 - - 1.051 ms\n2022-06-17T14:44:33.265Z dss GET /data/HoloLens2MachineLearningWebRTC 404 - - 0.073 ms\n2022-06-17T14:44:33.768Z dss GET /data/HoloLens2MachineLearningWebRTC 404 - - 0.085 ms\n2022-06-17T14:44:34.270Z dss GET /data/HoloLens2MachineLearningWebRTC 404 - - 0.066 ms\n2022-06-17T14:44:34.773Z dss GET /data/HoloLens2MachineLearningWebRTC 404 - - 0.065 ms\n```\n\n- Now, open the `WebRTCNetCoreSandbox` project in Visual studio\n- In the `Program.cs` file, make the following adjustments based on your own setup\n  1. Update the file path to the location of the `onnx-models/` folder\n  ```c#\n  // Update the file path to point to the location of the onnx-models/ folder\n  const string basePath = \"C:/git/public/hmd-ego-pose/pytorch-sandbox/onnx-models/\";\n  ```\n  2. Change the `HttpServerAddress` to the IP of your PC\n  ```c#\n  // Initialize the signaler\n  signaler = new NodeDssSignaler()\n  {\n    // Set to your IP (same as in the Unity project), leave the \n    // LocalPeerId and RemotePeerId values\n      HttpServerAddress = \"http://192.168.2.29:3000/\",\n      LocalPeerId = \"WebRTCSandbox\",\n      RemotePeerId = \"HoloLens2MachineLearningWebRTC\",\n  };\n  ```\n- **After ensuring the Unity project is already running**, set the project to `Release` and `x64` and run the sample\n- As indicated in the below image, you can either create a 3D printed drill to test the network, OR aim your webcam at your computer monitor and view images/video from the surgical drill dataset\n- In the image, the left window is the Unity window running the `HoloLens2-Machine-Learning-WebRTC` Unity project; the right window is the `WebRTCNetCoreSandbox` project; and the bottom right window is the Git Bash window running `node-dss`\n- The predicted pose for the surgical drill identified in the webcam video is applied to the virtual drill model in the Unity scene (in red)\n\n![](fig/sample-prediction-desktop.PNG)\n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Run sample on the HoloLens 2 and companion desktop",
        "parent_header": [
          "HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance",
          "Run sample with the HoloLens 2 and a workstation"
        ],
        "type": "Text_excerpt",
        "value": "- Open `HoloLens2-Machine-Learning-WebRTC` project in Unity and load to the `HoloLensVideoLocal` scene\n- Navigate to the `NodeDssSignaler` component in the Unity window and adjust the `HttpServerAddress` to the IP of your current machine (desktop not the HoloLens 2)\n  - For example, `http://xxx.xxx.x.xx:3000/`, while keeping the port as 3000\n  - Leave the `LocalPeerId` and `RemotePeerId` values as is\n- Switch build platform to `Universal Windows Platform`, select `HoloLens` for target device, and `ARM` as the target platform\n- Build the Visual Studio project and deploy to the HoloLens 2\n- On opening the app on the HoloLens 2, you should be able to see that your program has begun polling in the node dss window as:\n\n```\n2022-06-17T15:22:56.034Z dss:boot online @ 3000\n2022-06-17T15:22:56.424Z dss GET /data/HoloLens2MachineLearningWebRTC 404 - - 0.908 ms\n2022-06-17T15:22:56.520Z dss GET /data/HoloLens2MachineLearningWebRTC 404 - - 0.074 ms\n2022-06-17T15:22:57.044Z dss GET /data/HoloLens2MachineLearningWebRTC 404 - - 0.075 ms\n2022-06-17T15:22:57.617Z dss GET /data/HoloLens2MachineLearningWebRTC 404 - - 0.070 ms\n2022-06-17T15:22:58.188Z dss GET /data/HoloLens2MachineLearningWebRTC 404 - - 0.062 ms\n```\n\n- Now, open the `WebRTCNetCoreSandbox` project in Visual studio\n- In the `Program.cs` file, make the following adjustments based on your own setup\n  1. Update the file path to the location of the `onnx-models/` folder\n  ```c#\n  // Update the file path to point to the location of the onnx-models/ folder\n  const string basePath = \"C:/git/public/hmd-ego-pose/pytorch-sandbox/onnx-models/\";\n  ```\n  2. Change the `HttpServerAddress` to the IP of your PC\n  ```c#\n  // Initialize the signaler\n  signaler = new NodeDssSignaler()\n  {\n    // Set to your IP (same as in the Unity project), leave the \n    // LocalPeerId and RemotePeerId values\n      HttpServerAddress = \"http://192.168.2.29:3000/\",\n      LocalPeerId = \"WebRTCSandbox\",\n      RemotePeerId = \"HoloLens2MachineLearningWebRTC\",\n  };\n  ```\n- **After ensuring the HoloLens 2 application is already running**, set the project to `Release` and `x64` and run the sample\n- As indicated in the below image, you can either create a 3D printed drill to test the network, OR aim your webcam at your computer monitor and view images/video from the surgical drill dataset\n- In the image, the virtual surgical drill model is visible in red, with the `WebRTCNetCoreSandbox` project; and the bottom right window is the Git Bash window running `node-dss` inb the background\n- The predicted pose for the surgical drill identified in the HoloLens 2 front facing camera video is applied to the virtual drill model in the Unity scene (in red)\n  - Though the drill pose was not perfect in this sample, there was some additional error introduced through the mixed reality capture API and the lack of camera extrinsic parameter streaming with WebRTC and the HoloLens\n\n![](fig/sample-prediction-hl2.jpg)\n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "*Optional*: Comparing C# ONNX results to PyTorch and ONNX results in Python",
        "parent_header": [
          "HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance",
          "Run sample with the HoloLens 2 and a workstation"
        ],
        "type": "Text_excerpt",
        "value": "Test data is included in the `onnx-models` folder. \n- The `scratchpad.py` script demonstrates loading of data and the ONNX model with basic inference in Python (no additional output filtering)\n- Filtering and regression of network outputs is demonstrated in `evaluate.py` to arrive at a rotation and translation prediction for the rigid object\n- The output of `scratchpad.py` and `evaluate.py` can be used to compare to the `OpenCVDNNSandboxNetCore` C# implementation to assess accuracy of predictions\n- Running `evaluate.py` with sample input image `000000.png`, we arrive at the following prediction result\n\n```C#\n// Desired final results of prediction (from evaluate.py script with input 000000.png)\ncamera_matrix:\n [[480.   0. 128.]\n [  0. 480. 128.]\n [  0.   0.   1.]]\nrotation_gt: [[-2.92229713  1.00365327  0.09353241]]\ntranslation_gt: [[-0.02927876 -0.05211956  0.49457447]]\nrotation_pred: [[-2.9054394  1.0276762  0.1723399]]\ntranslation_pred: [[-0.02811211 -0.05858146  0.48664188]]\n```\n\n- Running the C# `OpenCVDNNSandboxNetCore` project with sample input image `000000.png`, we arrive at the following pre-processed input frame:\n  - *Note:* You will need to adapt the paths in this project to be relevant to your local path structure\n\n![](fig/000000-sample.PNG)\n\n- And output to the terminal window, we can observe that the C# produced results are nearly identical to the Python produced results:\n\n![](fig/000000-output.PNG)\n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "*Optional:* Build ONNX Runtime from source",
        "parent_header": [
          "HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance",
          "Run sample with the HoloLens 2 and a workstation"
        ],
        "type": "Text_excerpt",
        "value": "- Build TensorRT from source for processing\n- Download Nuget command line (https://www.nuget.org/downloads) and add to environment variables\n- Resources from ONNX runtime on how to build the package [link 1](https://www.onnxruntime.ai/docs/how-to/build.html) and [link 2](https://www.onnxruntime.ai/docs/how-to/build/inferencing.html)\n- `xxx\\Release\\Release\\nuget-artifacts`\n- Requirements\n  - Cuda 11.0\n  - cuDNN v8.0.5\n  - TensorRT 7.2.3.4\n- Below are the commands used for building the ONNX runtime library from source on my hardware, successful completion of these builds will result in a NuGet package (like those in the `unity-sandbox/` directory)\n \n```powershell\n# CPU \n# Debug\n# Release\n.\\build.bat --config Release --build_shared_lib --parallel --cmake_generator \"Visual Studio 16 2019\" --build_nuget --build_dir cpu --skip_tests\n\n# CUDA\n# Using Cuda 11.0, cuDNN v8.0.5\n# Debug\n# Release\n.\\build.bat --config Release --build_shared_lib --parallel --cmake_generator \"Visual Studio 16 2019\" --build_nuget --skip_tests --use_cuda --cuda_home \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\" --cudnn_home \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\" --cuda_version 11.0 --build_dir cuda \n\n# TensorRT\n# Using Cuda 11.0, cuDNN v8.0.5, TensorRT 7.2.3.4\n# Debug\n# Release\n.\\build.bat --config Release --build_shared_lib --parallel --cmake_generator \"Visual Studio 16 2019\" --build_nuget --skip_tests --use_cuda --cuda_home \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\" --cudnn_home \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\" --use_tensorrt --tensorrt_home \"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.0\\TensorRT-7.2.3.4\" --cuda_version 11.0 --build_dir tensorrt \n\n# DirectML\n# Debug\n# Release\n.\\build.bat --config Release --build_shared_lib --parallel --cmake_generator \"Visual Studio 16 2019\" --build_nuget --use_dml --build_dir directml --skip_tests\n```\n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-04 14:04:46",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 28
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Begin training",
        "parent_header": [
          "HMD-EgoPose: head-mounted display-based egocentric marker-less tool and hand pose estimation for augmented surgical guidance",
          "Training"
        ],
        "type": "Text_excerpt",
        "value": "- From the `pytorch-sandbox/` directory, we can begin training HMD-EgoPose using the `EfficientNet-B0` backbone\n- To monitor training/validation accuracy/loss, a TensorBoard training log is output to the `runs/` folder\n- At the end of each epoch, the model weights with the lowest transformation error will be saved to the `train_weights` folder\n- After completing training on the dataset, the model weights which achieved the highest accuracy on the validation set are selected for evaluation on the test set\n\n```bash\n# Begin training on fold 0 of synthetic colibri dataset\npython main.py --dataset='syn_colibri' --iter=0 --img_size='256,256' --batch_size=16 --fold=0\n```\n"
      },
      "source": "https://raw.githubusercontent.com/doughtmw/hmd-ego-pose/main/README.md",
      "technique": "header_analysis"
    }
  ]
}