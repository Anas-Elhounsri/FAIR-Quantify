{
  "acknowledgement": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Acknowledgment",
        "parent_header": [
          "FBMSNet: Architecture"
        ],
        "type": "Text_excerpt",
        "value": "We thank Ravikiran Mane et al. for their useful [toolbox](https://github.com/ravikiran-mane/FBCNet). \n\n"
      },
      "source": "https://raw.githubusercontent.com/Want2Vanish/FBMSNet/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Cite:",
        "parent_header": [
          "FBMSNet: Architecture"
        ],
        "type": "Text_excerpt",
        "value": "If you find this architecture or toolbox useful then please cite this paper:\n\n*Liu, Ke and Yang, Mingzhao and Yu, Zhuliang and Wang, Guoyin and Wu, Wei. FBMSNet: A Filter-Bank Multi-Scale Convolutional Neural Network for EEG-Based Motor Imagery Decoding. IEEE Transactions on Biomedical Engineering, 70(2):436\u2013445, 2023. https://ieeexplore.ieee.org/document/9837422*\n\n"
      },
      "source": "https://raw.githubusercontent.com/Want2Vanish/FBMSNet/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "References:",
        "parent_header": [
          "FBMSNet: Architecture"
        ],
        "type": "Text_excerpt",
        "value": "*Ravikiran Mane, Effie Chew, Karen Chua, Kai Keng Ang, Neethu Robinson, A.P. Vinod, Seong-Whan Lee, and Cuntai Guan, **\"FBCNet: An Efficient Multi-view Convolutional Neural Network for Brain-Computer Interface,\"** arXiv preprint arXiv:2104.01233 (2021) https://arxiv.org/abs/2104.01233*\n"
      },
      "source": "https://raw.githubusercontent.com/Want2Vanish/FBMSNet/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/Want2Vanish/FBMSNet"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2021-11-30T11:42:02Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-10-04T08:41:33Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "This is the PyTorch implementation of the FBMSNet architecture for EEG-MI classification."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9957540733572422,
      "result": {
        "original_header": "FBMSNet: A Filter-Bank Multi-Scale Convolutional Neural Network for EEG-Based Motor Imagery Decoding",
        "type": "Text_excerpt",
        "value": "This is the PyTorch implementation of the FBMSNet architecture for EEG-MI classification. \n \n"
      },
      "source": "https://raw.githubusercontent.com/Want2Vanish/FBMSNet/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9884168052782518,
      "result": {
        "original_header": "FBMSNet: Architecture",
        "type": "Text_excerpt",
        "value": "FBMSNet consists of four blocks, (1) a temporal convolution block, (2) a spatial convolution block, (3) a temporal log-variance block, and (4) a fully connected layer for classification. The first block is designed to learn the multiscale temporal information from the multiview EEG representations, and the second block aims to learn the spatial information from each temporal feature map. Subsequently, the third block computes the temporal variance of each time series. Finally, all representations are flattened and fed to the fully connected layer with softmax as the activation function. An overview of FBMSNet is depicted in Fig. 1. \nFurthermore, to distinguish similar categories in a better way and decrease the influence of interclass dispersion and within-class variance, we not only minimize the cross entropy (CE) loss function but also introduce the center loss function. With this joint supervision, FBMSNet is capable of learning deep features with two key learning objectives as much as possible, interclass separability and intraclass compactness as much as possible, which are crucial to MI recognition\n \n"
      },
      "source": "https://raw.githubusercontent.com/Want2Vanish/FBMSNet/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8506869528853395,
      "result": {
        "original_header": "FBMSNet: Results",
        "type": "Text_excerpt",
        "value": "The classification results for FBMSNet and other competing architectures are as follows: \n"
      },
      "source": "https://raw.githubusercontent.com/Want2Vanish/FBMSNet/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/Want2Vanish/FBMSNet/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 7
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/Want2Vanish/FBMSNet/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Want2Vanish/FBMSNet"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "FBMSNet"
      },
      "source": "https://raw.githubusercontent.com/Want2Vanish/FBMSNet/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/Want2Vanish/FBMSNet/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "FBMSNet"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "Want2Vanish"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 179867,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Want2Vanish/FBMSNet/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "somef_missing_categories": [
    "installation",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "license",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-04 13:30:09",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 31
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "How to use",
        "parent_header": [
          "FBMSNet: Architecture"
        ],
        "type": "Text_excerpt",
        "value": "If you want to reproduce the experimental results reported in the paper, make sure you use the same runtime environment.\n\nThe package requirements to run all the codes are provided in file env.txt. The complete instructions for utilising this toolbox are provided in instructions.txt.\n\nAll the codes have been tested to work using a virtual environment mentioned in the env.txt file. \n\n1. Create a new virtual environment named \"env_fbmsnet\" with python3.7 using Anaconda3 and activate it\uff1a\n\n   ```\n   conda create --name env_fbmsnet python=3.7\n   conda activate env_fbmsnet\n   ```\n\n\n3. Install virtualenv package:\n\n   ```\n   conda install -c anaconda ujson=1.35\n   pip install -U scikit-learn\n   pip install pandas -i\n   pip install matplotlib\n   pip install torchsummary\n   pip install resampy\n   pip install seaborn\n   pip install pygame\n   pip install selenium\n   ```\n\n3. Change the current working path to the path where you have stored env.txt. Install required packages.\n\n   ```\n   E: // Change the current working path\n   pip install -r env.txt\n   ```\n\n4. Download the offline installation files for torch and torchvision from the following link: https://download.pytorch.org/whl/torch_stable.html, Use the \"Ctrl + F\" command to search for the installation files of the specified version of torch and torchvision and download it. It is recommended to store them under the same path as env.txt.\n\n   > torch version : torch-1.3.1-cp37-cp37m-win_amd64\n   > \n   > torchvision version : torchvision-0.4.2-cp37-cp37m-win_amd64\n\n5. When you have finished downloading, use the pip command to install them.\n\n   ```\n   pip install torch-1.3.1-cp37-cp37m-win_amd64.whl\n   pip install torchvision-0.4.2-cp37-cp37m-win_amd64.whl\n   ```\n\nCongratulations, you have completed all the steps to create the virtual environment needed to run the source code!\n"
      },
      "source": "https://raw.githubusercontent.com/Want2Vanish/FBMSNet/main/README.md",
      "technique": "header_analysis"
    }
  ]
}