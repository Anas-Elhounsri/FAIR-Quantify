{
  "acknowledgement": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Acknowledgments",
        "parent_header": [
          "Generative Model-Enhanced Human Motion Prediction"
        ],
        "type": "Text_excerpt",
        "value": "The codebase is built on that of https://github.com/wei-mao-2019/LearnTrajDep and depends heavily on their work in [_Learning Trajectory Dependencies for Human Motion Prediction_](https://arxiv.org/abs/1908.05436) (ICCV 2019), and [_History Repeats Itself: Human Motion Prediction via Motion Attention_](https://arxiv.org/abs/2007.11755) (ECCV 2020). Thus please also cite:\n\n```\n@inproceedings{wei2019motion,\n  title={Learning Trajectory Dependencies for Human Motion Prediction},\n  author={Wei, Mao and Miaomiao, Liu and Mathieu, Salzemann and Hongdong, Li},\n  booktitle={ICCV},\n  year={2019}\n}\n```\n\nand\n\n```\n@article{mao2020history,\n  title={History Repeats Itself: Human Motion Prediction via Motion Attention},\n  author={Mao, Wei and Liu, Miaomiao and Salzmann, Mathieu},\n  journal={arXiv preprint arXiv:2007.11755},\n  year={2020}\n}\n```\n"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citing",
        "parent_header": [
          "Generative Model-Enhanced Human Motion Prediction"
        ],
        "type": "Text_excerpt",
        "value": "If you use our code, and/or build on our work, please cite our paper:\n\n```\n@article{https://doi.org/10.1002/ail2.63,\nauthor = {Bourached, Anthony and Griffiths, Ryan-Rhys and Gray, Robert and Jha, Ashwani and Nachev, Parashkev},\ntitle = {Generative Model-Enhanced Human Motion Prediction},\njournal = {Applied AI Letters},\nvolume = {n/a},\nnumber = {n/a},\npages = {},\ndoi = {https://doi.org/10.1002/ail2.63},\nurl = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ail2.63},\neprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ail2.63},\nabstract = {Abstract The task of predicting human motion is complicated by the natural heterogeneity and compositionality of actions, necessitating robustness to distributional shifts as far as out-of-distribution (OoD). Here we formulate a new OoD benchmark based on the Human3.6M and CMU motion capture datasets, and introduce a hybrid framework for hardening discriminative architectures to OoD failure by augmenting them with a generative model. When applied to current state-of-theart discriminative models, we show that the proposed approach improves OoD robustness without sacrificing in-distribution performance. We suggest human motion predictors ought to be constructed with OoD challenges in mind, and provide an extensible general framework for hardening diverse discriminative architectures to extreme distributional shift. The code is available at https: //github.com/bouracha/OoDMotion.}\n}\n\n\n```\n"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Bourached, Anthony and Griffiths, Ryan-Rhys and Gray, Robert and Jha, Ashwani and Nachev, Parashkev",
        "doi": "https://doi.org/10.1002/ail2.63",
        "format": "bibtex",
        "title": "Generative Model-Enhanced Human Motion Prediction",
        "type": "Text_excerpt",
        "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/ail2.63",
        "value": "@article{https://doi.org/10.1002/ail2.63,\n    abstract = {Abstract The task of predicting human motion is complicated by the natural heterogeneity and compositionality of actions, necessitating robustness to distributional shifts as far as out-of-distribution (OoD). Here we formulate a new OoD benchmark based on the Human3.6M and CMU motion capture datasets, and introduce a hybrid framework for hardening discriminative architectures to OoD failure by augmenting them with a generative model. When applied to current state-of-theart discriminative models, we show that the proposed approach improves OoD robustness without sacrificing in-distribution performance. We suggest human motion predictors ought to be constructed with OoD challenges in mind, and provide an extensible general framework for hardening diverse discriminative architectures to extreme distributional shift. The code is available at https: //github.com/bouracha/OoDMotion.},\n    eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/ail2.63},\n    url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ail2.63},\n    doi = {https://doi.org/10.1002/ail2.63},\n    pages = {},\n    number = {n/a},\n    volume = {n/a},\n    journal = {Applied AI Letters},\n    title = {Generative Model-Enhanced Human Motion Prediction},\n    author = {Bourached, Anthony and Griffiths, Ryan-Rhys and Gray, Robert and Jha, Ashwani and Nachev, Parashkev},\n}"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Wei, Mao and Miaomiao, Liu and Mathieu, Salzemann and Hongdong, Li",
        "format": "bibtex",
        "title": "Learning Trajectory Dependencies for Human Motion Prediction",
        "type": "Text_excerpt",
        "value": "@inproceedings{wei2019motion,\n    year = {2019},\n    booktitle = {ICCV},\n    author = {Wei, Mao and Miaomiao, Liu and Mathieu, Salzemann and Hongdong, Li},\n    title = {Learning Trajectory Dependencies for Human Motion Prediction},\n}"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Mao, Wei and Liu, Miaomiao and Salzmann, Mathieu",
        "format": "bibtex",
        "title": "History Repeats Itself: Human Motion Prediction via Motion Attention",
        "type": "Text_excerpt",
        "value": "@article{mao2020history,\n    year = {2020},\n    journal = {arXiv preprint arXiv:2007.11755},\n    author = {Mao, Wei and Liu, Miaomiao and Salzmann, Mathieu},\n    title = {History Repeats Itself: Human Motion Prediction via Motion Attention},\n}"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/bouracha/OoDMotion"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-04-29T12:49:30Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-21T07:03:12Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "code for learning trajectory dependencies for human motion prediction"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9827391344740819,
      "result": {
        "original_header": "Generative Model-Enhanced Human Motion Prediction",
        "type": "Text_excerpt",
        "value": "This is the code for the paper \n"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9434071544875045,
      "result": {
        "original_header": "Training commands",
        "type": "Text_excerpt",
        "value": "All the running args are defined in [opt.py](utils/opt.py). We use following commands to train on different datasets and representations.\nTo train on angle space, in-distribution, H3.6M:\n```bash\npython3 main.py --data_dir \"[Path To Your H36M data]/h3.6m/dataset/\" --variational --lambda 0.003 --n_z 8 --dropout 0.3 --lr_gamma 1.0 --input_n 10 --output_n 10 --dct_n 20\n```\nin-distribution (CMU):\nBASH2*\nto train on 3D space for CMU, simply change the BASH3* to BASH4*. This flag is 'h3.6m' by default. \nTo train on 'walking' and test out-of-distribution (for h3.6M), include the extra flag:\n```bash\n--out_of_distribution 'walking' \n```\nidentically to train on 'basketball' and test out-of-distribution (for CMU), include the extra flag:\nBASH6*\nThe same models may be trained (or used for inference independent of how they were trained) without the VGAE branch by removing the \nBASH7* \nflag.\n \n"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/bouracha/OoDMotion/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 4
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/bouracha/OoDMotion/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "bouracha/OoDMotion"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Generative Model-Enhanced Human Motion Prediction"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9907026240532526,
      "result": {
        "original_header": "Inference on latent spaces for trained model, saves to latents.csv (also save DCT inputs, to inputs.csv)",
        "type": "Text_excerpt",
        "value": "```python\npython3 interpretability.py --dataset 'cmu_mocap' --model_path \"[Path To Your Trained Model].pth.tar\"\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8245407166392643,
      "result": {
        "original_header": "Hyperparameter search can be conducted via:",
        "type": "Text_excerpt",
        "value": "```\npython3 hyperparameter_search.py --num_trials 10 --epoch 100 --variational\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9599885721554077,
      "result": {
        "original_header": "Inference on latent spaces for trained model, saves to latents.csv (also save DCT inputs, to inputs.csv)",
        "type": "Text_excerpt",
        "value": "```python\npython3 interpretability.py --dataset 'cmu_mocap' --model_path \"[Path To Your Trained Model].pth.tar\"\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/bouracha/OoDMotion/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "MIT License\n\nCopyright (c) 2019 Wei Mao\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/LICENSE",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Licence",
        "parent_header": [
          "Generative Model-Enhanced Human Motion Prediction"
        ],
        "type": "Text_excerpt",
        "value": "MIT\n"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "OoDMotion"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "bouracha"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 156715,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1908.05436"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2010.11699"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2007.11755"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Dependencies",
        "parent_header": [
          "Generative Model-Enhanced Human Motion Prediction"
        ],
        "type": "Text_excerpt",
        "value": "Some older versions may work. But we used the following:\n\n* cuda 10.1\n* Python 3.6.9\n* [Pytorch](https://github.com/pytorch/pytorch) 1.6.0\n* [progress 1.5](https://pypi.org/project/progress/)\n"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "run",
    "download",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-04 14:17:08",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 10
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Get the data",
        "parent_header": [
          "Generative Model-Enhanced Human Motion Prediction"
        ],
        "type": "Text_excerpt",
        "value": "[Human3.6m](http://vision.imar.ro/human3.6m/description.php) in exponential map can be downloaded from [here](http://www.cs.stanford.edu/people/ashesh/h3.6m.zip).\n\n[CMU mocap](http://mocap.cs.cmu.edu/) was obtained from the [repo](https://github.com/chaneyddtt/Convolutional-Sequence-to-Sequence-Model-for-Human-Dynamics) of ConvSeq2Seq paper.\n"
      },
      "source": "https://raw.githubusercontent.com/bouracha/OoDMotion/master/README.md",
      "technique": "header_analysis"
    }
  ]
}