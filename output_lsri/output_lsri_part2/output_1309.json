{
  "application_domain": [
    {
      "confidence": 22.61,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_of_conduct": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "\n# Code of Conduct\n\n## Overview\n\nThis document defines the Code of Conduct followed and enforced for NVIDIA C++\n  Core Compute Libraries.\n\n### Intended Audience\n\n* Community\n* Developers\n* Project Leads\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\n  contributors and maintainers pledge to making participation in our project and\n  our community a harassment-free experience for everyone, regardless of age,\n  body size, disability, ethnicity, sex characteristics, gender identity and\n  expression, level of experience, education, socio-economic status, nationality,\n  personal appearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment include:\n\n- Using welcoming and inclusive language.\n- Being respectful of differing viewpoints and experiences.\n- Gracefully accepting constructive criticism.\n- Focusing on what is best for the community.\n- Showing empathy towards other community members.\n\nExamples of unacceptable behavior by participants include:\n\n- The use of sexualized language or imagery and unwelcome sexual attention or\n    advances.\n- Trolling, insulting/derogatory comments, and personal or political attacks.\n- Public or private harassment.\n- Publishing others\u2019 private information, such as a physical or electronic\n    address, without explicit permission.\n- Other conduct which could reasonably be considered inappropriate.\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\n  behavior and are expected to take appropriate and fair corrective action in\n  response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\n  reject comments, commits, code, wiki edits, issues, and other contributions\n  that are not aligned to this Code of Conduct, or to ban temporarily or\n  permanently any contributor for other behaviors that they deem inappropriate,\n  threatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\n  when an individual is representing the project or its community.\nExamples of representing a project or community include using an official\n  project email address, posting via an official social media account, or acting\n  as an appointed representative at an online or offline event.\nRepresentation of a project may be further defined and clarified by project\n  maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\n  reported by contacting [cpp-conduct@nvidia.com](mailto:cpp-conduct@nvidia.com).\nAll complaints will be reviewed and investigated and will result in a response\n  that is deemed necessary and appropriate to the circumstances.\nThe project team is obligated to maintain confidentiality with regard to the\n  reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\n  faith may face temporary or permanent repercussions as determined by other\n  members of the project\u2019s leadership.\n\n## Attribution\n\nThis Code of Conduct was taken from the [NVIDIA RAPIDS] project, which was\n  adapted from the [Contributor Covenant version 1.4].\n\nPlease see this [FAQ] for answers to common questions about this Code of Conduct.\n\n## Contact\n\nPlease email [cpp-conduct@nvidia.com] for any Code of Conduct related matters.\n\n\n[cpp-conduct@nvidia.com]: mailto:cpp-conduct@nvidia.com\n\n[FAQ]: https://www.contributor-covenant.org/faq\n\n[NVIDIA RAPIDS]: https://docs.rapids.ai/resources/conduct/\n[Contributor Covenant]: https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n libcudacxx-conduct@nvidia.com\n"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/src/gpu_utils/cub/CODE_OF_CONDUCT.md",
      "technique": "file_exploration"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/alncat/cryoem"
      },
      "technique": "GitHub_API"
    }
  ],
  "contributing_guidelines": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# Table of Contents\n\n1. [Contributing to CUB](#contributing-to-cub)\n1. [CMake Options](#cmake-options)\n1. [Development Model](#development-model)\n\n# Contributing to CUB\n\nCUB uses Github to manage all open-source development, including bug tracking,\npull requests, and design discussions. This document details how to get\nstarted as a CUB contributor.\n\nAn overview of this process is:\n\n1. [Clone the CUB repository](#clone-the-cub-repository)\n1. [Setup a fork of CUB](#setup-a-fork-of-cub)\n1. [Setup your environment](#setup-your-environment)\n1. [Create a development branch](#create-a-development-branch)\n1. [Local development loop](#local-development-loop)\n1. [Push development branch to your fork](#push-development-branch-to-your-fork)\n1. [Create pull request](#create-pull-request)\n1. [Address feedback and update pull request](#address-feedback-and-update-pull-request)\n1. [When your PR is approved...](#when-your-pr-is-approved)\n\n## Clone the CUB Repository\n\nTo get started, clone the main repository to your local computer:\n\n```\ngit clone https://github.com/NVIDIA/cub.git\ncd cub\n```\n\n## Setup a Fork of CUB\n\nYou'll need a fork of CUB on Github to create a pull request. To setup your\nfork:\n\n1. Create a Github account (if needed)\n2. Go to [the CUB Github page](https://github.com/NVIDIA/cub)\n3. Click \"Fork\" and follow any prompts that appear.\n\nOnce your fork is created, setup a new remote repo in your local CUB clone:\n\n```\ngit remote add github-fork git@github.com:<GITHUB_USERNAME>/cub.git\n```\n\n## Setup Your Environment\n\n### Git Environment\n\nIf you haven't already, this is a good time to tell git who you are. This\ninformation is used to fill out authorship information on your git commits.\n\n```\ngit config --global user.name \"John Doe\"\ngit config --global user.email johndoe@example.com\n```\n\n### Configure CMake builds\n\nCUB uses [CMake](https://www.cmake.org) for its developer build system. To\nconfigure, build, and test your checkout of CUB with default settings:\n\n```\n# Create build directory:\nmkdir build\ncd build\n\n# Configure -- use one of the following:\ncmake ..   # Command line interface.\nccmake ..  # ncurses GUI (Linux only)\ncmake-gui  # Graphical UI, set source/build directories in the app\n\n# Build:\ncmake --build . -j <num jobs>   # invokes make (or ninja, etc)\n\n# Run tests and examples:\nctest\n```\n\nSee [CMake Options](#cmake-options) for details on customizing the build.\n\n## Create a Development Branch\n\nAll work should be done in a development branch (also called a \"topic branch\")\nand not directly in the `main` branch. This makes it easier to manage multiple\nin-progress patches at once, and provides a descriptive label for your patch\nas it passes through the review system.\n\nTo create a new branch based on the current `main`:\n\n```\n# Checkout local main branch:\ncd /path/to/cub/sources\ngit checkout main\n\n# Sync local main branch with github:\ngit pull\n\n# Create a new branch named `my_descriptive_branch_name` based on main:\ngit checkout -b my_descriptive_branch_name\n\n# Verify that the branch has been created and is currently checked out:\ngit branch\n```\n\nCUB branch names should follow a particular pattern:\n\n- For new features, name the branch `feature/<name>`\n- For bugfixes associated with a github issue, use `bug/github/<bug-description>-<bug-id>`\n  - Internal nvidia and gitlab bugs should use `nvidia` or `gitlab` in place of\n    `github`.\n\n## Local Development Loop\n\n### Edit, Build, Test, Repeat\n\nOnce the topic branch is created, you're all set to start working on CUB\ncode. Make some changes, then build and test them:\n\n```\n# Implement changes:\ncd /path/to/cub/sources\nemacs cub/some_file.cuh # or whatever editor you prefer\n\n# Create / update a unit test for your changes:\nemacs tests/some_test.cu\n\n# Check that everything builds and tests pass:\ncd /path/to/cub/build/directory\ncmake --build . -j <num_jobs> # or make, ninja, etc\nctest\n```\n\n### Creating a Commit\n\nOnce you're satisfied with your patch, commit your changes:\n\n```\n# Manually add changed files and create a commit:\ncd /path/to/cub\ngit add cub/some_file.cuh\ngit add tests/some_test.cu\ngit commit\n\n# Or, if possible, use git-gui to review your changes while building your patch:\ngit gui\n```\n\n#### Writing a Commit Message\n\nYour commit message will communicate the purpose and rationale behind your\npatch to other developers, and will be used to populate the initial description\nof your Github pull request.\n\nWhen writing a commit message, the following standard format should be used,\nsince tools in the git ecosystem are designed to parse this correctly:\n\n```\nFirst line of commit message is a short summary (<80 char)\n<Second line left blank>\nDetailed description of change begins on third line. This portion can\nspan multiple lines, try to manually wrap them at something reasonable.\n\nBlank lines can be used to separate multiple paragraphs in the description.\n\nIf your patch is associated with another pull request or issue in the main\nCUB repository, you should reference it with a `#` symbol, e.g.\n#1023 for issue 1023.\n\nFor issues / pull requests in a different github repo, reference them using\nthe full syntax, e.g. NVIDIA/thrust#4 for issue 4 in the NVIDIA/thrust repo.\n\nMarkdown is recommended for formatting more detailed messages, as these will\nbe nicely rendered on Github, etc.\n```\n\n## Push Development Branch to your Fork\n\nOnce you've committed your changes to a local development branch, it's time to\npush them to your fork:\n\n```\ncd /path/to/cub/checkout\ngit checkout my_descriptive_branch_name # if not already checked out\ngit push --set-upstream github-fork my_descriptive_branch_name\n```\n\n`--set-upstream github-fork` tells git that future pushes/pulls on this branch\nshould target your `github-fork` remote by default.\n\n## Create Pull Request\n\nTo create a pull request for your freshly pushed branch, open your github fork\nin a browser by going to `https://www.github.com/<GITHUB_USERNAME>/cub`. A\nprompt may automatically appear asking you to create a pull request if you've\nrecently pushed a branch.\n\nIf there's no prompt, go to \"Code\" > \"Branches\" and click the appropriate\n\"New pull request\" button for your branch.\n\nIf you would like a specific developer to review your patch, feel free to\nrequest them as a reviewer at this time.\n\nThe CUB team will review your patch, test it on NVIDIA's internal CI, and\nprovide feedback.\n\n## Address Feedback and Update Pull Request\n\nIf the reviewers request changes to your patch, use the following process to\nupdate the pull request:\n\n```\n# Make changes:\ncd /path/to/cub/sources\ngit checkout my_descriptive_branch_name\nemacs cub/some_file.cuh\nemacs tests/some_test.cu\n\n# Build + test\ncd /path/to/thrust/build/directory\ncmake --build . -j <num jobs>\nctest\n\n# Amend commit:\ncd /path/to/cub/sources\ngit add cub/some_file.cuh\ngit add tests/some_test.cu\ngit commit --amend\n# Or\ngit gui # Check the \"Amend Last Commit\" box\n\n# Update the branch on your fork:\ngit push -f\n```\n\nAt this point, the pull request should show your recent changes.\n\n## When Your PR is Approved\n\nOnce your pull request is approved by the CUB team, no further action is\nneeded from you. We will handle integrating it since we must coordinate changes\nto `main` with NVIDIA's internal perforce repository.\n\n# CMake Options\n\nA CUB build is configured using CMake options. These may be passed to CMake\nusing\n\n```\ncmake -D<option_name>=<value> /path/to/cub/sources\n```\n\nor configured interactively with the `ccmake` or `cmake-gui` interfaces.\n\nThe configuration options for CUB are:\n\n- `CMAKE_BUILD_TYPE={Release, Debug, RelWithDebInfo, MinSizeRel}`\n  - Standard CMake build option. Default: `RelWithDebInfo`\n- `CUB_ENABLE_HEADER_TESTING={ON, OFF}`\n  - Whether to test compile public headers. Default is `ON`.\n- `CUB_ENABLE_TESTING={ON, OFF}`\n  - Whether to build unit tests. Default is `ON`.\n- `CUB_ENABLE_EXAMPLES={ON, OFF}`\n  - Whether to build examples. Default is `ON`.\n- `CUB_ENABLE_DIALECT_CPPXX={ON, OFF}`\n  - Toggle whether a specific C++ dialect will be targeted.\n  - Multiple dialects may be targeted in a single build.\n  - Possible values of `XX` are `{11, 14, 17}`.\n  - By default, only C++14 is enabled.\n- `CUB_ENABLE_COMPUTE_XX={ON, OFF}`\n  - Controls the targeted CUDA architecture(s)\n  - Multiple options may be selected when using NVCC as the CUDA compiler.\n  - Valid values of `XX` are:\n    `{35, 37, 50, 52, 53, 60, 61, 62, 70, 72, 75, 80}`\n  - Default value depends on `CUB_DISABLE_ARCH_BY_DEFAULT`:\n- `CUB_ENABLE_COMPUTE_FUTURE={ON, OFF}`\n  - If enabled, CUDA objects will target the most recent virtual architecture\n    in addition to the real architectures specified by the\n    `CUB_ENABLE_COMPUTE_XX` options.\n  - Default value depends on `CUB_DISABLE_ARCH_BY_DEFAULT`:\n- `CUB_DISABLE_ARCH_BY_DEFAULT={ON, OFF}`\n  - When `ON`, all `CUB_ENABLE_COMPUTE_*` options are initially `OFF`.\n  - Default: `OFF` (meaning all architectures are enabled by default)\n- `CUB_ENABLE_TESTS_WITH_RDC={ON, OFF}`\n  - Whether to enable Relocatable Device Code when building tests.\n    Default is `OFF`.\n- `CUB_ENABLE_EXAMPLES_WITH_RDC={ON, OFF}`\n  - Whether to enable Relocatable Device Code when building examples.\n    Default is `OFF`.\n- `CUB_ENABLE_INSTALL_RULES={ON, OFF}`\n  - If true, installation rules will be generated for CUB. Default is `ON` when\n    building CUB alone, and `OFF` when CUB is a subproject added via CMake's\n    `add_subdirectory`.\n\n# Development Model\n\nThe following is a description of the basic development process that CUB follows. This is a living\ndocument that will evolve as our process evolves.\n\nCUB is distributed in three ways:\n\n   * On GitHub.\n   * In the NVIDIA HPC SDK.\n   * In the CUDA Toolkit.\n\n## Trunk Based Development\n\nCUB uses [trunk based development](https://trunkbaseddevelopment.com). There is a single long-lived\nbranch called `main`. Engineers may create branches for feature development. Such branches always\nmerge into `main`. There are no release branches. Releases are produced by taking a snapshot of\n`main` (\"snapping\"). After a release has been snapped from `main`, it will never be changed.\n\n## Repositories\n\nAs CUB is developed both on GitHub and internally at NVIDIA, there are three main places where code lives:\n\n   * The Source of Truth, the [public CUB repository](https://github.com/NVIDIA/cub), referred to as\n     `github` later in this document.\n   * An internal GitLab repository, referred to as `gitlab` later in this document.\n   * An internal Perforce repository, referred to as `perforce` later in this document.\n\n## Versioning\n\nCUB has its own versioning system for releases, independent of the versioning scheme of the NVIDIA\nHPC SDK or the CUDA Toolkit.\n\nToday, CUB version numbers have a specific [semantic meaning](https://semver.org/).\nReleases prior to 1.10.0 largely, but not strictly, followed these semantic meanings.\n\nThe version number for a CUB release uses the following format: `MMM.mmm.ss-ppp`, where:\n\n   * `CUB_VERSION_MAJOR`/`MMM`: Major version, up to 3 decimal digits. It is incremented\n     when the fundamental nature of the library evolves, leading to widespread changes across the\n     entire library interface with no guarantee of API, ABI, or semantic compatibility with former\n     versions.\n   * `CUB_VERSION_MINOR`/`mmm`: Minor version, up to 3 decimal digits. It is incremented when\n     breaking API, ABI, or semantic changes are made.\n   * `CUB_VERSION_SUBMINOR`/`ss`: Subminor version, up to 2 decimal digits. It is incremented\n     when notable new features or bug fixes or features that are API, ABI, and semantic backwards\n     compatible are added.\n   * `CUB_PATCH_NUMBER`/`ppp`: Patch number, up to 3 decimal digits. It is incremented if any\n     change in the repo whatsoever is made and no other version component has been incremented.\n\nThe `<cub/version.h>` header defines `CUB_*` macros for all of the version components mentioned\nabove. Additionally, a `CUB_VERSION` macro is defined, which is an integer literal containing all\nof the version components except for `CUB_PATCH_NUMBER`.\n\n## Branches and Tags\n\nThe following tag names are used in the CUB project:\n\n  * `github/nvhpc-X.Y`: the tag that directly corresponds to what has been shipped in the NVIDIA HPC SDK release X.Y.\n  * `github/cuda-X.Y`: the tag that directly corresponds to what has been shipped in the CUDA Toolkit release X.Y.\n  * `github/A.B.C`: the tag that directly corresponds to CUB version A.B.C.\n  * `github/A.B.C-rcN`: the tag that directly corresponds to CUB version A.B.C release candidate N.\n\nThe following branch names are used in the CUB project:\n\n  * `github/main`: the Source of Truth development branch of CUB.\n  * `github/old-master`: the old Source of Truth branch, before unification of public and internal repositories.\n  * `github/feature/<name>`: feature branch for a feature under development.\n  * `github/bug/<bug-system>/<bug-description>-<bug-id>`: bug fix branch, where `bug-system` is `github` or `nvidia`.\n  * `gitlab/main`: mirror of `github/main`.\n  * `perforce/private`: mirrored `github/main`, plus files necessary for internal NVIDIA testing systems.\n\nOn the rare occasion that we cannot do work in the open, for example when developing a change specific to an\nunreleased product, these branches may exist on `gitlab` instead of `github`. By default, everything should be\nin the open on `github` unless there is a strong motivation for it to not be open.\n"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/src/gpu_utils/cub/CONTRIBUTING.md",
      "technique": "file_exploration"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2018-05-16T15:30:12Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2023-10-18T06:21:18Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "OPUS-SSRI"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9780931206186796,
      "result": {
        "original_header": "Compilation",
        "type": "Text_excerpt",
        "value": "You can compile this program with fftw in external directory!\nFirst, modify the ```CMakeLists.txt``` file, replace the cuda architecture related variables to make it compatible with your GPU! and set the BASH2* to the mpi related paths on your system! Note that the default setting of cuda architecture is BASH3*. To execute this program on Nvidia GPUs with other CUDA architectures, you should change all BASH3* and BASH4* variables in BASH5* to the corresponding architecture of your GPUs. \n"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9995079726900994,
      "result": {
        "original_header": "OPUS-SSRI",
        "type": "Text_excerpt",
        "value": "\nOPUS-SSRI(Sparsity and Smoothness Regularized Imaging) is a stand-alone computer\nprogram for Maximum A Posteriori refinement of (multiple) 3D reconstructions in cryo-electron microscopy. It is developed in the\nresearch group of Jianpeng Ma in Baylor College of Medicine. This implementation is based on the software [RELION](https://www.ncbi.nlm.nih.gov/pubmed/22100448).\n \n"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9590424072463497,
      "result": {
        "original_header": "CTF refinement <a name=\"ctf\"></a>",
        "type": "Text_excerpt",
        "value": "We added the capacity to perform per-particle CTF refinement in a new version. You can use this program to perform consensus 3D refinement with per-particle CTF refinement. The program can be obtained as a docker image by\n```\ndocker pull alncat/ssri-torch:ctf1\n```\nTo run the CTF refinement, you need to create a docker container using the downloaded image by\n```\nsudo docker run --name ssri-ctf --gpus all  -it -v /data:/data alncat/ssri-torch:ctf1 bash\n```\nBASH3* is the name of container, BASH4* argument is used to bind directory in host to the directory in container, the first option before colon refers to the directory in host, the second option refers to the directory in the container.\nThe docker container can be accessed later via\n```\nsudo docker start ssri-ctf && sudo docker exec -it ssri-ctf bash\n```\n \nThe compiled program with CTF refinement capability is located at ```/root/gpu/ssri_remote/build/bin``` in your docker container. And the source code can be found at BASH7* .\nNote that the program is compiled against Nvidia GPU with compute capacity BASH8*. To execute this program on Nvidia GPUs with other CUDA architectures, you should recompile this program by changing all BASH8* and BASH9* variables in BASH10* to the corresponding architecture of your GPUs. You can then recompile the program by first loading the compilation toolset,\n```\nscl enable devtoolset-7 bash\n```\n \nOption | Function\n------------ | -------------\n--ctf_order   | Default is 2, start CTF refinement when the healpix sampling order is larger than this value\n--refine_ctf_angle | Add this option to your command to refine the defoucs angle\n--ctf_defocus_dev | Default is 1.0, the restraint strength of the deviation of defocus values from previous refinement\n--ctf_defocus_iso | Default is 0.5, the restraint strength of the difference between two defocus depths \nAn exmaple of the command for running OPUS-SSRI with CTF refinement is shown below,\n```\nmpiexec --allow-run-as-root -n 3 /root/gpu/ssri/remote/build/bin/relion_refine_mpi --o /output-folder --i particle-stack --ini_high 40 \\\n--dont_combine_weights_via_disc --pool 4 --ctf --ctf_corrected_ref --iter 25 --particle_diameter 256 \\\n--flatten_solvent --zero_mask --oversampling 1 --healpix_order 2 --offset_range 5 --offset_step 2 \\\n--norm --scale --j 8 --gpu 0,1,2,3 --tv_alpha 1.0 --tv_beta 2.0 --tv_eps 0.01 --tv_epsp 0.01 \\\n--tv_weight 0.1 --tv_lr 0.5 --tv_iters 150 --ref initial-map --free_gpu_memory 256 --auto_refine \\\n--split_random_halves --low_resol_join_halves 50 --tv --adaptive_fraction 0.94 --preread_images --sym C4 --sigma2_fudge 0.5 --ctf_defocus_iso 0.5\n\n```\nYou should replace BASH14* with your own path and files, respectively. You should also set the BASH15* according to the parameters of your system. For very large datasets that cannot fit in the memory, you should remove the BASH16* option.\n \n"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/alncat/cryoem/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 3
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/alncat/cryoem/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "alncat/cryoem"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "We ported SSRI to Relion 3.0! Check out https://github.com/alncat/ssri!!!"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://render.githubusercontent.com/render/math?math=%5Calpha%5Cfrac%7B%7Cx_j%7C%7D%7B(%7Cx_j%5Ei%7C%2B%5Cepsilon)%7D%20%2B%20%5Cbeta%5Cfrac%7B%5C%7C%5Cnabla%20x_j%5C%7C_2%7D%7B(%5C%7C%5Cnabla%20x_j%5Ei%5C%7C_2%5C%2B%5Cepsilon%5E')%7D%20%2B%20%5Cgamma%5C%7Cx_j-x_j%5Ei%20%5C%7C%5E2"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://render.githubusercontent.com/render/math?math=%5Cepsilon"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://render.githubusercontent.com/render/math?math=%5Cepsilon%5E'"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://render.githubusercontent.com/render/math?math=%5Calpha"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://render.githubusercontent.com/render/math?math=%5Cbeta"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://render.githubusercontent.com/render/math?math=%5Cgamma"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "parent_header": [
          "OPUS-SSRI"
        ],
        "type": "Text_excerpt",
        "value": "OPUS-SSRI can be obtained as a docker image by executing command below.\n\n```\ndocker pull alncat/opus-ssri:first\n```\nAnother important step is setting up gpu support for docker.\nYou can follow the instruction at https://www.tensorflow.org/install/docker .\nWe can then create a docker container from the docker image via\n```\nsudo docker run --name ssri-test --gpus all  -it -v /data:/data alncat/opus-ssri:first bash\n```\nAfter entering the docker containter, the OPUS-SSRI program is located at ```/relion-luo/build/bin```. You can perform the 3D refinement using the program in that directory.\n"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.8105472264211961,
      "result": {
        "original_header": "We ported SSRI to Relion 3.0! Check out https://github.com/alncat/ssri!!!",
        "type": "Text_excerpt",
        "value": "Our latest version can do CTF refinement with 3D refinement simultaneously!!! checkout [CTF refinement](#ctf)\n \n"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9999874344042391,
      "result": {
        "original_header": "Compilation",
        "type": "Text_excerpt",
        "value": "You can compile this program with fftw in external directory!\nFirst, modify the ```CMakeLists.txt``` file, replace the cuda architecture related variables to make it compatible with your GPU! and set the BASH2* to the mpi related paths on your system! Note that the default setting of cuda architecture is BASH3*. To execute this program on Nvidia GPUs with other CUDA architectures, you should change all BASH3* and BASH4* variables in BASH5* to the corresponding architecture of your GPUs. \nWe recommend using cmake >= 3.14, gnu8 and cuda 10.1 !\n```\nmkdir build & cd build\n```\nBASH7*\nRemember to replace the environment variables with your settings! \n"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9808809402669335,
      "result": {
        "original_header": "CTF refinement <a name=\"ctf\"></a>",
        "type": "Text_excerpt",
        "value": "We added the capacity to perform per-particle CTF refinement in a new version. You can use this program to perform consensus 3D refinement with per-particle CTF refinement. The program can be obtained as a docker image by\n```\ndocker pull alncat/ssri-torch:ctf1\n```\nTo run the CTF refinement, you need to create a docker container using the downloaded image by\n```\nsudo docker run --name ssri-ctf --gpus all  -it -v /data:/data alncat/ssri-torch:ctf1 bash\n```\nBASH3* is the name of container, BASH4* argument is used to bind directory in host to the directory in container, the first option before colon refers to the directory in host, the second option refers to the directory in the container.\nThe docker container can be accessed later via\n```\nsudo docker start ssri-ctf && sudo docker exec -it ssri-ctf bash\n```\n \nThe compiled program with CTF refinement capability is located at ```/root/gpu/ssri_remote/build/bin``` in your docker container. And the source code can be found at BASH7* .\nNote that the program is compiled against Nvidia GPU with compute capacity BASH8*. To execute this program on Nvidia GPUs with other CUDA architectures, you should recompile this program by changing all BASH8* and BASH9* variables in BASH10* to the corresponding architecture of your GPUs. You can then recompile the program by first loading the compilation toolset,\n```\nscl enable devtoolset-7 bash\n```\n \nthen executing\n```\ncd build && make\n```\nThe options for controlling the CTF refinement are listed below, \nAn exmaple of the command for running OPUS-SSRI with CTF refinement is shown below,\n```\nmpiexec --allow-run-as-root -n 3 /root/gpu/ssri/remote/build/bin/relion_refine_mpi --o /output-folder --i particle-stack --ini_high 40 \\\n--dont_combine_weights_via_disc --pool 4 --ctf --ctf_corrected_ref --iter 25 --particle_diameter 256 \\\n--flatten_solvent --zero_mask --oversampling 1 --healpix_order 2 --offset_range 5 --offset_step 2 \\\n--norm --scale --j 8 --gpu 0,1,2,3 --tv_alpha 1.0 --tv_beta 2.0 --tv_eps 0.01 --tv_epsp 0.01 \\\n--tv_weight 0.1 --tv_lr 0.5 --tv_iters 150 --ref initial-map --free_gpu_memory 256 --auto_refine \\\n--split_random_halves --low_resol_join_halves 50 --tv --adaptive_fraction 0.94 --preread_images --sym C4 --sigma2_fudge 0.5 --ctf_defocus_iso 0.5\n\n```\nYou should replace BASH14* with your own path and files, respectively. You should also set the BASH15* according to the parameters of your system. For very large datasets that cannot fit in the memory, you should remove the BASH16* option.\n \n"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/alncat/cryoem/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "GNU General Public License v2.0",
        "spdx_id": "GPL-2.0",
        "type": "License",
        "url": "https://api.github.com/licenses/gpl-2.0",
        "value": "https://api.github.com/licenses/gpl-2.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "cryoem"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "alncat"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "C++",
        "size": 3730849,
        "type": "Programming_language",
        "value": "C++"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Cuda",
        "size": 2625568,
        "type": "Programming_language",
        "value": "Cuda"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "C",
        "size": 386918,
        "type": "Programming_language",
        "value": "C"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "CMake",
        "size": 279432,
        "type": "Programming_language",
        "value": "CMake"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Makefile",
        "size": 6685,
        "type": "Programming_language",
        "value": "Makefile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 5833,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-04 13:56:44",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 10
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Usage",
        "parent_header": [
          "OPUS-SSRI"
        ],
        "type": "Text_excerpt",
        "value": "OPUS-SSRI introduces several more options to the 3D refinement program of RELION.\nOPUS-SSRI solves the 3D reconstruction problem in the maximization step with penalty function of the form,\n\n![\\alpha\\frac{|x_j|}{(|x_j^i|+\\epsilon)} + \\beta\\frac{\\|\\nabla x_j\\|_2}{(\\|\\nabla x_j^i\\|_2\\+\\epsilon^')} + \\gamma\\|x_j-x_j^i \\|^2](https://render.githubusercontent.com/render/math?math=%5Calpha%5Cfrac%7B%7Cx_j%7C%7D%7B(%7Cx_j%5Ei%7C%2B%5Cepsilon)%7D%20%2B%20%5Cbeta%5Cfrac%7B%5C%7C%5Cnabla%20x_j%5C%7C_2%7D%7B(%5C%7C%5Cnabla%20x_j%5Ei%5C%7C_2%5C%2B%5Cepsilon%5E')%7D%20%2B%20%5Cgamma%5C%7Cx_j-x_j%5Ei%20%5C%7C%5E2)\n\nor in text format,\n\n```\n\u03b1|x_j|/(|x_j^i|+\u03f5) + \u03b2\u2016\u2207x_j\u2016_2/(\u2016\u2207x_j^i\u2016_2+\u03f5') + \u03b3\u2016x_j-x_j^i \u2016^2.\n```\n\nThe new options are:\n\nOption | Function\n------------ | -------------\n--tv |toggle on the OPUS-SSRI 3D refinement protocol\n--tv_eps |the ![\\epsilon](https://render.githubusercontent.com/render/math?math=%5Cepsilon) in the above equation\n--tv_epsp |the ![\\epsilon^'](https://render.githubusercontent.com/render/math?math=%5Cepsilon%5E') in the above equation\n--tv_alpha |propotional to the ![\\alpha\\epsilon](https://render.githubusercontent.com/render/math?math=%5Calpha) in the above equation\n--tv_beta |propotional to the ![\\beta\\epsilon^'](https://render.githubusercontent.com/render/math?math=%5Cbeta) in the above equation\n--tv_weight |propotional to the ![\\gamma](https://render.githubusercontent.com/render/math?math=%5Cgamma) in the above equation\n--tv_iters |the number of iterations of the optimization algorithm\n--tv_lr |propotional to the learning rate of the optimization algorithm\n\nThe command for running OPUS-SSRI is shown below,\n\n```\nmpiexec --allow-run-as-root -n 3 /relion-luo/build/bin/relion_refine_mpi --o /output-folder --i particle-stack --ini_high 40 \\\n--dont_combine_weights_via_disc --pool 4 --ctf --ctf_corrected_ref --iter 25 --particle_diameter 256 \\\n--flatten_solvent --zero_mask --oversampling 1 --healpix_order 2 --offset_range 5 --offset_step 2 \\\n--norm --scale --j 8 --gpu 0,1,2,3 --tv_alpha 1.0 --tv_beta 2.0 --tv_eps 0.01 --tv_epsp 0.01 \\\n--tv_weight 0.1 --tv_lr 0.5 --tv_iters 150 --ref initial-map --free_gpu_memory 256 --auto_refine \\\n--split_random_halves --low_resol_join_halves 50 --tv --adaptive_fraction 0.94 --preread_images --sym C4\n\n```\nYou should replace ```output-folder, particle-stack and initial-map``` with your own path and files, respectively. You should also set the ```particle_diameter and sym``` according to the parameters of your system. For very large datasets that cannot fit in the memory, you should remove the ```preread_images``` option. ```tv_eps``` and ```tv_epsp``` are two adjustable parameters. You can set them around the level of density as ```the threshold for creating mask```. You can also experiment with different ```tv_alpha```, ```tv_beta``` and ```tv_weight```.\n"
      },
      "source": "https://raw.githubusercontent.com/alncat/cryoem/master/README.md",
      "technique": "header_analysis"
    }
  ]
}