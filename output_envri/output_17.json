{
  "application_domain": [
    {
      "confidence": 12.31,
      "result": {
        "type": "String",
        "value": "Audio"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/adobe-research/DeepAFx"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2021-05-07T17:52:50Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-07T11:54:16Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Third-party audio effects plugins as differentiable layers within deep neural networks."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9796314680569573,
      "result": {
        "original_header": "DeepAFx: Deep Audio Effects",
        "type": "Text_excerpt",
        "value": "Audio signal processing effects (FX) are used to manipulate sound characteristics across a variety of media. Many FX, however, can be difficult or tedious to use, particularly for novice users. In our work, we aim to simplify how audio FX are used by training a machine to use FX directly and perform automatic audio production tasks. By using familiar and existing tools for processing and suggesting control parameters, we can create a unique paradigm that blends the power of AI with human creative control to empower creators. For a quick demonstration, please see our demo video: \n\nTo combine deep learning and audio plugins together, we have developed a new method to incorporate third-party, audio signal processing effects (FX) plugins as layers within deep neural networks. We then use a deep encoder to analyze sounds and learn to control audio FX that themselves performs signal manipulation. To train our network with non-differentiable FX layers, we compute FX layer gradients via a fast, parallel stochastic approximation scheme within a standard automatic differentiation graph, enabling efficient end-to-end backpropagation for deep learning training. \n \n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8504272715411991,
      "result": {
        "original_header": "Paper",
        "type": "Text_excerpt",
        "value": "For technical details of the work, please see: \n\"[Differentiable Signal Processing with Black-Box Audio Effects.](https://arxiv.org/abs/2105.04752)\"\n[Marco A. Mart\u00ednez Ram\u00edrez](https://m-marco.com/about/), [Oliver Wang](http://www.oliverwang.info/), [Paris Smaragdis](https://paris.cs.illinois.edu/), and [Nicholas J. Bryan](https://ccrma.stanford.edu/~njb/). \nIEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2021. \n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9908612450381157,
      "result": {
        "original_header": "Developer",
        "type": "Text_excerpt",
        "value": "DeepAFX is built using [Tensorflow 2.2.0](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf) for deep learning and [Linux Audio Developer's Simple Plugin API v2](https://lv2plug.in) (LV2) audio effects plugins. LV2 is a royalty-free, open standard for audio plugins for both synthesis, processing, and host applications written in the C programming language.  To bridge the language gap between the LV2 plugins and Python, we use the [lilv](http://drobilla.net/docs/lilv/python/) LV2 library together our own research python pip package called `deepafx`. Our package provides a custom TF Keras layer that internally loads LV2 plugins and does gradient approximation during backpropagation.\n \n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9978141719833021,
      "result": {
        "original_header": "Custom TF Keras Operators and Gradients",
        "type": "Text_excerpt",
        "value": "At the core of this project is a custom TF keras layer with custom gradients. To develop this, we started with a series of small examples that compared finite different gradients with gradients from automatic differentiation via TF. We then built this up with more complex setups in the examples listed below.\n \n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Dataset Download",
        "parent_header": [
          "Developer",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "In this work, we developed three DeepAFX models: tube amplifier emulation (distortion), automatic non-speech sound removal (nonspeech) and automatic music mastering (mastering). For each of these tasks, we have provide scripts to download the necessary datasets to train each model. \n\nTo download all datasets (about 50GB), type:\n\n```\n# Within the docker container + deepafx code folder\ncd /home/code-base/runtime/deepafx/deepafx\npython download.py all\n\n```\n\nAlternatively, to download all datasets individually, type:\n\n```\n# Within the docker container + deepafx code folder\ncd /home/code-base/runtime/deepafx/deepafx\n\n# Download tube amplifier emulation/distortion dataset\npython download.py distortion\n\n# Download the nonspeech dataset\npython download.py nonspeech\n\n# Download the mastering dataset\npython download.py mastering\n\n# Note the mastering dataset is built on-the-fly and results much change depending on when you run the command.\n# The reconstruction is done can also be done individually via:\npython download.py mastering --mode download\npython download.py mastering --mode align\npython download.py mastering --mode resample\npython download.py mastering --mode all\n```\n\n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/adobe-research/DeepAFx/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/notebooks/notebook_plots_paper_Fig2.ipynb"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/notebooks/notebook_plots_paper_Fig2.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/notebooks/cmd_kill_functions.ipynb"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/notebooks/cmd_kill_functions.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/notebooks/testing.ipynb"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/notebooks/testing.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/notebooks/notebook_plots.ipynb"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/notebooks/notebook_plots.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 26
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/adobe-research/DeepAFx/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "adobe-research/DeepAFx"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "DeepAFx: Deep Audio Effects"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_build_file": [
    {
      "confidence": 1,
      "result": {
        "format": "docker_compose",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/docker-compose.yml"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/docker-compose.yml",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/Dockerfile",
      "technique": "file_exploration"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/images/video.png"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Docker Setup",
        "parent_header": [
          "Developer"
        ],
        "type": "Text_excerpt",
        "value": "Given that our work deeply combines python and Linux binaries (i.e. pre-compiled audio plugins), we provide a Dockerfile to fully reproduce our development environment. Docker is a set of tools and ecosystem to develop software in packages called containers, which act as light-weight virtual machines. You can directly use our dockerfile to run a Linux-based containerized development environment on Windows, MacOS, or Linux. \n\nOur dockerfile builds off of the `tensorflow/tensorflow:2.2.0-jupyter` docker image, adds our necessary LV2 dependencies and installs our `deepafx` python pip package for training and inference. In addition to the default [JupyterLab IDE](https://jupyterlab.readthedocs.io/en/stable/), we also install [code-server](https://github.com/cdr/code-server) into our development environment, which provides a variant of the popular [VS Code](https://github.com/Microsoft/vscode) IDE for development as well.  If you don't like [our Dockerfile](./Dockerfile), you can just use it as a recipe to recreate our development environment elsewhere.\n\n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/adobe-research/DeepAFx/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "ADOBE RESEARCH LICENSE\n\nThis license (the \"License\") between Adobe Inc., having a place of business at 345 Park Avenue, San Jose, California 95110-2704 (\"Adobe\"), and you, the individual or entity exercising rights under this License (\"you\" or \"your\"), sets forth the terms for your use of certain research materials that are owned by Adobe (the \"Licensed Materials\"). By exercising rights under this License, you accept and agree to be bound by its terms. If you are exercising rights under this license on behalf of an entity, then \"you\" means you and such entity, and you (personally) represent and warrant that you (personally) have all necessary authority to bind that entity to the terms of this License.\n\n1.\tGRANT OF LICENSE.\n\n1.1\tAdobe grants you a nonexclusive, worldwide, royalty-free, fully paid license to (A) reproduce, use, modify, and publicly display and perform the Licensed Materials for noncommercial research purposes only; and (B) redistribute the Licensed Materials, and modifications or derivative works thereof, for noncommercial research purposes only, provided that you give recipients a copy of this License.\n\n1.2\tYou may add your own copyright statement to your modifications and may provide additional or different license terms for use, reproduction, modification, public display and performance, and redistribution of your modifications and derivative works, provided that such license terms limit the use, reproduction, modification, public display and performance, and redistribution of such modifications and derivative works to noncommercial research purposes only.\n\n1.3\tFor purposes of this License, noncommercial research purposes include academic research, teaching, and testing, but do not include commercial licensing or distribution, development of commercial products, or any other activity which results in commercial gain.\n\n2.\tOWNERSHIP AND ATTRIBUTION. Adobe and its licensors own all right, title, and interest in the Licensed Materials. You must keep intact any copyright or other notices or disclaimers in the Licensed Materials.\n\n3.\tDISCLAIMER OF WARRANTIES. THE LICENSED MATERIALS ARE PROVIDED \"AS IS\" WITHOUT WARRANTY OF ANY KIND. THE ENTIRE RISK AS TO THE RESULTS AND PERFORMANCE OF THE LICENSED MATERIALS IS ASSUMED BY YOU. ADOBE DISCLAIMS ALL WARRANTIES, EXPRESS, IMPLIED OR STATUTORY, WITH REGARD TO ANY LICENSED MATERIALS PROVIDED UNDER THIS LICENSE, INCLUDING, BUT NOT LIMITED TO, ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NONINFRINGEMENT OF THIRD-PARTY RIGHTS.\n\n4.\tLIMITATION OF LIABILITY. IN NO EVENT WILL ADOBE BE LIABLE FOR ANY ACTUAL, INCIDENTAL, SPECIAL OR CONSEQUENTIAL DAMAGES OF ANY NATURE WHATSOEVER, INCLUDING WITHOUT LIMITATION, LOSS OF PROFITS OR OTHER COMMERCIAL LOSS, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF ANY LICENSED MATERIALS PROVIDED UNDER THIS LICENSE, EVEN IF ADOBE HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n5.\tTERM AND TERMINATION.  \n\n5.1\tThe License is effective upon acceptance by you and will remain in effect unless terminated earlier as permitted under this License.\n\n5.2\tIf you breach any material provision of this License, then your rights will terminate immediately.\n\n5.3\tAll clauses which by their nature should survive the termination of this License will survive such termination. In addition, and without limiting the generality of the preceding sentence, Sections 2 (Ownership and Attribution), 3 (Disclaimer of Warranties), 4 (Limitation of Liability) will survive termination of this License.\n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/LICENSE",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License",
        "parent_header": [
          "Custom TF Keras Operators and Gradients"
        ],
        "type": "Text_excerpt",
        "value": "Copyright (c) Adobe Systems Incorporated. All rights reserved.\n\nLicensed under [ADOBE RESEARCH LICENSE](./LICENSE).\n\n\n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "DeepAFx"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "adobe-research"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 5273808,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 443715,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Dockerfile",
        "size": 5336,
        "type": "Programming_language",
        "value": "Dockerfile"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_documentation": [
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "https://jupyterlab.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2105.04752"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2105.04752](https://arxiv.org/abs/2105.04752"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Build &amp; Run Development Environment",
        "parent_header": [
          "Developer",
          "Docker Setup"
        ],
        "type": "Text_excerpt",
        "value": "Install [Docker](https://docs.docker.com/get-docker/) and [docker-compose](https://docs.docker.com/compose/install/) on your local system. To verify, make sure you can see a Docker icon in your OS toolbar and/or confirm via running `docker --version` and `docker-compose -v`. Once confirmed, you can \n\n\n```\n# Clone this repository\ngit clone http://github.com/adobe-research/deepafx\n\n# Move into git folder\ncd <deepafx>\n\n# Build and run the docker image -> container\ndocker-compose up --build -d\n\n# Specify a shared directory between your local machine and docker to share data\n# Note: You can alternatively update the docker-compose.yml to use a Named Volume by \n# changing $DEEPAFX_DATA/:/home/code-base/scratch_space to $data/:/home/code-base/scratch_space\nexport DEEPAFX_DATA=<path/for/shared/data>\n\n# Run an existing image\ndocker-compose up \n\n# Open your IDE of choice\n# For VS Code, open a web browser at http://0.0.0.0:8887 (password is dsp)\n# For Jupyter, open a web browser at http://127.0.0.1:8888 (password is dsp)\n\n# Within the IDE, open a terminal and navigate to the code within the container\ncd /home/code-base/runtime\n\n# Please change the passwords for any remote development.\n```\n\nFor command line SSH access to the container when running locally, open a second terminal, find the running container id, and enter it\n\n```\ndocker container ls\ndocker exec -it <CONTAINER ID> bash\n```\n\nOnce you open the web IDE or ssh into the container, everything is installed as needed, and you can start using DeepAFX as discussed below. \n\n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier"
  ],
  "somef_provenance": {
    "date": "2024-10-03 22:54:16",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 187
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Usage",
        "parent_header": [
          "Developer"
        ],
        "type": "Text_excerpt",
        "value": "This repository is intended for educational and research purposes (full license below). We overview downloading our pre-trained models and datasets for our tasks as well as training and evaluating the models. Further below, we also provide several examples on developing TF Keras layers with custom gradients.\n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Training &amp; Configurations",
        "parent_header": [
          "Developer",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "To train one or more of the models, \n\n```\n# Within the docker container + deepafx code folder\ncd /home/code-base/runtime/deepafx/deepafx\n\n# Train the tube amplifier emulation/distortion task\npython train_distortion.py\n\n# Train the nonspeech removal tasks\npython train_nonspeech.py\n\n# Train the music mastering task\npython train_mastering.py\n```\n\nThese scripts will train for the specifc task and save the trained models, training history, config files and gradient logs (if enabled). The training scripts will also use the evaluate function from `evaluate.py` to test the trained model. The function computes the objective metrics and saves the input, target and output audio samples and the parameter automation. The notebook `notebook_plots.ipynb` plots and saves specific test audio samples and parameter automation curves for a given model.\n\nYou can edit the respective training configuration files before training each task, such as selecting the type of encoder, audio plugins, trainable parameters, values of non-trainable parameters, new range of parameters, etc. via editing the following files: `config_distortion.py`, `config_nonspeech.py`, and `config_mastering.py`.\n\n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Evaluation",
        "parent_header": [
          "Developer",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "The script `evaluate.py` allows evaluating a trained model with the test dataset(s). The config file saved from training (numpy dictionary *.params.npy) and the weights (*.h5) are required for this, along with the task name and output directory.\n\nThe script receives the following command line positional arguments:\n  `task` - string from 'distortion', 'nonspeech' or 'mastering'\n  `model_path` - absolute filepath to the model .h5 file.\n  `params_path` - absolute filepath to the params.npy file.\n  `output_dir` - absolute path to output folder \n  `dafx_wise` (optional) = integer that indicates which audio plugins from the Fx chain are going to be used. For example, for the mastering task; FxChain = Compressor, EQ, Limiter; If `dafx_wise=2` the script will only use the Compressor and the EQ. Useful for testing audio fxchain and progressive training. Default is 0, which means all audio plugins are used.\n\nExamples:\n\n```\n# Within the docker container + deepafx code folder\ncd /home/code-base/runtime/deepafx/deepafx\n\n# Evaluate \n# Note: Pretrained models can be found /home/code-base/runtime/deepafx/models\npython evaluate.py nonspeech </path/to/model>.h5 </path/to/params>.npy /path/to/output/folder\n\n# Evaluate distortion with only output with the first two LV2 plugins in the DSP chain\ncd /home/code-base/runtime/deepafx/deepafx\npython evaluate.py distortion </path/to/model>.h5 </path/to/params>.npy /path/to/output/folder --dafx 2\n```\n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Inference",
        "parent_header": [
          "Developer",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "To run our models on any audio file, you can use type:\n\n```\n# Within the docker container + deepafx code folder\ncd /home/code-base/runtime/deepafx/deepafx\n\n# <task> below can be one of: distortion, nonspeech, mastering\npython inference.py <task> --input_file <path/to/input>.wav --output_file <path/to/output>.wav\n```\n\n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Notes",
        "parent_header": [
          "Developer",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "- To save the gradient during training, enable the global variable `kLogGradient` from `dafx_layer.py`\n\n- The function `set_param` from the class `Parallel_Batch` doesn't work when using an audio fx chain (`kFxChain=True`) due to threading communication issues. This means that the values for the non-trainable parameters can only be set at the constructor (using values from `config_*.py` and not after the dafx layer is created.\n\n- TODO FIX: The function `layers.compute_time_shifting()` has the argument `samples_delay_max` which corresponds to the max delay we considered as group delay. For the distortion and nonspeech tasks its value is 100 samples. For the mastering task it yields better results as 300 samples (due to the longer group delay of the Fx chain). This change is not happening automatically. \n\n- The evaluate function from `evaluate.py` and `evaluate_trained_model.py` renders up to 50 seconds from each audio file, but this constant can be changed manually. \n\n- For the distortion task, the models were trained with an FxChain of Compressor and Limiter (to see whether the Limiter would help to obtain a better matching, it didn't). So when you test these models, add the `--dafx 1` command line argument. This command line arg is not needed for the other ICASSP models. \n\n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Examples",
        "parent_header": [
          "Custom TF Keras Operators and Gradients"
        ],
        "type": "Text_excerpt",
        "value": "* [Example 1: Custom Gradient of Scalar](scripts/custom_grad_example1.py)\n* [Example 2: Custom Gradient of Two Scalars](scripts/custom_grad_example2.py)\n* [Example 3: Custom Gradient of a Vector and Scalar](scripts/custom_grad_example3.py)\n* [Example 4: Custom Gradient of Two Vectors](scripts/custom_grad_example4.py)\n* [Example 5: Custom Gradient of Batches of Two Vectors](scripts/custom_grad_example5.py)\n* [Example 6: Custom Gradient in Keras Layer](scripts/custom_grad_example6.py)\n\nOnce we built this up enough, we moved over to building a custom keras layer that loads an LV2 plugin and approximates the gradient. You can see our custom LV2 TF Keras layer and a very basic toy example below\n\n* [Example 7: LV2 Keras Layer](deepafx/dafx_layer.py)\n* [Example 8: Learning to Normalize Toy Example](deepafx/train_toy.py)\n\n"
      },
      "source": "https://raw.githubusercontent.com/adobe-research/DeepAFx/main/README.md",
      "technique": "header_analysis"
    }
  ]
}