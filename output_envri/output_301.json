{
  "application_domain": [
    {
      "confidence": 36.41,
      "result": {
        "type": "String",
        "value": "Natural Language Processing"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "How to cite this dataset:",
        "type": "Text_excerpt",
        "value": "Our paper: \n```\n@Article{epidemiologia2030024,\nAUTHOR = {Banda, Juan M. and Tekumalla, Ramya and Wang, Guanyu and Yu, Jingyuan and Liu, Tuo and Ding, Yuning and Artemova, Ekaterina and Tutubalina, Elena and Chowell, Gerardo},\nTITLE = {A Large-Scale COVID-19 Twitter Chatter Dataset for Open Scientific Research\u2014An International Collaboration},\nJOURNAL = {Epidemiologia},\nVOLUME = {2},\nYEAR = {2021},\nNUMBER = {3},\nPAGES = {315--324},\nURL = {https://www.mdpi.com/2673-3986/2/3/24},\nISSN = {2673-3986},\nDOI = {10.3390/epidemiologia2030024}\n}\n```\n\nVersion 162\n\n```\n@dataset{banda_juan_m_2020_3757272,\n  author       = {Banda, Juan M. and\n                  Tekumalla, Ramya and\n                  Wang, Guanyu and\n                  Yu, Jingyuan and\n                  Liu, Tuo and\n                  Ding, Yuning and\n                  Artemova, Katya and\n                  Tutubalin\u0430, Elena and\n                  Chowell, Gerardo},\n  title        = {{A large-scale COVID-19 Twitter chatter dataset for \n                   open scientific research - an international\n                   collaboration}},\n  month        = Feb,\n  year         = 2023,\n  note         = {{This dataset will be updated bi-weekly at least \n                   with additional tweets, look at the github repo\n                   for these updates. Release: We have standardized\n                   the name of the resource to match our pre-print\n                   manuscript and to not have to update it every\n                   week.}},\n  publisher    = {Zenodo},\n  version      = {145},\n  doi          = {10.5281/zenodo.3723939},\n  url          = {https://doi.org/10.5281/zenodo.3723939}\n}\n\n```\n"
      },
      "source": "https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Banda, Juan M. and Tekumalla, Ramya and Wang, Guanyu and Yu, Jingyuan and Liu, Tuo and Ding, Yuning and Artemova, Ekaterina and Tutubalina, Elena and Chowell, Gerardo",
        "doi": "10.3390/epidemiologia2030024",
        "format": "bibtex",
        "title": "A Large-Scale COVID-19 Twitter Chatter Dataset for Open Scientific Research\u2014An International Collaboration",
        "type": "Text_excerpt",
        "url": "https://www.mdpi.com/2673-3986/2/3/24",
        "value": "@article{epidemiologia2030024,\n    doi = {10.3390/epidemiologia2030024},\n    issn = {2673-3986},\n    url = {https://www.mdpi.com/2673-3986/2/3/24},\n    pages = {315--324},\n    number = {3},\n    year = {2021},\n    volume = {2},\n    journal = {Epidemiologia},\n    title = {A Large-Scale COVID-19 Twitter Chatter Dataset for Open Scientific Research\u2014An International Collaboration},\n    author = {Banda, Juan M. and Tekumalla, Ramya and Wang, Guanyu and Yu, Jingyuan and Liu, Tuo and Ding, Yuning and Artemova, Ekaterina and Tutubalina, Elena and Chowell, Gerardo},\n}"
      },
      "source": "https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/thepanacealab/covid19_twitter"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-03-23T00:15:24Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-22T03:24:34Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Covid-19 Twitter dataset for non-commercial research use and pre-processing scripts - under active development"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9631850265044433,
      "result": {
        "original_header": "Mainted by:",
        "type": "Text_excerpt",
        "value": "[Panacea Lab](www.panacealab.org) - [Georgia State University](www.gsu.edu) - [Juan M. Banda](www.jmbanda.com), Ramya Tekumalla, and Gerardo Chowell-Puente.\nAdditional data provided by: Guanyu Wang (Missouri school of journalism, University of Missouri), Jingyuan Yu (Department of social psychology, Universitat Aut\u00f2noma de Barcelona), Tuo Liu (Department of psychology, Carl von Ossietzky Universit\u00e4t Oldenburg), Yuning Ding (Language technology lab, Universit\u00e4t Duisburg-Essen), Katya Artemova (NRU HSE) and Elena Tutubalina (KFU)\n \n"
      },
      "source": "https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9031229459485262,
      "result": {
        "original_header": "Version 153 release notes",
        "type": "Text_excerpt",
        "value": "Version 153 of the dataset. In this release we introduced splitting of the main dataset files for easier upload to Zenodo. The dataset files: full_dataset.tsv.gz and full_dataset_clean.tsv.gz have been split in 1 GB parts using the Linux utility called Split. So make sure to join the parts before unzipping. We had to make this change as we had huge issues uploading files larger than 2GB's (hence the delay in the dataset releases) \n \n"
      },
      "source": "https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/thepanacealab/covid19_twitter/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/COVID_19_dataset_Tutorial.ipynb"
      },
      "source": "https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/COVID_19_dataset_Tutorial.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 186
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/thepanacealab/covid19_twitter/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "thepanacealab/covid19_twitter"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "source": "https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "identifier": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://doi.org/10.5281/zenodo.7633479"
      },
      "source": "https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9785253754403442,
      "result": {
        "original_header": "Version 153 release notes",
        "type": "Text_excerpt",
        "value": "Version 153 of the dataset. In this release we introduced splitting of the main dataset files for easier upload to Zenodo. The dataset files: full_dataset.tsv.gz and full_dataset_clean.tsv.gz have been split in 1 GB parts using the Linux utility called Split. So make sure to join the parts before unzipping. We had to make this change as we had huge issues uploading files larger than 2GB's (hence the delay in the dataset releases) \n \n"
      },
      "source": "https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/thepanacealab/covid19_twitter/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "dataset, dissemination, frequent-terms, ngrams, retweets, tweets, tweets-acquired, twitter-stream"
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "covid19_twitter"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "thepanacealab"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 358796,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 23991,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "R",
        "size": 15657,
        "type": "Programming_language",
        "value": "R"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "license",
    "faq",
    "support",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-03 23:58:59",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 474
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Covid-19 Twitter chatter dataset for scientific use",
        "type": "Text_excerpt",
        "value": "Due to the relevance of the COVID-19 global pandemic, we are releasing our dataset of tweets acquired from the Twitter Stream related to COVID-19 chatter. The first 9 weeks of data (from January 1st, 2020 to March 11th, 2020) contain very low tweet counts as we filtered other data we were collecting for other research purposes, however, one can see the dramatic increase as the awareness for the virus spread. Dedicated data gathering started from March 11th yielding over 4 million tweets a day.\n\nThe data collected from the stream captures all languages, but the higher prevalence are:  English, Spanish, and French. We release all tweets and retweets on the full dataset, and a cleaned version with no retweets. There are several practical reasons for us to leave the retweets, tracing important tweets and their dissemination is one of them. For NLP tasks we provide the top 1000 frequent terms, the top 1000 bigrams, and the top 1000 trigrams. Some general statistics per day are included for both datasets.\n\nWe will continue to update the dataset every two days here and weekly in Zenodo. \n\nFor more information on processing and visualizations please visit: www.panacealab.org/covid19\n"
      },
      "source": "https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Usage",
        "type": "Text_excerpt",
        "value": "All tweets ids found in full_dataset.tsv and full_dataset-clean.tsv need to be hydrated using a tool like get_metada.py from the Social Media Toolkit (SMMT) released by our lab or Twarc. \n\nNote: All the code in the /processing_code folder is provided as-is, it was used to generate the provided files from the source Tweet JSON files. Documentation will be gradually added for these scripts. \n\nWe added a [Colab Notebook tutorial](COVID_19_dataset_Tutorial.ipynb) with some code to help you hydrate and pre-process the dataset. Note that this is just for illustration and will not download and process the whole dataset for you.\n\n"
      },
      "source": "https://raw.githubusercontent.com/thepanacealab/covid19_twitter/master/README.md",
      "technique": "header_analysis"
    }
  ]
}