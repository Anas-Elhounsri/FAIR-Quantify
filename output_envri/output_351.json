{
  "acknowledgement": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Acknowledgments",
        "type": "Text_excerpt",
        "value": "Any published work is built on top of the work of many other people, and the credit belongs to too many people to list here.\n* The Python and PyTorch developer communities have shared many invaluable insights, examples and ideas on the Web.\n* The authors of the research papers implemented in the [Distiller model-zoo](https://intellabs.github.io/distiller/model_zoo.html) have shared their research ideas, theoretical background and results.\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Built With",
        "parent_header": [
          "Acknowledgments"
        ],
        "type": "Text_excerpt",
        "value": "* [PyTorch](http://pytorch.org/) - The tensor and neural network framework used by Distiller.\n* [Jupyter](http://jupyter.org/) - Notebook serving.\n* [TensorBoard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) - Used to view training graphs.\n* [Cadene](https://github.com/Cadene/pretrained-models.pytorch) - Pretrained PyTorch models.\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "application_domain": [
    {
      "confidence": 19.69,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "author": "Neta Zmora and\nGuy Jacob and\nLev Zlotnik and\nBar Elharar and\nGal Novik",
        "format": "bibtex",
        "title": "Neural Network Distiller: A Python Package For DNN Compression Research",
        "type": "Text_excerpt",
        "url": "https://arxiv.org/abs/1910.12232",
        "value": "@article{nzmora2019distiller,\n    url = {https://arxiv.org/abs/1910.12232},\n    year = {2019},\n    month = {October},\n    title = {Neural Network Distiller: A Python Package For DNN Compression Research},\n    author = {Neta Zmora and\nGuy Jacob and\nLev Zlotnik and\nBar Elharar and\nGal Novik},\n}"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/IntelLabs/distiller"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2018-04-24T14:58:12Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-30T04:31:43Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Neural Network Distiller by Intel AI Lab: a Python package for neural network compression research.  https://intellabs.github.io/distiller"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9995277047271495,
      "result": {
        "original_header": "Highlighted features",
        "type": "Text_excerpt",
        "value": "* **Automatic Compression**\n  - [Automated Model Compression](https://github.com/IntelLabs/distiller/blob/master/examples/auto_compression/amc) (AMC)\n* **Weight pruning**\n  - Element-wise pruning using magnitude thresholding, sensitivity thresholding, target sparsity level, and activation statistics \n* **Structured pruning**\n  - Convolution: 2D (kernel-wise), 3D (filter-wise), 4D (layer-wise), and channel-wise structured pruning.  \n  - Fully-connected: column-wise and row-wise structured pruning.\n  - Structure groups (e.g. structures of 4 filters).\n  - Structure-ranking with using weights or activations criteria (Lp-norm, APoZ, gradients, random, etc.).\n  - Support for new structures (e.g. block pruning)\n* **Control**\n  - Soft (mask on forward-pass only) and hard pruning (permanently disconnect neurons)\n  - Dual weight copies (compute loss on masked weights, but update unmasked weights)  \n  - Model thinning (AKA \"network garbage removal\") to permanently remove pruned neurons and connections.\n* **Schedule**\n  - Flexible scheduling of pruning, regularization, and learning rate decay (compression scheduling)\n  - One-shot and iterative pruning (and fine-tuning) are supported.\n  - Easily control what is performed each training step (e.g. greedy layer by layer pruning to full model pruning). \n  - Automatic gradual schedule (AGP) for pruning individual connections and complete structures.\n  - The compression schedule is expressed in a YAML file so that a single file captures the details of experiments.  This [dependency injection](https://en.wikipedia.org/wiki/Dependency_injection) design decouples the Distiller scheduler and library from future extensions of algorithms.\n* Element-wise and filter-wise pruning **sensitivity analysis** (using L1-norm thresholding). Examine the data from some of the networks we analyzed, using [this notebook](https://github.com/IntelLabs/distiller/blob/master/jupyter/sensitivity_analysis.ipynb).\n* **Regularization**\n  - L1-norm element-wise regularization\n  - Group Lasso an group variance regularization   \n* **Quantization**\n  - Automatic mechanism to transform existing models to quantized versions, with customizable bit-width configuration for different layers. No need to re-write the model for different quantization methods.\n  - [Post-training quantization](https://intellabs.github.io/distiller/usage.html#post-training-quantization) of trained full-precision models, dynamic and static (statistics-based)\n  - Support for [quantization-aware training](https://intellabs.github.io/distiller/algo_quantization.html#quantization-aware-training) in the loop\n* **Knowledge distillation**\n  - Training with [knowledge distillation](https://intellabs.github.io/distiller/knowledge_distillation.html), in conjunction with the other available pruning / regularization / quantization methods.\n* **Conditional computation**\n  - Sample implementation of Early Exit\n* **Low rank decomposition**\n  - Sample implementation of [truncated SVD](https://github.com/IntelLabs/distiller/blob/master/jupyter/truncated_svd.ipynb)\n* Lottery Ticket Hypothesis training \n* Export statistics summaries using Pandas dataframes, which makes it easy to slice, query, display and graph the data.\n* A set of [Jupyter notebooks](https://intellabs.github.io/distiller/jupyter.html) to plan experiments and analyze compression results.  The graphs and visualizations you see on this page originate from the included Jupyter notebooks.  \n  + Take a look at [this notebook](https://github.com/IntelLabs/distiller/blob/master/jupyter/alexnet_insights.ipynb), which compares visual aspects of dense and sparse Alexnet models.\n  + [This notebook](https://github.com/IntelLabs/distiller/blob/master/jupyter/model_summary.ipynb) creates performance indicator graphs from model data.\n* Sample implementations of published research papers, using library-provided building blocks.  See the  research papers discussions in our [model-zoo](https://intellabs.github.io/distiller/model_zoo.html).\n* Logging to the console, text file and TensorBoard-formatted file.\n* Export to **ONNX** (export of quantized models pending ONNX standardization)\n \n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8029427501755885,
      "result": {
        "original_header": "Versioning",
        "type": "Text_excerpt",
        "value": "We use [SemVer](http://semver.org/) for versioning. For the versions available, see the [tags on this repository](https://github.com/IntelLabs/distiller/tags).\n \n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8986337161396536,
      "result": {
        "original_header": "Community",
        "type": "Text_excerpt",
        "value": "- [TorchFI](https://github.com/bfgoldstein/torchfi) - TorchFI is a fault injection framework build on top of PyTorch for research purposes. \n- Shangqian Gao , Cheng Deng , and Heng Huang.<br>\n *[Cross Domain Model Compression by Structurally Weight Sharing](http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html),*<br>\n In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8973-8982.\n \n- Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass.<br>\n  *[FAKTA: An Automatic End-to-End Fact Checking System](https://arxiv.org/abs/1906.04164),*<br>\n  In North American Chapter of the Association for Computational Linguistics (NAACL), 2019. \n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9978311253162279,
      "result": {
        "original_header": "Disclaimer",
        "type": "Text_excerpt",
        "value": "Distiller is released as a reference code for research purposes. It is not an official Intel product, and the level of quality and support may not be as expected from an official product. Additional algorithms and features are planned to be added to the library. Feedback and contributions from the open source and research communities are more than welcome.\n \n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9645400918049928,
      "result": {
        "type": "Text_excerpt",
        "value": "> :warning: **DISCONTINUATION OF PROJECT** - *This project will no longer be maintained by Intel.  This project has been identified as having known security escapes.  Intel has ceased development and contributions including, but not limited to, maintenance, bug fixes, new releases, or updates, to this project.* **Intel no longer accepts patches to this project.** \n\n**Distiller** is an open-source Python package for neural network compression research. \nNetwork compression can reduce the memory footprint of a neural network, increase its inference speed and save energy. Distiller provides a  environment for prototyping and analyzing compression algorithms, such as sparsity-inducing methods and low-precision arithmetic. \n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/IntelLabs/distiller/tree/master/docs"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/IntelLabs/distiller/tree/master/docs-src/docs"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Generating the HTML documentation site",
        "type": "Text_excerpt",
        "value": "Install mkdocs and the required packages by executing:\n\n```\n$ pip3 install -r doc-requirements.txt\n```\n\nTo build the project documentation run:\n```\n$ cd distiller/docs-src\n$ mkdocs build --clean\n```\nThis will create a folder named 'site' which contains the documentation website.\nOpen distiller/docs/site/index.html to view the documentation home page.\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "format": "wiki",
        "type": "Url",
        "value": "https://github.com/IntelLabs/distiller/wiki"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/IntelLabs/distiller/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/greedy_pruning/greedy_pruning.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/greedy_pruning/greedy_pruning.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/word_language_model/quantize_lstm.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/word_language_model/quantize_lstm.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/auto_compression/amc/jupyter/amc_resnet20.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/auto_compression/amc/jupyter/amc_resnet20.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/auto_compression/amc/jupyter/amc_random.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/auto_compression/amc/jupyter/amc_random.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/auto_compression/amc/jupyter/amc_worksheet.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/auto_compression/amc/jupyter/amc_worksheet.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/auto_compression/amc/jupyter/amc_plain20.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/auto_compression/amc/jupyter/amc_plain20.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/GNMT/quantize_gnmt.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/GNMT/quantize_gnmt.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/L1-regularization.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/L1-regularization.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/compare_executions.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/compare_executions.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/save_intermediate_feature_maps.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/save_intermediate_feature_maps.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/compression_insights.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/compression_insights.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/language_model.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/language_model.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/experimental.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/experimental.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/interactive_lr_scheduler.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/interactive_lr_scheduler.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/performance.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/performance.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/pruning_channels_and_filters.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/pruning_channels_and_filters.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/agp_schedule.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/agp_schedule.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/activation_histograms.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/activation_histograms.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/post_train_quant_convert_pytorch.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/post_train_quant_convert_pytorch.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/model_summary.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/model_summary.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/what_are_you_looking_at.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/what_are_you_looking_at.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/distiller_jupyter_helpers.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/distiller_jupyter_helpers.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/truncated_svd.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/truncated_svd.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/parameter_histograms.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/parameter_histograms.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/alexnet_insights.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/alexnet_insights.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/tutorial__adding_new_model.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/tutorial__adding_new_model.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/sensitivity_analysis.ipynb"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/jupyter/sensitivity_analysis.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 799
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/IntelLabs/distiller/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "IntelLabs/distiller"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/ncf/extract_dataset.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/ncf/download_dataset.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/ncf/verify_dataset.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/quantization/preact_resnet_cifar_quant_distill_tests.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/quantization/post_train_quant/run_all_ptq_cmdline_samples.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/object_detection_compression/data/download_dataset.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/GNMT/download_dataset.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/GNMT/verify_dataset.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/GNMT/download_trained_model.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "identifier": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://zenodo.org/badge/latestdoi/130871393"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/imgs/banner1.png"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/imgs/simplenet_training.png"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/imgs/ch_sparsity_stats.png"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/imgs/ch_sparsity_stats_barchart.png"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/imgs/ch_compute_stats.png"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/imgs/resnet18-sensitivity.png"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "## Spinningup installation\n\nSpinup require that we use exactly Python 3.6 so if you are not using this Python version see the instructions here:\n    http://ubuntuhandbook.org/index.php/2017/07/install-python-3-6-1-in-ubuntu-16-04-lts/\n```\n    $ sudo update-alternatives --config python3\n```\n\nFor Python 3.6 you may also need to install a new virtual-env:\n```\n    $ sudo apt-get install python3.6-venv\n```\n\nThen create and activate your venv, and populate it with the Distiller packages:\n```\n    $ python3 -m venv  distiller_env_python3.6\n    $ source distiller_env_python3.6/bin/activate\n    $ pip3 install -r requirements.txt\n```\n\nFinally, you need to install Spinup into this venv.  First clone Spinup and then install it into your venv:\n```\n    $ cd <spinningup-repo>\n    $ sudo apt-get install python3.6-dev\n    $ pip3 install -e .\n```\n\nSee also:\nhttps://spinningup.openai.com/en/latest/user/installation.html?highlight=license\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/examples/auto_compression/amc/rl_libs/spinningup/installation.md",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# Distiller Installation\n\nThese instructions will help get Distiller up and running on your local machine.\n\nYou may also want to refer to these resources:\n\n* [Image classification dataset installation instructions](https://github.com/IntelLabs/distiller/tree/master/examples/classifier_compression#datasets-supported)\n* [Jupyter installation instructions](https://intellabs.github.io/distiller/jupyter.html#installation)\n\nNotes:\n- Distiller has only been tested on Ubuntu 16.04 LTS, and with Python 3.5.\n- If you are not using a GPU, you might need to make small adjustments to the code.\n\n## Clone Distiller\nClone the Distiller code repository from github:\n```\n$ git clone https://github.com/IntelLabs/distiller.git\n```\nThe rest of the documentation that follows, assumes that you have cloned your repository to a directory called ```distiller```. <br>\n\n## Create a Python virtual environment\nWe recommend using a [Python virtual environment](https://docs.python.org/3/library/venv.html#venv-def), but that of course, is up to you.\nThere's nothing special about using Distiller in a virtual environment, but we provide some instructions, for completeness.<br>\nBefore creating the virtual environment, make sure you are located in directory ```distiller```.  After creating the environment, you should see a directory called ```distiller/env```.\n<br>\n### Using virtualenv\nIf you don't have virtualenv installed, you can find the installation instructions [here](https://packaging.python.org/guides/installing-using-pip-and-virtualenv/).\n\nTo create the environment, execute:\n```\n$ python3 -m virtualenv env\n```\nThis creates a subdirectory named ```env``` where the python virtual environment is stored, and configures the current shell to use it as the default python environment.\n\n### Using venv\nIf you prefer to use ```venv```, then begin by installing it:\n```\n$ sudo apt-get install python3-venv\n```\nThen create the environment:\n```\n$ python3 -m venv env\n```\nAs with virtualenv, this creates a directory called ```distiller/env```.<br>\n\n### Activate the environment\nThe environment activation and deactivation commands for ```venv``` and ```virtualenv``` are the same.<br>\n**!NOTE: Make sure to activate the environment, before proceeding with the installation of the dependency packages:<br>**\n```\n$ source env/bin/activate\n```\n\n## Install the package\nFinally, install the Distiller package and its dependencies using ```pip3```:\n```\n$ cd distiller\n$ pip3 install -e .\n```\nThis installs Distiller in \"development mode\", meaning any changes made in the code are reflected in the environment without re-running the install command (so no need to re-install after pulling changes from the Git repository).\n\nPyTorch is included in the ```requirements.txt``` file, and will currently download PyTorch version 1.0.1 for CUDA 9.0.  This is the setup we've used for testing Distiller.\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/docs-src/docs/install.md",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "type": "Text_excerpt",
        "value": "These instructions will help get Distiller up and running on your local machine.\n\n<details><summary><b>1. Clone Distiller</b></summary>\t\n<p>\n  \nClone the Distiller code repository from github:\n```\n$ git clone https://github.com/IntelLabs/distiller.git\n```\nThe rest of the documentation that follows, assumes that you have cloned your repository to a directory called ```distiller```. <br>\n</p>\n</details>\n  \n<details><summary><b>2. Create a Python virtual environment</b></summary>\t\n<p>\n\nWe recommend using a [Python virtual environment](https://docs.python.org/3/library/venv.html#venv-def), but that of course, is up to you.\nThere's nothing special about using Distiller in a virtual environment, but we provide some instructions, for completeness.<br>\nBefore creating the virtual environment, make sure you are located in directory ```distiller```.  After creating the environment, you should see a directory called ```distiller/env```.\n<br>\n#### Using virtualenv\nIf you don't have virtualenv installed, you can find the installation instructions [here](https://packaging.python.org/guides/installing-using-pip-and-virtualenv/).\n\nTo create the environment, execute:\n```\n$ python3 -m virtualenv env\n```\nThis creates a subdirectory named ```env``` where the python virtual environment is stored, and configures the current shell to use it as the default python environment.\n\n#### Using venv\nIf you prefer to use ```venv```, then begin by installing it:\n```\n$ sudo apt-get install python3-venv\n```\nThen create the environment:\n```\n$ python3 -m venv env\n```\nAs with virtualenv, this creates a directory called ```distiller/env```.<br>\n\n#### Activate the environment\nThe environment activation and deactivation commands for ```venv``` and ```virtualenv``` are the same.<br>\n**!NOTE: Make sure to activate the environment, before proceeding with the installation of the dependency packages:<br>**\n```\n$ source env/bin/activate\n```\n</p>\n</details>\n\n<details><summary><b>3. Install the Distiller package</b></summary>\t\n<p>\n  \nFinally, install the Distiller package and its dependencies using ```pip3```:\n```\n$ cd distiller\n$ pip3 install -e .\n```\nThis installs Distiller in \"development mode\", meaning any changes made in the code are reflected in the environment without re-running the install command (so no need to re-install after pulling changes from the Git repository).\n\nNotes:\n- Distiller has only been tested on Ubuntu 16.04 LTS, and with Python 3.5.\n- If you are not using a GPU, you might need to make small adjustments to the code.\n\n</p>\n</details>\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Required PyTorch Version",
        "parent_header": [
          "Installation"
        ],
        "type": "Text_excerpt",
        "value": "Distiller is tested using the default installation of PyTorch 1.3.1, which uses CUDA 10.1. We use TorchVision version 0.4.2. These are included in Distiller's `requirements.txt` and will be automatically installed when installing the Distiller package as listed above.\n\nIf you do not use CUDA 10.1 in your environment, please refer to [PyTorch website](https://pytorch.org/get-started/locally/) to install the compatible build of PyTorch 1.3.1 and torchvision 0.4.2.\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9351360543374216,
      "result": {
        "original_header": "Versioning",
        "type": "Text_excerpt",
        "value": "We use [SemVer](http://semver.org/) for versioning. For the versions available, see the [tags on this repository](https://github.com/IntelLabs/distiller/tags).\n \n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8249148649625875,
      "result": {
        "type": "Text_excerpt",
        "value": "If you used Distiller for your work, please use the following citation: \n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/IntelLabs/distiller/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "automl-for-compression, deep-neural-networks, distillation, early-exit, group-lasso, jupyter-notebook, network-compression, onnx, pruning, pruning-structures, pytorch, quantization, regularization, truncated-svd"
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Apache License 2.0",
        "spdx_id": "Apache-2.0",
        "type": "License",
        "url": "https://api.github.com/licenses/apache-2.0",
        "value": "https://api.github.com/licenses/apache-2.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/LICENSE.md",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License",
        "type": "Text_excerpt",
        "value": "This project is licensed under the Apache License 2.0 - see the [LICENSE.md](LICENSE.md) file for details\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "distiller"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "IntelLabs"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 3281683,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 1035469,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "CSS",
        "size": 97,
        "type": "Programming_language",
        "value": "CSS"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2002.07686"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2003.06902"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2003.00146"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2002.12620, 2020.\n\n- Alexander Kozlov, Ivan Lazarevich, Vasily Shamporov, Nikolay Lyalyushkin, Yury Gorbachev.<br>\n*[Neural Network Compression Framework for fast model inference](https://arxiv.org/abs/2002.08679)*,<br>\nhttps://arxiv.org/abs/2002.08679, 2020.\n\n- Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yuri Nahshan, Alex Bronstein, Uri Weiser.<br>\n*[Robust Quantization: One Model to Rule Them All](https://arxiv.org/abs/2002.07686)*,<br>\nhttps://arxiv.org/abs/2002.07686, 2020.\n\n- Muhammad Abdullah Hanif, Muhammad Shafique.<br>\n*[SalvageDNN: salvaging deep neural network accelerators with permanent faults through saliency-driven fault-aware mapping](https://royalsocietypublishing.org/doi/10.1098/rsta.2019.0164)*,<br>\nIn Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering SciencesVolume 378, Issue 2164, 2019.<br>\nhttps://doi.org/10.1098/rsta.2019.0164\n\n- Meiqi Wang, Jianqiao Mo, Jun Lin, Zhongfeng Wang, Li Du.<br>\n*[DynExit: A Dynamic Early-Exit Strategy for Deep Residual Networks](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020551)*,<br>\nIn IEEE International Workshop on Signal Processing Systems (SiPS), 2019.\n\n- Vinu Joseph, Saurav Muralidharan, Animesh Garg, Michael Garland, Ganesh Gopalakrishnan.<br>\n*[A Programmable Approach to Model Compression](https://arxiv.org/abs/1911.02497),*<br>\nhttps://arxiv.org/abs/1911.02497, 2019<br>\n[code](https://github.com/NVlabs/condensa)\n\n- Hui Guan, Lin Ning, Zhen Lin, Xipeng Shen, Huiyang Zhou, Seung-Hwan Lim.<br>\n*[In-Place Zero-Space Memory Protection for CNN](https://arxiv.org/abs/1910.14479)*,<br>\nIn Conference on Neural Information Processing Systems (NeurIPS), 2019.<br>\nhttps://arxiv.org/abs/1910.14479, 2019<br>\n[code](https://github.com/guanh01/wot)\n\n- Hossein Baktash, Emanuele Natale, Laurent Viennot.<br>\n*[A Comparative Study of Neural Network Compression](https://arxiv.org/abs/1910.11144)*,<br>\nhttps://arxiv.org/abs/1910.11144, 2019.\n\n- Maxim Zemlyanikin, Alexander Smorkalov, Tatiana Khanova, Anna Petrovicheva, Grigory Serebryakov.<br>\n*[512KiB RAM Is Enough! Live Camera Face Recognition DNN on MCU](http://openaccess.thecvf.com/content_ICCVW_2019/html/LPCV/Zemlyanikin_512KiB_RAM_Is_Enough_Live_Camera_Face_Recognition_DNN_on_ICCVW_2019_paper.html)*,<br>\nIn IEEE International Conference on Computer Vision (ICCV), 2019.\n\n- Ziheng Wang, Jeremy Wohlwend, Tao Lei.<br>\n*[Structured Pruning of Large Language Models](https://arxiv.org/abs/1910.04732)*,<br>\nhttps://arxiv.org/abs/1910.04732, 2019.\n\n- Soroush Ghodrati, Hardik Sharma, Sean Kinzer, Amir Yazdanbakhsh, Kambiz Samadi, Nam Sung Kim, Doug Burger, Hadi Esmaeilzadeh.<br>\n*[Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through Interleaved Bit-Partitioned Arithmetic](https://arxiv.org/abs/1906.11915)*,<br>\nhttps://arxiv.org/abs/1906.11915, 2019.\n\n- Gil Shomron, Tal Horowitz, Uri Weiser.<br>\n*[SMT-SA: Simultaneous Multithreading in Systolic Arrays](https://ieeexplore.ieee.org/document/8742541)*,<br>\nIn IEEE Computer Architecture Letters (CAL), 2019.\n\n- Shangqian Gao , Cheng Deng , and Heng Huang.<br>\n *[Cross Domain Model Compression by Structurally Weight Sharing](http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html),*<br>\n In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8973-8982.\n \n- Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass.<br>\n  *[FAKTA: An Automatic End-to-End Fact Checking System](https://arxiv.org/abs/1906.04164),*<br>\n  In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n  *[SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training](https://arxiv.org/abs/1905.01416),*<br>\n  https://arxiv.org/abs/1905.01416, 2019.\n  [code](https://github.com/sinreq/sinreq_code)\n  \n- Goncharenko A., Denisov A., Alyamkin S., Terentev E.<br>\n*[Trainable Thresholds for Neural Network Quantization](https://rd.springer.com/chapter/10.1007/978-3-030-20518-8_26),*<br>\nIn: Rojas I., Joya G., Catala A. (eds) Advances in Computational Intelligence Lecture Notes in Computer Science, vol 11507. Springer, Cham.  International Work-Conference on Artificial Neural Networks (IWANN 2019).\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n*[Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks](https://arxiv.org/abs/1906.06033),*<br>\nhttps://arxiv.org/abs/1906.06033, 2019\n\n- Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.<br>\n  *[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](https://arxiv.org/abs/1901.09504),*<br>\n  https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1911.02497, 2019<br>\n[code](https://github.com/NVlabs/condensa)\n\n- Hui Guan, Lin Ning, Zhen Lin, Xipeng Shen, Huiyang Zhou, Seung-Hwan Lim.<br>\n*[In-Place Zero-Space Memory Protection for CNN](https://arxiv.org/abs/1910.14479)*,<br>\nIn Conference on Neural Information Processing Systems (NeurIPS), 2019.<br>\nhttps://arxiv.org/abs/1910.14479, 2019<br>\n[code](https://github.com/guanh01/wot)\n\n- Hossein Baktash, Emanuele Natale, Laurent Viennot.<br>\n*[A Comparative Study of Neural Network Compression](https://arxiv.org/abs/1910.11144)*,<br>\nhttps://arxiv.org/abs/1910.11144, 2019.\n\n- Maxim Zemlyanikin, Alexander Smorkalov, Tatiana Khanova, Anna Petrovicheva, Grigory Serebryakov.<br>\n*[512KiB RAM Is Enough! Live Camera Face Recognition DNN on MCU](http://openaccess.thecvf.com/content_ICCVW_2019/html/LPCV/Zemlyanikin_512KiB_RAM_Is_Enough_Live_Camera_Face_Recognition_DNN_on_ICCVW_2019_paper.html)*,<br>\nIn IEEE International Conference on Computer Vision (ICCV), 2019.\n\n- Ziheng Wang, Jeremy Wohlwend, Tao Lei.<br>\n*[Structured Pruning of Large Language Models](https://arxiv.org/abs/1910.04732)*,<br>\nhttps://arxiv.org/abs/1910.04732, 2019.\n\n- Soroush Ghodrati, Hardik Sharma, Sean Kinzer, Amir Yazdanbakhsh, Kambiz Samadi, Nam Sung Kim, Doug Burger, Hadi Esmaeilzadeh.<br>\n*[Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through Interleaved Bit-Partitioned Arithmetic](https://arxiv.org/abs/1906.11915)*,<br>\nhttps://arxiv.org/abs/1906.11915, 2019.\n\n- Gil Shomron, Tal Horowitz, Uri Weiser.<br>\n*[SMT-SA: Simultaneous Multithreading in Systolic Arrays](https://ieeexplore.ieee.org/document/8742541)*,<br>\nIn IEEE Computer Architecture Letters (CAL), 2019.\n\n- Shangqian Gao , Cheng Deng , and Heng Huang.<br>\n *[Cross Domain Model Compression by Structurally Weight Sharing](http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html),*<br>\n In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8973-8982.\n \n- Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass.<br>\n  *[FAKTA: An Automatic End-to-End Fact Checking System](https://arxiv.org/abs/1906.04164),*<br>\n  In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n  *[SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training](https://arxiv.org/abs/1905.01416),*<br>\n  https://arxiv.org/abs/1905.01416, 2019.\n  [code](https://github.com/sinreq/sinreq_code)\n  \n- Goncharenko A., Denisov A., Alyamkin S., Terentev E.<br>\n*[Trainable Thresholds for Neural Network Quantization](https://rd.springer.com/chapter/10.1007/978-3-030-20518-8_26),*<br>\nIn: Rojas I., Joya G., Catala A. (eds) Advances in Computational Intelligence Lecture Notes in Computer Science, vol 11507. Springer, Cham.  International Work-Conference on Artificial Neural Networks (IWANN 2019).\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n*[Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks](https://arxiv.org/abs/1906.06033),*<br>\nhttps://arxiv.org/abs/1906.06033, 2019\n\n- Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.<br>\n  *[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](https://arxiv.org/abs/1901.09504),*<br>\n  https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1812.07872"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2002.08679, 2020.\n\n- Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yuri Nahshan, Alex Bronstein, Uri Weiser.<br>\n*[Robust Quantization: One Model to Rule Them All](https://arxiv.org/abs/2002.07686)*,<br>\nhttps://arxiv.org/abs/2002.07686, 2020.\n\n- Muhammad Abdullah Hanif, Muhammad Shafique.<br>\n*[SalvageDNN: salvaging deep neural network accelerators with permanent faults through saliency-driven fault-aware mapping](https://royalsocietypublishing.org/doi/10.1098/rsta.2019.0164)*,<br>\nIn Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering SciencesVolume 378, Issue 2164, 2019.<br>\nhttps://doi.org/10.1098/rsta.2019.0164\n\n- Meiqi Wang, Jianqiao Mo, Jun Lin, Zhongfeng Wang, Li Du.<br>\n*[DynExit: A Dynamic Early-Exit Strategy for Deep Residual Networks](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020551)*,<br>\nIn IEEE International Workshop on Signal Processing Systems (SiPS), 2019.\n\n- Vinu Joseph, Saurav Muralidharan, Animesh Garg, Michael Garland, Ganesh Gopalakrishnan.<br>\n*[A Programmable Approach to Model Compression](https://arxiv.org/abs/1911.02497),*<br>\nhttps://arxiv.org/abs/1911.02497, 2019<br>\n[code](https://github.com/NVlabs/condensa)\n\n- Hui Guan, Lin Ning, Zhen Lin, Xipeng Shen, Huiyang Zhou, Seung-Hwan Lim.<br>\n*[In-Place Zero-Space Memory Protection for CNN](https://arxiv.org/abs/1910.14479)*,<br>\nIn Conference on Neural Information Processing Systems (NeurIPS), 2019.<br>\nhttps://arxiv.org/abs/1910.14479, 2019<br>\n[code](https://github.com/guanh01/wot)\n\n- Hossein Baktash, Emanuele Natale, Laurent Viennot.<br>\n*[A Comparative Study of Neural Network Compression](https://arxiv.org/abs/1910.11144)*,<br>\nhttps://arxiv.org/abs/1910.11144, 2019.\n\n- Maxim Zemlyanikin, Alexander Smorkalov, Tatiana Khanova, Anna Petrovicheva, Grigory Serebryakov.<br>\n*[512KiB RAM Is Enough! Live Camera Face Recognition DNN on MCU](http://openaccess.thecvf.com/content_ICCVW_2019/html/LPCV/Zemlyanikin_512KiB_RAM_Is_Enough_Live_Camera_Face_Recognition_DNN_on_ICCVW_2019_paper.html)*,<br>\nIn IEEE International Conference on Computer Vision (ICCV), 2019.\n\n- Ziheng Wang, Jeremy Wohlwend, Tao Lei.<br>\n*[Structured Pruning of Large Language Models](https://arxiv.org/abs/1910.04732)*,<br>\nhttps://arxiv.org/abs/1910.04732, 2019.\n\n- Soroush Ghodrati, Hardik Sharma, Sean Kinzer, Amir Yazdanbakhsh, Kambiz Samadi, Nam Sung Kim, Doug Burger, Hadi Esmaeilzadeh.<br>\n*[Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through Interleaved Bit-Partitioned Arithmetic](https://arxiv.org/abs/1906.11915)*,<br>\nhttps://arxiv.org/abs/1906.11915, 2019.\n\n- Gil Shomron, Tal Horowitz, Uri Weiser.<br>\n*[SMT-SA: Simultaneous Multithreading in Systolic Arrays](https://ieeexplore.ieee.org/document/8742541)*,<br>\nIn IEEE Computer Architecture Letters (CAL), 2019.\n\n- Shangqian Gao , Cheng Deng , and Heng Huang.<br>\n *[Cross Domain Model Compression by Structurally Weight Sharing](http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html),*<br>\n In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8973-8982.\n \n- Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass.<br>\n  *[FAKTA: An Automatic End-to-End Fact Checking System](https://arxiv.org/abs/1906.04164),*<br>\n  In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n  *[SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training](https://arxiv.org/abs/1905.01416),*<br>\n  https://arxiv.org/abs/1905.01416, 2019.\n  [code](https://github.com/sinreq/sinreq_code)\n  \n- Goncharenko A., Denisov A., Alyamkin S., Terentev E.<br>\n*[Trainable Thresholds for Neural Network Quantization](https://rd.springer.com/chapter/10.1007/978-3-030-20518-8_26),*<br>\nIn: Rojas I., Joya G., Catala A. (eds) Advances in Computational Intelligence Lecture Notes in Computer Science, vol 11507. Springer, Cham.  International Work-Conference on Artificial Neural Networks (IWANN 2019).\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n*[Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks](https://arxiv.org/abs/1906.06033),*<br>\nhttps://arxiv.org/abs/1906.06033, 2019\n\n- Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.<br>\n  *[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](https://arxiv.org/abs/1901.09504),*<br>\n  https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1910.14479"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1910.11144"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1905.01416"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1906.06033"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1910.14479, 2019<br>\n[code](https://github.com/guanh01/wot)\n\n- Hossein Baktash, Emanuele Natale, Laurent Viennot.<br>\n*[A Comparative Study of Neural Network Compression](https://arxiv.org/abs/1910.11144)*,<br>\nhttps://arxiv.org/abs/1910.11144, 2019.\n\n- Maxim Zemlyanikin, Alexander Smorkalov, Tatiana Khanova, Anna Petrovicheva, Grigory Serebryakov.<br>\n*[512KiB RAM Is Enough! Live Camera Face Recognition DNN on MCU](http://openaccess.thecvf.com/content_ICCVW_2019/html/LPCV/Zemlyanikin_512KiB_RAM_Is_Enough_Live_Camera_Face_Recognition_DNN_on_ICCVW_2019_paper.html)*,<br>\nIn IEEE International Conference on Computer Vision (ICCV), 2019.\n\n- Ziheng Wang, Jeremy Wohlwend, Tao Lei.<br>\n*[Structured Pruning of Large Language Models](https://arxiv.org/abs/1910.04732)*,<br>\nhttps://arxiv.org/abs/1910.04732, 2019.\n\n- Soroush Ghodrati, Hardik Sharma, Sean Kinzer, Amir Yazdanbakhsh, Kambiz Samadi, Nam Sung Kim, Doug Burger, Hadi Esmaeilzadeh.<br>\n*[Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through Interleaved Bit-Partitioned Arithmetic](https://arxiv.org/abs/1906.11915)*,<br>\nhttps://arxiv.org/abs/1906.11915, 2019.\n\n- Gil Shomron, Tal Horowitz, Uri Weiser.<br>\n*[SMT-SA: Simultaneous Multithreading in Systolic Arrays](https://ieeexplore.ieee.org/document/8742541)*,<br>\nIn IEEE Computer Architecture Letters (CAL), 2019.\n\n- Shangqian Gao , Cheng Deng , and Heng Huang.<br>\n *[Cross Domain Model Compression by Structurally Weight Sharing](http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html),*<br>\n In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8973-8982.\n \n- Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass.<br>\n  *[FAKTA: An Automatic End-to-End Fact Checking System](https://arxiv.org/abs/1906.04164),*<br>\n  In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n  *[SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training](https://arxiv.org/abs/1905.01416),*<br>\n  https://arxiv.org/abs/1905.01416, 2019.\n  [code](https://github.com/sinreq/sinreq_code)\n  \n- Goncharenko A., Denisov A., Alyamkin S., Terentev E.<br>\n*[Trainable Thresholds for Neural Network Quantization](https://rd.springer.com/chapter/10.1007/978-3-030-20518-8_26),*<br>\nIn: Rojas I., Joya G., Catala A. (eds) Advances in Computational Intelligence Lecture Notes in Computer Science, vol 11507. Springer, Cham.  International Work-Conference on Artificial Neural Networks (IWANN 2019).\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n*[Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks](https://arxiv.org/abs/1906.06033),*<br>\nhttps://arxiv.org/abs/1906.06033, 2019\n\n- Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.<br>\n  *[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](https://arxiv.org/abs/1901.09504),*<br>\n  https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1906.06033, 2019\n\n- Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.<br>\n  *[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](https://arxiv.org/abs/1901.09504),*<br>\n  https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1901.09504"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1910.04732"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1906.11915, 2019.\n\n- Gil Shomron, Tal Horowitz, Uri Weiser.<br>\n*[SMT-SA: Simultaneous Multithreading in Systolic Arrays](https://ieeexplore.ieee.org/document/8742541)*,<br>\nIn IEEE Computer Architecture Letters (CAL), 2019.\n\n- Shangqian Gao , Cheng Deng , and Heng Huang.<br>\n *[Cross Domain Model Compression by Structurally Weight Sharing](http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html),*<br>\n In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8973-8982.\n \n- Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass.<br>\n  *[FAKTA: An Automatic End-to-End Fact Checking System](https://arxiv.org/abs/1906.04164),*<br>\n  In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n  *[SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training](https://arxiv.org/abs/1905.01416),*<br>\n  https://arxiv.org/abs/1905.01416, 2019.\n  [code](https://github.com/sinreq/sinreq_code)\n  \n- Goncharenko A., Denisov A., Alyamkin S., Terentev E.<br>\n*[Trainable Thresholds for Neural Network Quantization](https://rd.springer.com/chapter/10.1007/978-3-030-20518-8_26),*<br>\nIn: Rojas I., Joya G., Catala A. (eds) Advances in Computational Intelligence Lecture Notes in Computer Science, vol 11507. Springer, Cham.  International Work-Conference on Artificial Neural Networks (IWANN 2019).\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n*[Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks](https://arxiv.org/abs/1906.06033),*<br>\nhttps://arxiv.org/abs/1906.06033, 2019\n\n- Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.<br>\n  *[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](https://arxiv.org/abs/1901.09504),*<br>\n  https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1910.04732, 2019.\n\n- Soroush Ghodrati, Hardik Sharma, Sean Kinzer, Amir Yazdanbakhsh, Kambiz Samadi, Nam Sung Kim, Doug Burger, Hadi Esmaeilzadeh.<br>\n*[Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through Interleaved Bit-Partitioned Arithmetic](https://arxiv.org/abs/1906.11915)*,<br>\nhttps://arxiv.org/abs/1906.11915, 2019.\n\n- Gil Shomron, Tal Horowitz, Uri Weiser.<br>\n*[SMT-SA: Simultaneous Multithreading in Systolic Arrays](https://ieeexplore.ieee.org/document/8742541)*,<br>\nIn IEEE Computer Architecture Letters (CAL), 2019.\n\n- Shangqian Gao , Cheng Deng , and Heng Huang.<br>\n *[Cross Domain Model Compression by Structurally Weight Sharing](http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html),*<br>\n In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8973-8982.\n \n- Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass.<br>\n  *[FAKTA: An Automatic End-to-End Fact Checking System](https://arxiv.org/abs/1906.04164),*<br>\n  In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n  *[SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training](https://arxiv.org/abs/1905.01416),*<br>\n  https://arxiv.org/abs/1905.01416, 2019.\n  [code](https://github.com/sinreq/sinreq_code)\n  \n- Goncharenko A., Denisov A., Alyamkin S., Terentev E.<br>\n*[Trainable Thresholds for Neural Network Quantization](https://rd.springer.com/chapter/10.1007/978-3-030-20518-8_26),*<br>\nIn: Rojas I., Joya G., Catala A. (eds) Advances in Computational Intelligence Lecture Notes in Computer Science, vol 11507. Springer, Cham.  International Work-Conference on Artificial Neural Networks (IWANN 2019).\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n*[Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks](https://arxiv.org/abs/1906.06033),*<br>\nhttps://arxiv.org/abs/1906.06033, 2019\n\n- Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.<br>\n  *[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](https://arxiv.org/abs/1901.09504),*<br>\n  https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1906.04164"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1906.11915"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2003.00146, 2020.\n\n- Ziqing Yang, Yiming Cui, Zhipeng Chen, Wanxiang Che, Ting Liu, Shijin Wang, Guoping Hu.<br>\n*[TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing](https://arxiv.org/abs/2002.12620)*,<br>\nhttps://arxiv.org/abs/2002.12620, 2020.\n\n- Alexander Kozlov, Ivan Lazarevich, Vasily Shamporov, Nikolay Lyalyushkin, Yury Gorbachev.<br>\n*[Neural Network Compression Framework for fast model inference](https://arxiv.org/abs/2002.08679)*,<br>\nhttps://arxiv.org/abs/2002.08679, 2020.\n\n- Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yuri Nahshan, Alex Bronstein, Uri Weiser.<br>\n*[Robust Quantization: One Model to Rule Them All](https://arxiv.org/abs/2002.07686)*,<br>\nhttps://arxiv.org/abs/2002.07686, 2020.\n\n- Muhammad Abdullah Hanif, Muhammad Shafique.<br>\n*[SalvageDNN: salvaging deep neural network accelerators with permanent faults through saliency-driven fault-aware mapping](https://royalsocietypublishing.org/doi/10.1098/rsta.2019.0164)*,<br>\nIn Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering SciencesVolume 378, Issue 2164, 2019.<br>\nhttps://doi.org/10.1098/rsta.2019.0164\n\n- Meiqi Wang, Jianqiao Mo, Jun Lin, Zhongfeng Wang, Li Du.<br>\n*[DynExit: A Dynamic Early-Exit Strategy for Deep Residual Networks](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020551)*,<br>\nIn IEEE International Workshop on Signal Processing Systems (SiPS), 2019.\n\n- Vinu Joseph, Saurav Muralidharan, Animesh Garg, Michael Garland, Ganesh Gopalakrishnan.<br>\n*[A Programmable Approach to Model Compression](https://arxiv.org/abs/1911.02497),*<br>\nhttps://arxiv.org/abs/1911.02497, 2019<br>\n[code](https://github.com/NVlabs/condensa)\n\n- Hui Guan, Lin Ning, Zhen Lin, Xipeng Shen, Huiyang Zhou, Seung-Hwan Lim.<br>\n*[In-Place Zero-Space Memory Protection for CNN](https://arxiv.org/abs/1910.14479)*,<br>\nIn Conference on Neural Information Processing Systems (NeurIPS), 2019.<br>\nhttps://arxiv.org/abs/1910.14479, 2019<br>\n[code](https://github.com/guanh01/wot)\n\n- Hossein Baktash, Emanuele Natale, Laurent Viennot.<br>\n*[A Comparative Study of Neural Network Compression](https://arxiv.org/abs/1910.11144)*,<br>\nhttps://arxiv.org/abs/1910.11144, 2019.\n\n- Maxim Zemlyanikin, Alexander Smorkalov, Tatiana Khanova, Anna Petrovicheva, Grigory Serebryakov.<br>\n*[512KiB RAM Is Enough! Live Camera Face Recognition DNN on MCU](http://openaccess.thecvf.com/content_ICCVW_2019/html/LPCV/Zemlyanikin_512KiB_RAM_Is_Enough_Live_Camera_Face_Recognition_DNN_on_ICCVW_2019_paper.html)*,<br>\nIn IEEE International Conference on Computer Vision (ICCV), 2019.\n\n- Ziheng Wang, Jeremy Wohlwend, Tao Lei.<br>\n*[Structured Pruning of Large Language Models](https://arxiv.org/abs/1910.04732)*,<br>\nhttps://arxiv.org/abs/1910.04732, 2019.\n\n- Soroush Ghodrati, Hardik Sharma, Sean Kinzer, Amir Yazdanbakhsh, Kambiz Samadi, Nam Sung Kim, Doug Burger, Hadi Esmaeilzadeh.<br>\n*[Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through Interleaved Bit-Partitioned Arithmetic](https://arxiv.org/abs/1906.11915)*,<br>\nhttps://arxiv.org/abs/1906.11915, 2019.\n\n- Gil Shomron, Tal Horowitz, Uri Weiser.<br>\n*[SMT-SA: Simultaneous Multithreading in Systolic Arrays](https://ieeexplore.ieee.org/document/8742541)*,<br>\nIn IEEE Computer Architecture Letters (CAL), 2019.\n\n- Shangqian Gao , Cheng Deng , and Heng Huang.<br>\n *[Cross Domain Model Compression by Structurally Weight Sharing](http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html),*<br>\n In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8973-8982.\n \n- Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass.<br>\n  *[FAKTA: An Automatic End-to-End Fact Checking System](https://arxiv.org/abs/1906.04164),*<br>\n  In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n  *[SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training](https://arxiv.org/abs/1905.01416),*<br>\n  https://arxiv.org/abs/1905.01416, 2019.\n  [code](https://github.com/sinreq/sinreq_code)\n  \n- Goncharenko A., Denisov A., Alyamkin S., Terentev E.<br>\n*[Trainable Thresholds for Neural Network Quantization](https://rd.springer.com/chapter/10.1007/978-3-030-20518-8_26),*<br>\nIn: Rojas I., Joya G., Catala A. (eds) Advances in Computational Intelligence Lecture Notes in Computer Science, vol 11507. Springer, Cham.  International Work-Conference on Artificial Neural Networks (IWANN 2019).\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n*[Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks](https://arxiv.org/abs/1906.06033),*<br>\nhttps://arxiv.org/abs/1906.06033, 2019\n\n- Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.<br>\n  *[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](https://arxiv.org/abs/1901.09504),*<br>\n  https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1910.11144, 2019.\n\n- Maxim Zemlyanikin, Alexander Smorkalov, Tatiana Khanova, Anna Petrovicheva, Grigory Serebryakov.<br>\n*[512KiB RAM Is Enough! Live Camera Face Recognition DNN on MCU](http://openaccess.thecvf.com/content_ICCVW_2019/html/LPCV/Zemlyanikin_512KiB_RAM_Is_Enough_Live_Camera_Face_Recognition_DNN_on_ICCVW_2019_paper.html)*,<br>\nIn IEEE International Conference on Computer Vision (ICCV), 2019.\n\n- Ziheng Wang, Jeremy Wohlwend, Tao Lei.<br>\n*[Structured Pruning of Large Language Models](https://arxiv.org/abs/1910.04732)*,<br>\nhttps://arxiv.org/abs/1910.04732, 2019.\n\n- Soroush Ghodrati, Hardik Sharma, Sean Kinzer, Amir Yazdanbakhsh, Kambiz Samadi, Nam Sung Kim, Doug Burger, Hadi Esmaeilzadeh.<br>\n*[Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through Interleaved Bit-Partitioned Arithmetic](https://arxiv.org/abs/1906.11915)*,<br>\nhttps://arxiv.org/abs/1906.11915, 2019.\n\n- Gil Shomron, Tal Horowitz, Uri Weiser.<br>\n*[SMT-SA: Simultaneous Multithreading in Systolic Arrays](https://ieeexplore.ieee.org/document/8742541)*,<br>\nIn IEEE Computer Architecture Letters (CAL), 2019.\n\n- Shangqian Gao , Cheng Deng , and Heng Huang.<br>\n *[Cross Domain Model Compression by Structurally Weight Sharing](http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html),*<br>\n In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8973-8982.\n \n- Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass.<br>\n  *[FAKTA: An Automatic End-to-End Fact Checking System](https://arxiv.org/abs/1906.04164),*<br>\n  In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n  *[SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training](https://arxiv.org/abs/1905.01416),*<br>\n  https://arxiv.org/abs/1905.01416, 2019.\n  [code](https://github.com/sinreq/sinreq_code)\n  \n- Goncharenko A., Denisov A., Alyamkin S., Terentev E.<br>\n*[Trainable Thresholds for Neural Network Quantization](https://rd.springer.com/chapter/10.1007/978-3-030-20518-8_26),*<br>\nIn: Rojas I., Joya G., Catala A. (eds) Advances in Computational Intelligence Lecture Notes in Computer Science, vol 11507. Springer, Cham.  International Work-Conference on Artificial Neural Networks (IWANN 2019).\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n*[Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks](https://arxiv.org/abs/1906.06033),*<br>\nhttps://arxiv.org/abs/1906.06033, 2019\n\n- Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.<br>\n  *[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](https://arxiv.org/abs/1901.09504),*<br>\n  https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1910.12232}\n}\n```\n\n\n## Acknowledgments\n\nAny published work is built on top of the work of many other people, and the credit belongs to too many people to list here.\n* The Python and PyTorch developer communities have shared many invaluable insights, examples and ideas on the Web.\n* The authors of the research papers implemented in the [Distiller model-zoo](https://intellabs.github.io/distiller/model_zoo.html"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1911.02497"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2002.12620"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2002.08679"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2003.06902, 2020.\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Fatemehsadat Mireshghallah, Tarek Elgindi, Charles-Alban Deledalle, Hadi Esmaeilzadeh.<br>\n*[Gradient-Based Deep Quantization of Neural Networks through Sinusoidal\nAdaptive Regularization](https://arxiv.org/abs/2003.00146)*,<br>\nhttps://arxiv.org/abs/2003.00146, 2020.\n\n- Ziqing Yang, Yiming Cui, Zhipeng Chen, Wanxiang Che, Ting Liu, Shijin Wang, Guoping Hu.<br>\n*[TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing](https://arxiv.org/abs/2002.12620)*,<br>\nhttps://arxiv.org/abs/2002.12620, 2020.\n\n- Alexander Kozlov, Ivan Lazarevich, Vasily Shamporov, Nikolay Lyalyushkin, Yury Gorbachev.<br>\n*[Neural Network Compression Framework for fast model inference](https://arxiv.org/abs/2002.08679)*,<br>\nhttps://arxiv.org/abs/2002.08679, 2020.\n\n- Moran Shkolnik, Brian Chmiel, Ron Banner, Gil Shomron, Yuri Nahshan, Alex Bronstein, Uri Weiser.<br>\n*[Robust Quantization: One Model to Rule Them All](https://arxiv.org/abs/2002.07686)*,<br>\nhttps://arxiv.org/abs/2002.07686, 2020.\n\n- Muhammad Abdullah Hanif, Muhammad Shafique.<br>\n*[SalvageDNN: salvaging deep neural network accelerators with permanent faults through saliency-driven fault-aware mapping](https://royalsocietypublishing.org/doi/10.1098/rsta.2019.0164)*,<br>\nIn Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering SciencesVolume 378, Issue 2164, 2019.<br>\nhttps://doi.org/10.1098/rsta.2019.0164\n\n- Meiqi Wang, Jianqiao Mo, Jun Lin, Zhongfeng Wang, Li Du.<br>\n*[DynExit: A Dynamic Early-Exit Strategy for Deep Residual Networks](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020551)*,<br>\nIn IEEE International Workshop on Signal Processing Systems (SiPS), 2019.\n\n- Vinu Joseph, Saurav Muralidharan, Animesh Garg, Michael Garland, Ganesh Gopalakrishnan.<br>\n*[A Programmable Approach to Model Compression](https://arxiv.org/abs/1911.02497),*<br>\nhttps://arxiv.org/abs/1911.02497, 2019<br>\n[code](https://github.com/NVlabs/condensa)\n\n- Hui Guan, Lin Ning, Zhen Lin, Xipeng Shen, Huiyang Zhou, Seung-Hwan Lim.<br>\n*[In-Place Zero-Space Memory Protection for CNN](https://arxiv.org/abs/1910.14479)*,<br>\nIn Conference on Neural Information Processing Systems (NeurIPS), 2019.<br>\nhttps://arxiv.org/abs/1910.14479, 2019<br>\n[code](https://github.com/guanh01/wot)\n\n- Hossein Baktash, Emanuele Natale, Laurent Viennot.<br>\n*[A Comparative Study of Neural Network Compression](https://arxiv.org/abs/1910.11144)*,<br>\nhttps://arxiv.org/abs/1910.11144, 2019.\n\n- Maxim Zemlyanikin, Alexander Smorkalov, Tatiana Khanova, Anna Petrovicheva, Grigory Serebryakov.<br>\n*[512KiB RAM Is Enough! Live Camera Face Recognition DNN on MCU](http://openaccess.thecvf.com/content_ICCVW_2019/html/LPCV/Zemlyanikin_512KiB_RAM_Is_Enough_Live_Camera_Face_Recognition_DNN_on_ICCVW_2019_paper.html)*,<br>\nIn IEEE International Conference on Computer Vision (ICCV), 2019.\n\n- Ziheng Wang, Jeremy Wohlwend, Tao Lei.<br>\n*[Structured Pruning of Large Language Models](https://arxiv.org/abs/1910.04732)*,<br>\nhttps://arxiv.org/abs/1910.04732, 2019.\n\n- Soroush Ghodrati, Hardik Sharma, Sean Kinzer, Amir Yazdanbakhsh, Kambiz Samadi, Nam Sung Kim, Doug Burger, Hadi Esmaeilzadeh.<br>\n*[Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through Interleaved Bit-Partitioned Arithmetic](https://arxiv.org/abs/1906.11915)*,<br>\nhttps://arxiv.org/abs/1906.11915, 2019.\n\n- Gil Shomron, Tal Horowitz, Uri Weiser.<br>\n*[SMT-SA: Simultaneous Multithreading in Systolic Arrays](https://ieeexplore.ieee.org/document/8742541)*,<br>\nIn IEEE Computer Architecture Letters (CAL), 2019.\n\n- Shangqian Gao , Cheng Deng , and Heng Huang.<br>\n *[Cross Domain Model Compression by Structurally Weight Sharing](http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html),*<br>\n In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8973-8982.\n \n- Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass.<br>\n  *[FAKTA: An Automatic End-to-End Fact Checking System](https://arxiv.org/abs/1906.04164),*<br>\n  In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n  *[SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training](https://arxiv.org/abs/1905.01416),*<br>\n  https://arxiv.org/abs/1905.01416, 2019.\n  [code](https://github.com/sinreq/sinreq_code)\n  \n- Goncharenko A., Denisov A., Alyamkin S., Terentev E.<br>\n*[Trainable Thresholds for Neural Network Quantization](https://rd.springer.com/chapter/10.1007/978-3-030-20518-8_26),*<br>\nIn: Rojas I., Joya G., Catala A. (eds) Advances in Computational Intelligence Lecture Notes in Computer Science, vol 11507. Springer, Cham.  International Work-Conference on Artificial Neural Networks (IWANN 2019).\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n*[Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks](https://arxiv.org/abs/1906.06033),*<br>\nhttps://arxiv.org/abs/1906.06033, 2019\n\n- Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.<br>\n  *[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](https://arxiv.org/abs/1901.09504),*<br>\n  https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2002.07686, 2020.\n\n- Muhammad Abdullah Hanif, Muhammad Shafique.<br>\n*[SalvageDNN: salvaging deep neural network accelerators with permanent faults through saliency-driven fault-aware mapping](https://royalsocietypublishing.org/doi/10.1098/rsta.2019.0164)*,<br>\nIn Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering SciencesVolume 378, Issue 2164, 2019.<br>\nhttps://doi.org/10.1098/rsta.2019.0164\n\n- Meiqi Wang, Jianqiao Mo, Jun Lin, Zhongfeng Wang, Li Du.<br>\n*[DynExit: A Dynamic Early-Exit Strategy for Deep Residual Networks](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020551)*,<br>\nIn IEEE International Workshop on Signal Processing Systems (SiPS), 2019.\n\n- Vinu Joseph, Saurav Muralidharan, Animesh Garg, Michael Garland, Ganesh Gopalakrishnan.<br>\n*[A Programmable Approach to Model Compression](https://arxiv.org/abs/1911.02497),*<br>\nhttps://arxiv.org/abs/1911.02497, 2019<br>\n[code](https://github.com/NVlabs/condensa)\n\n- Hui Guan, Lin Ning, Zhen Lin, Xipeng Shen, Huiyang Zhou, Seung-Hwan Lim.<br>\n*[In-Place Zero-Space Memory Protection for CNN](https://arxiv.org/abs/1910.14479)*,<br>\nIn Conference on Neural Information Processing Systems (NeurIPS), 2019.<br>\nhttps://arxiv.org/abs/1910.14479, 2019<br>\n[code](https://github.com/guanh01/wot)\n\n- Hossein Baktash, Emanuele Natale, Laurent Viennot.<br>\n*[A Comparative Study of Neural Network Compression](https://arxiv.org/abs/1910.11144)*,<br>\nhttps://arxiv.org/abs/1910.11144, 2019.\n\n- Maxim Zemlyanikin, Alexander Smorkalov, Tatiana Khanova, Anna Petrovicheva, Grigory Serebryakov.<br>\n*[512KiB RAM Is Enough! Live Camera Face Recognition DNN on MCU](http://openaccess.thecvf.com/content_ICCVW_2019/html/LPCV/Zemlyanikin_512KiB_RAM_Is_Enough_Live_Camera_Face_Recognition_DNN_on_ICCVW_2019_paper.html)*,<br>\nIn IEEE International Conference on Computer Vision (ICCV), 2019.\n\n- Ziheng Wang, Jeremy Wohlwend, Tao Lei.<br>\n*[Structured Pruning of Large Language Models](https://arxiv.org/abs/1910.04732)*,<br>\nhttps://arxiv.org/abs/1910.04732, 2019.\n\n- Soroush Ghodrati, Hardik Sharma, Sean Kinzer, Amir Yazdanbakhsh, Kambiz Samadi, Nam Sung Kim, Doug Burger, Hadi Esmaeilzadeh.<br>\n*[Mixed-Signal Charge-Domain Acceleration of Deep Neural networks through Interleaved Bit-Partitioned Arithmetic](https://arxiv.org/abs/1906.11915)*,<br>\nhttps://arxiv.org/abs/1906.11915, 2019.\n\n- Gil Shomron, Tal Horowitz, Uri Weiser.<br>\n*[SMT-SA: Simultaneous Multithreading in Systolic Arrays](https://ieeexplore.ieee.org/document/8742541)*,<br>\nIn IEEE Computer Architecture Letters (CAL), 2019.\n\n- Shangqian Gao , Cheng Deng , and Heng Huang.<br>\n *[Cross Domain Model Compression by Structurally Weight Sharing](http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Cross_Domain_Model_Compression_by_Structurally_Weight_Sharing_CVPR_2019_paper.html),*<br>\n In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 8973-8982.\n \n- Moin Nadeem, Wei Fang, Brian Xu, Mitra Mohtarami, James Glass.<br>\n  *[FAKTA: An Automatic End-to-End Fact Checking System](https://arxiv.org/abs/1906.04164),*<br>\n  In North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n  *[SinReQ: Generalized Sinusoidal Regularization for Low-Bitwidth Deep Quantized Training](https://arxiv.org/abs/1905.01416),*<br>\n  https://arxiv.org/abs/1905.01416, 2019.\n  [code](https://github.com/sinreq/sinreq_code)\n  \n- Goncharenko A., Denisov A., Alyamkin S., Terentev E.<br>\n*[Trainable Thresholds for Neural Network Quantization](https://rd.springer.com/chapter/10.1007/978-3-030-20518-8_26),*<br>\nIn: Rojas I., Joya G., Catala A. (eds) Advances in Computational Intelligence Lecture Notes in Computer Science, vol 11507. Springer, Cham.  International Work-Conference on Artificial Neural Networks (IWANN 2019).\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n*[Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks](https://arxiv.org/abs/1906.06033),*<br>\nhttps://arxiv.org/abs/1906.06033, 2019\n\n- Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.<br>\n  *[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](https://arxiv.org/abs/1901.09504),*<br>\n  https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1905.01416, 2019.\n  [code](https://github.com/sinreq/sinreq_code)\n  \n- Goncharenko A., Denisov A., Alyamkin S., Terentev E.<br>\n*[Trainable Thresholds for Neural Network Quantization](https://rd.springer.com/chapter/10.1007/978-3-030-20518-8_26),*<br>\nIn: Rojas I., Joya G., Catala A. (eds) Advances in Computational Intelligence Lecture Notes in Computer Science, vol 11507. Springer, Cham.  International Work-Conference on Artificial Neural Networks (IWANN 2019).\n\n- Ahmed T. Elthakeb, Prannoy Pilligundla, Hadi Esmaeilzadeh.<br>\n*[Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized Training of Neural Networks](https://arxiv.org/abs/1906.06033),*<br>\nhttps://arxiv.org/abs/1906.06033, 2019\n\n- Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, Zhiru Zhang.<br>\n  *[Improving Neural Network Quantization without Retraining using Outlier Channel Splitting](https://arxiv.org/abs/1901.09504),*<br>\n  https://arxiv.org/abs/1901.09504, 2019<br>\n  [Code](https://github.com/cornell-zhang/dnn-quant-ocs)\n\n- Angad S. Rekhi, Brian Zimmer, Nikola Nedovic, Ningxi Liu, Rangharajan Venkatesan, Miaorong Wang, Brucek Khailany, William J. Dally, C. Thomas Gray.<br>\n*[Analog/Mixed-Signal Hardware Error Modeling for Deep Learning Inference](https://research.nvidia.com/sites/default/files/pubs/2019-06_Analog/Mixed-Signal-Hardware-Error/40_2_Rekhi_AMS_ML.pdf)*,<br>\nNvidia Research, 2019.\n\n- Norio Nakata.<br>\n*[Recent Technical Development of Artificial Intelligence for Diagnostic Medical Imaging]( https://rd.springer.com/article/10.1007/s11604-018-0804-6)*,<br>\nIn Japanese Journal of Radiology, February 2019, Volume 37, Issue 2, pp 103\u2013108.\n\n-  Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, Evgeny Terentev.<br>\n   *[Fast Adjustable Threshold For Uniform Neural Network Quantization](https://arxiv.org/abs/1812.07872)*,<br>\n   https://arxiv.org/abs/1812.07872, 2018\n</p>\t\n</details>\n\nIf you used Distiller for your work, please use the following citation:\n\n```\n@article{nzmora2019distiller,\n  author       = {Neta Zmora and\n                  Guy Jacob and\n                  Lev Zlotnik and\n                  Bar Elharar and\n                  Gal Novik"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "nzmora",
          "type": "User"
        },
        "date_created": "2019-04-01T14:51:46Z",
        "date_published": "2019-04-01T15:00:42Z",
        "description": "Tagging the 'master' branch before performing a few API-breaking changes.",
        "html_url": "https://github.com/IntelLabs/distiller/releases/tag/v0.3.1",
        "name": "Intermediate release",
        "release_id": 16478677,
        "tag": "v0.3.1",
        "tarball_url": "https://api.github.com/repos/IntelLabs/distiller/tarball/v0.3.1",
        "type": "Release",
        "url": "https://api.github.com/repos/IntelLabs/distiller/releases/16478677",
        "value": "https://api.github.com/repos/IntelLabs/distiller/releases/16478677",
        "zipball_url": "https://api.github.com/repos/IntelLabs/distiller/zipball/v0.3.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "nzmora",
          "type": "User"
        },
        "date_created": "2019-02-28T12:09:00Z",
        "date_published": "2019-02-28T12:11:09Z",
        "description": "* Supports PyTorch 1.0.1\r\n* Supports installation as a Python package\r\n* Many features (TBD) since release v0.2.0 (PyTorch 0.4)",
        "html_url": "https://github.com/IntelLabs/distiller/releases/tag/v0.3.0",
        "name": "v0.3.0",
        "release_id": 15831082,
        "tag": "v0.3.0",
        "tarball_url": "https://api.github.com/repos/IntelLabs/distiller/tarball/v0.3.0",
        "type": "Release",
        "url": "https://api.github.com/repos/IntelLabs/distiller/releases/15831082",
        "value": "https://api.github.com/repos/IntelLabs/distiller/releases/15831082",
        "zipball_url": "https://api.github.com/repos/IntelLabs/distiller/zipball/v0.3.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "nzmora",
          "type": "User"
        },
        "date_created": "2018-06-25T12:03:15Z",
        "date_published": "2018-06-25T12:12:39Z",
        "description": "* PyTorch 0.4 support\r\n* An implementation of Baidu's RNN pruning paper from ICLR 2017\r\n   Narang, Sharan & Diamos, Gregory & Sengupta, Shubho & Elsen, Erich. (2017).\r\n   <i>Exploring Sparsity in Recurrent Neural Networks</i>. (https://arxiv.org/abs/1704.05119)\r\n* Add a word language model pruning example using AGP and Baidu RNN pruning\r\n* Quantization aware training (4-bit quantization)\r\n* New models: pre-activation ResNet for ImageNet and CIFAR, and AlexNet with batch-norm\r\n* New quantization documentation content",
        "html_url": "https://github.com/IntelLabs/distiller/releases/tag/v0.2.0",
        "name": "PyTorch 0.4 support and new features",
        "release_id": 11631687,
        "tag": "v0.2.0",
        "tarball_url": "https://api.github.com/repos/IntelLabs/distiller/tarball/v0.2.0",
        "type": "Release",
        "url": "https://api.github.com/repos/IntelLabs/distiller/releases/11631687",
        "value": "https://api.github.com/repos/IntelLabs/distiller/releases/11631687",
        "zipball_url": "https://api.github.com/repos/IntelLabs/distiller/zipball/v0.2.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "nzmora",
          "type": "User"
        },
        "date_created": "2018-05-15T14:27:31Z",
        "date_published": "2018-05-16T09:21:13Z",
        "description": "We're tagging this version which uses PyTorch 0.3, and we want to want to move the 'master' branch to support PyTorch 0.4 and its API changes. ",
        "html_url": "https://github.com/IntelLabs/distiller/releases/tag/v0.1.0",
        "name": "Initial version",
        "release_id": 11022913,
        "tag": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/IntelLabs/distiller/tarball/v0.1.0",
        "type": "Release",
        "url": "https://api.github.com/repos/IntelLabs/distiller/releases/11022913",
        "value": "https://api.github.com/repos/IntelLabs/distiller/releases/11022913",
        "zipball_url": "https://api.github.com/repos/IntelLabs/distiller/zipball/v0.1.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Running the tests",
        "type": "Text_excerpt",
        "value": "We are currently light-weight on test and this is an area where contributions will be much appreciated.<br>\nThere are two types of tests: system tests and unit-tests.  To invoke the unit tests:\n```\n$ cd distiller/tests\n$ pytest\n```\n\nWe use CIFAR10 for the system tests, because its size makes for quicker tests.  To invoke the system tests, you need to provide a path to the CIFAR10 dataset which you've already downloaded.  Alternatively, you may invoke ```full_flow_tests.py``` without specifying the location of the CIFAR10 dataset and let the test download the dataset (for the first invocation only).  Note that ```--cifar1o-path``` defaults to the current directory. <br>\nThe system tests are not short, and are even longer if the test needs to download the dataset.  \n\n```\n$ cd distiller/tests\n$ python full_flow_tests.py --cifar10-path=<some_path>\n```\n\nThe script exits with status 0 if all tests are successful, or status 1 otherwise.\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "download",
    "requirements",
    "contact",
    "contributors",
    "faq",
    "support",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-04 00:26:52",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 4342
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Getting Started",
        "type": "Text_excerpt",
        "value": "Distiller comes with sample applications and tutorials covering a range of model types:\n\n| Model Type | Sparsity | Post-training quantization | Quantization-aware training | Auto Compression (AMC) | Knowledge Distillation |\n|------------|:--------:|:--------------------------:|:---------------------------:|:----------------------:|:--------:|\n| [Image classification](https://github.com/IntelLabs/distiller/tree/master/examples/classifier_compression) | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| [Word-level language model](https://github.com/IntelLabs/distiller/tree/master/examples/word_language_model)| :white_check_mark: | :white_check_mark: | | |\n| [Translation (GNMT)](https://github.com/IntelLabs/distiller/tree/master/examples/GNMT) | | :white_check_mark: | | |\n| [Recommendation System (NCF)](https://github.com/IntelLabs/distiller/tree/master/examples/ncf) | |  :white_check_mark: | | |\n| [Object Detection](https://github.com/IntelLabs/distiller/tree/master/examples/object_detection_compression) |  :white_check_mark: | | | |\n\nHead to the [examples](https://github.com/IntelLabs/distiller/tree/master/examples) directory for more details.\n\nOther resources to refer to, beyond the examples:\n+ [Frequently-asked questions (FAQ)](https://github.com/IntelLabs/distiller/wiki/Frequently-Asked-Questions-(FAQ))\n+ [Model zoo](https://intellabs.github.io/distiller/model_zoo.html)\n+ [Compression scheduling](https://intellabs.github.io/distiller/schedule.html)\n+ [Usage](https://intellabs.github.io/distiller/usage.html)\n+ [Preparing a model for quantization](https://intellabs.github.io/distiller/prepare_model_quant.html)\n+ [Tutorial: Pruning Filters & Channels](https://intellabs.github.io/distiller/tutorial-struct_pruning.html)\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Basic Usage Examples",
        "parent_header": [
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": "The following are simple examples using Distiller's image classifcation sample, showing some of Distiller's capabilities.\n\n+ [Training-only](#training-only)\n+ [Getting parameter statistics of a sparsified model](#getting-parameter-statistics-of-a-sparsified-model)\n+ [Post-training quantization](#post-training-quantization)\n\n<details><summary><b>Example: Simple training-only session (no compression)</b></summary>\t\n<p>\n\nThe following will invoke training-only (no compression) of a network named 'simplenet' on the CIFAR10 dataset.  This is roughly based on TorchVision's sample Imagenet training application, so it should look familiar if you've used that application.  In  this example we don't invoke any compression mechanisms: we just train because for fine-tuning after pruning, training is an essential part.<br>  \nNote that the first time you execute this command, the CIFAR10 code will be downloaded to your machine, which may take a bit of time - please let the download process proceed to completion.\n\nThe path to the CIFAR10 dataset is arbitrary, but in our examples we place the datasets in the same directory level as distiller (i.e. ```../../../data.cifar10```).\n\nFirst, change to the sample directory, then invoke the application:\n```\n$ cd distiller/examples/classifier_compression\n$ python3 compress_classifier.py --arch simplenet_cifar ../../../data.cifar10 -p 30 -j=1 --lr=0.01\n```\n\nYou can use a TensorBoard backend to view the training progress (in the diagram below we show a couple of training sessions with different LR values).  For compression sessions, we've added tracing of activation and parameter sparsity levels, and regularization loss.\n<center> <img src=\"imgs/simplenet_training.png\"></center>\n\n</p>\n</details>\n\n<details><summary><b>Example: Getting parameter statistics of a sparsified model</b></summary>\t\n<p>\n\nWe've included in the git repository a few checkpoints of a ResNet20 model that we've trained with 32-bit floats.  Let's load the checkpoint of a model that we've trained with channel-wise Group Lasso regularization.<br>\nWith the following command-line arguments, the sample application loads the model (```--resume```)  and prints statistics about the model weights (```--summary=sparsity```).  This is useful if you want to load a previously pruned model, to examine the weights sparsity statistics, for example.  Note that when you *resume* a stored checkpoint, you still need to tell the application which network architecture the checkpoint uses (```-a=resnet20_cifar```):\n```\n$ python3 compress_classifier.py --resume=../ssl/checkpoints/checkpoint_trained_ch_regularized_dense.pth.tar -a=resnet20_cifar ../../../data.cifar10 --summary=sparsity\n```\n<center> <img src=\"imgs/ch_sparsity_stats.png\"></center>\n\nYou should see a text table detailing the various sparsities of the parameter tensors.  The first column is the parameter name, followed by its shape, the number of non-zero elements (NNZ) in the dense model, and in the sparse model.  The next set of columns show the column-wise, row-wise, channel-wise, kernel-wise, filter-wise and element-wise sparsities.\n<br>\nWrapping it up are the standard-deviation, mean, and mean of absolute values of the elements.\n\nIn the [Compression Insights notebook](https://github.com/IntelLabs/distiller/blob/master/jupyter/compression_insights.ipynb) we use matplotlib to plot a bar chart of this summary, that indeed show non-impressive footprint compression.\n\n<center> <img src=\"imgs/ch_sparsity_stats_barchart.png\"></center>\n\n\nAlthough the memory footprint compression is very low, this model actually saves 26.6% of the MACs compute.  \n```\n$ python3 compress_classifier.py --resume=../ssl/checkpoints/checkpoint_trained_channel_regularized_resnet20_finetuned.pth.tar -a=resnet20_cifar ../../../data.cifar10 --summary=compute\n```\n<center> <img src=\"imgs/ch_compute_stats.png\"></center>\n</p>\n</details>\n\n<details><summary><b>Example: Post-training quantization</b></summary>\t\n<p>\n\nThis example performs 8-bit quantization of ResNet20 for CIFAR10.  We've included in the git repository the checkpoint of a ResNet20 model that we've trained with 32-bit floats, so we'll take this model and quantize it:\n\n```\n$ python3 compress_classifier.py -a resnet20_cifar ../../../data.cifar10 --resume ../ssl/checkpoints/checkpoint_trained_dense.pth.tar --quantize-eval --evaluate\n```\n\nThe command-line above will save a checkpoint named `quantized_checkpoint.pth.tar` containing the quantized model parameters. See more examples [here](https://github.com/IntelLabs/distiller/blob/master/examples/quantization/post_train_quant/command_line.md).\n</p>\n</details>\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Explore the sample Jupyter notebooks",
        "parent_header": [
          "Getting Started"
        ],
        "type": "Text_excerpt",
        "value": "The set of notebooks that come with Distiller is described [here](https://intellabs.github.io/distiller/jupyter.html#using-the-distiller-notebooks), which also explains the steps to install the Jupyter notebook server.<br>\nAfter installing and running the server, take a look at the [notebook](https://github.com/IntelLabs/distiller/blob/master/jupyter/sensitivity_analysis.ipynb) covering pruning sensitivity analysis.\n\nSensitivity analysis is a long process and this notebook loads CSV files that are the output of several sessions of sensitivity analysis.\n<center> <img src=\"imgs/resnet18-sensitivity.png\"></center>\n"
      },
      "source": "https://raw.githubusercontent.com/IntelLabs/distiller/master/README.md",
      "technique": "header_analysis"
    }
  ]
}