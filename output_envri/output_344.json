{
  "application_domain": [
    {
      "confidence": 18.51,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Friston",
        "parent_header": [
          "Deep Predictive Coding for Multimodal Representation Learning",
          "References"
        ],
        "type": "Text_excerpt",
        "value": "Friston, K., & Kiebel, S. (2009). [Predictive coding under the free-energy principle](http://rstb.royalsocietypublishing.org/content/364/1521/1211). Philosophical Transactions of the Royal Society B: Biological Sciences, 364(1521), 1211-1221.\n"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Friston_",
        "parent_header": [
          "Deep Predictive Coding for Multimodal Representation Learning",
          "References"
        ],
        "type": "Text_excerpt",
        "value": "Friston, K. (2010). The free-energy principle: a unified brain theory? Nature Reviews Neuroscience, 11(2):127.\n"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Lotter",
        "parent_header": [
          "Deep Predictive Coding for Multimodal Representation Learning",
          "References"
        ],
        "type": "Text_excerpt",
        "value": "Lotter, W., Kreiman, G., & Cox, D. (2016). [Deep predictive coding networks for video prediction and unsupervised learning](https://arxiv.org/abs/1605.08104). arXiv preprint arXiv:1605.08104.\n"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Monfort",
        "parent_header": [
          "Deep Predictive Coding for Multimodal Representation Learning",
          "References"
        ],
        "type": "Text_excerpt",
        "value": "Monfort, M., Zhou, B., Bargal, S. A., Andonian, A., Yan, T., Ramakrishnan, K., ... & Oliva, A. (2018). [Moments in Time Dataset: one million videos for event understanding](https://arxiv.org/abs/1801.03150). arXiv preprint arXiv:1801.03150.\n"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Rao",
        "parent_header": [
          "Deep Predictive Coding for Multimodal Representation Learning",
          "References"
        ],
        "type": "Text_excerpt",
        "value": "Rao, R. P. and Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):79.\n"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Soomro",
        "parent_header": [
          "Deep Predictive Coding for Multimodal Representation Learning",
          "References"
        ],
        "type": "Text_excerpt",
        "value": "Soomro, K., Zamir, A. R., and Shah, M. (2012). Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402.\n"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Stiles",
        "parent_header": [
          "Deep Predictive Coding for Multimodal Representation Learning",
          "References"
        ],
        "type": "Text_excerpt",
        "value": "Stiles, N. R. and Shimojo, S. (2015). Auditory sensory substitution is intuitive and automatic with texture stimuli. Scientific reports, 5:15628.\n"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/thefonseca/predictive-coding"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2018-03-27T21:03:11Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-06-28T10:52:03Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Research of a deep predictive coding model for unsupervised representation learning from video and audio data."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9975866947383764,
      "result": {
        "original_header": "Abstract",
        "type": "Text_excerpt",
        "value": "In machine learning parlance, common sense reasoning relates to the capacity of _learning representations_ that disentangle hidden factors behind spatiotemporal sensory data. In this work, we hypothesise that the predictive coding theory of perception and learning from neuroscience literature may be a good candidate for implementing such common sense inductive biases. We build upon a previous deep learning implementation of predictive coding by [Lotter et al. (2016)](#lotter) and extend its application to the challenging task of inferring abstract, everyday human actions such as _cooking_ and _diving_. Furthermore, we propose a novel application of the same architecture to process auditory data, and find that with a simple sensory substitution trick, the predictive coding model can learning useful representations. Our transfer learning experiments also demonstrate good generalisation of learned representations on the UCF-101 action classification dataset.\n \n"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9783710709122817,
      "result": {
        "original_header": "Research questions",
        "type": "Text_excerpt",
        "value": "To investigate the design of machines that acquire common sense by observing the world, we capitalise on a deep learning implementation of the predictive coding model published by [Lotter et al. (2016)](#lotter). Their deep predictive coding network was shown to learn representations that disentangle latent variables correlated to the movement of objects in synthetic and natural images. We extend their study to address the following questions:\n* Can unsupervised predictive coding models learn higher-level spatiotemporal concepts, namely quotidian activities such as _driving_ or _exercising_?\n* Are predictive coding inductive biases general enough so that these models can also learn from auditory information?\n* What are the limitations of the deep predictive coding implementation with respect to the original neuroscience theory proposed by [Friston and Kiebel (2009)](#friston) and [Rao and Ballard (1999)](#rao)?\n \n"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9822047733915886,
      "result": {
        "original_header": "Contributions",
        "type": "Text_excerpt",
        "value": "* Based on a theoretical review of the free-energy principle [(Friston, 2010)](#friston_), we analyse some of the architectural limitations of [Lotter et al. (2016)](#lotter) deep learning implementation, in particular, regarding the inference of hidden causes via free energy minimisation.\n* We extend the work of [Lotter et al. (2016)](#lotter) by using predictive coding representations to decode higher-level concepts that require the understanding of world dynamics. The learned representations are evaluated on small-scale tasks and on UCF-101 [(Soomro et al., 2012)](#soomro), a popular action recognition benchmark.\n* We train the predictive coding model on a dataset about 60 times larger than the one used in previous work ([Lotter et al., 2016](#lotter)) and show that model continues to improve future frame predictions, even when the training dataset includes a large number of unrelated classes. \n* Inspired by sensory substitution literature from neuroscience [(Stiles and Shimojo, 2015)](#stiles), a novel application of the predictive coding model is proposed for unsupervised representation learning from audio data. Our results suggest that the different modalities provide complementary information that is useful for the action classification task. \n"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9939017198690828,
      "result": {
        "original_header": "Project folders",
        "type": "Text_excerpt",
        "value": "* [datasets](./datasets): includes scripts for downloading and preprocessing of the datasets used in the experiments, including the Moments in Time and UCF-101 datasets.\n* [models/prednet](./models/prednet): the primary model implementation for our study. The model code is adapted from the implementation provided by [Lotter, 2016](#lotter). All the pipeline was reimplemented to fit our experimental needs.\n* [models/classifier](./models/classifier): implementation of simple SVM and LSTM classifiers used on top of predictive coding representations. \n"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/thefonseca/predictive-coding/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/classifier/scratch.ipynb"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/classifier/scratch.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/classifier/ensemble.ipynb"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/classifier/ensemble.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/prednet/frame_prediction_ucf.ipynb"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/prednet/frame_prediction_ucf.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/prednet/frame_prediction.ipynb"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/prednet/frame_prediction.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/prednet/scratch.ipynb"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/prednet/scratch.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/prednet/visualization.ipynb"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/prednet/visualization.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/prednet/frame_prediction_audio.ipynb"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/prednet/frame_prediction_audio.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 2
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/thefonseca/predictive-coding/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "thefonseca/predictive-coding"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Deep Predictive Coding for Multimodal Representation Learning"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/datasets/kitti_download.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/datasets/moments_download.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/datasets/moments_frames.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/classifier/train.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/models/prednet/kitti/kitti_model_download.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/./images/next-frame.png"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8176616892351387,
      "result": {
        "original_header": "Contributions",
        "type": "Text_excerpt",
        "value": "<p align=\"center\">\n  <img width=\"700\" src=\"./images/next-frame.png\">\n</p> \n"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/thefonseca/predictive-coding/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "action-recognition, deep-learning, free-energy-principle, predictive-coding, representation-learning"
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "predictive-coding"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "thefonseca"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 12382729,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 195206,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 2366,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1212.0402.\n\n##### Stiles\nStiles, N. R. and Shimojo, S. (2015). Auditory sensory substitution is intuitive and automatic with texture stimuli. Scientific reports, 5:15628."
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1605.08104"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1801.03150.\n\n##### Rao\nRao, R. P. and Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):79.\n\n##### Soomro\nSoomro, K., Zamir, A. R., and Shah, M. (2012). Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint https://arxiv.org/abs/1212.0402.\n\n##### Stiles\nStiles, N. R. and Shimojo, S. (2015). Auditory sensory substitution is intuitive and automatic with texture stimuli. Scientific reports, 5:15628."
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1605.08104.\n\n##### Monfort\nMonfort, M., Zhou, B., Bargal, S. A., Andonian, A., Yan, T., Ramakrishnan, K., ... & Oliva, A. (2018). [Moments in Time Dataset: one million videos for event understanding](https://arxiv.org/abs/1801.03150). arXiv preprint https://arxiv.org/abs/1801.03150.\n\n##### Rao\nRao, R. P. and Ballard, D. H. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature neuroscience, 2(1):79.\n\n##### Soomro\nSoomro, K., Zamir, A. R., and Shah, M. (2012). Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint https://arxiv.org/abs/1212.0402.\n\n##### Stiles\nStiles, N. R. and Shimojo, S. (2015). Auditory sensory substitution is intuitive and automatic with texture stimuli. Scientific reports, 5:15628."
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1801.03150"
      },
      "source": "https://raw.githubusercontent.com/thefonseca/predictive-coding/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "somef_missing_categories": [
    "installation",
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-04 00:25:30",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 27
      },
      "technique": "GitHub_API"
    }
  ]
}