{
  "application_domain": [
    {
      "confidence": 6.82,
      "result": {
        "type": "String",
        "value": "Audio"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/SunnyShikhar/music-datamining"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2016-10-23T18:59:47Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2023-12-19T10:52:19Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "An exploratory research to find any correlation between a listener's preference in music and their mental health."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9247415721155929,
      "result": {
        "original_header": "Mental Health as a Function of Music - Data Mining",
        "type": "Text_excerpt",
        "value": "An exploratory research to find any correlation between a listener's preference in music and their mental health. \nPython and Spotipy (Python Spotify Web Wrapper) were used to create the data mining models and to retrieve song information from Spotify's Web API.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.907674573046955,
      "result": {
        "original_header": "Data Collection",
        "type": "Text_excerpt",
        "value": "The primary data was collected through a survey that asked surveyers to list their 3 favourite songs at the moment and to rate their mental health using a likert scale. The following questions were asked as they are described to be good indicators of an individual's mental health by the [Canadian Mental Health Association](https://www.cmha.ca/mental_health/mental-health-meter/): \nThe survey also collected general data such as: \nOnce the survey had more than 300 entries (and approximately 1000 songs), a Python script was made to fetch data from Spotify's music catalog using Spotify Web API. The  [Get Audio Features for a Track](https://developer.spotify.com/web-api/get-audio-features/) endpoint was used to retrieve song information in a JSON format. An example of this JSON output is shown below for a song. \nFor the purposes of this study, only the following features were retrieved:  \n| Feature  | Definition by Spotify  |\n|:-:|---|\n| Energy |Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. |\n| Danceability | Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.  |\n| Valence  |  A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).  |\n| Tempo  | The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration. |\n| Popularity  | The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are.  |\n| Instrumentalness  | \tPredicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.  |\n|  Acousticness | \tA confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.  |\n| Liveness  | Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live. | \nOnce the data was collected, it was cleaned to ensure that there were no missing fields. Furthermore, each listener's total mental health score was calculated as a sum of the individual mental health answers and an average of their musical properties were taken to reflect the listener's overall musical behaviour.  \nNow that all the data has been collected and cleaned, time to get into the more interesting material!\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9182701035815548,
      "result": {
        "original_header": "Exploratory Data Analysis",
        "type": "Text_excerpt",
        "value": "Let's begin by plotting bar charts and histograms of each variable obtained from the survey and spotify to understand it's distribution, mode, variance etc. \n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9563799709294943,
      "result": {
        "original_header": "General Graphs",
        "type": "Text_excerpt",
        "value": "<p align=\"center\">\n  <img src=\"/images/genderHistogram.png?raw=true\" width=\"400\" /> \n  <img src=\"/images/hoursDurationHisto.png?raw=true\" width=\"400\" /> \n  <img src=\"/images/ageHistogram.png?raw=true\" width=\"400\" />\n</p>\n53.8% of the data set consists of females, 45.3% males and less than 1% chose not to specify. \nSurprisingly, <b>46.6%</b> of the data set listens to 2+ hours of music, <b>35%</b> listen to 1-2 hours and <b>18.4%</b> listen to 0 - 1 hours of music. It was expected that most people would be listening to 1-2 hours, but we underestimated the number of avid music listeners.  \nSimilarly, our data set primariy consisted of 18-30 year olds. We realzied later that this was too large of a range and should have been divided further.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9964538728909241,
      "result": {
        "original_header": "Music Graphs",
        "type": "Text_excerpt",
        "value": "Tempo, popularity, energy, dance and valence have a nice normal distribution which shows that listeners listen to a variety of music hovering around a mean of <b> 123 bpm, 61 popularity, 0.65 energy, 0.60 danceability, 0.45 valence</b>. The entire dataset is primarily not listening to instrumental tracks as shown by the instrumentalness graph which means we can drop this feature as it is least likely to be contributing to mental health. However let's keep exploring the data set before removing any features.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8949461948004408,
      "result": {
        "original_header": "Mental Health Graph",
        "type": "Text_excerpt",
        "value": "\nThe mental health histogram is also normally distributed around a mental health\nscore between 21-23. There are significantly less people with a low mental health\nscore as majority of the people have a medium to high mental health score.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9741151862833831,
      "result": {
        "original_header": "Scatter Plots",
        "type": "Text_excerpt",
        "value": "The scatter plot does not show any clear relationship between mental health and any musical feature (such as linear, logarithmic etc). However, we are able to conclude that instrumentalness is a weak feature in potentially predicting mental health as mental health ranges from 5 to 30 for really low instrumental values. To say this conclusively, let's do some feature engineering to ensure the right attributes are being used to predict mental health.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9725272756016781,
      "result": {
        "original_header": "Recursive Feature Elimination (RFE)",
        "type": "Text_excerpt",
        "value": "The Recursive Feature Elimination (RFE) method is a feature selection approach. It works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute. Using sklearn's LogisticRegression and RFE library, the following features were found to have the most impact on the mental health class variable.  \nAs expected, instrumentalness is the least important feature along with livness and acousticness. Therefore, these three features were removed for the remainder of the study in building more models.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9613180306106726,
      "result": {
        "original_header": "Association",
        "type": "Text_excerpt",
        "value": "The purpose of association was to find patterns in the data along with analyzing the relationships between various attributes. Based on the scatter plots analyzed previously, it was hypothesized that with a high confidence level, the categorical variable mental health will not be part of any rules. This is because mental health did not consistently correlate with any of the song attributes. In addition, 87.5% of the dataset consisted of people from ages 18-20 years old. It is assumed that the age range categorical variable will be associated with a lot of attributes due to it abundance of 18-20 year olds in the dataset. \nAs expected, association rules for mental health had a very low confidence. There are several reasons as to why the APRIORI algorithm did not generate good association rules for mental health based on the dataset. One factor could be that music may not be a dominant influence on a person\u2019s mental health leading to poor association rules. Another factor could be the lack of song data. However, some interesting associations amongst features are shown below:  \nThese were a few of the sensible rules out of thousands of rules. \"High Popularity + High Valence -> High Danceability\" and vice versa could indicate that songs that perhaps songs that get popular on Spotify may primarily be highly danceable songs. The other rules are interesting as well, such as \"High Danceability + Medium Popularity + High Valence\" has a 70% confidence of being a female. Let's see what type of clusters we can identify to see what \"type\" of people exist in the dataset. \n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.878292685551598,
      "result": {
        "original_header": "Clustering",
        "type": "Text_excerpt",
        "value": "Using the top 5 features (energy, danceability, popularity, tempo and valence) the clustering alogrithm was run on the data set to first find two clusters. When searching for two clusters, <b>tempo</b> was found to be the biggest factor that divided the clusters as shown below:\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.969953364552878,
      "result": {
        "original_header": "2 Clusters",
        "type": "Text_excerpt",
        "value": "It's informative to know that tempo is the main factor that divides the entire dataset into two clusters, telling us that there exist two main types of people in the data set- those who listen to mid to high tempo, and those who listen to mid to low tempo songs. But what's the optimal number of clusters? To answer this question and determine the best number of clusters, The Elbow Method was used. The Elbow method looks at the percentage of variance explained as a function of the number of clusters. The optimal number should form an \"elbow\", essentially showing that the increase with an additional cluster has less of an impact on the cluster center. The graph to find the \"elbow\" is shown below:  \nThe \"elbow\" can be spotted most visibly at cluster = 4. Therefore the clustering algorithm was run again with k = 4, to find 4 clusters. The graphs were difficult to interpret with 4 clusters as the clusters are being plotted in 4 dimensions and it is tough to interpret them in a 2D plot. For example, the 2D graphs overalp into clusters which shouldn't happen, looking like this:\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9240488991095166,
      "result": {
        "original_header": "4 Clusters",
        "type": "Text_excerpt",
        "value": "Since the graphs prove to be of little help, it is much more useful to analyze the cluster centroids for the 4 clusters that were formed. These clusters are summarized by the following table:  \n<b>Cluster 1: Energetic Radio Listeners</b>. People who listen to high danceability and energetic songs, that are just popular to be on radio or were once popular. (For example: <b> Stay - Kygo, Beibs In The Trap - Travis Scott </b> from cluster 1 listeners) \n<b>Clsuter 2: Soothing Underground Listeners</b>. People who listen to low energy, danceability and popularity songs that are relaxing and soothing. (For example: <b> Honey, Save Me From My Falsehoods - Asha Jefferies</b> from cluster 2 listeners) \n<b>Cluster 3: Upbeat Dancers</b>. People listening to fast tempo and highly danceable songs, but lacking energy and intensity. (For example: <b>Location - Khalid, Drama - Roy Woods, Dat $tick - Rich Chigga (how!?)</b> from cluster 3 listeners.) \n<b>Cluster 4: Underground Energetic/Indie Listeners</b>. People listening to high energy and low danceability music, often alternative rock/screamo/indie music that's not popular on Spotify or energetic underground rap/hip-hop music. <b>(For example: Chloroform - Phoenix, Aftermanth - Crown The Empire, Just Might Be - Young Thug</b> from cluster 4 listeners.)\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9702405952773411,
      "result": {
        "original_header": "Linear Regression",
        "type": "Text_excerpt",
        "value": "As noticed through scatter plots, there is no graph that distinctly shows any linear or non-linear relationship. Therefore, more scatter plots were investigated. When plotting danceability of songs for people who said they went through a traumatic experience and those who didn't with mental health hinted that there might be a relationship present. Although the scatter plot is still difficult to interpret, the filtered plot is shown below: \nBased on the observations of the scatter plots, it was hypothesized that linear regression model would not be the best model to represent the dataset.To conclude this hypothesis, a linear regression analysis was conducted on mental health score relative to the danceability factor for individuals who recently encountered a traumatic experience. Danceability song attribute was chosen as the scatter plot resembled the most to a\nlinear trend in comparison to other song attributes.  \nThe residual plot of mental health scores against danceability showed a normal distribution concentrated around zero. This validates that a linear regression model is an appropriate model although not an applicable one due to such a wide spread of data points. The R-squared value obtained for the linear regression model was 14% which is fairly low. This further shows that the model does not account for much variability in\nthe data.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9883401529313893,
      "result": {
        "original_header": "Multiple Linear Regression",
        "type": "Text_excerpt",
        "value": "In order to improve the r-squared value, a multiple regression was conducted using the top 5 features from RFE to predict Mental Health. Sklearn's linear_model library was used which uses the Ordinary Least Squares (OLS) method to conduct it's multiple linear regression. OLS or linear least squares method computes the least squares solution using a singular value decomposition of X. This means the algorithm attempts to minimize the sum of squares of residuals. The output is shown below.  \nThe R-squared value increases from 13% in the linear regression to 20.5% in the multi linear regression. However, it is still a weak r-squared value. However, the adjusted r-squared is significantly lower at 15.8%. The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. This could mean that even 5 features are too many (or too few) to predict mental health and that the gap between r-squared and adjusted r-squared could be less by adding or removing features.  \nRegardless, it is conclusive that a linear model is not an accurate model to represent mental health and musical features. Let's keep searching! \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9548008796679843,
      "result": {
        "original_header": "Naive Bayes Model",
        "type": "Text_excerpt",
        "value": "The last model that was considered to make predictions was Naive Bayes model. Naive Bayes is a classification technique based on Bayes\u2019 Theorem with an assumption of independence amongst its features/predictors.Bayes\u2019 Theorem can be used to calculate the probability of a person\u2019s mental health category given the numerical song attributes of their preferred songs. It is important to note the assumption that each song attribute is independent of another for Na\u00efve Bayes\u2019 theorem. \nThe surprising weakness was that the model incorrectly predicted 33 data points to have a high mental health when they have a low mental health. This is the most glaring problem with the model. Since medium health falls in the middle of high and low health, some overlap in prediction of medium mental health is expected. However, incorrectly predicting high mental health when it is in fact low mental health, or vice versa is a significant flaw. This is due to the scattered distribution of the data points. \nTo ensure that the model is not overfitting, the data was cross validated with 10 folds. The mean of the 10 fold cross-validation was calculated to be 0.38. Therefore, the model is slightly overfitting. However, it is within 10% so the model is not extremely overfitting. A potential solution to reduce overfitting would be to gather more data from people who have low mental health since the dataset has few people with low mental health, as seen in the histogram. The Naive Bayes\u2019 model was used to predict mental health using the 4 features; energy, danceability, popularity and tempo. These predictions are shown below:  \n|  Description | Energy  | Dance  | Popularity   | Tempo  | Result Health   | Confidence   |\n|:-:|---|---|---|---|---|---|\n| High Value Features | 0.95 | 0.95 | 100 | 150 | High | 70.3% |\n| Mid-High Value Features | 0.7 | 0.7 | 70 | 140 | High | 40.2% |\n| Medium Value Features | 0.6 | 0.6 | 60 | 120 | Medium | 47.1% |\n| Low Value Features | 0.2 | 0.2 | 20 | 100 | Low | 93.3% | \nAlthough the accuracy of the model is weak, the model of the output is as hypothesized: users who listen to songs with high dance, energy, tempo and popularity are 63% probable to have a high mental health. Conversely, those who listen to all features with extremely low values are 93.3% probable to have low mental health. Therefore the model is useful to predict mentalhealth when the feature values are extreme, either extremely high or low. However, the Naive Bayes\u2019 model struggles to predict mental health when the song attributes hover around 0.5, as shown by the probability of prediction for each cluster. The model struggles to predict mid-ranged values as most values in the dataset are centered around the middle, as seen by the scatterplot figures.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9809270038685683,
      "result": {
        "original_header": "Recommendations",
        "type": "Text_excerpt",
        "value": "Naive Bayes\u2019 can be used as a powerful tool to recommend songs to users to improve their mental health since it predicts low and high mental health with a strong probability for extreme values of each song attributes. Therefore, if it is known that a person has low mental health, songs can be recommended to users that have a high probability of improving mental health due to the Naive Bayes\u2019 prediction. In addition to individual songs, a playlist which averages out to yield a high mental health prediction could be recommended. For example: \n- Juju On That Beat (TZ Anthem) - Zay Hilfigerrr\n- This Is What You Came For - Calvin Harris\n- La Bicicleta - Carlos Vives  \nThe combination of the above songs can improve your stress level and mental health according to our model, as these songs combined are predicted to have a high mental health with a probability of 60%\u201d.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9616492986114487,
      "result": {
        "original_header": "Conclusion",
        "type": "Text_excerpt",
        "value": "Data mining procedures such as linear regression, association, clustering and Na\u00efve Bayes\u2019 were used to analyze the effect that music has on a person\u2019s overall mental health. The song attributes of each song inputted by individuals were obtained by using Spotify's Web API. \nBased on the results of association rule mining, zero song attributes were associated with any of the categorical mental health variables if a high confidence level was used. In addition, the best linear regression model (mental health vs danceability) has an R-squared value of 13%\nwhich does not account for much variability in the dataset. Na\u00efve Bayes\u2019 model has a low accuracy but provides usable predictions and matches the initial hypothesis. However, there are several reasons as to why the results of the data mining procedures were not favorable. This could be due to the lack of song data as only three songs were collected per individual. Three songs are not an accurate representation of an individual\u2019s music preferences. \nFurthermore, lyrical content was also not considered in this project. For instance, a song may have a high tempo, energy and danceability factors, but the song may have negative lyrical content. Another reason may be due to the lack of diversity of people who completed the survey. Only 12% of the dataset consisted of people who were under 17 or above 30 years old. The rest of the individuals were between the age ranges of 18-30 years old. \nIn addition, a major potential reason as to why the data mining techniques did not yield good results may be because music is not major influence on a person\u2019s mental health. Other factors such as relationship and financial status could have more of an effect on a person\u2019s mental health. However, Na\u00efve Bayes\u2019 provided the best model given the flaws in the data set to predict mental health and use to improve individuals\u2019 mental health by recommending songs. \nIn the future, we can attempt Logistic Regression or Support Vector Machines to classify our data set. \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/SunnyShikhar/music-datamining/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/bayes.ipynb"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/bayes.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/data-collection.ipynb"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/data-collection.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/regression.ipynb"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/regression.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/eda.ipynb"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/eda.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/assoc.ipynb"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/assoc.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/clust.ipynb"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/clust.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/rfe.ipynb"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/jupyter/rfe.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 5
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/SunnyShikhar/music-datamining/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "SunnyShikhar/music-datamining"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Mental Health as a Function of Music - Data Mining"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/genderHistogram.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/hoursDurationHisto.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/ageHistogram.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/tempoHistogram.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/popularityHistogram.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/valenceHistogram.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/livenessHistogram.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/acousticHistogram.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/instrumentalnessHistogram.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/sunnyshikhar/music-datamining/master/images/mentalHealthHisto.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/healthVsTempo.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/healthVsPopularity.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/healthVsEnergy.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/healthVsDanceability.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/healthVsValence.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/healthVsLiveness.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/healthVsAcousticness.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/healthVsInstrumentalness.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/energyVsTempoCluster.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/danceVsTempoCluster.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/popularityVsTempoCluster.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/valenceVsTempoCluster.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/sunnyshikhar/music-datamining/master/images/danceVsEnergyCluster.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/sunnyshikhar/music-datamining/master/images/numberOfClusters.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/danceVsTempoClusterK4.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/popularityVsTempoK4.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/sunnyshikhar/music-datamining/master/images/clusterCatK4.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/sunnyshikhar/music-datamining/master/images/traumaScatter.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/traumaRegression.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/traumaResidualHisto.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch//images/residualDistribution.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/sunnyshikhar/music-datamining/master/images/OLSregression.png?raw=true"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9999478685743296,
      "result": {
        "original_header": "Data Collection",
        "type": "Text_excerpt",
        "value": "[{\n- \"track_href\": \"https://api.spotify.com/v1/tracks/7MXVkk9YMctZqd1Srtv4MB\",\n- \"type\": \"audio_features\",\n- \"analysis_url\": \"https://api.spotify.com/v1/audio-analysis/7MXVkk9YMctZqd1Srtv4MB\",\n- \"acousticness\": 0.168,\n- \"speechiness\": 0.284,\n- \"liveness\": 0.136,\n- \"uri\": \"spotify:track:7MXVkk9YMctZqd1Srtv4MB\",\n- \"id\": \"7MXVkk9YMctZqd1Srtv4MB\",\n- \"key\": 7,\n- \"mode\": 1,\n- \"time_signature\": 4,\n- \"duration_ms\": 230453,\n- \"tempo\": 185.998,\n- \"energy\": 0.595,\n- \"danceability\": 0.675,\n- \"valence\": 0.49,\n- \"instrumentalness\": 3.36e-06\n}] \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8801261378331628,
      "result": {
        "original_header": "General Graphs",
        "type": "Text_excerpt",
        "value": "<p align=\"center\">\n  <img src=\"/images/genderHistogram.png?raw=true\" width=\"400\" /> \n  <img src=\"/images/hoursDurationHisto.png?raw=true\" width=\"400\" /> \n  <img src=\"/images/ageHistogram.png?raw=true\" width=\"400\" />\n</p>\n53.8% of the data set consists of females, 45.3% males and less than 1% chose not to specify. \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9224414576645178,
      "result": {
        "original_header": "Music Graphs",
        "type": "Text_excerpt",
        "value": "<p align=\"center\">\n  <img src=\"/images/tempoHistogram.png?raw=true\" width=\"400\" />\n  <img src=\"/images/popularityHistogram.png?raw=true\" width=\"400\" /> \n</p>\n<p align=\"center\">\n  <img src=\"/images/valenceHistogram.png?raw=true\" width=\"400\" />\n  <img src=\"/images/livenessHistogram.png?raw=true\" width=\"400\" /> \n</p>\n<p align=\"center\">\n  <img src=\"/images/acousticHistogram.png?raw=true\" width=\"400\" />\n  <img src=\"/images/instrumentalnessHistogram.png?raw=true\" width=\"400\" /> \n</p> \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9224946702116746,
      "result": {
        "original_header": "Scatter Plots",
        "type": "Text_excerpt",
        "value": "The following graphs plot mental health as a function of each musical feature.\n<p align=\"center\">\n  <img src=\"/images/healthVsTempo.png?raw=true\" width=\"400\" />\n  <img src=\"/images/healthVsPopularity.png?raw=true\" width=\"400\" /> \n</p>\n<p align=\"center\">\n  <img src=\"/images/healthVsEnergy.png?raw=true\" width=\"400\" />\n  <img src=\"/images/healthVsDanceability.png?raw=true\" width=\"400\" /> \n</p>\n<p align=\"center\">\n  <img src=\"/images/healthVsValence.png?raw=true\" width=\"400\" />\n  <img src=\"/images/healthVsLiveness.png?raw=true\" width=\"400\" /> \n</p>\n<p align=\"center\">\n  <img src=\"/images/healthVsAcousticness.png?raw=true\" width=\"400\" />\n  <img src=\"/images/healthVsInstrumentalness.png?raw=true\" width=\"400\" /> \n</p> \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9224414576645178,
      "result": {
        "original_header": "2 Clusters",
        "type": "Text_excerpt",
        "value": "<p align=\"center\">\n  <img src=\"/images/energyVsTempoCluster.png?raw=true\" width=\"400\" />\n  <img src=\"/images/danceVsTempoCluster.png?raw=true\" width=\"400\" /> \n</p>\n<p align=\"center\">\n  <img src=\"/images/popularityVsTempoCluster.png?raw=true\" width=\"400\" />\n  <img src=\"/images/valenceVsTempoCluster.png?raw=true\" width=\"400\" /> \n</p> \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9224414576645178,
      "result": {
        "original_header": "4 Clusters",
        "type": "Text_excerpt",
        "value": "<p align=\"center\">\n  <img src=\"/images/danceVsTempoClusterK4.png?raw=true\" width=\"400\" />\n  <img src=\"/images/popularityVsTempoK4.png?raw=true\" width=\"400\" /> \n</p> \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9244910990473627,
      "result": {
        "original_header": "Linear Regression",
        "type": "Text_excerpt",
        "value": "<p align=\"center\">\n  <img src=\"/images/traumaRegression.png?raw=true\" width=\"400\" />\n  <img src=\"/images/traumaResidualHisto.png?raw=true\" width=\"400\" /> \n  <img src=\"/images/residualDistribution.png?raw=true\" width=\"400\" /> \n</p> \n"
      },
      "source": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/SunnyShikhar/music-datamining/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "music-datamining"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "SunnyShikhar"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 1983923,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 28839,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SunnyShikhar/music-datamining/presentation_branch/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-04 00:30:05",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 10
      },
      "technique": "GitHub_API"
    }
  ]
}