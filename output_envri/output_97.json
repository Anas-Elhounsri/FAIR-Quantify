{
  "application_domain": [
    {
      "confidence": 36.69,
      "result": {
        "type": "String",
        "value": "Natural Language Processing"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/Fryingpannn/WallStreetBets_BigDataAnalysis"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2021-03-15T02:45:14Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-29T15:57:42Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Research project aimed to classify the best stock research posts from r/WallStreetBets for you. \ud83d\ude0f"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "**II. Introduction**",
        "type": "Text_excerpt",
        "value": "With the influx of retail investors during the 2021 Coronavirus pandemic and GameStop fiasco, it is more important now than ever for people to educate themselves on investment decisions. The largest financial subreddit, r/WallStreetBets, is home to many memes, however, many\u00a0**DD*** posts are also present.\n\nUnlike professional investors, retail investors have very limited time to conduct their own research as they usually have a separate career. In this project, we aim to further democratize stock trading by filtering and classifying stock research such that every regular investor has the opportunity to quickly access them.\n\nWe will be able to provide a quick filter on the top research posts as well as a classification on whether any given post may or may not be worth your time to read.\n\nA related work is a website called SwaggyStocks.com, in which sentiment analysis of the r/WallStreetBets subreddit is done and visualized. However, we differ from this as we are not trying to explicitly analyze sentiment of the subreddit. Instead, we will base off our filtering upon historical performance of the stocks each post is discussing.\n\n***DD: stands for \"Due Diligence\". Represents the investigation and research a person has done for a potential investment.**\n"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9955509715588261,
      "result": {
        "original_header": "**I. Abstract**",
        "type": "Text_excerpt",
        "value": "One of the most popular platforms for stocks and financial discussion is a subreddit called WallStreetBets. We would like to help further democratize stock trading by making the access to insightful stock research easier. Most people already know about the mainstream news outlets, this is why we choose this subreddit, where insightful yet unpopularized research information lay in a sea of memes. We will use the posts on r/WallStreetBets as our dataset and aim to classify them as either valuable or less valuable in order to help any user quickly filter out posts he or she may want to read. This project aims to be particularly useful towards those, such as ourselves, who are interested in trading and investing in the stock market, but have little time to do their own research.\n \n"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9890602095562431,
      "result": {
        "original_header": "**III. Materials and Methods**",
        "type": "Text_excerpt",
        "value": "Our dataset consist of the posts from the r/WallStreetBets subreddit. They are extracted by using the Pushshift API (we previously used the Reddit PRAW API, but due to API limitations we had to revamp all our code with Pushshift instead which proved to be more flexible). The Pushshift API is able to provide us with a lot of information about a given post such as the title, score, upvote ratio, author, text, URL, created time, comments and more. Except for upvote ratio, score, created time, and number of comments, all other data are textual. \nWe also used the iexfinance API to obtain financial data. This API provides the closing price, ticker, financials, cash-flow, volumes of a specific ticker and more.\n \n"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9872286456787809,
      "result": {
        "original_header": "Data Preprocessing",
        "type": "Text_excerpt",
        "value": "Upon receiving text data from Pushshift, we used regular expression to extract tickers from the posts. We used the iexfinance API to validate the ticker and get its closing price at the post creation date and present date. To label data, we calculated the growth percentage between these two dates. We set the standard to 6%, any post above 6% are labeled as 1 (good) else 0 (bad). We believe if whatever stock a post is talking about has risen 6%, it is worth taking a look at, plus, this number becomes crucial later in our model training step when splitting training and test sets. It allowed us to avoid data imbalance as we had a perfect split of 50% of our data as class 1 and the other 50% as class 0. \nThese posts are filtered and cleaned with many conditions, some of which you may see in these two functions. *(WallStreetBets-Preprocessing.ipynb)* \n"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9982720962472763,
      "result": {
        "original_header": "Algorithms",
        "type": "Text_excerpt",
        "value": "To extract features, we first lemmatized the texts of each post with Spark NLP, an extension package which provides a pre-trained NLP model. we then used CountVectorizer and HashingTF from PySpark to transform textual data into vectors. CountVectorizer generates the Document-Term Matrix and contains the size of the entire vocabulary, whereas HashingTF also generates this text vector but with lower number of features (performs dimensionality reduction). \n \n"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9861637837223843,
      "result": {
        "original_header": "Naive Bayes",
        "type": "Text_excerpt",
        "value": "The first algorithm we used is Naive Bayes from PySpark's machine learning library. Naive Bayes is a supervised learning classification model based on applying Bayes' theorem. The features are the results from CountVectorizer and HashingTF. We tested with two hyperparameters, the smoothing value and the model type. The smoothing value is used to replace 0 probability event, as it makes the entire event impossible due to 1 missing feature. We also used the two most appropriate Naive Bayes model types, Complement, and Multinomial. Each model type changes the algorithm with a different method to compute the model\u2019s coefficients, where Complement Naive Bayes is more suited for imbalanced datasets than Multinomial Naive Bayes. All these factors were randomized in many iterations to obtain the best combination.\n \n"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9993497009425478,
      "result": {
        "original_header": "Clustering",
        "type": "Text_excerpt",
        "value": "The second algorithm we used is k-means++ clustering from both PySpark's machine learning library and scikit-learn, combined with PCA and TruncatedSVD/t-SNE from scikit-learn for dimensionality reduction. Because our features are vectors with thousands of dimensions (words), to visualize and plot the data we need to perform dimensionality reduction. We tested our dataset with PCA, and TruncatedSVD/t-SNE. Principal Component Analysis (PCA) maps features to an N-dimensional space, and chooses a custom number of main axis to represent the data. For PCA, the decisions of using which axis is based on the minimized square distances between the data points and the axis, whereas t-SNE uses the probability of neighboring points, and computes the best components to represent that probability. \n \n"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9943250787282235,
      "result": {
        "original_header": "Discussion",
        "type": "Text_excerpt",
        "value": "Numerically, with 65% accuracy, our Naive Bayes model doesn't seem tremendously \"accurate\" or \"precise\". However, it must be noted that the goal of this classifier is simply to indicate which posts has a higher chance of being worth the user's time to read. Remember that this result is also obtained after having already filtered the posts with many variables as previously mentioned, thus the final classified posts actually have very good chances to be worth a read.\n \n"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9966921446132845,
      "result": {
        "original_header": "Potential Issues",
        "type": "Text_excerpt",
        "value": "1. Bias\n    - In the data we used to train our Naive Bayes classifier, a lot of the posts were discussing about the same stock, GME. This can cause some degree of bias as the words in those posts have a higher chance to relate to each other.\n    - Although our ticker extraction algorithm is very accurate, sometimes the ticker identified for a given post is still inaccurate. This can also cause bias as then the computed growth percentage (label) would not be relevant for the associated post.\n2. Lack of data\n    - From around 40 000 posts, we filtered and cleaned it down to 1141 data points. Although these remaining 1141 posts we used to train the model were of good quality due to all the filtering we did, it is still a small quantity. We believe we may get even better results by acquiring more data, which was very difficult due to API limitations.\n \n"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9943263068870408,
      "result": {
        "original_header": "Closing thoughts",
        "type": "Text_excerpt",
        "value": "As for utility, we believe our solution, with just a bit more tuning, can actually be useful and create value. In the goal of making access to stock research easier, we plan to create a website, on which many features would be present. This Naive Bayes classifier can easily be one of the features: by displaying a list of filtered research posts, we may indicate beside each of them, the probability that they would be worth a read. \nWe believe this would be of great interest to those, such as ourselves, who like trading and investing in the stock market, but have little time to find and do research of their own. \nIn conclusion, this project was incredibly interesting for us. We had the chance to go through essentially the whole data science workflow, from finding a unique problem statement, gathering data, training models and presenting, as well as somewhat accomplish our goal to create something useful. As undergraduate students, it was a tremendous learning experience and we hope to create more exciting projects like this one. \n"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/Fryingpannn/WallStreetBets_BigDataAnalysis/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/ML%20Model%20Training/WallStreetBets-CountVec-NaiveBayes.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/ML%20Model%20Training/WallStreetBets-CountVec-NaiveBayes.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/ML%20Model%20Training/WallStreetBets-HashingTF-NaiveBayes.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/ML%20Model%20Training/WallStreetBets-HashingTF-NaiveBayes.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/ML%20Model%20Training/WallStreetBets-Clustering.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/ML%20Model%20Training/WallStreetBets-Clustering.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/ML%20Model%20Training/NotGood/NotGood-PySpark-RandomForest.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/ML%20Model%20Training/NotGood/NotGood-PySpark-RandomForest.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/ML%20Model%20Training/NotGood/NotGood-PySpark-Clustering.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/ML%20Model%20Training/NotGood/NotGood-PySpark-Clustering.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/Preprocessing/WallStreetBets-CreateLemmas.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/Preprocessing/WallStreetBets-CreateLemmas.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/Preprocessing/WallStreetBets-Preprocessing.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/Preprocessing/WallStreetBets-Preprocessing.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 42
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/Fryingpannn/WallStreetBets_BigDataAnalysis/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Fryingpannn/WallStreetBets_BigDataAnalysis"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ":rocket: WallStreetBets Stock Research Posts Classifier :rocket:"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://user-images.githubusercontent.com/59063950/115100523-381fd900-9f0b-11eb-9fd7-a1cc39944389.png"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://user-images.githubusercontent.com/59063950/115100526-3bb36000-9f0b-11eb-9c94-431ca0c6ea91.png"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://user-images.githubusercontent.com/59063950/115100544-538ae400-9f0b-11eb-928a-8db23119abd4.png"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://user-images.githubusercontent.com/59063950/115100549-584f9800-9f0b-11eb-91b8-63a991b2f7f1.png"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://user-images.githubusercontent.com/59063950/115100562-6ac9d180-9f0b-11eb-8722-b949ffdf24c8.png"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://user-images.githubusercontent.com/59063950/115100567-774e2a00-9f0b-11eb-98e3-50296c9c61d2.png"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://user-images.githubusercontent.com/59063950/115100569-7ddca180-9f0b-11eb-88af-1a85a0b947e2.png"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://user-images.githubusercontent.com/59063950/115100573-82a15580-9f0b-11eb-8d84-82e16e88814d.png"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9995000205564116,
      "result": {
        "original_header": ":rocket: WallStreetBets Stock Research Posts Classifier :rocket:",
        "type": "Text_excerpt",
        "value": "Project presentation: https://youtu.be/spDbteQAlbE  <br>\nProject website: https://wsbrecommender.web.app/ \n"
      },
      "source": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/Fryingpannn/WallStreetBets_BigDataAnalysis/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "clustering, naive-bayes, pyspark, python, reddit-api, scikit-learn, stocks"
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "WallStreetBets_BigDataAnalysis"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "Fryingpannn"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 1533860,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "CSS",
        "size": 47387,
        "type": "Programming_language",
        "value": "CSS"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "JavaScript",
        "size": 32706,
        "type": "Programming_language",
        "value": "JavaScript"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 12241,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "HTML",
        "size": 11545,
        "type": "Programming_language",
        "value": "HTML"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Fryingpannn/WallStreetBets_BigDataAnalysis/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-03 23:14:01",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 173
      },
      "technique": "GitHub_API"
    }
  ]
}