{
  "application_domain": [
    {
      "confidence": 59.94,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "author": "Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, Matthias Grundmann",
        "format": "bibtex",
        "title": "Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations",
        "type": "Text_excerpt",
        "value": "@article{objectron2021,\n    year = {2021},\n    journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n    author = {Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, Matthias Grundmann},\n    title = {Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations},\n}"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/google-research-datasets/Objectron"
      },
      "technique": "GitHub_API"
    }
  ],
  "contributing_guidelines": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# How to Contribute\n\nWe'd love to accept your patches and contributions to this project. There are\njust a few small guidelines you need to follow.\n\n## Contributor License Agreement\n\nContributions to this project must be accompanied by a Contributor License\nAgreement. You (or your employer) retain the copyright to your contribution;\nthis simply gives us permission to use and redistribute your contributions as\npart of the project. Head over to <https://cla.developers.google.com/> to see\nyour current agreements on file or to sign a new one.\n\nYou generally only need to submit a CLA once, so if you've already submitted one\n(even if it was for a different project), you probably don't need to do it\nagain.\n\n## Code reviews\n\nAll submissions, including submissions by project members, require review. We\nuse GitHub pull requests for this purpose. Consult\n[GitHub Help](https://help.github.com/articles/about-pull-requests/) for more\ninformation on using pull requests.\n\n## Community Guidelines\n\nThis project follows [Google's Open Source Community\nGuidelines](https://opensource.google.com/conduct/).\n"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/CONTRIBUTING.md",
      "technique": "file_exploration"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-10-19T21:58:41Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-10-02T02:45:53Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Objectron is a dataset of short, object-centric video clips. In addition, the videos also contain AR session metadata including camera poses, sparse point-clouds and planes. In each video, the camera moves around and above the object and captures it from different views. Each object is annotated with a 3D bounding box. The 3D bounding box describes the object\u2019s position, orientation, and dimensions. The dataset contains about 15K annotated video clips and 4M annotated images in the following categories: bikes, books, bottles, cameras, cereal boxes, chairs, cups, laptops, and shoes"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9889163072182624,
      "result": {
        "original_header": "Key Features",
        "type": "Text_excerpt",
        "value": "- 15000 annotated videos and 4M annotated images\n- All samples include high-res images, object pose, camera pose, point-cloud, and surface planes.\n- Ready to use examples in various tf.record formats, which can be used in Tensorflow/PyTorch.\n- Object-centric multi-views, observing the same object from different angles.\n- Accurate evaluation metrics, like 3D IoU for oriented 3D bounding boxes. \n"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9341812700812485,
      "result": {
        "original_header": "Dataset Format",
        "type": "Text_excerpt",
        "value": "The data is stored in the [objectron bucket](https://storage.googleapis.com/objectron) on Google Cloud storage. Check out the [Download Data](https://github.com/google-research-datasets/Objectron/blob/master/notebooks/Download%20Data.ipynb) notebook for a quick review of how to download/access the dataset. The following assets are available: \n- The video sequences (located in `/videos/class/batch-i/j/video.MOV` files)\n- The annotation labels containing the 3D bounding boxes for objects. The annotation protobufs are located in `/videos/class/batch-i/j/geometry.pbdata` files. They are formatted using the [object.proto](https://github.com/google-research-datasets/Objectron/blob/master/objectron/schema/object.proto). See [example] on how to parse the annotation files.\n- AR metadata (such as camera poses, point clouds, and planar surfaces). They are based on [a_r_capture_metadata.proto](https://github.com/google-research-datasets/Objectron/blob/master/objectron/schema/a_r_capture_metadata.proto). See [example](https://github.com/google-research-datasets/Objectron/blob/master/notebooks/objectron-geometry-tutorial.ipynb) on how to parse these files.\n- Processed dataset: sharded and shuffled `tf.records` of the annotated frames, in tf.example format and videos in `tf.SequenceExample` format. These are used for creating the input data pipeline to your models. These files are located in `/v1/records_shuffled/class/` and `/v1/sequences/class/`.\n- Supporting scripts to run evaluation based on the 3D IoU metric.\n- Supporting scripts to load the data into Tensorflow, Jax and Pytorch and visualize the dataset, including \u201cHello World\u201d examples.\n- Supporting Apache Beam jobs to process the datasets on Google Cloud infrastructure.\n- The index of all available samples, as well as train/test splits for easy access and download. \nRaw dataset size is 1.9TB (including videos and their annotations). Total dataset size is 4.4TB (including videos, records, sequences, etc.). This repository provides the required schemas and tools to parse the dataset. \n"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9243072773962991,
      "result": {
        "original_header": "BibTeX",
        "type": "Text_excerpt",
        "value": "**This is not an officially supported Google product.** If you have any question, you can email us at objectron@google.com or join our mailing list at objectron@googlegroups.com \n"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9658222228504928,
      "result": {
        "type": "Text_excerpt",
        "value": "**Objectron is a dataset of short object centric video clips with pose annotations.** \n\nThe Objectron dataset is a collection of short, object-centric video clips, which are accompanied by AR session metadata that includes camera poses, sparse point-clouds and characterization of the planar surfaces in the surrounding environment. In each video, the camera moves around the object, capturing it from different angles. The data also contain manually annotated 3D bounding boxes for each object, which describe the object\u2019s position, orientation, and dimensions. The dataset consists of 15K annotated video clips supplemented with over 4M annotated images in the following categories: `bikes, books, bottles, cameras, cereal boxes, chairs, cups, laptops`, and `shoes`. In addition, to ensure geo-diversity, our dataset is collected from 10 countries across five continents. Along with the dataset, we are also sharing a  for four categories of objects \u2014 shoes, chairs, mugs, and cameras. These models are trained using this dataset, and are released in , Google's open source framework for cross-platform customizable ML solutions for live and streaming media. \n"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/google-research-datasets/Objectron/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/Hello%20World.ipynb"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/Hello%20World.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/3D_IOU.ipynb"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/3D_IOU.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/SequenceExamples.ipynb"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/SequenceExamples.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/Parse%20Annotations.ipynb"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/Parse%20Annotations.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/Objectron_Pytorch_tutorial.ipynb"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/Objectron_Pytorch_tutorial.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/objectron-geometry-tutorial.ipynb"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/objectron-geometry-tutorial.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/objectron-3dprojection-hub-tutorial.ipynb"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/objectron-3dprojection-hub-tutorial.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/Objectron_NeRF_Tutorial.ipynb"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/Objectron_NeRF_Tutorial.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/Download%20Data.ipynb"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/notebooks/Download%20Data.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 263
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/google-research-datasets/Objectron/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "google-research-datasets/Objectron"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/docs/images/objectron_samples.gif"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.8686803247252387,
      "result": {
        "type": "Text_excerpt",
        "value": "  Website \u2022\n  Dataset Format \u2022\n  Tutorials \u2022\n  License \n"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/google-research-datasets/Objectron/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "3d, 3d-reconstruction, 3d-vision, ai, augmented-reality, computer-vision, dataset, deep-learning, machine-learning, neural-network, python, pytorch, tensorflow"
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# Computational Use of Data Agreement v1.0\n\nThis is the Computational Use of Data Agreement, Version 1.0 (the \u201cC-UDA\u201d). Capitalized terms are defined in Section 5. Data Provider and you agree as follows:\n\n1. **Provision of the Data**\n\n    1.1. You may use, modify, and distribute the Data made available to you by the Data Provider under this C-UDA for Computational Use if you follow the C-UDA's terms.\n\n    1.2. Data Provider will not sue you or any Downstream Recipient for any claim arising out of the use, modification, or distribution of the Data provided you meet the terms of the C-UDA.\n\n    1.3 This C-UDA does not restrict your use, modification, or distribution of any portions of the Data that are in the public domain or that may be used, modified, or distributed under any other legal exception or limitation.\n\n2. **Restrictions**\n\n    2.1  You agree that you will use the Data solely for Computational Use.\n\n\t  2.2 The C-UDA does not impose any restriction with respect to the use, modification, or distribution of Results.\n\n3.\t**Redistribution of Data**\n\n    3.1. You may redistribute the Data, so long as:\n\n      3.1.1. You include with any Data you redistribute all credit or attribution information that you received with the Data, and your terms require any Downstream Recipient to do the same; and\n\n      3.1.2. You bind each recipient to whom you redistribute the Data to the terms of the C-UDA.\n\n4.\t**No Warranty, Limitation of Liability**\n\n    4.1. Data Provider does not represent or warrant that it has any rights whatsoever in the Data.\n\n    4.2. THE DATA IS PROVIDED ON AN \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE.\n\n    4.3. NEITHER DATA PROVIDER NOR ANY UPSTREAM DATA PROVIDER SHALL HAVE ANY LIABILITY FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING WITHOUT LIMITATION LOST PROFITS), HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE DATA OR RESULTS, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n5.\t**Definitions**\n\n    5.1. \u201cComputational Use\u201d means activities necessary to enable the use of Data (alone or along with other material) for analysis by a computer.\n\n    5.2.\u201cData\u201d means the material you receive under the C-UDA in modified or unmodified form, but not including Results.\n\n    5.3. \u201cData Provider\u201d means the source from which you receive the Data and with whom you enter into the C-UDA.\n\n    5.4. \u201cDownstream Recipient\u201d means any person or persons who receives the Data directly or indirectly from you in accordance with the C-UDA.\n\n    5.5. \u201cResult\u201d means anything that you develop or improve from your use of Data that does not include more than a de minimis portion of the Data on which the use is based. Results may include de minimis portions of the Data necessary to report on or explain use that has been conducted with the Data, such as figures in scientific papers, but do not include more. Artificial intelligence models trained on Data (and which do not include more than a de minimis portion of Data) are Results.\n\n    5.6. \u201cUpstream Data Providers\u201d means the source or sources from which the Data Provider directly or indirectly received, under the terms of the C-UDA, material that is included in the Data.\n"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/LICENSE",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License",
        "type": "Text_excerpt",
        "value": "Objectron is released under [Computational Use of Data Agreement 1.0 (C-UDA-1.0)](https://github.com/microsoft/Computational-Use-of-Data-Agreement). A [copy](https://github.com/google-research-datasets/Objectron/blob/master/LICENSE) of the license is available in this repository.\n\n"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Objectron"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "google-research-datasets"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 11990044,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 55115,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2012.09988"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "ahmadyan",
          "type": "User"
        },
        "date_created": "2021-06-10T20:03:34Z",
        "date_published": "2021-07-07T23:00:09Z",
        "description": "The full set of models (for EfficientNet and MobilePose) are available for download at Objectron bucket.\r\nThese models can be used to predict the 3D object poses from RGB images. Example usage, including Mobile, Python, and Web API are available via [Mediapipe](https://google.github.io/mediapipe/solutions/objectron).\r\nTo list/download the models, use `gsutil ls gs://objectron/models`",
        "html_url": "https://github.com/google-research-datasets/Objectron/releases/tag/v0.1.1",
        "name": "3D Object Detection Models",
        "release_id": 45871205,
        "tag": "v0.1.1",
        "tarball_url": "https://api.github.com/repos/google-research-datasets/Objectron/tarball/v0.1.1",
        "type": "Release",
        "url": "https://api.github.com/repos/google-research-datasets/Objectron/releases/45871205",
        "value": "https://api.github.com/repos/google-research-datasets/Objectron/releases/45871205",
        "zipball_url": "https://api.github.com/repos/google-research-datasets/Objectron/zipball/v0.1.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "lzhang57",
          "type": "User"
        },
        "date_created": "2021-02-19T22:49:44Z",
        "date_published": "2021-04-07T20:31:35Z",
        "description": "# Major Updates\r\n\r\n**Check our latest [newsletter](https://groups.google.com/g/objectron/c/quE06MrNJfY/m/dDmbBYdWAQAJ) for more details.**\r\n\r\n- Objectron [Paper](https://arxiv.org/abs/2012.09988) to appear in CVPR-2021. \r\n- Python and Web API for Objectron models (available via [Mediapipe](https://google.github.io/mediapipe/solutions/objectron#python-solution-api), [Python colab](https://mediapipe.page.link/objectron_py_colab)).\r\n  - Objectron models now offer a ready-to-use yet customizable Python solution as part of the prebuilt Mediapipe Python package.\r\n  - The Mediapipe Python package is available on [PyPI]( https://pypi.org/project/mediapipe/) for Linux, macOS and Windows.\r\n- Camera pose refinement via offline [bundle adjustment](https://en.wikipedia.org/wiki/Bundle_adjustment).\r\n  - We updated the camera pose with more accurate poses obtained from running offline global bundle adjustment on the video. \r\n  - The updated pose is very useful for applications that require more accurate and consistent camera poses, such as [NeRF](https://www.matthewtancik.com/nerf).\r\n- Community spotlight:  Objectron is available via [ActiveLoop hub](https://www.activeloop.ai/).\r\n\r\n\r\n\r\n",
        "html_url": "https://github.com/google-research-datasets/Objectron/releases/tag/v0.1.0",
        "name": "Objectron v0.1.0 - camera pose refinement + Python and Web API",
        "release_id": 40851826,
        "tag": "v0.1.0",
        "tarball_url": "https://api.github.com/repos/google-research-datasets/Objectron/tarball/v0.1.0",
        "type": "Release",
        "url": "https://api.github.com/repos/google-research-datasets/Objectron/releases/40851826",
        "value": "https://api.github.com/repos/google-research-datasets/Objectron/releases/40851826",
        "zipball_url": "https://api.github.com/repos/google-research-datasets/Objectron/zipball/v0.1.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-03 23:11:34",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 2227
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Tutorials",
        "type": "Text_excerpt",
        "value": "- [Downloading the dataset](https://github.com/google-research-datasets/Objectron/blob/master/notebooks/Download%20Data.ipynb)\n- [Hello, World example: Loading examples in Tensorflow](https://github.com/google-research-datasets/Objectron/blob/master/notebooks/Hello%20World.ipynb)\n- [Loading data in PyTorch](https://github.com/google-research-datasets/Objectron/blob/master/notebooks/Objectron_Pytorch_tutorial.ipynb)\n- [Parsing raw annotation files](https://github.com/google-research-datasets/Objectron/blob/master/notebooks/Parse%20Annotations.ipynb)\n- [Parsing the AR metadata](https://github.com/google-research-datasets/Objectron/blob/master/notebooks/objectron-geometry-tutorial.ipynb)\n- [Evaluating the model performance with 3D IoU](https://github.com/google-research-datasets/Objectron/blob/master/notebooks/3D_IOU.ipynb)\n- [SequenceExample tutorial](https://github.com/google-research-datasets/Objectron/blob/master/notebooks/SequenceExamples.ipynb)\n- [Training a NeRF model](https://github.com/google-research-datasets/Objectron/blob/master/notebooks/Objectron_NeRF_Tutorial.ipynb)\n"
      },
      "source": "https://raw.githubusercontent.com/google-research-datasets/Objectron/master/README.md",
      "technique": "header_analysis"
    }
  ]
}