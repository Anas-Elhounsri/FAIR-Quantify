{
  "application_domain": [
    {
      "confidence": 11.49,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_of_conduct": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "## Code of Conduct\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/CODE_OF_CONDUCT.md",
      "technique": "file_exploration"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/aws-samples/research-workshop-fits-datalake-code"
      },
      "technique": "GitHub_API"
    }
  ],
  "contributing_guidelines": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# Contributing Guidelines\n\nThank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional\ndocumentation, we greatly value feedback and contributions from our community.\n\nPlease read through this document before submitting any issues or pull requests to ensure we have all the necessary\ninformation to effectively respond to your bug report or contribution.\n\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n* A reproducible test case or series of steps\n* The version of our code being used\n* Any modifications you've made relevant to the bug\n* Anything unusual about your environment or deployment\n\n\n## Contributing via Pull Requests\nContributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:\n\n1. You are working against the latest source on the *main* branch.\n2. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.\n3. You open an issue to discuss any significant work - we would hate for your time to be wasted.\n\nTo send us a pull request, please:\n\n1. Fork the repository.\n2. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.\n3. Ensure local tests pass.\n4. Commit to your fork using clear commit messages.\n5. Send us a pull request, answering any default questions in the pull request interface.\n6. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.\n\nGitHub provides additional document on [forking a repository](https://help.github.com/articles/fork-a-repo/) and\n[creating a pull request](https://help.github.com/articles/creating-a-pull-request/).\n\n\n## Finding contributions to work on\nLooking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.\n\n\n## Code of Conduct\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n\n## Security issue notifications\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/CONTRIBUTING.md",
      "technique": "file_exploration"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-12-02T02:30:37Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2023-03-21T02:11:21Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Introduction",
        "parent_header": [
          "Welcome to FITS Data Lake project."
        ],
        "type": "Text_excerpt",
        "value": "Flexible Image Transport System (FITS) is the most commonly used file format in astronomy. Each FITS file contains one or more ASCII headers (metadata) and a binary data section as multi-dimensional array or tables. \nInformation about the data itself, such as instrument configuration, observation details, image origin, rize and coordinates, are stored in the headers as key value pairs. \n\nSelf-contained metadata format makes it possible for easy transport and sharing. It also provides challenges on the searchability and managibility of the data, especially when you are working on a large amount of data from different satallites or instruments. \n\nData lake is a centralized data repository, which has gain popularity in recently years as the size of the data grow exponentially in all industries. Unlike traditional data warehouse where high peformance relational databases ( can be very expensive at large scale ) are used to store data in a central location and with pre-determined schema, data lake takes advantage of existing storage and centralize data schema in a data catalog. For example, to build a data lake on AWS, you can put structured or unstructed data in Amazon Simple Storage Service (Amazon S3) with different schema, automate an ETL crawler to aggregate the schemas into a fully managed data catalog. Then you can run sql querie against the data catalog and process your data. \n\nData lake has been used mostly in data analytics so far. In this solution, however, we will apply the concept and practice of building a serverless data lake in AWS cloud for FITS data. This approach can be applied to most of the scientific data, which containes metadata within or with external attachment of metadata files. \n\n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9925045196964897,
      "result": {
        "original_header": "About CDK",
        "type": "Text_excerpt",
        "value": "This project is built with AWS Cloud Development Kit (CDK) using Python. AWS CDK is an open source software development framework you can use to build and deploy your cloud infrastrures and application resources using your favorite programming language (as an alternative to using AWS CloudFormation). For more detailed information about AWS CDK, please visit https://aws.amazon.com/cdk/.  \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9857920764193454,
      "result": {
        "original_header": "Step 1. Create a virtual environment",
        "type": "Text_excerpt",
        "value": "This project is set up like a standard Python project. To create the virtualenv it assumes that there is a `python3` (or `python` for Windows) executable in your path with access to the `venv`\npackage.  \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9946923664533726,
      "result": {
        "original_header": "Step 2. Modify project parameters in app.py",
        "type": "Text_excerpt",
        "value": "In app.py, there are three parameters you can customize: \n```\n##### Begin customization\n# This is the name of the bucket where your source FITS files are stored or will be stored. The bucket must exist already and must be unique. Must be changed\nsource_bucket_name = \"<changeme>\"\n\n# This is the name of the database for the data catalog, you can leave it as is \nglue_database_name = \"fits_datalake\"\n\n# stack id, you can leave as is\nstack_id = \"my-fits-datalake\"\n###### End customization\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8896887809519296,
      "result": {
        "original_header": "Step 3. Synthesize and deploy the project",
        "type": "Text_excerpt",
        "value": "At this point you can now synthesize the CloudFormation template for this code. The result templates can be viewed in cdk.out (auto generated) folder\n```\n$ cdk synth     # this will synthesize the CloudFormation templates\n$ cdk deploy    # this will deploy the tack via CloudFormation\n```\nMain resources created will be ( not including IAM Roles, policies, lambda layers, permissions etc),\n \nSubsequent execution of `cdk deploy` will create a CloudFormation stack changeset and only update the existing stack. \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8377556869347067,
      "result": {
        "original_header": "Step 4. Test your deployment",
        "type": "Text_excerpt",
        "value": "Thene copy the content of the folder into your existing S3 bucket (replace `<source_bucket_name>` with your bucket name): \nCreate and/or updateing files in the <source_bucket_name> bucket will trigger the header extraction lambda function to create header csv files in the <target_bucket_name>.  \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9436946065731072,
      "result": {
        "original_header": "Architecture",
        "type": "Text_excerpt",
        "value": "- S3PutObject/S3RemoveObject Event triggers the header extractor lambda function <header_extractor_lambda>\n- FITS header is extracted and saved into a csv file to the destination bucket <target_bucket_name>\n- AWS Glue crawler <glue_crawler> updates the database tables in AWS Glue data catalog <glue_database> \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/aws-samples/research-workshop-fits-datalake-code/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/notebooks/fits_datalake_notebook.ipynb"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/notebooks/fits_datalake_notebook.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 3
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/aws-samples/research-workshop-fits-datalake-code/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "aws-samples/research-workshop-fits-datalake-code"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Welcome to FITS Data Lake project."
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/bootstrap/bootstrap-cdk.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/images/science_datalake.png"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9999999999951683,
      "result": {
        "original_header": "About CDK",
        "type": "Text_excerpt",
        "value": "Assume you have installed cdk and aws cli on the computer you are working on. For instructions on how to install AWS CDK and AWS cli, please visit https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html and  https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html\n \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9999999999831743,
      "result": {
        "original_header": "Project Structure",
        "type": "Text_excerpt",
        "value": "<pre>\nproject_folder/\n|-- fits_data/                      # sample FITS files\n    |-- REAEME.md                   # instruction on how to download the test data from public sources\n|-- images/\n|-- my_fits_datalake/\n    |-- my_fits_datalake_stack.py   # the FitsDatalakeStack code\n!-- notebooks/                      # test jupyter notebook\n|-- resources/\n    |-- fits_header_extractor       # lambda code for extracting FITS header info\n|-- app.py                          # CDK app code\n|-- cdk.json                        # CDK config\n|-- test/\n|-- README.md\n|-- requirements.txt\n|-- setup.py\n|-- source.bat \n</pre>\n \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9999714441059895,
      "result": {
        "original_header": "Step 1. Create a virtual environment",
        "type": "Text_excerpt",
        "value": "This project is set up like a standard Python project. To create the virtualenv it assumes that there is a `python3` (or `python` for Windows) executable in your path with access to the `venv`\npackage.  \nTo create a virtualenv on MacOS and Linux:\n```\n$ python3 -m venv .env\n```\nAfter the init process completes and the virtualenv is created, you can use the following\nstep to activate your virtualenv.\n```\n$ source .env/bin/activate\n```\nIf you are a Windows platform, you would activate the virtualenv like this:\n```\n% .env\\Scripts\\activate.bat\n```\nOnce the virtualenv is activated, you can install the required dependencies.\n```\n$ pip3 install -r requirements.txt\n```\nTo add additional dependencies, for example other CDK libraries, just add\nthem to your `setup.py` file and rerun the `pip3 install -r requirements.txt`\ncommand.\n \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9969248738825003,
      "result": {
        "original_header": "Step 2. Modify project parameters in app.py",
        "type": "Text_excerpt",
        "value": "The `cdk.json` file tells the CDK Toolkit how to execute your app when you run `cdk deploy`. \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9977873255140839,
      "result": {
        "original_header": "Step 4. Test your deployment",
        "type": "Text_excerpt",
        "value": "If you want to use some public FITS data, navigate to the 'fits_data\" folder, run the following commands to download the data\n```bash\nwget https://www.spacetelescope.org/static/projects/fits_liberator/datasets/eagle/502nmos.zip\nunzip -d hubble_samples 502nmos.zip\nrm 502nmos.zip\nwget -P fits_samples https://fits.gsfc.nasa.gov/samples/WFPC2u5780205r_c0fx.fits\nwget -P fits_samples https://fits.gsfc.nasa.gov/samples/FOCx38i0101t_c0f.fits\nwget -P fits_samples https://fits.gsfc.nasa.gov/samples/FOSy19g0309t_c2f.fits\nwget -P fits_samples https://fits.gsfc.nasa.gov/samples/HRSz0yd020fm_c2f.fits\nwget -P fits_samples https://fits.gsfc.nasa.gov/samples/WFPC2ASSNu5780205bx.fits\nwget -P tutorials http://data.astropy.org/tutorials/FITS-images/HorseHead.fits\ncd ..\n```\n \nThene copy the content of the folder into your existing S3 bucket (replace `<source_bucket_name>` with your bucket name): \nIf you already have FITS files in your <source_bucket_name> bucket, run the following aws cli command\n```\naws s3 cp --recursive --metadata {\\\"touched\\\":\\\"true\\\"} s3://<source_bucket_name>/ s3://<source_bucket_name>/\n```\nThis will \"touch\" all the existing FITS files in the source bucket. \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.999996638128574,
      "result": {
        "original_header": "How to Create a layer for the lambda function",
        "type": "Text_excerpt",
        "value": "```\n$ mkdir resources_layer\n$ mkdir python\n$ pip3 install aws_cdk.aws_lambda aws_cdk.aws_s3\n$ pip3 install astropy --target python\n$ zip -r9 resources_layer/astropy.zip python\n```\nuse resources_layer/astropy.zip to create a new layer\n```\n        layer_astropy = lambda_.LayerVersion(self, 'AstroFitsioLayer', \n            code=lambda_.Code.from_asset(\"resources_layer/astropy.zip\"),\n            compatible_runtimes=[lambda_.Runtime.PYTHON_3_7]\n        )\n       # use an AWS provided layer for numpy\n        layer_numpy = lambda_.LayerVersion.from_layer_version_arn(self, \"NumpyLayer\", \"arn:aws:lambda:us-east-1:668099181075:layer:AWSLambda-Python37-SciPy1x:22\")\n```\nAstropy depends on Numpy package, furtunately AWS has a public layer available (arn:aws:lambda:us-east-1:668099181075:layer:AWSLambda-Python37-SciPy1x:22)\n \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.9365771531397169,
      "result": {
        "original_header": "Project Structure",
        "type": "Text_excerpt",
        "value": "<pre>\nproject_folder/\n|-- fits_data/                      # sample FITS files\n    |-- REAEME.md                   # instruction on how to download the test data from public sources\n|-- images/\n|-- my_fits_datalake/\n    |-- my_fits_datalake_stack.py   # the FitsDatalakeStack code\n!-- notebooks/                      # test jupyter notebook\n|-- resources/\n    |-- fits_header_extractor       # lambda code for extracting FITS header info\n|-- app.py                          # CDK app code\n|-- cdk.json                        # CDK config\n|-- test/\n|-- README.md\n|-- requirements.txt\n|-- setup.py\n|-- source.bat \n</pre>\n \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8622013253655825,
      "result": {
        "original_header": "Step 3. Synthesize and deploy the project",
        "type": "Text_excerpt",
        "value": "| Resource Type | Name | Alias | \n|---------------|------|-------|\n| S3::Bucket | <stack_id>-fitsstorebucket<guid> | <target_bucket_name> |  \n| Lambda::Function | <stack_id>-FITSHeaderExtractorHandler\\<guid\\> | <header_extractor_lambda> |\n| Glue::Database | fits_datalake | <glue_database> | <glue_database>\n| Glue::Crawler | fitsdatalakecrawler-\\<guid\\> | <glue_crawler> | \n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/aws-samples/research-workshop-fits-datalake-code/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT No Attribution",
        "spdx_id": "MIT-0",
        "type": "License",
        "url": "https://api.github.com/licenses/mit-0",
        "value": "https://api.github.com/licenses/mit-0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this\nsoftware and associated documentation files (the \"Software\"), to deal in the Software\nwithout restriction, including without limitation the rights to use, copy, modify,\nmerge, publish, distribute, sublicense, and/or sell copies of the Software, and to\npermit persons to whom the Software is furnished to do so.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\nINCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A\nPARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\nHOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\nOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/LICENSE",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License",
        "parent_header": [
          "Welcome to FITS Data Lake project."
        ],
        "type": "Text_excerpt",
        "value": "This library is licensed under the MIT-0 License. See the LICENSE file.\n"
      },
      "source": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "research-workshop-fits-datalake-code"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "aws-samples"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 14097,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 2747,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Batchfile",
        "size": 434,
        "type": "Programming_language",
        "value": "Batchfile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 428,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/aws-samples/research-workshop-fits-datalake-code/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-03 23:43:37",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 5
      },
      "technique": "GitHub_API"
    }
  ]
}