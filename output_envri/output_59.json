{
  "application_domain": [
    {
      "confidence": 100.7,
      "result": {
        "type": "String",
        "value": "Natural Language Processing"
      },
      "technique": "supervised_classification"
    },
    {
      "confidence": 8.42,
      "result": {
        "type": "String",
        "value": "Audio"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/hanhanwu/Hanhan_NLP"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2016-06-29T06:32:36Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-02-15T02:58:35Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "NLP research and implementation"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9474753141629644,
      "result": {
        "original_header": "Hanhan_NLP",
        "type": "Text_excerpt",
        "value": "NLP research and implementation\n \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9981817946239533,
      "result": {
        "original_header": "Somethig About NLP",
        "type": "Text_excerpt",
        "value": "* Basic TOP k NLP Operations\n  * Stemming\n    * Shorten the words. \n    * Such as \"beautiful\" and \"beautifully\" are stemmed to \"beauti\"\n  * Lemmatisation\n    * Reduce words into their lemma/dictionary form. \n    * Such as \"good\", \"better\" and \"best\" are lemmatised to \"good\"\n  * Words Inflection\n    * Looks like the opposite operation of Lemmatisation/Stemming\n    * It adds the characters to a word base format, so that we get more formats, such as singular or plural\n  * Words Embedding\n    * A word or a phase is represented in a fixed dimensional vector of length X\n  * Part-of-Speech Tagging\n    * Tag word type (such as Noun, Verb, Adj. etc.) fo each word in the text\n  * Named Entity Disambiguation\n \u00a0 \u00a0* Try to link the entity in the text with knowledge\n    * such as, \"Apple has best phone\", machine needs to figure out \"Apple\" is a company or fruit or something else\n  * Named Entity Recognition\n    * Give the entity a category\n    * Such as Location, Person, Date, Organization\n  * Sentiment Analysis\n  * Semantic Text Similarity\n    * Semantic Text Similarity is the process of analysing similarity between two pieces of text with respect to the <b>meaning and essence</b> of the text rather than analysing the syntax of the two pieces of text\n  * Text Summarisation\n    * Identify important points to form a summary\n    * The goal of Text Summarisation is to retain maximum information along with maximum shortening of text without altering the meaning of the text.\n  * refernece: https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n    * It has Python code for implementation of each task\n    * Also has relavant papers, application\n  * My code: https://github.com/hanhanwu/Basic_But_Useful/blob/master/NLP_basic_preprocessing.ipynb\n    * It has more\n    * Major libraries you can try for NLP tasks\n      * nltk - very basic\n      * Spacy\n      * Standard core NLP\n      * gensim - topic modeling\n  * My code2: https://github.com/hanhanwu/Hanhan_NLP/blob/master/NLP_textblob.ipynb\n    * It is using an NLP library - TextBlob, which is build upon nltk\n    * TextBlob makes calling nltk functions easier, but the functions are very basic. Spacy is fasteer and may provides more NLP data manipulation functions; classifiers are also basic\n    * However, <b>I like the Language Translation in TextBlob</b>. It's built upon Google Translate, you don't need to download any language package, and it can do the translation, looks not bad\n    * Sentiment Analysis tells not only positive & negative, but also trying to tell how subjective a statement is. \n      * Language codes supported: https://cloud.google.com/translate/docs/languages\n    * TextBlob document: http://textblob.readthedocs.io/en/dev/\n    * reference: https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29 \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9602650614540323,
      "result": {
        "original_header": "PRACTICE",
        "type": "Text_excerpt",
        "value": "* When dealing with special characters in text, ASCII and Binary Characteristics: http://roubaixinteractive.com/PlayGround/Binary_Conversion/The_Characters.asp \n* Keywords Search for Short Context\n  * When the text context is very short, and you cannot build the search engine by simply calculating query terms distance, query terms frequency or query terms position. \n  * In the code here, I am trying different methods to allow better key words search for short context. https://github.com/hanhanwu/Hanhan_NLP/blob/master/short_context_search.py\n  * In the code, I have implemented 5 methods to do the search. <b>METHOD 1</b> - calculate the score based on the number of search query tokens in the short context; <b>METHOD 2</b> - calculate the score based on how similar the words in short context to the query tokens; <b>METHOD 3</b> - calculate the score based on query tokens order appeard in the short context; <b>METHOD 4</b> - calculate the score based on the query token totoal min physical distances in the short context; <b>METHOD 5</b> - Adjust the score basde on which output users have chosen. Among these 5 mtthonds, method 1 to 4 can be used together by setting different weights, method 5 can be used in real time learning to make the output more intelligent based on all the users choices (using 1 hidden layer neural network)\n  * By rescaling the score in this method, we will be able to use method 1, 2, 3, 4 together, but don't rescale method 4, because using method 4 has to order method 1 results first, which means method 4 output is cannot simply be normalized with max and min. Just use the unnormalized method 4 results with normalized others is fine here. https://github.com/hanhanwu/Hanhan_NLP/blob/master/rescale_score.py\n  * NOTE: If the user input contains plural, stemming the query tokens is better\n  * NOTE: However, in many cases, the semmed tokens cannot be found in the text with python, for example \"beauty\" will be stemmed into \"beauti\", in these cases, no stemming returns better result. This is especially important in method 4, since it will be ordered by the number of query token existence first, then will be orered by the token distance \n* Text Feature Selection Example\n  * Python code: text_feature_selection_example.py\n  * This is just an example to do feature engineering on text data, which means, besides the individual words we could use as features, we could also generate other features based on the text to help later data analysis work. \n* Login Spider\n  * Crawl web pages that need login: https://github.com/hanhanwu/Hanhan_NLP/blob/master/basic_authentication_spider.py\n  * When running the code with Scrapy, use terminal command. cd to the top folder of your project, then type `scrapy runspider [spider file path]/[spider file name].py`\n  * NOTE: this method does not work for all the web pages that needs login...\n \n* Scikit_Learn NLP Go Through\n  * Text Matching - Distance Matching, Phonetic Matching, Flexible String Matching, Cosine Similarity \n  * In Distance Matching, besides Levenshtein Distance, there are other distance methods: [Jaro Distance][13], [Hammig Distance][14]\n  * For Phonetic Matching, python Fuzzy contains Soundex, DMetaphone and nysiis. It seems that when it comes to similar pronuncation but different spelling, DMetaphone works better. But if you care about both spelling and pronouncation differences, Soundex and nysiis maybe better\n  * Flexible String Matching, I will try regular expression, lemmatized matching (StanfordNLP), compact matching (python fuzzyfinder)\n  * Cosine Similarity, when the text is represented as vector notation, a general cosine similarity can also be applied in order to measure vectorized similarity. <b>It seems that this method works very well when the words in 2 strings are not in the same order</b>, if we use Levenshtein Distance, it cares about words order and will output larger distance.\n  * <b>Coreference Resolution</b> is a process of finding relational links among the words (or phrases) within the sentences. it automatically figures out what does \"he\", \"it\" mean in the sentences. [Standford NLP Coreference Resolution][15], [Standford NLP Python][16] \n  * [Reference][17]\n  * [my code][18]\n \n* R Web Mining\n  * Learning Resource: https://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n  * My R code: https://github.com/hanhanwu/Hanhan_NLP/blob/master/R_web_parsing.R\n  * The way I parse the data is totally different from the methods used in learning resources. In the learning resource, they are using class name, and when there are multiple web elements with the same class names, their method may not be able to find the content in order. In my code, I have found that using `XPATH` is much better for this case, and with `Firebug`, you can simply right click to copy the `XPATH` of your web element, write a function to get the content in order \n \n*********************************************************************************************\n<b>RESAERCH AND DEVELOPMENT</b> \n* How to improve search accuracy with Google Search\n  * Simple video explains how google search works: https://www.youtube.com/watch?v=KyCYyoGusqs\n  * Operators used in google search query, looks helpful: https://support.google.com/websearch/answer/2466433?visit_id=1-636146162684675642-3633116773&rd=1\n  \n* <b>Word Embedding Methods</b>\n  * Reference: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n  * Frequency based Embedding\n    * Count Vectors\n      * Imagine you have a table, each column is a unique word, each row is a document\n      * In a corpus, unique words are top couted words, otherwise that's too much\n      * For the same document, the word count for each word - Document Vector\n      * For the same word, its count in each document - Word Vector\n    * TF-IDF\n      * It checks not only how many times a word appears in one document, but also checks the time it appears in the whole corpus. And it will penalize those common words which appeared in large number of documents, in this way, it is more likely to find real important words for a subset of documents\n      * `TF = the number of time word t appears in 1 document`\n      * `IDF = log(N/n)`, N is the number of documents, n is the number of documents that t appeared. In this way, when n is large, IDF tend to get closer to 0, and therefore those stop words will get low score (`TF * IDF`)\n      * Higher score, more important the word to the document\n    * Co-Occurrence Matrix & Context Window\n      * With context window, it defines the number of words together, and even a direction. For example, 2 words together\n      * Co-occurance will be the count of word tuples, the number of words ina tuple is defined by context window. For example, count the co-occurance of (ice, cream)\n      * Co-occurrence matrix can be huge even remove stop words. It uses algorithms such as PCA to decompose.\n        * If you perform PCA on a V*V matrix, you get V principle components. Choose k components out of V, you get a new matrix of V*k\n        * It preserves semantic relationships between words. For example \"ice\" and \"cream\" is closer than \"ice\" and \"human\"\n        * You just compute it once and can use multiple times, so it's fast\n  * Prediction based Embedding (methods here need you manually create lables)\n    * Word2Vector = CBOW (Continuous bag of Words) + Skip-Gram\n      * CBOW: \n        * Probabilistic, better than deterministic. \n        * Lower memory requirement\n        * It's neural network and the hidden layer taks advantage of the context\n      * Embedding in ANN: https://www.analyticsvidhya.com/blog/2018/04/introductory-guide-understand-how-anns-conceptualize-new-ideas/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n        * In this article, it mentioned word2vector can be expensive for the softmax function at the end, since the output size equals to the vocabulary size. So better to use pretrained word2vec matrix\n        * Therefore, there is another method called <b>Negative Sampling</b>, which converts the problem into binary problem and made softmax calculation efficient\n    * Skip-Gram\n      * It's also neural network, it predicts context given a word\n    * A visualization tool: https://docs.google.com/document/d/1qUH1LvNcp5msoh2FEwTQAUX8KfMq2faGpNv4s4WXhgg/pub\n  * My code: https://github.com/hanhanwu/Hanhan_NLP/blob/master/try_words_embedding.ipynb\n    * word vector pre-trained model from Google\n    * train your own model by providing sentences\n    * with words embedding, you can search for similar tokens, tokens that always appear together, or tokens that has lower probability to appear with other tokens \n* Google search API sets limitations per day, only could get 32 search reuslts each day\n* <b>xgoogle</b>, it has a pretty good tutorial, but google has blocked it, since Google doesn't allow automated search... http://www.catonmat.net/blog/python-library-for-google-search/\n* <b>pygoogle</b> is no longer available either, it recommends to use Google Custom Search\n* <b>YQL</b>\n  * Yahoo YQL Guide: https://developer.yahoo.com/yql/guide/usage_info_limits.html\n  * Yahoo Dev Center, to get the key and secret for YQL boss.search: https://developer.yahoo.com/apps/\n  * yql open tables: https://github.com/hanhanwu/yql-tables\n  * yql health checker, to check the current situations of yql open tables (I really like this feature, when you are clicking the tables here, it leads to YQL Console to allow you run the test query immediately): https://www.datatables.org/healthchecker/\n  * <b>Example</b>, yql with Bing Web Search: https://developer.yahoo.com/yql/console/?env=store://datatables.org/alltableswithkeys&q=SELECT+*+FROM+microsoft.bing.web+WHERE+query%3D%27stackoverflow%27#h=SELECT+*+FROM+microsoft.bing.web+WHERE+query%3D'stackoverflow'\n  * But, yql always returns empty results for my queries....\n* <b>Bing</b>\n  * Bing APIs: https://www.microsoft.com/cognitive-services/en-us/bing-web-search-api/documentation\n  * Bing Search API Guide: https://msdn.microsoft.com/en-us/library/dn760781.aspx\n  * In order to use Bing Search API, need to get the subscription key here: https://www.microsoft.com/cognitive-services/en-us/subscriptions\n  * Bing Search API Parameters (cannot compete with Google...): https://dev.cognitive.microsoft.com/docs/services/56b43eeccf5ff8098cef3807/operations/56b4447dcf5ff8098cef380d\n  * Bing API Test Console: https://dev.cognitive.microsoft.com/docs/services/56b43eeccf5ff8098cef3807/operations/56b4447dcf5ff8098cef380d/console\n  * Python Bing Search (it seems that the api key here should be got form Azure Market Place....): https://github.com/hanhanwu/py-bing-search\n  * When I was using Python Bing Search, it was showing the authorization method is not valid, I guess this maybe the reason:\n https://datamarket.azure.com/dataset/bing/search\n  * It changes v2 to v5, and in order to get all those info, I need to purchase in Azure Market Place: https://msdn.microsoft.com/en-US/library/mt707570.aspx\n  * Bing Web Search API Pricing: https://www.microsoft.com/cognitive-services/en-us/bing-web-search-api\n* <b>Google Custom Search Engine (CSE)</b>\n  * It's super easy and super cool! If you want to custom your own search engine, on your website\n  * The tutorial provides all the useful and educational urls: https://developers.google.com/custom-search/docs/topical\n  * The control panel is easy and fast to use, you can generate customized search engine within seconds, and google will create a web version as well as html code and tell you how to copy the code to html page: https://cse.google.com/cse/all\n  * My CSE test html code, I just opened JSBin console, opened a HTML template and copy the generate code between <body></body>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/google_cse_sample.html\n  * If you need the search results obey a certain url pattern, check google search query operator, very helpful!: https://support.google.com/websearch/answer/2466433?visit_id=1-636146162684675642-3633116773&rd=1\n  * Now I want to get the search results from pyhton code, it also simple. This is the detail tutorials about where to get your cx (Custom Search Engine id), key (API Key): https://developers.google.com/custom-search/json-api/v1/reference/cse/list\n  * In order to get cx, you need to visit this website: https://cse.google.com/cse/all\n  * Create a Custom Search Engine, it's very fast! Once you have created one, you will find cx=.... through the url in your browser\n  * In order to get Google API Key, you need to go to Google Dev Center: https://console.developers.google.com/apis/dashboard\n  * In Google Dev Center, first click Dashboard, then Click 'ENABLE API' and enable 'Custom Search API'. Next, click 'Credentials', and click 'Create Credential' to create an API Key\n  * Pricing if you want to have more search requests: https://developers.google.com/custom-search/json-api/v1/overview#key\n  * About the pricing, one thing is a little weird, if you are using $5 per 1000 query based on the free version, it will prevent you from calling more queries each day when you haven't reached to 10k queirs. Maybe this is because I'm not the one who paid the price and cannot see the settings...\n  * My sample code about how to get the results through python, with FREE version: https://github.com/hanhanwu/Hanhan_NLP/blob/master/google_cse_sample_python_call.py\n  * Google CSE is great, cheaper than many other web search apis, even cheaper than Google Search API, and it's well documented. One thing need to note that google CSE didn't mention is their limitations per second. Bing Search API has 5 query limitation per second. I guess Google CSE also has limitation, therefore only using for loop to execute multiple queries will get errors. You can simply add `time.sleep(1)` in your for loop. Check my code: https://github.com/hanhanwu/Hanhan_NLP/blob/master/multiple_query_google_cse.py\n  \n* <b>Gigya Javascript Web Parser</b>\n  * One year ago, Amazon Camel-Camel-Camel suddenly changed its website source code and everything parsable has become Javascript, therefore I lost many data, and thought Javascript written website may not be parsable. Today, I just tried the Gigya API (the researchers I'm working with may have paid this API...), it will return all the content hidden behind the Javascript code!\n  * How does this work: http://developers.gigya.com/display/GD/Developer%27s+Guide\n  * Parameters: http://developers.gigya.com/display/GD/comments.getComments+JS\n  * One thing about the parameters, some website has hundreds or even thoughds of comments, but you need to click a button such as \"More Comments\" to see more comments, and the url has never changed. When you are using Gigya API, gave to set parameter \"threadLimit\", otherwise it cannot return all the comments.\n* <b>DIY JavaScript Parser - with selenium</b>\n  * In above you will find Gigya works well to get the content from JS written website, however it doesn't work all the time. Recently, I just met a problem, that when Globe and Mail is updating their website, all the old comments have been hidden from the view, only new comments posted after Nov. 28, 2016 will appear with added new features. However, by using Gigya, I could get all the old comments, but no new comments, no new feature in the output data as well. After 2 days experiments, finally I somewhat realized why this happened, and at least I have found the basic solution to get the content data of this JS written website.\n  * <b>My code 1</b>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/DIY_JSParser.py\n  * JS written website is not impossible for parsing without other API, we cannot see content from the source code, but we should be able to get the HTML content from the JS page.\n  * <b> First of all</b>, install <b>Firebug!</b>(the name is really cool!) on your Firefox, it's an Addon. With Firebug, when you open the url in Firefox, click `HTML` in Firebug, you will see all the content. With Firebug search function, it's easy to locate and find the tags of the content you want to parse. Then, write code similar to mine, we can get the content.\n  * <b>Then</b>, as you can see, in my code, I am using selenium library, it's super cool. It can get HTML page from JS written website, add, with `click()` method, you can click the chosen element. This is very imortant is because some elements such as reactions counts in my sample url will not appear until you click each reaction image.\n  * selenium elements locating: http://selenium-python.readthedocs.io/locating-elements.html\n  * NOTE: selenium has get_element_* and get_elements_*, pay attention to this detail.\n  * <b>My code 2</b>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/DIY_JS_Parser2.py\n  * In my part 2 code, I was dealing with multiple clickable elements that have the same class name. When you clicked one, without closing it or scroll down, you cannot click another one. In this code, the code closed each popup, before clicking the next one.\n  * Also in my code 2, I'm using this method `driver.execute_script(\"arguments[0].scrollIntoView();\", clk)`, each time you need to interact with the next element, it will move to that position first. Otherwise you will get an error for not be able to see that (x,y) point, even if you maximize the browser.\n  * <b>Getting parents web element</b>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/DIY_JS_Parser_get_parents.py\n  * <b>My code 3</b>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/DIY_JS_Parser3.py\n  * My code 3 has finished a complex process, that is to extract hierarchical data and recover them back to the original hierarchial structure. If you copy the url to firefox and have firebug on it, you will find for each main comment author, the class name is \"c29cjTJ\", the replied author of each main comment has class name \"c29cjTJ c3fk6Wf\". In fact \"c29cjTJ c3fk6Wf\" is a compact class, you cannot find the element through this class name, but you can find the element through `driver.find_element_by_css_selector(\".c29cjTJ.c3fk6Wf\")`. However, there is a even simpler way, that is what I wrote in my code 3, you use `driver.find_element_by_class_name(\"c29cjTJ\")`, it will include all the authors for main comment and replies. Later, just try to trace back to find the root class id to decide which reply belongs to which main comment.\n  * NOTE: In some machines, it may require `geckodriver`, and sometimes, adding its path into $PATH doesn't work, so in your code, instead of using `driver = webdriver.Firefox()`, you should use `driver = webdriver.Firefox(executable_path='[your path]/geckodriver')`\n    * To deal with more errors related to `geckodriver`, check https://github.com/hanhanwu/Hanhan_Break_the_Limits/blob/master/Ideas_Spark/track_siliconvalley_vancouver_trend.ipynb\n  * <b>My code 4</b>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/DIY_JS_Parser4.py\n  * I thought my code 3 was good, no it's not. When I found there was a \"Show More Comments\" button at the bottom, and tried to expand all the comments, I have found that the expanded comments had no HTML location id, which was the ids I used to map comment authors, post time text, reactions and replies together. I tried some methods, hoping to adding something on my code 3 to solve the problem, but only made things more complex. Maybe, sometimes when the problem becomes more complex, the solution will in fact become easier. I redisigned the code, and just found that, instead of using driver to get all the web elements, I could use a web elements to locate its children, which makes my mapping much easier. Although the website have some inconsistent display, I put many \"try... except\" in the code, at least for nnow, there is no error. Here are the test urls: https://github.com/hanhanwu/Hanhan_NLP/blob/master/test_urls.txt\n  * Note: in my code 4, I am using `driver.quit()`, instead of `driver.close()`, this is because in some machine, `driver.close()` does not work, this may be a bug in selenium, here is the discussion: https://github.com/seleniumhq/selenium/issues/654 \n* Sample url \"http://news.nationalpost.com/full-comment/marni-soupcoff-bob-dylans-nobel-silence-is-golden\"\n* I need to get the comments and I don't want to use its HTML data, looks simple, but no. Because this website embed everything in WordPress, however, traditional APIs cannot get the right comments data.\n* <b>Gigya</b> got empty comments\n* <b>WordPress API</b>: https://developer.wordpress.com/docs/api/\n  * WordPress API console: https://developer.wordpress.com/docs/api/console/\n  * But for each NationalPost article, both WordPress post and comment all point to the article, and the `$site` has to be the domain. `https://public-api.wordpress.com/rest/v1/sites/news.nationalpost.com/comments/307452/`, `https://public-api.wordpress.com/rest/v1/sites/news.nationalpost.com/posts/1282447/replies`, others could not help either.\n* <b>PostMedia open source</b> is not useful either: https://github.com/Postmedia\n* <b>Python Twill</b>: Python Twill does not work well for JavaScript. http://twill.idyll.org/,  http://twill.idyll.org/python-api.html\n* <b>Zombie</b>: I tried Zombie first because it's browserless, which means when it is parsing the data, there is no browser popup. First of all, its Python API does not work for external url not built by me. So I had to try Zombie.js, it finally only returned me those only in web source code, still cannot get comments data. But a good learning experience.\n  * Zombie CoffeeScript Stackoverflow reference: http://stackoverflow.com/questions/11628636/i-cant-get-the-whole-source-code-of-an-html-page\n  * In order to use Zombie.js, I have checked: how to use node.js: http://www.dannemanne.com/posts/tutorial_get_started_with_node_js_on_mac_beginner_\n  * In order to use Zombie.js, I have checked: coffeescript - TRY COFFEESCRIPT: http://coffeescript.org/\n  * In order to use Zombie.js, I have checked: zombie.js: http://zombie.js.org/\n  * In order to use Zombie.js, I have checked: zombie functions: http://zombie.readthedocs.io/en/latest/pythonzombie.html\n  * In order to use Zombie.js, I have checked: CSS selector string: http://www.w3schools.com/jsref/met_document_queryselector.asp\n* <b>Mechanize</b>: It is built on python urllib2, therefore it cannot get the comments data which is not showing in web source code... But it is very convenient to get article title. http://wwwsearch.sourceforge.net/mechanize/\n* <b>Selenium</b>: Finally I got the comments by using selenium and firebug. They were my last choice because of the popup browsers, but, I have to admit, they are good to use.\n  * My code: https://github.com/hanhanwu/Hanhan_NLP/blob/master/get_national_post_comments.py\n  * In my code, I have learned that, when using XPath, `//iframe[@class='fb_ltr']` means this element ins directly under the root, if not, use `.//iframe[@class='fb_ltr']`\n \n \n<b>Python Scrapy</b>\n \n* Scrapy Web Crawler\n  * Scrapy General: https://github.com/scrapy/scrapy\n  * Scrapy documents: https://doc.scrapy.org/en/latest/\n  * Pyhton yield: http://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n* Scrapy Pipeline\n  * The Scrapy Pipeline aims to let you execute a series activities through a pipeline of process\n  * If you check Scrapy Pipeline document here, it's difficult to really get thing done: https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n  * Here is my detailed readme about what did I try and worked: https://github.com/hanhanwu/Hanhan_NLP/blob/master/HanhanScrapt/Hanhan_Scrapy_Pipeline_ReadMe.md\n  * All my Scrapy Pipeline code: https://github.com/hanhanwu/Hanhan_NLP/tree/master/HanhanScrapt\n* Other Scrapy Resources\n  * Web Scrapign: https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n    * Extract web elements such as images, text, votes, etc. with Scrapy Spider\n    * Based on my experience, Scrapy only works well when the webiste does not use any bot to prevent customized spiders (even if you change Scrapy bot settings). Python selenium often works even when there is bot.\n  \n<b>Entity Recognizer</b>\n \n* Stanford Named Entity Recognizer (NER)\n  * NER is able to regognize names of different things/people in English words\n  * Download NER package: http://nlp.stanford.edu/software/CRF-NER.shtml#Download\n  * My code to extract people names with NER: https://github.com/hanhanwu/Hanhan_NLP/blob/master/extract_people_names.py \n* It helps find relevant information/documents that can satisfiy readers requirements\n* You can use this method in:\n  * Search Engine\n  * Content Based Image Retrieval\n  * Recommendation System\n* KNN vs KD Tree\n  * KNN takes O(N) time for N data points; KNN with K Neighbour search takes O(log(K)*N). In both cases, when there are large amount of data points, KNN can be time consuming\n  * KD Tree is an improvement on KNN, it uses both decision tree and KNN to  calculate nearest neighbours\n    * Similar to binary search tree, you build a decision tree by spliting training data based on certain conditions. Now when there is a testing data, it travels from root to the node its condition satisfied with.\n    * But locating the node for your testing data doesn't mean data points in that node are the nearest neighbour. You need to check sibling nodes by tracing back to 1 level parent root and gets to sibling nodes each time, compare the nearest distance with current nearest data point\n    * This method can help you get rid of subtress and improve the time efficiency\n    * Also KD Tree allows you to split and organize data based on certain conditions\n    * Average time complexity O(log(N)), worst case O(N) when the data structure and conditions make the tress like a list\n    * KDTree may not work well in high dimensions (lots of features). When there are many features, you still need to check many partitions in the tree, won't be very efficient. I would do dimensional reduction first and and try KDTree, and compare with other algorithms\n  * My code: https://github.com/hanhanwu/Hanhan_NLP/blob/master/KDTree_Information_Retrieval.ipynb\n    * In this code, I'm using KDTree to retrieve text close to 'Emmanuel'! It works well!\n  * reference: https://www.analyticsvidhya.com/blog/2017/11/information-retrieval-using-kdtree/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n* Distance Methods used in KNN\n  * The paper: https://github.com/hanhanwu/readings/blob/master/knn_distance_algs.pdf\n  * There are definitions of 11 distance methods\n  * They have compared 8 datasets with 11 distance methods and found:\n    * Hamming and Jaccard perform worst since these 2 methods will be affected by the ratio of the members of each class, while other methods won't be affected\n    * The top 6 distance methods a they tried in KNN are: City-block, Chebychev, Euclidean, Mahalanobis, Minkowski, Standardized Enuclidean techniques \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9984616133502439,
      "result": {
        "original_header": "R Sentiment Analysis",
        "type": "Text_excerpt",
        "value": "* Reference - Sentiment analysis on movie and tweet: https://www.analyticsvidhya.com/blog/2017/03/measuring-audience-sentiments-about-movies-using-twitter-and-text-analytics/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n  * This tutorial is worthy to try is because of the `NRC sentiment dictionary` they use. In R, if you use `library syuzhet` you can use the sentiment dictionary. It has multiple sentiment levels: <b>\"Positive\",\"Anger\",\"Anticipation\",\"Disgust\",\"Fear\",\"Joy\",\"Sadness\",\"Surprise\",\"Trust\",\"Negative\"</b>\n  * <b>My code</b>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/tweets_sentiment_analysis.R\n  * My code includes Tweet data collection, sentiment analysis and visualization\n  * Resource about collect tweets through R - How tweet search query works, and tutorial for generating tweets in R: http://bogdanrau.com/blog/collecting-tweets-using-r-and-the-twitter-search-api/ \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.951805232526334,
      "result": {
        "original_header": "seealsology - Wikipedia Web Crawler &amp; Auto Graph Building",
        "type": "Text_excerpt",
        "value": "* It's tool crawling links in given Wiki pages and will generate a URL graph automatically, you can also download the graph for further analysis.\n  * The strength of graph is, it allows to you analyze the relationship between pages\n* https://densitydesign.github.io/strumentalia-seealsology/\n \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9039918735548446,
      "result": {
        "original_header": "NLP Framework",
        "type": "Text_excerpt",
        "value": "* The frameworks support multiple NLP tasks, such as machine translation, chatbot, question answering, etc.\n* Multi-purpose Tools\n  * ULMFiT - FastAI\n  * Transformer - Google\n  * BERT - Google\n  * [DistilBERT][6] - A light weighted BERT\n  * [ShuffleNet Series][7] - Computational Efficient CNN Architecture\n  * Transformer-XL - Google\n  * GPT-2 - OpenAI\n* Words Embedding Tools\n  * ELMo\n  * Flair - It combines words embedding methods from multiple tools such as GloVe, BERT, ELMo, etc.\n* References\n  * [8 pretrained NLP models][5] - Not only the description of each tool, but also the Github link and some tutorials \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9923341165020487,
      "result": {
        "original_header": "Transformers",
        "type": "Text_excerpt",
        "value": "* [What is transformer][10]\n  * \"The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\"\n  * \"Self-attention, sometimes called intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\"\n* [Reformer][11]\n  * It performs as well as those giant Transformer models, but it does so while using far less resources and money.\n \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8676935906450461,
      "result": {
        "original_header": "UW NLP TOOLS",
        "type": "Text_excerpt",
        "value": "* Reverb  (works well on raw text)\n  * About Reverb: https://github.com/hanhanwu/reverb\n  * My code using reverb for extraction: https://github.com/hanhanwu/Hanhan_NLP/blob/master/reverb_extraction.py\n  * Sample data for my code: https://github.com/hanhanwu/Hanhan_NLP/blob/master/reverb_sample_data.txt\n  * How to run my code: 1) Download the latest reverb .jar file 2) open your terminal, cd to the folder where you have your text data input and reverb .jar file 3) run this command in your terminal `java -Xmx512m -jar reverb-latest.jar [input file name] > [output file name]` 4) The run my Python code, in the python code, `r_pre` is the file path prefix for data input, `f_path` is the output data path full name \n* Ollie (works better than Reverb for longer/more complex raw text)\n  * About Ollie: https://github.com/hanhanwu/ollie\n  * Sample text data: https://github.com/hanhanwu/Hanhan_NLP/blob/master/ollie_sample_data.txt\n  * Ollie extracted results: https://github.com/hanhanwu/Hanhan_NLP/blob/master/ollie_extracted_output.txt\n  * Check Ollie Quick Start to run do the extraction, if you want to extract the results into a file, run command line `java -Xmx512m -jar ollie-app-latest.jar ollie_sample_data.txt >> extracted_ollie_sample_data.txt`, and remember to `cd` to the folder that include both latest Ollie .jar and engmalt.linear-1.7.mco\n  * Ollie really did a good job in my example, in this sample text example, there must be grammer mistake and each sentence is very long. But Ollie could find large amount of relational noun. I am also surprised by its N-ary extractions, some of them could extract those parallized phrases accurately. Check my ollie_extracted_output.txt, you will find the examples \n* R/Python TEXT MINING PACKAGE\n  * R text mining basics: https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html\n  * Helpful resource - sorting R matrix: https://www.r-bloggers.com/sorting-rows-and-colums-in-a-matrix-with-some-music-and-some-magic/\n  * More about LDA with Python code (still for beginners): https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n  \n \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9902092348691071,
      "result": {
        "original_header": "RESEARCH PAPERS",
        "type": "Text_excerpt",
        "value": "* <b>Analysis of Collaborative Writing Processes Using Hidden Markov Models and Semantic Heuristics</b> - It analysis the PROCESS of collaborative writing and predict writing performance\n* Inspiration\n  * Maybe I can try HMM in my semantic layer, see what kind of key components will lead to what kind of result\n  * It's a good idea to record writing behaviour and do analysis, plus time/duration may be better \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "http://textblob.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "http://selenium-python.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "http://zombie.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/hanhanwu/Hanhan_NLP/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/KDTree_Information_Retrieval.ipynb"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/KDTree_Information_Retrieval.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/text_classification_feature_generation.ipynb"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/text_classification_feature_generation.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/try_Spacy.ipynb"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/try_Spacy.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/NLP%20with%20Scikit%20Learn.ipynb"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/NLP%20with%20Scikit%20Learn.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/try_words_embedding.ipynb"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/try_words_embedding.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/NLP_textblob.ipynb"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/NLP_textblob.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/NLP_dependency_trees.ipynb"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/NLP_dependency_trees.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/hanhanwu/Hanhan_NLP/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "hanhanwu/Hanhan_NLP"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Hanhan_NLP"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/try_Stanford_CoreNLP/parsing_with_OO/run_source_code.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/HanhanScrapt/source/run_news_scraper.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "installation": [
    {
      "confidence": 0.9874107975718832,
      "result": {
        "original_header": "Somethig About NLP",
        "type": "Text_excerpt",
        "value": "* Computational Linguistics Basics: https://www.analyticsvidhya.com/blog/2017/12/introduction-computational-linguistics-dependency-trees/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n  * Dependency trees\n  * Stanford Dependencies Manual: https://nlp.stanford.edu/software/dependencies_manual.pdf\n  * Coreference Resolution or Anaphora Resolution\n    * Identify all the expressions that refer to the same entity \n* Some NLP methods for feature extraction in text classification\n  * My code: https://github.com/hanhanwu/Hanhan_NLP/blob/master/text_classification_feature_generation.ipynb\n* Other tools similar to tools used above\n  * Flair: https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n    * pos tag\n    * words embedding\n     \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9734958396568396,
      "result": {
        "original_header": "PRACTICE",
        "type": "Text_excerpt",
        "value": "* When dealing with special characters in text, ASCII and Binary Characteristics: http://roubaixinteractive.com/PlayGround/Binary_Conversion/The_Characters.asp \n* Spam Detection with Akismet\n  * Akismet Python examples: http://www.programcreek.com/python/example/56583/akismet.verify_key\n  * My code with Akismet for spam detection: https://github.com/hanhanwu/Hanhan_NLP/blob/master/spam_detection_Akismet.py \n* Google search API sets limitations per day, only could get 32 search reuslts each day\n* <b>xgoogle</b>, it has a pretty good tutorial, but google has blocked it, since Google doesn't allow automated search... http://www.catonmat.net/blog/python-library-for-google-search/\n* <b>pygoogle</b> is no longer available either, it recommends to use Google Custom Search\n* <b>YQL</b>\n  * Yahoo YQL Guide: https://developer.yahoo.com/yql/guide/usage_info_limits.html\n  * Yahoo Dev Center, to get the key and secret for YQL boss.search: https://developer.yahoo.com/apps/\n  * yql open tables: https://github.com/hanhanwu/yql-tables\n  * yql health checker, to check the current situations of yql open tables (I really like this feature, when you are clicking the tables here, it leads to YQL Console to allow you run the test query immediately): https://www.datatables.org/healthchecker/\n  * <b>Example</b>, yql with Bing Web Search: https://developer.yahoo.com/yql/console/?env=store://datatables.org/alltableswithkeys&q=SELECT+*+FROM+microsoft.bing.web+WHERE+query%3D%27stackoverflow%27#h=SELECT+*+FROM+microsoft.bing.web+WHERE+query%3D'stackoverflow'\n  * But, yql always returns empty results for my queries....\n* <b>Bing</b>\n  * Bing APIs: https://www.microsoft.com/cognitive-services/en-us/bing-web-search-api/documentation\n  * Bing Search API Guide: https://msdn.microsoft.com/en-us/library/dn760781.aspx\n  * In order to use Bing Search API, need to get the subscription key here: https://www.microsoft.com/cognitive-services/en-us/subscriptions\n  * Bing Search API Parameters (cannot compete with Google...): https://dev.cognitive.microsoft.com/docs/services/56b43eeccf5ff8098cef3807/operations/56b4447dcf5ff8098cef380d\n  * Bing API Test Console: https://dev.cognitive.microsoft.com/docs/services/56b43eeccf5ff8098cef3807/operations/56b4447dcf5ff8098cef380d/console\n  * Python Bing Search (it seems that the api key here should be got form Azure Market Place....): https://github.com/hanhanwu/py-bing-search\n  * When I was using Python Bing Search, it was showing the authorization method is not valid, I guess this maybe the reason:\n https://datamarket.azure.com/dataset/bing/search\n  * It changes v2 to v5, and in order to get all those info, I need to purchase in Azure Market Place: https://msdn.microsoft.com/en-US/library/mt707570.aspx\n  * Bing Web Search API Pricing: https://www.microsoft.com/cognitive-services/en-us/bing-web-search-api\n* <b>Google Custom Search Engine (CSE)</b>\n  * It's super easy and super cool! If you want to custom your own search engine, on your website\n  * The tutorial provides all the useful and educational urls: https://developers.google.com/custom-search/docs/topical\n  * The control panel is easy and fast to use, you can generate customized search engine within seconds, and google will create a web version as well as html code and tell you how to copy the code to html page: https://cse.google.com/cse/all\n  * My CSE test html code, I just opened JSBin console, opened a HTML template and copy the generate code between <body></body>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/google_cse_sample.html\n  * If you need the search results obey a certain url pattern, check google search query operator, very helpful!: https://support.google.com/websearch/answer/2466433?visit_id=1-636146162684675642-3633116773&rd=1\n  * Now I want to get the search results from pyhton code, it also simple. This is the detail tutorials about where to get your cx (Custom Search Engine id), key (API Key): https://developers.google.com/custom-search/json-api/v1/reference/cse/list\n  * In order to get cx, you need to visit this website: https://cse.google.com/cse/all\n  * Create a Custom Search Engine, it's very fast! Once you have created one, you will find cx=.... through the url in your browser\n  * In order to get Google API Key, you need to go to Google Dev Center: https://console.developers.google.com/apis/dashboard\n  * In Google Dev Center, first click Dashboard, then Click 'ENABLE API' and enable 'Custom Search API'. Next, click 'Credentials', and click 'Create Credential' to create an API Key\n  * Pricing if you want to have more search requests: https://developers.google.com/custom-search/json-api/v1/overview#key\n  * About the pricing, one thing is a little weird, if you are using $5 per 1000 query based on the free version, it will prevent you from calling more queries each day when you haven't reached to 10k queirs. Maybe this is because I'm not the one who paid the price and cannot see the settings...\n  * My sample code about how to get the results through python, with FREE version: https://github.com/hanhanwu/Hanhan_NLP/blob/master/google_cse_sample_python_call.py\n  * Google CSE is great, cheaper than many other web search apis, even cheaper than Google Search API, and it's well documented. One thing need to note that google CSE didn't mention is their limitations per second. Bing Search API has 5 query limitation per second. I guess Google CSE also has limitation, therefore only using for loop to execute multiple queries will get errors. You can simply add `time.sleep(1)` in your for loop. Check my code: https://github.com/hanhanwu/Hanhan_NLP/blob/master/multiple_query_google_cse.py\n  \n<b>Java Script Parser</b> \n* <b>Gigya Javascript Web Parser</b>\n  * One year ago, Amazon Camel-Camel-Camel suddenly changed its website source code and everything parsable has become Javascript, therefore I lost many data, and thought Javascript written website may not be parsable. Today, I just tried the Gigya API (the researchers I'm working with may have paid this API...), it will return all the content hidden behind the Javascript code!\n  * How does this work: http://developers.gigya.com/display/GD/Developer%27s+Guide\n  * Parameters: http://developers.gigya.com/display/GD/comments.getComments+JS\n  * One thing about the parameters, some website has hundreds or even thoughds of comments, but you need to click a button such as \"More Comments\" to see more comments, and the url has never changed. When you are using Gigya API, gave to set parameter \"threadLimit\", otherwise it cannot return all the comments.\n* <b>DIY JavaScript Parser - with selenium</b>\n  * In above you will find Gigya works well to get the content from JS written website, however it doesn't work all the time. Recently, I just met a problem, that when Globe and Mail is updating their website, all the old comments have been hidden from the view, only new comments posted after Nov. 28, 2016 will appear with added new features. However, by using Gigya, I could get all the old comments, but no new comments, no new feature in the output data as well. After 2 days experiments, finally I somewhat realized why this happened, and at least I have found the basic solution to get the content data of this JS written website.\n  * <b>My code 1</b>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/DIY_JSParser.py\n  * JS written website is not impossible for parsing without other API, we cannot see content from the source code, but we should be able to get the HTML content from the JS page.\n  * <b> First of all</b>, install <b>Firebug!</b>(the name is really cool!) on your Firefox, it's an Addon. With Firebug, when you open the url in Firefox, click `HTML` in Firebug, you will see all the content. With Firebug search function, it's easy to locate and find the tags of the content you want to parse. Then, write code similar to mine, we can get the content.\n  * <b>Then</b>, as you can see, in my code, I am using selenium library, it's super cool. It can get HTML page from JS written website, add, with `click()` method, you can click the chosen element. This is very imortant is because some elements such as reactions counts in my sample url will not appear until you click each reaction image.\n  * selenium elements locating: http://selenium-python.readthedocs.io/locating-elements.html\n  * NOTE: selenium has get_element_* and get_elements_*, pay attention to this detail.\n  * <b>My code 2</b>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/DIY_JS_Parser2.py\n  * In my part 2 code, I was dealing with multiple clickable elements that have the same class name. When you clicked one, without closing it or scroll down, you cannot click another one. In this code, the code closed each popup, before clicking the next one.\n  * Also in my code 2, I'm using this method `driver.execute_script(\"arguments[0].scrollIntoView();\", clk)`, each time you need to interact with the next element, it will move to that position first. Otherwise you will get an error for not be able to see that (x,y) point, even if you maximize the browser.\n  * <b>Getting parents web element</b>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/DIY_JS_Parser_get_parents.py\n  * <b>My code 3</b>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/DIY_JS_Parser3.py\n  * My code 3 has finished a complex process, that is to extract hierarchical data and recover them back to the original hierarchial structure. If you copy the url to firefox and have firebug on it, you will find for each main comment author, the class name is \"c29cjTJ\", the replied author of each main comment has class name \"c29cjTJ c3fk6Wf\". In fact \"c29cjTJ c3fk6Wf\" is a compact class, you cannot find the element through this class name, but you can find the element through `driver.find_element_by_css_selector(\".c29cjTJ.c3fk6Wf\")`. However, there is a even simpler way, that is what I wrote in my code 3, you use `driver.find_element_by_class_name(\"c29cjTJ\")`, it will include all the authors for main comment and replies. Later, just try to trace back to find the root class id to decide which reply belongs to which main comment.\n  * NOTE: In some machines, it may require `geckodriver`, and sometimes, adding its path into $PATH doesn't work, so in your code, instead of using `driver = webdriver.Firefox()`, you should use `driver = webdriver.Firefox(executable_path='[your path]/geckodriver')`\n    * To deal with more errors related to `geckodriver`, check https://github.com/hanhanwu/Hanhan_Break_the_Limits/blob/master/Ideas_Spark/track_siliconvalley_vancouver_trend.ipynb\n  * <b>My code 4</b>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/DIY_JS_Parser4.py\n  * I thought my code 3 was good, no it's not. When I found there was a \"Show More Comments\" button at the bottom, and tried to expand all the comments, I have found that the expanded comments had no HTML location id, which was the ids I used to map comment authors, post time text, reactions and replies together. I tried some methods, hoping to adding something on my code 3 to solve the problem, but only made things more complex. Maybe, sometimes when the problem becomes more complex, the solution will in fact become easier. I redisigned the code, and just found that, instead of using driver to get all the web elements, I could use a web elements to locate its children, which makes my mapping much easier. Although the website have some inconsistent display, I put many \"try... except\" in the code, at least for nnow, there is no error. Here are the test urls: https://github.com/hanhanwu/Hanhan_NLP/blob/master/test_urls.txt\n  * Note: in my code 4, I am using `driver.quit()`, instead of `driver.close()`, this is because in some machine, `driver.close()` does not work, this may be a bug in selenium, here is the discussion: https://github.com/seleniumhq/selenium/issues/654 \n* Sample url \"http://news.nationalpost.com/full-comment/marni-soupcoff-bob-dylans-nobel-silence-is-golden\"\n* I need to get the comments and I don't want to use its HTML data, looks simple, but no. Because this website embed everything in WordPress, however, traditional APIs cannot get the right comments data.\n* <b>Gigya</b> got empty comments\n* <b>WordPress API</b>: https://developer.wordpress.com/docs/api/\n  * WordPress API console: https://developer.wordpress.com/docs/api/console/\n  * But for each NationalPost article, both WordPress post and comment all point to the article, and the `$site` has to be the domain. `https://public-api.wordpress.com/rest/v1/sites/news.nationalpost.com/comments/307452/`, `https://public-api.wordpress.com/rest/v1/sites/news.nationalpost.com/posts/1282447/replies`, others could not help either.\n* <b>PostMedia open source</b> is not useful either: https://github.com/Postmedia\n* <b>Python Twill</b>: Python Twill does not work well for JavaScript. http://twill.idyll.org/,  http://twill.idyll.org/python-api.html\n* <b>Zombie</b>: I tried Zombie first because it's browserless, which means when it is parsing the data, there is no browser popup. First of all, its Python API does not work for external url not built by me. So I had to try Zombie.js, it finally only returned me those only in web source code, still cannot get comments data. But a good learning experience.\n  * Zombie CoffeeScript Stackoverflow reference: http://stackoverflow.com/questions/11628636/i-cant-get-the-whole-source-code-of-an-html-page\n  * In order to use Zombie.js, I have checked: how to use node.js: http://www.dannemanne.com/posts/tutorial_get_started_with_node_js_on_mac_beginner_\n  * In order to use Zombie.js, I have checked: coffeescript - TRY COFFEESCRIPT: http://coffeescript.org/\n  * In order to use Zombie.js, I have checked: zombie.js: http://zombie.js.org/\n  * In order to use Zombie.js, I have checked: zombie functions: http://zombie.readthedocs.io/en/latest/pythonzombie.html\n  * In order to use Zombie.js, I have checked: CSS selector string: http://www.w3schools.com/jsref/met_document_queryselector.asp\n* <b>Mechanize</b>: It is built on python urllib2, therefore it cannot get the comments data which is not showing in web source code... But it is very convenient to get article title. http://wwwsearch.sourceforge.net/mechanize/\n* <b>Selenium</b>: Finally I got the comments by using selenium and firebug. They were my last choice because of the popup browsers, but, I have to admit, they are good to use.\n  * My code: https://github.com/hanhanwu/Hanhan_NLP/blob/master/get_national_post_comments.py\n  * In my code, I have learned that, when using XPath, `//iframe[@class='fb_ltr']` means this element ins directly under the root, if not, use `.//iframe[@class='fb_ltr']`\n \n \n<b>Python Scrapy</b>\n \n* Scrapy Web Crawler\n  * Scrapy General: https://github.com/scrapy/scrapy\n  * Scrapy documents: https://doc.scrapy.org/en/latest/\n  * Pyhton yield: http://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n* Scrapy Pipeline\n  * The Scrapy Pipeline aims to let you execute a series activities through a pipeline of process\n  * If you check Scrapy Pipeline document here, it's difficult to really get thing done: https://doc.scrapy.org/en/latest/topics/item-pipeline.html\n  * Here is my detailed readme about what did I try and worked: https://github.com/hanhanwu/Hanhan_NLP/blob/master/HanhanScrapt/Hanhan_Scrapy_Pipeline_ReadMe.md\n  * All my Scrapy Pipeline code: https://github.com/hanhanwu/Hanhan_NLP/tree/master/HanhanScrapt\n* Other Scrapy Resources\n  * Web Scrapign: https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n    * Extract web elements such as images, text, votes, etc. with Scrapy Spider\n    * Based on my experience, Scrapy only works well when the webiste does not use any bot to prevent customized spiders (even if you change Scrapy bot settings). Python selenium often works even when there is bot.\n  \n<b>Entity Recognizer</b>\n \n* Stanford Named Entity Recognizer (NER)\n  * NER is able to regognize names of different things/people in English words\n  * Download NER package: http://nlp.stanford.edu/software/CRF-NER.shtml#Download\n  * My code to extract people names with NER: https://github.com/hanhanwu/Hanhan_NLP/blob/master/extract_people_names.py \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9885555990880206,
      "result": {
        "original_header": "nltk Sentiment Analysis",
        "type": "Text_excerpt",
        "value": "* NLTK Sentiment Analysis: http://www.nltk.org/howto/sentiment.html \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9999999853085343,
      "result": {
        "original_header": "Vader Sentiment Analysis",
        "type": "Text_excerpt",
        "value": "* Download Vader .zip file here: https://github.com/hanhanwu/vaderSentiment\n* After downloading Vader, import all the 3 files in vaderSentiment folder to your python SDK, under the same project you are going to do sentiment analysis\n* Vader can also handle emoji, slang and emoticons: https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f\n  * <b>Python 2.x only</b>\n  * My vader sentiment analysis test code: https://github.com/hanhanwu/Hanhan_NLP/blob/master/nltk_sentiment_analysis_test.py \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9999999980496739,
      "result": {
        "original_header": "Web Annotatio Tools",
        "type": "Text_excerpt",
        "value": "* WebAnno: https://webanno.github.io/webanno/\n  * WebAnno source code, more about web development: https://github.com/webanno/webanno\n* Brat: http://brat.nlplab.org/examples.html\n* <b>Format JSON Output</b>, when you don't have notepad++: http://jsonviewer.stack.hu/ \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9918047873890642,
      "result": {
        "original_header": "UW NLP TOOLS",
        "type": "Text_excerpt",
        "value": "* Reverb  (works well on raw text)\n  * About Reverb: https://github.com/hanhanwu/reverb\n  * My code using reverb for extraction: https://github.com/hanhanwu/Hanhan_NLP/blob/master/reverb_extraction.py\n  * Sample data for my code: https://github.com/hanhanwu/Hanhan_NLP/blob/master/reverb_sample_data.txt\n  * How to run my code: 1) Download the latest reverb .jar file 2) open your terminal, cd to the folder where you have your text data input and reverb .jar file 3) run this command in your terminal `java -Xmx512m -jar reverb-latest.jar [input file name] > [output file name]` 4) The run my Python code, in the python code, `r_pre` is the file path prefix for data input, `f_path` is the output data path full name \n* Ollie (works better than Reverb for longer/more complex raw text)\n  * About Ollie: https://github.com/hanhanwu/ollie\n  * Sample text data: https://github.com/hanhanwu/Hanhan_NLP/blob/master/ollie_sample_data.txt\n  * Ollie extracted results: https://github.com/hanhanwu/Hanhan_NLP/blob/master/ollie_extracted_output.txt\n  * Check Ollie Quick Start to run do the extraction, if you want to extract the results into a file, run command line `java -Xmx512m -jar ollie-app-latest.jar ollie_sample_data.txt >> extracted_ollie_sample_data.txt`, and remember to `cd` to the folder that include both latest Ollie .jar and engmalt.linear-1.7.mco\n  * Ollie really did a good job in my example, in this sample text example, there must be grammer mistake and each sentence is very long. But Ollie could find large amount of relational noun. I am also surprised by its N-ary extractions, some of them could extract those parallized phrases accurately. Check my ollie_extracted_output.txt, you will find the examples \n* Open Source Statistical Machine Translation: http://www.statmt.org/ \n* SFU Parser: https://github.com/sfu-natlang/glm-parser \n* Crowd Sourcing for Sentiment Analysis\n  * CrowdFlower: https://www.crowdflower.com/use-case/sentiment-analysis/ \n* R/Python TEXT MINING PACKAGE\n  * R text mining basics: https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html\n  * Helpful resource - sorting R matrix: https://www.r-bloggers.com/sorting-rows-and-colums-in-a-matrix-with-some-music-and-some-magic/\n  * More about LDA with Python code (still for beginners): https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n  \n \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8335430983072825,
      "result": {
        "original_header": "Facebook Research: https://github.com/facebookresearch",
        "type": "Text_excerpt",
        "value": "  * It includes not only NLP, but also AI\n  * FastText: https://github.com/facebookresearch/fastText\n    * It can help supervised text classification\n    * I didn't try it, sounds fast... But you need to give the label...\n \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9999999616227918,
      "result": {
        "original_header": "Chat Bot - RASA-NLU",
        "type": "Text_excerpt",
        "value": "  * RASA-NLU: https://github.com/RasaHQ/rasa_nlu\n    * You need to provide the training data yourself\n  * How other use RASA-NLU to build chat-bot app: https://www.analyticsvidhya.com/blog/2018/01/faq-chatbots-the-future-of-information-searching/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n    * source code: https://github.com/yogeshhk/gstfaqbot\n     \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.998083881578065,
      "result": {
        "original_header": "XLNet",
        "type": "Text_excerpt",
        "value": "* https://github.com/zihangdai/xlnet \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9965089314136877,
      "result": {
        "original_header": "My NLP Presentation",
        "type": "Text_excerpt",
        "value": "* 2016/8/24: https://github.com/hanhanwu/Hanhan_NLP/blob/master/Hanhan_NLP_Presentation.pdf \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9953450245021265,
      "result": {
        "original_header": "RESEARCH PAPERS",
        "type": "Text_excerpt",
        "value": "* <b>NLP Research Papers collection on Github</b>: https://github.com/dair-ai/nlp_paper_summaries \n* <b>Computational Linguistics</b> - MIT Press: http://www.mitpressjournals.org/loi/coli \n* <b>SO-CAL in NLP</b>: https://github.com/hanhanwu/Hanhan_NLP/blob/master/Taboada_etal_SO-CAL.pdf \n\n* <b>Text - Independent Speaker Identification using Hidden Markov Models</b>\n* <b>Hidden Markov Models in Text Recognition</b>\n* Python HMM\n  * http://scikit-learn.sourceforge.net/stable/modules/hmm.html\n  * https://github.com/hanhanwu/hmmlearn/blob/master/examples/plot_hmm_stock_analysis.py\n* R HMM\n  * https://cran.r-project.org/web/packages/HMM/HMM.pdf \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9284453339545061,
      "result": {
        "original_header": "OTHER RESOURCES",
        "type": "Text_excerpt",
        "value": "* SFU CMPT-825 NLP, 2008: http://www.cs.sfu.ca/~anoop/courses/CMPT-825-Spring-2008/\n* SFU CMPT-825 NLP, 2014: http://anoopsarkar.github.io/nlp-class/syllabus.html\n* Deep Leanring Course with NLP: http://cs224d.stanford.edu/syllabus.html\n* Deep Learning in NLP & Speech/Audio: https://www.analyticsvidhya.com/blog/2016/08/deep-learning-path/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n* Stanford Question Answering Dataset: https://rajpurkar.github.io/SQuAD-explorer/\n  * A new about Alibaba and MSR: https://www.analyticsvidhya.com/blog/2018/01/alibabas-neural-network-model-beat-highest-human-score-stanfords-reading-test/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n  \n   \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 1.0,
      "result": {
        "original_header": "Regex Cheatsheet",
        "type": "Text_excerpt",
        "value": "* `re.findall(r\"[\\w.-]+@[\\w.-]+\", text)` to extract the email address from the text\n  \n  \n[1]:https://github.com/chartbeat-labs/textacy/blob/master/textacy/preprocess.py\n[2]:https://github.com/chartbeat-labs/textacy/blob/master/textacy/extract.py\n[3]:https://github.com/hanhanwu/Hanhan_NLP/blob/master/textacy_explore/go_through_features.py\n[4]:https://www.analyticsvidhya.com/blog/2017/01/introduction-to-structuring-customer-complaints/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n[5]:https://www.analyticsvidhya.com/blog/2019/03/pretrained-models-get-started-nlp/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n[6]:https://github.com/huggingface/pytorch-transformers/tree/master/examples/distillation\n[7]:https://github.com/megvii-model/ShuffleNet-Series\n[8]:https://github.com/PhantomInsights/mexican-government-report\n[9]:http://www.nltk.org/howto/sentiment.html\n[10]:https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/?utm_source=blog&utm_medium=5-open-source-machine-learning-projects-data-scientist\n[11]:https://github.com/lucidrains/reformer-pytorch\n[12]:https://www.analyticsvidhya.com/blog/2021/02/uninformed-search-algorithms-in-ai/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n[13]:https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance\n[14]:https://en.wikipedia.org/wiki/Hamming_distance\n[15]:http://nlp.stanford.edu/software/dcoref.shtml\n[16]:https://github.com/Wordseer/stanford-corenlp-python\n[17]:https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n[18]:https://github.com/hanhanwu/Hanhan_NLP/blob/master/NLP%20with%20Scikit%20Learn.ipynb\n \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8119955565998913,
      "result": {
        "original_header": "UW NLP TOOLS",
        "type": "Text_excerpt",
        "value": "* Reverb  (works well on raw text)\n  * About Reverb: https://github.com/hanhanwu/reverb\n  * My code using reverb for extraction: https://github.com/hanhanwu/Hanhan_NLP/blob/master/reverb_extraction.py\n  * Sample data for my code: https://github.com/hanhanwu/Hanhan_NLP/blob/master/reverb_sample_data.txt\n  * How to run my code: 1) Download the latest reverb .jar file 2) open your terminal, cd to the folder where you have your text data input and reverb .jar file 3) run this command in your terminal `java -Xmx512m -jar reverb-latest.jar [input file name] > [output file name]` 4) The run my Python code, in the python code, `r_pre` is the file path prefix for data input, `f_path` is the output data path full name \n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/hanhanwu/Hanhan_NLP/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Hanhan_NLP"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "hanhanwu"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 119300,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 97648,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "R",
        "size": 13446,
        "type": "Programming_language",
        "value": "R"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 721,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "HTML",
        "size": 576,
        "type": "Programming_language",
        "value": "HTML"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-03 23:06:43",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 5
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Stanford Sentiment Analysis Example: http://stackoverflow.com/questions/32879532/stanford-nlp-for-python",
        "parent_header": [
          "Hanhan_NLP",
          "PRACTICE",
          "Sentiment Analysis"
        ],
        "type": "Text_excerpt",
        "value": "  * In Stanford sentiment analysis, their sentiment levels are: <b>VeryPositive, Positive, Nuetral, Negative, VeryNegative</b>\n    \n    \n* Python Flash Text - Search, Replace words in text, faster than regex\n  * reference: https://www.analyticsvidhya.com/blog/2017/11/flashtext-a-library-faster-than-regular-expressions/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n  * flashtext: https://pypi.python.org/pypi/flashtext/2.2\n    * I don't really like that 'remove keywords' feature\n  * It's using <b>Trie Data Structure</b> for the words\n  * <b>Single Pass vs No Single Pass</b>\n    * With single pass, each word replacing will use the origial text/string as input\n    * Without single pass, each replace is built based on the last replacing\n  * FlashText is faster than regex is because, it checks whether the keyword exists in keyword dictionary, rather than checking the whole text\n  * my code: https://github.com/hanhanwu/Basic_But_Useful/blob/master/try_flashtext.ipynb\n\n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Feature Extraction, for Clustering Use",
        "parent_header": [
          "Hanhan_NLP",
          "NLP Tools Research"
        ],
        "type": "Text_excerpt",
        "value": "* Stanford CoreNLP: http://stanfordnlp.github.io/CoreNLP/\n\n* Spacy\n  * https://spacy.io/\n  * Spacy GitHub: https://github.com/hanhanwu/spaCy\n  * Spack API Reference: https://spacy.io/docs/api/\n  * NLP Tutorials: https://spacy.io/docs/usage/tutorials\n  * Spacy Workflow: https://spacy.io/docs/usage/language-processing-pipeline\n  * My code for basic try spacy: https://github.com/hanhanwu/Hanhan_NLP/blob/master/try_Spacy.ipynb\n  * Reference: https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A+AnalyticsVidhya+%28Analytics+Vidhya%29\n    * For the machine learning part in this tutorial, I didn't try, since I think only the pipeline part is special but I don't see any meaning for the author to create `class predictors`. If I have to use pipeline in my personal projects, I'd rather to use Spark + Stanford CoreNLP\n \n* Tools built on Spacy (maybe they are even better)\n  * Overall: https://spacy.io/docs/usage/showcase\n* <b>Textacy</b>\n  * Textacy Document: https://media.readthedocs.org/pdf/textacy/latest/textacy.pdf\n  * [Text Preprocessing][1]: Their preprocessing choices are very rich, removing emails, remove urls, fix unicode, deal with Egnlish contractions and so on. Meanwhile, if you ues `textacy.preprocess.preprocess_text()`, you can choose which choices to use, very flexible.\n  * [Extract][2]: They are using Part-of-speech tagging as Spacy dose. The part-of-speech tagger uses the OntoNotes 5 version of the Penn Treebank tag set. We also map the tags to the simpler Google Universal POS tag set. \n  * [My textacy explore code][3]\n \n* NLTK\n  * NLP basic operations go through: http://clarkgrubb.com/nlp\n  * [NLTK also supports Vader][9]\n    * Although Vader seems still only works in Python 2.*, with NLTK, it also works in python 3\n    * To download `vader_text`:\n      * This may not work:\n        * `import nltk`\n        * `nltk.download('vader_lexicon')`\n      * This works for me:\n        * `python -m nltk.downloader vader_lexicon`, run this in the terminal\n \n* Other Methods\n  * [Feature Summary with Python][4] (the data preprocessing methods here are too basic, however, it's very interesting to learn there is `tf-isf` and see the way they calculate it. The only pity is, they got Cue Words manually...)\n\n* Methods used in Published Papers\n  * Threat Comments Detection (check their features selection): https://www.semanticscholar.org/paper/Threat-detection-in-online-discussions-Wester-%C3%98vrelid/f4150e2fb4d8646ebc2ea84f1a86afa1b593239b\n \n* Python Gensim - Topic Modeling\n  * General Process:\n    * Preprocess the data, and use different word embedding methods to convert text into vector (tfidf, text-to_vectors also count)\n    * Then apply topic modeling methods\n  * https://github.com/hanhanwu/gensim\n  * To check algorithm details, strength: https://radimrehurek.com/gensim/apiref.html\n  * An example of preprocessing the data and topic modeling: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/lda_training_tips.ipynb\n  * Gensim also supports different word embedding and other text to vector methods: https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#word-embeddings\n  * Supported Topoc Modeling methods: https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#topic-modeling\n \n* Pyhthon Pattern - NLP and Machine Learning\n  * https://github.com/hanhanwu/pattern\n\n* Python TextBlob - NLP, built upon NLTK and Pattern\n  * https://github.com/hanhanwu/TextBlob\n\n"
      },
      "source": "https://raw.githubusercontent.com/hanhanwu/Hanhan_NLP/master/README.md",
      "technique": "header_analysis"
    }
  ]
}