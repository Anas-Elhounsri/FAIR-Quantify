{
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/Hanson-Research-Lab/TRI_STILT"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-07-22T20:30:52Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2021-04-22T04:37:09Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 0.8379411151521324,
      "result": {
        "original_header": "STILT for TRI Modeling:",
        "type": "Text_excerpt",
        "value": "[Greg Lee](https://greglee1905.github.io/personalpage/), [Heidi Hanson](https://medicine.utah.edu/surgery/research/research-sections/uinquire/research-groups/hanson/), [Joemy Ramsay](https://medicine.utah.edu/surgery/research/research-sections/uinquire/research-groups/hanson/members.php), [Derek Malia](http://home.chpc.utah.edu/~u0703457/dereks_homepage/) and [Ben Fasoli](https://benfasoli.com/)<br>\nSeptember 26th 2020 \n"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9984327992190608,
      "result": {
        "original_header": "Structure &amp; Organization",
        "type": "Text_excerpt",
        "value": "    - _TRI_STILT_ - A cloned directory to handle all pre/post data processing and visualization\n        - **_data_** - Houses data only. Data descriptions/origins are found in data_origin.txt which should be modified if data is added. \n            - _processed_ - Processed raw data. Often this will take the form of cleaned data, stilt inputs and stilt outputs. \n            - _raw_ - Immutable raw data. All results should be reproducible from this subset of data. \n            - _validation_ - Data to validate model results. A workflow is in development to script this process. \n        - **_figures_** - Storage for figures. For consistency, please use the same name for the stilt_input.rds, figures, stilt_run.r and output netcdf/csv/shapefiles\n        - **_notebooks_** - Jupyter notebooks for data exploration, visualization and testing. It is preferred that code in the pipeline be transformed to a scripted form to ensure the project retains its repeatability and readability. \n            - _descriptiveanalysis.ipynb_ - Loading and exploring the raw TRI data, exploring the distributions of chemicals and spacial locations through time. \n            - _epavalidation.ipynb_ - A start to validation results from STILT simulations. Within, the nearest 100 TRI releases to every EPA sensor are found and saved in data/validation. A test simulation is run with 100 relevant TRI releases using a non-standard data isolation procedure (before 2015 due to not all NARR data being converted). Relevant data sets include `data/validation/092920_epa_valid_2014.csv`, `data/processed/stilt_input/092920_epa_valid_2014.rds`, `src/validation/092920_epa_validation.r` (convert csv to rds file) and `src/stilt_run/092920_epa_validation.r` (run_stilt.r file for STILT control).\n            - _hysplitvsstilt.ipynb_ - Comparing results from HYSPLIT and STILT. HYSPLIT simulations were saved under validation data. If you need to recreate these simulations, please look into the Environmental Exposome and set a HYSPLIT model of 100 puffs.\n            - _postprocessing.ipynb_ - A test script to demonstrate the basic post processing necessary after make stilt_output_conversion. This details how to extract and visualize portions of the saved .csv file. \n            - _stiltparametertuning.ipynb_ - An exploration into metrics to optimize STILT hyperparameters. At this point, Ben estimates it will be unnecessary unless the computational load becomes too heavy for the traditional program. \n        - **_src_** - folder to house all scripts to run pre and post-processing of STILT inputs\n            - _data_ - Clean, link and process data to become ammenable to STILT modeling\n            - _stiltpostprocessing_ - Converts netCDF to csv and links back to STILT simulations\n            - _stiltrun_ - scripts to copy and past as `run_stilt.r` to control simulation. For base case see `src/stilt_run/template_run.r`\n            - _validation_ - Scripts related to validation. Widely unfinished at this point in time. \n        - **_gitignore_** - git file to allocate files and folders which should not be added to the repo\n        - **_chpc help.txt_** - a quick guide on how to use chpc. \n        - _LICENSE_ - License for code: MIT license\n        - **_Makefile_** - Control file to run simulations - change to desired parameters and paths before running simulations\n        - **_README.md_** - This file!\n        - **_requirements.txt_** - pip requirements for python venv\n        - _setup.py_ - A setup script to verify a virutal environment is setup correctly \n"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9309527334410751,
      "result": {
        "original_header": "Pre-Processing",
        "type": "Text_excerpt",
        "value": "All steps are built utilizing a make style system. Before running, please edit the `Makefile` with the desired directories and variables. If choosing to run this on CHPC, you will need to use an interactive compute node or slurm job. For an interactive node use `srun --time=1:00:00 --ntasks=16 --nodes=1 --account=hanson --partition=notchpeak --pty /bin/bash -l`.  \n1. **Clean Data** \n    - **Description:** <br>Executes src/data/make_data.py. This script cleans and converts all TRI raw data into a single csv, with RSEI and Pubchem information attached. Change the inputs within the makefile as you deem fit for your project. For code details please visit src/data/make_data.py.\n    - **Assumptions:**\n        1. Keeps only Fugitive and Stack Air Releases\n        2. Removes any columns with over the threshold amount for missing data\n        3. Keeps select columns of use: YEAR, TRIFD, FRSID, FACILITYNAME, CITY, COUNTRY, ST, ZIP, LATITUDE, LONGITUDE, INDUSTRYSECTORCODE, CHEMICAL, CAS#/COMPOUNDID, METAL, CARCINOGEN, UNITOFMEASURE, 51-FUGITIVEAIR and 52-STACKAIR. \n        4. IARC does not exist for the following chemicals so they are filled accordingly.  \n            - Strong-inorganic-acid mists containing sulfuric acid (see Acid mists) - CLASS 1 \n            - Bis(2-ethylhexyl) phthalate (see Di(2-ethylhexyl) phthalate) - CLASS 2B\n        5. Pubchem merge is conditionally dependent on all chemical files being present within the `TRI_Pubchem_CIDS.csv` file. If chemicals are not found, this step is skipped. Within the CIDS.csv several of the chemicals do not have an ID (Creosote, PCB, Sulfuric Acid (1994..) and methyl isobutyl ketone). This indicates the chemical does not have a functional or easily defineable pubchem CID. \n        6. The primary purpose of the RSEI merge is to estimate stack height of fugitive releases\n        7. Keeping all data which is not of IARC class 3 (known to not be carcinogenic)\n    - **Makefile Command:** `make data`\n    - **Inputs:**\n        1. _Source Script_ - src/data/make_data.py\n        2. _Input Filepath_ - Path to TRI data. All TRI release files must be labeled as tri_YEAR_ut.csv.\n        3. _Output Filepath_ - Path to export cleaned data. Note: only the label of the file is needed as the years of the simulation and csv are added to the export ie `data/processed/test_clean` will fill to `data/processed/test_clean_1990_2018.csv`.\n        4. _Min Year_ - An integer filter to keep only tri releases from the min year on (>=)\n        5. _Max Year_ - An integer filter to keep only tri releases from the max year and below (<=)\n        6. _Threshold_ - Missing values threshold. Throws out any variables which have greater than that percentage of missing data. 0.2 ~ 20% of rows are missing within that column\n        7. _IARC Path_ - Path to IARC chemical data \n        8. _Pubchem Path_ - Path to Pubchem path. NOTE: pubchem is linked via a pubchem ID to the name of the chemical. This file is done for chemicals from 1990-1999. If these simulations are run in the future, a user will need to edit this file to include pubchem IDs for all new chemicals. If information is not available for all chemicals, this step is skipped and no pubchem information is linked. \n        9. _RSEI Path_ - Path to RSEI data from the EPA. \n    - **Outputs:** \n        1. Single csv file of name output_filepath_name_min_year_max_year.csv. I recommend placing this output within data/processed \n2. **Convert to STILT Input Format:** \n    - **Description:** <br> Takes the cleaned TRI filepath and extracts the height, latitude,  longitude and time for STILT simulations. Aggregations occur for simulations and fugitive and stack releases are seperated per simulation run. Columns are renamed to lati, long, zagl and run_times. DO NOT ALTER THESE OR THE SIMULATIONS WILL SHOW AN IMPORT NULL ERROR. Per each TRI year, an expansion step is performed so releases happen on a daily basis. \n    - **Assumptions:**\n        1. Currently date expansion is accounting for leap years\n        2. No dates in the year are being omitted. If you wanted to change this edit the RDF File Conversion script\n        3. Simulations which have identical year, lati, long and release height can be considered identical for modeling purposes\n    - **Makefile Command:** `make stilt_input`\n    - **Inputs:**\n        1. Python Processing\n            - _Source Script_ - `src/data/make_stilt_data_1.py`\n            - _TRI Filepath_ - Path to the cleaned TRI data csv file aka: output_filepath_name_min_year_max_year.csv\n            - _Output Filepath_ - File path and name for the exported data. Two outputs are produced to compress the number of simulations run. For example if output filepath is labeled data/processed/temp_tri, two outputs would appear within data/processed titled:\n                - `temp_tri_RUN.csv` - input for STILT\n                - `temp_tri_IDMAPPING.csv` - Compression map to find which simulations pair with what chemicals. Selections are removed based upon identical simulation height, lat/long and year (time). \n            - _Min Year_ - An additional year filter (in case you want to run different inputs from the same TRI file)\n            - _Max Year_ - An additional year filter\n        2. RDF File Conversion \n            - _Source Script_ - `src/data/make_stilt_data_2.r`\n            - _Input path_ - Path to the `_RUN.csv` file\n            - _Output path_ - Save path with rds extension. Recommended save in `data/processed/stilt_input/temp_name.rds`\n            - _Random Sample_ - Boolean to indicate whether a subsample should be taken of the original data. [CURRENTLY UNIMPLEMENTED!]\n    - **Outputs:**\n        1. `_RUN.csv` - unique TRI releases based upon lati, long, zagl and year\n        2. `_IDMAPPING.csv` - map to connect stilt simulations back to TRI releases\n        3. `temp_name.rds` - Conversion of _RUN.csv to rds file format with extension of dates from year to all days within the year \n"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.955842376577139,
      "result": {
        "original_header": "Post-Processing",
        "type": "Text_excerpt",
        "value": "These steps become more intensive in terms of processing. It is best to either use slurm batch scripting or an interactive node (see pre-processing for creating an interactive node session) if you choose to run this portion of the program on CHPC \n1. **Pair netCDF Footprints to TRI Releases** \n    - **Description:** <br> In pre-processing, the TRI release amounts were removed to eliminate simulation overlap. Now these concentrations must be multiplied on a lbs/day basis to the stilt 0.01 x 0.01 km flux field. Furthermore, for ease of use, the individual release dataframes are combined into a single output frame for easier post-processing. \n    - **Assumptions:**\n        1. Not yet accounting for leap year\n        2. csv is a workable output\n        3. This is still a slow process. Needs acceleration for large batches to not create a bottleneck. Consider join instead of merge within make_stilt_outputs.py or a better method for compression of data into a single UT_exposure dataframe. \n        4. The footprint files export ppm/flux, summed over the total simulation window timing. When we multiply by lbs/day this gives the mixing ratio. I am not sure that units work particularly well for validation. SO THIS IS SOMETHING THAT SHOULD BE LOOKED AT AGAIN. \n    - **Makefile Command:** `make stilt_input`\n    - **Inputs:**\n        1. _stilt netcdf file path_ - file path to folder of STILT footprints (netcdf .nc format) \n        2. _save file csv_ - filepath and name to save the final csv \n        3. _run file path_ - filepath to the csv file used to create the stilt outputs `_RUN.csv`\n        4. _id mappings file path_ - filepath to the csv file with the TRI mappings ie `_IDMAPPINGS.csv` \n        5. _gridding threshold_ - Threshold to limit values which are included from the netcdf files (exported as a grid with 0's for those grid cells with nothing in them) \n2. **Visualizing Output** \n    - **Description:** <br> In order to visualize and explore outputs from post-processing, a simple jupyter notebook was put together called post_processing.ipynb. Within the notebook, an output .csv can be loaded into a geodataframe and visualized.  \n"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9264495582416085,
      "result": {
        "original_header": "Validation",
        "type": "Text_excerpt",
        "value": "**Parameter Tuning** <br>\nIntially, we believed there was some tuning processes to elucidate the ideal model parameters for our case. To evaluate how particles and smoothing affected model we came up with three metrics.  Note that these parameters also enable comparison between simulations to guage how the shape is changing (IOU between two plumes) and how much stability there is on a cell to cell basis (COV)  \n1. The plume area as determined by convex hull\n2. The mean distance between the source point and grid cells > 0\n3. The coeffecient of variation for the foot of the simulation \nAfter chatting with Ben, it was decided that 1,000 particles was alright for our use case. If we ever need to return to those intial metrics, they are housed in `notebooks/stilt_parameter_tuning.ipynb`. \n**Comparison to Hysplit** <br>\nSTILT and HYSPLIT are both programs capable of tracking particles in time. Initially they started as the same program and have diverged over time. We wanted to sanity check our STILT simulations by comparins HYSPLIT and STILT simulations. This work is done within `notebooks/hysplit_vs_stilt.ipynb` with figures in `figures/hysplit_v_stilt`.  \n**EPA Monitor Validation** <br>\nThere exist some EPA sensors which monitor chemicals within the air back to 1990 and are within reasonable proximity to a TRI release. In order to boost the strength of our validation, we plan to model relevant TRI releases through 2018 with comparable EPA sensor data to validate how well our model estimates chemical concentration. {IN PROGRESS --> see `notebooks/epa_validation.ipynb`} \nThis section is where the most work needs to be done. Specifically, the lead sensors need to be isolated, and simulations run. Then we need to start to distill the signal from noise utilizing windows where lead is being produced and not produced \nNote: This requires an added datafile from the UBOX (TRI_ValidationSet.csv into `data/validation/`) \n"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/Hanson-Research-Lab/TRI_STILT/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/descriptive_analysis.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/descriptive_analysis.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/stilt_parameter_tuning.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/stilt_parameter_tuning.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/post_processing.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/post_processing.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/hysplit_vs_stilt.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/hysplit_vs_stilt.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/epa_validation.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/epa_validation.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/Plotting/NetCDFfilesExample.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/Plotting/NetCDFfilesExample.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/Plotting/PlotFoot.ipynb"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/notebooks/Plotting/PlotFoot.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 0
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/Hanson-Research-Lab/TRI_STILT/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Hanson-Research-Lab/TRI_STILT"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "STILT for TRI Modeling:"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/logo.png"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/figures/hysplit_v_stilt/all_simulations_with_source.png"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Setup:",
        "parent_header": [
          "STILT for TRI Modeling:"
        ],
        "type": "Text_excerpt",
        "value": "All steps create an environment on CHPC to run STILT simulations. \n\n1. **[CHPC](https://www.chpc.utah.edu/documentation/software/r-language.php)**\n    - login to chpc `ssh uXXXXXXXX@XXXXpeak.chpc.utah.edu`\n    - Create a directory for modules (unless you already have one built)\n        - `mkdir ./gl_modules`\n    - Setup a custom R environment to download the required libraries\n        - `module load R`\n        - `mkdir -p ~/gl_modules/myR` (replace if you want, any env is fine except R!!)\n        - `ls /uufs/chpc.utah.edu/sys/modulefiles/CHPC-18/Core/R/` (and look for the most recent version of R. Here will use )\n        - `cp /uufs/chpc.utah.edu/sys/modulefiles/CHPC-18/Core/R/4.0.2.lua ~/gl_modules/myR/`\n        - `mkdir -p ~/RLibs/4.0.2/` (creating a place to install any new libraries)\n        - `vim ~/gl_modules/myR/4.0.2.lua`\n            - add anywhere: \n            - `setenv(\"R_LIBS_USER\",pathJoin(\"/uufs/chpc.utah.edu/common/home\",os.getenv(\"USER\"),\"RLibs\",myModuleVersion()))`\n        - Reload Terminal to update all scripts\n        - Enable Modules (This should be done a new CHPC instance is accessed, unless custom.sh is modified to include these commands) \n            - `module use ~/gl_modules`\n            - `module load myR`\n            - `module load netcdf-c`\n        - To check the installation use `echo $R_LIBS_USER` and make sure this points to your RLibs\n\n2. **[STILT](https://github.com/uataq/stilt)**\n    - Install the library using an R terminal\n        - `install.packages(c(\"rslurm\",\"splitstackshape\"),lib=c(paste(\"/uufs/chpc.utah.edu/common/home/\",Sys.getenv(\"USER\"),\"/RLibs/\",Sys.getenv(\"R_VERSION\"),sep=\"\")), repos=c(\"http://cran.us.r-project.org\"),verbose=TRUE)`\n        - `if (!require('devtools')) install.packages('devtools')`\n        - `devtools::install_github('benfasoli/uataq')`\n    - Create a project in the root directory\n        - `Rscript -e  \"uataq::stilt_init('my_folder_name')\"`\n        - For the purposes of this tutorial, name the project STILT\n    - Test simulation (Necessary to configure all files)\n        - `bash ./test/test_setup.sh`\n        - `bash ./test/test_run_stilt.sh`\n\n3. **Python Virtual Environment on CHPC**  \n    - Load the python version of interest\n        - `which python (view current version of python)`\n        - `module spider python`\n    - Activate python 3.7\n        - `module load python/3.7.3`\n        - `which python` (should see a chpc origin)\n    - Gather python3.7 and install with system site packages (this will create the virtual environment within gl_modules -- mystiltenv)\n        - `python3.7 -m venv --system-site-packages ~/gl_modules/mystilt_env`\n    - Launch the virtual environemnt\n        - `module unload python/3.7.3`\n        - `source ~/gl_modules/mystilt_env/bin/activate.csh`\n    - You should see the virtual environment active - see a (mystilt_env) [uxxxxx@kingspeak1:~]\n\n4. **TRI_STILT**\n    - Install the Repo\n        -`git clone https://github.com/Hanson-Research-Lab/TRI_STILT.git`\n    - Install Libraries\n        - `make clean` cleans the existing python caches\n        - `make requirements` uses pip to install all requirements to run the src code\n            - Currently an issue with RTree and CHPC!\n\n5. **Extracting Utah Data from NARR Files** \n    - If available use the following to sync the UT NARR files: \n        - `rsync -azv /uufs/chpc.utah.edu/common/home/u0890227/STILT/UT_NARR ~/`\n    - If not available, the LAIR group holds versions of NARR data which can be converted to get the same NARR files using [xtrct-grid](https://github.com/benfasoli/xtrct-grid), a software package from Ben Fasoli. \n        1. git clone https://github.com/benfasoli/xtrct-grid.git\n        2. access using python3 ./xtrct-grid/entrypoint.py\n        3. Isolate the NARR data for Utah (based upon these grid points:(45.3210,117,0937) (45.3210,105.0937) (33.3210,117,093) (33.3210,105.0937))\n        4. To run: \n            - create new script within xtrct-grid called batch_extract_grid.py\n            - Insert the following code: \n                ```\n                import glob\n                import subprocess\n                import os\n\n                #Double check this data is still available with Derrick Malia of LAIR\n                import_path = '/uufs/chpc.utah.edu/common/home/lin-group5/NARR/'\n                count=0\n\n                for filename in glob.glob(import_path+'NARR199[0-9]*'):\n                        narr_filename = filename.split('/')[-1]\n                        out_name = \"UT_\"+narr_filename\n\n                        #Take all the names and run them through the python 3 script. This is intended to be run from the xtrct-grid folder  \n                        print(narr_filename)\n                        p=subprocess.Popen('python3 ./entrypoint.py \\\n                                                --input_dir=$INPUT_DIR \\\n                    --input={0} \\\n                    --output_dir=$OUTPUT_DIR \\\n                    --output={1} \\\n                    --xmin=-117.0937 \\\n                    --xmax=-105.0937 \\\n                    --ymin=33.3210 \\\n                    --ymax=45.3210'.format(narr_filename,out_name),stdout=subprocess.PIPE,shell=True)\n\n                        p.wait()\n\n                        print(narr_filename + ' conversion complete')\n                        count +=1\n                ```\n            - create a directory where you want to store the data: \n                - `mkdir /uufs/chpc.utah.edu/common/home/u0890227/STILT/UT_NARR`\n            - Write the input and output directories \n                - `export INPUT_DIR=/uufs/chpc.utah.edu/common/home/lin-group5/NARR`\n                - `export OUTPUT_DIR=/uufs/chpc.utah.edu/common/home/u0890227/STILT/UT_NARR`\n            - Navigate to xtrct-grid directory and execute: \n                - Activate a compute node for ~ 4 hrs\n                - `python3 batch_extract_grid.py`\n\n<br>\n\n---"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9658491616536793,
      "result": {
        "original_header": "STILT for TRI Modeling:",
        "type": "Text_excerpt",
        "value": "<p align=\"center\">\n  <a href=\"https://github.com/Hanson-Research-Lab\">\n    <img src=\"logo.png\"/>\n  </a>\n</p>\n \n"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 1.0,
      "result": {
        "original_header": "Structure &amp; Organization",
        "type": "Text_excerpt",
        "value": "- CHPC HOME DIRECTORY \n    - _STILT_ - Initialized via `Rscript -e  \"uataq::stilt_init('my_folder_name',branch='hysplit-merge')\"`. Bolded names represent changes from the cloned repo\n        - _bin_ - Initialized STILT Directory (no modifications)\n        - _Dockerfile_ - Initialized STILT Directory (no modifications)\n        - _docs_ - Initialized STILT Directory (no modifications)\n        - _README.md_ - Initialized STILT Directory (no modifications)\n        - _exe_ - Initialized STILT Directory (no modifications)\n        - **_out_** - STILT Model Outputs\n            - _particles_ - Particle trajectories and simulation configuration information are packaged and saved in a compressed .rds\n            - _footprints_ - ppm/flux output placed on a netCDF grid (where grid cells are represented by centroids)\n            - _by ids_ - simulation details\n        - **_r_** - Houses scripts used to execute stilt\n            - _run stilt.r_ - Main script utilized to execute STILT. Note, when building out a template in TRI_STILT/src/stilt_run, copy the file but leave the run nomenclature the same\n            - _src_ - Initialized STILT Directory (no modifications) containing all STILT code\n            - _stilt cli.r_ - Initialized STILT script (no modifications)\n            - _dependencies.r_ - Initialized STILT script (no modifications)\n        - _rslurm STILT_ - Initialized STILT Directory (no modifications)\n        - _setup_ - Initialized STILT Directory (no modifications)\n        - _stilt-tutorials_ - Initialized STILT Directory (no modifications) created when the test setup scripts are run\n        - _test_ - Initialized STILT Directory (no modifications) created when the test setup scripts are run\n        - **_UT NARR_** - xtrct-grid for NARR data within the Utah boundaries (+/- 6 degrees lat/long from Utah Center)  \n"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9998368577955611,
      "result": {
        "original_header": "Pre-Processing",
        "type": "Text_excerpt",
        "value": "All steps are built utilizing a make style system. Before running, please edit the `Makefile` with the desired directories and variables. If choosing to run this on CHPC, you will need to use an interactive compute node or slurm job. For an interactive node use `srun --time=1:00:00 --ntasks=16 --nodes=1 --account=hanson --partition=notchpeak --pty /bin/bash -l`.  \n"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9658491616536793,
      "result": {
        "original_header": "Post-Processing",
        "type": "Text_excerpt",
        "value": "<p align=\"center\">\n  <a href=\"https://github.com/Hanson-Research-Lab\">\n    <img src=\"figures/hysplit_v_stilt/all_simulations_with_source.png\"/>\n  </a>\n</p> \n"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9570427978806577,
      "result": {
        "original_header": "Validation",
        "type": "Text_excerpt",
        "value": "This section is where the most work needs to be done. Specifically, the lead sensors need to be isolated, and simulations run. Then we need to start to distill the signal from noise utilizing windows where lead is being produced and not produced \n"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/Hanson-Research-Lab/TRI_STILT/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "MIT License\n\nCopyright (c) 2020 Hanson Research Group\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/LICENSE",
      "technique": "file_exploration"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "TRI_STILT"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "Hanson-Research-Lab"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 144124,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "R",
        "size": 84743,
        "type": "Programming_language",
        "value": "R"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 20183,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Makefile",
        "size": 3534,
        "type": "Programming_language",
        "value": "Makefile"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Running Simulations on CHPC",
        "parent_header": [
          "STILT for TRI Modeling:"
        ],
        "type": "Text_excerpt",
        "value": "1. **Create a run_stilt.r file** \n    - **Description:** <br> In order to run stilt, a run_stilt.r file need to provided to the STILT program which governs all of the configuration [variables](https://uataq.github.io/stilt/#/configuration). For clarity, please create this file under and `src/stilt_run/date_description.r`, changing date and description for the run accordingly. I find it easiest to copy the `/src/stilt_run/template_run.r` file and only change what is necessary within a local editor, than push the changes to github and sync with CHPC.\n    - **Changes:** \n        - _slurm options_ - change the time, account and partition based upon your CHPC account. Account reflects the lab name typically and partition represents the CHPC compute node you will use to run the program.\n        - _run times_ - Add in the name of the rds file from the previous step (`temp_name.rds`). No filepath is necessary here as this file will be moved into the stilt main directory. \n        - _time integrate_ - Changed to TRUE (not default) so STILT will create a sum over the allocated n_hour time window\n        - _xmn xmx ymn ymx and xres_ - changed to reflect the boundary of Utah with a bit of a cushion. \n        - _met directory_ - changed to reflect the directory within the STILT project where the UT NARR data is stored. \n        - _met file format_ - changed to reflect the naming nomenclature of UT_NARR files\n        - _n hours_ - changed to be 24 to reflect the simulations are running for 24 hours following the release\n        - _numpar_ - changed to 1000 on recommendation from Ben Fasoli. For highest efficiency this parameter may need to be tuned (see `notebooks/stilt_parameter_tuning.ipynb` for more on this). \n        - _emisshrs_ - set to 24 to model a continuous release instead of a single release. This may require more particles. \n        - _kmix0_ - variable (50-300). Note this marks one of the instabilities of stilt is nightime meteorology\n\n2. **Starting a STILT Simulation** \n    - **Description:** <br> All steps up to this point have been run within the TRI_STILT project folder. These steps can be run on CHPC on locally, whichever suits you best. The following steps require the CHPC user to be within their home directory (both TRI_STILT and STILT directories should be visible with the `ls` command). This portion showcases all the steps to batch a simulation run out to slurm. I would recommend collapsing all these steps into bash script (sh)\n    - **Procedure**\n        1. Clear any previous STILT outputs\n            - `rm ./STILT/out/particles/*`\n            - `rm ./STILT/out/footprints/*`\n            - `rm ./STILT/out/by_id/*`\n        2. Copy over the file to run stilt and rename to typical stilt convention\n            - `cp ./TRI_STILT/src/stilt_run/date_description.r ./STILT/r/run_stilt.r`\n        3. Copy over the data for the run\n            - `cp ./TRI_STILT/data/processed/stilt_input/temp_name.rds ./STILT/`\n        4. Load module and run the program \n            - `module use ~/gl_modules`\n            - `module load netcdf-c`\n            - `module load myR`\n            - `cd ./STILT/`\n            - `Rscript r/run_stilt.r`\n        5. STILT automatically batches in the simulations to CHPC (Thanks Ben and UATAQ team!)\n    - **CHPC Helpful Commands** \n        - `squeue -u uXXXXXXX` : shows jobs related to you \n        - `sinfo` - status of different partitions\n        - `scancel job_id` - cancel a job\n        - `scontrol show job job_number` - shows information about a run\n\n3. **Collecting Simulation Footprints** \n    - **Description:** <br> As the job processes simulation data will be processed into the `./STILT/out/footprints/` directory. Once completed (check with squeue - should see no active jobs) the data needs to be moved to TRI_STILT for storage and post-processing. This involves a quick move of the data! Again, this can be modified into a single bash script or kept single. Whichever works best for you!\n    - **Procedure:**\n        1. Make a new directory for the simulation run within TRI_STILT\n            - `mkdir ./TRI_STILT/data/processed/stilt_output/netcdf/run_name`\n        2. Copy the files into this new directory\n            - `cp ./STILT/out/footprints/* ./TRI_STILT/data/processed/stilt_output/netcdf/run_name/`\n\n<br>\n\n---"
      },
      "source": "https://raw.githubusercontent.com/Hanson-Research-Lab/TRI_STILT/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-03 23:08:41",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 0
      },
      "technique": "GitHub_API"
    }
  ]
}