{
  "application_domain": [
    {
      "confidence": 66.29,
      "result": {
        "type": "String",
        "value": "Natural Language Processing"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/tawells/PETM_LDA"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-08-28T13:54:00Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-03-07T18:02:51Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Using LDA to uncover topics in Paleocene Eocene Thermal Maximum research"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9234257498485423,
      "result": {
        "original_header": "An exploration into using unsupervised learning methods to pull out the themes or topics of a large body of scientific text.",
        "type": "Text_excerpt",
        "value": "\nGlobal warming, attributed to increasing carbon dioxide in the atmosphere, causes climate shifts and sea-level rises that have great social and economic impacts on populations.   \nGeologists have studied these ancient effects to better understand what is happening today. \nOne example is here in Bastrop County outside of Austin, TX.  Resulting publications that directly address this topic are now covering a wide range of data types, from sediments to fossils to carbon isotopes, but with uneven worldwide coverage. \n \nAn evaluation and summary of this data is currently being undertaken to provide a snapshot of current knowledge and to suggest future research directions.   With the amount of information involved there is the risk of under-emphasizing seminal publications, over-emphasizing peripheral publications, seeing patterns where none exist, and missing links where they do exist.  \n__Resources Used:__\n- Data Collection and Cleaning\n  - pyPDF2\n  - Gensim\n  - text cleaning\n- Unsupervised Learning\n  - NLP\n  - Topic Modeling\n  - Word2Vec \n"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9813301824832094,
      "result": {
        "original_header": "Objective",
        "type": "Text_excerpt",
        "value": "__Using Natural Language Processing and Machine Learning, what can we learn about latent knowledge from Paleocene Eocene boundary research and can this give us future avenues to explore?__ \n"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9706431889157349,
      "result": {
        "original_header": "Data",
        "type": "Text_excerpt",
        "value": "__1. Find source for PETM research__\n  - The data source was generated out of documents collected over 30 years by Christoper Denison PhD. The issue lies in that a preponderence of scientific knowledge is in the form of text, housed in journals and publications.  In the last few years research has now been able to be read online and downloaded with a fee.  The pdf downloads are not consistent and most researchers turn to scanning their old journals.  \n__2. Turn the pdfs of research into usable text__\n  - To generate a text file of the pdf's, I used the package pyPDF2 to read pdf's individually and save them, again individually into their own text files. \n```python\npdfFiles = []\n\nfor filename in glob.glob('./data/input_pdfs/**/*.pdf', recursive = True):  \n    if filename.endswith('.pdf'):   \n        pdfFiles.append(filename) \n        \nfor filename in pdfFiles:\n   \n    try:\n        f =  open(f'{filename}', 'rb') \n        f2 = open(f'./data/output_txt/{filename.rsplit(\"/\",1)[-1][:-4]}.txt', 'w') \n        pdf = PdfFileReader(f)\n        print(f'reading {filename}')\n\n        for pageNum in range(0,pdf.numPages):\n            page = pdf.getPage(pageNum)\n            text = page.extractText()\n            f2.write(text)\n            #print(f'page{pageNum}')\n    except:\n        print (f\"busted file {filename}\")\n```\n \n__3. Data Cleaning__\n  - Once each pdf had been read and saved as an individual text file it was necessary to clean up the text a bit. I chose to use the preprocessing module and example from gensim documentation to strip out punctuation, numbers, short words, multiple whitespaces and english stopwords.   I also removed 115 corpus specific stop words.   These were journal names, research places, publisher names and image data.  I chose to leave in the reference section of each document.  \n  \n  ```python\nfrom gensim import utils\nimport gensim.parsing.preprocessing as gsp\n\nfilters = [\n    gsp.strip_tags,     \n    gsp.strip_punctuation,\n    gsp.strip_numeric,\n    gsp.remove_stopwords, \n    gsp.strip_short, \n    gsp.stem_text,\n    gsp.strip_multiple_whitespaces \n          ]\n\ndef clean_text(s):\n    s = s.lower()\n    s = utils.to_unicode(s)\n    for f in filters:\n        s = f(s)\n    return s\n``` \n  -  There were a total of 2031 documents out of over 10,000 that eventually were usable in modeling. It turned out that 80% of the pdf's had read in as blank text files.   Most were due to the way the document had been orginally scanned into a pdf.   Some were screen shots from the internet and some were just unreadable due to age, such as one small book from 1860.     \n"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9274017495656426,
      "result": {
        "original_header": "Topic Modeling",
        "type": "Text_excerpt",
        "value": "To conduct topic modeling, Latent Dirichlet Allocation (LDA) was run from two different python libraries.  \n  - sklearn\n  - gensim\n  \nAim of using the LDA method is to exhibit latent topics, which are distributions of word frequency, among all documents \nEach model type was fed the TFIDF vectorizer results. At first, the  decision on how many number of topics to use was difficult to isolate on where to draw the line. To help this, I performed a gridsearch in sklearn and used the coherence and perplexity scores to map how well the topics were being clustered in relevant groups. \n"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9541364081077324,
      "result": {
        "original_header": "Futurework",
        "type": "Text_excerpt",
        "value": "There is need to continue this work for the geology/sedimentology scientific research.  What we can discover can help catapult the discovery of latent knowledge in the research.  In the future this project will encorporate the following.  \n- PDF reader debugging or find a new reader that can handle a wider range of pdf images and translate into text.  \n- Train / test using prior work to train models and the lastest work to test and model for predictions\n- Utilize Word2Vec and Doc2Vec more thoroughly to pull out words and associated papers that look unique in the topic environments.  \n"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9853313207787192,
      "result": {
        "original_header": "Conclusion",
        "type": "Text_excerpt",
        "value": "Topics were able to be assigned to each document and showed a distinct and unique flow.  This does not indicate there were any gaps associated in research at this time.  Training a larger vocabulary associated with PETM research can improve these results.    \n"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/tawells/PETM_LDA/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/Word2Vec.ipynb"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/Word2Vec.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/Sklearn%20-%20cos.ipynb"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/Sklearn%20-%20cos.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/data_collection.ipynb"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/data_collection.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/Gensim%20LDA.ipynb"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/Gensim%20LDA.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/data_cleaning.ipynb"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/data_cleaning.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 0
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/tawells/PETM_LDA/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "tawells/PETM_LDA"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Latent knowledge in the Paleocene Eocene boundary research"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9088157364879714,
      "result": {
        "original_header": "An exploration into using unsupervised learning methods to pull out the themes or topics of a large body of scientific text.",
        "type": "Text_excerpt",
        "value": "__Python Packages Used:__\n- Pandas\n- sklearn\n- gensim\n- NLTK\n- regex\n- pyLDAvis \n"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8668735544377922,
      "result": {
        "original_header": "Data",
        "type": "Text_excerpt",
        "value": "__2. Turn the pdfs of research into usable text__\n  - To generate a text file of the pdf's, I used the package pyPDF2 to read pdf's individually and save them, again individually into their own text files. \n```python\npdfFiles = []\n\nfor filename in glob.glob('./data/input_pdfs/**/*.pdf', recursive = True):  \n    if filename.endswith('.pdf'):   \n        pdfFiles.append(filename) \n        \nfor filename in pdfFiles:\n   \n    try:\n        f =  open(f'{filename}', 'rb') \n        f2 = open(f'./data/output_txt/{filename.rsplit(\"/\",1)[-1][:-4]}.txt', 'w') \n        pdf = PdfFileReader(f)\n        print(f'reading {filename}')\n\n        for pageNum in range(0,pdf.numPages):\n            page = pdf.getPage(pageNum)\n            text = page.extractText()\n            f2.write(text)\n            #print(f'page{pageNum}')\n    except:\n        print (f\"busted file {filename}\")\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8394788159987951,
      "result": {
        "original_header": "Topic Modeling",
        "type": "Text_excerpt",
        "value": "\n```python\nfrom gensim.models.coherencemodel import CoherenceModel\n\ncoherence_model_lda = CoherenceModel(model=lda_model,\n                                     texts=texts,\n                                     dictionary=dictionary,\n                                     coherence='c_v')\n\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('Coherence Score: ', coherence_lda)   # higher is better\nprint('Perplexity:', lda_model.log_perplexity(corpus))  #lower is better\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/tawells/PETM_LDA/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "PETM_LDA"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "tawells"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 217610,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "HTML",
        "size": 85603,
        "type": "Programming_language",
        "value": "HTML"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/tawells/PETM_LDA/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-03 23:51:16",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 0
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "notebook-application"
      },
      "technique": "software_type_heuristics"
    }
  ]
}