{
  "application_domain": [
    {
      "confidence": 0.9457348466603406,
      "result": {
        "type": "String",
        "value": "Graphs"
      },
      "technique": "supervised_classification"
    },
    {
      "confidence": 73.73,
      "result": {
        "type": "String",
        "value": "Natural Language Processing"
      },
      "technique": "supervised_classification"
    },
    {
      "confidence": 70.78,
      "result": {
        "type": "String",
        "value": "Reinforcement Learning"
      },
      "technique": "supervised_classification"
    },
    {
      "confidence": 8.05,
      "result": {
        "type": "String",
        "value": "Audio"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/stevenrouk/evolution-of-machine-learning"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-10-14T15:54:48Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2023-08-24T11:44:08Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Using natural language processing techniques such as topic modeling to analyze the evolution of machine learning as a field, using ArXiv research paper descriptions as the dataset."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9696034520035174,
      "result": {
        "original_header": "Steven Rouk - The Evolution of Machine Learning",
        "type": "Text_excerpt",
        "value": "_Analysis of the evolution of the field of machine learning as discovered through natural language processing (NLP) techniques applied to research papers on [arXiv.org](https://arxiv.org/)._ \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9905607471614792,
      "result": {
        "original_header": "Goal of the Project",
        "type": "Text_excerpt",
        "value": "The goal of this project was to identify subfields of machine learning through topic modeling, and to see how interest in those subfields has changed over time.\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9629455796298109,
      "result": {
        "original_header": "A Brief History",
        "type": "Text_excerpt",
        "value": "It's no overstatement to say that machine learning\u2014and data science more broadly\u2014is revolutionizing our society. \nJust for a bit of perspective: \n- The first web browser was made available to the public in 1991.\n- Deep Blue (by IBM) beat Garry Kasparov in chess in 1997.\n- Google was founded in 1998.\n- The first iPhone was released in 2007.\n- Watson (once again by IBM) beat Jeopardy champions Brad Rutter and Ken Jennings in 2011.\n- AlphaGo (by Google/DeepMind) beat a 9-dan professional Go player in 2016.\n- The first self-driving car completed the first DARPA Grand Challenge in 2005. By the end of 2016, Google's fleet of self-driving cars had completed over 2,000,000 autonomous miles.\n- In 2018, an NLP AI by Alibaba outperformed Stanford students on a reading and comprehension test, and Google releases \"Duplex\", an AI assistant with an incredibly human-like voice that can make reservations for you. \nAnd keep in mind that thirty years ago, most homes in the US didn't have a personal computer. Ten years ago, most people in the US weren't using smartphones. These days, the computing power available to the average person is astronomically more than was available in previous years, which is democratizing access to the computing power needed to accomplish incredible technological feats using data. As one article puts it, thanks to Moore's Law [\"Your smartphone is millions of times more powerful than all of NASA\u2019s combined computing in 1969\"](https://www.zmescience.com/research/technology/smartphone-power-compared-to-apollo-432/). \nWhereas previously you needed supercomputers and teams of researchers to create algorithms even capable of recognizing hand-written digits, these days millions of people have the computing power needed to create sophisticated facial recognition algorithms. (And for those who don't possess the computing power on their personal machines, they can simply purchase it through platforms like [AWS](https://aws.amazon.com/machine-learning/).) And according to Google search volume, people are taking notice. \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9928316380092036,
      "result": {
        "original_header": "A Rapidly Evolving Field",
        "type": "Text_excerpt",
        "value": "With this kind of quickly changing technological landscape, I was curious as to the characteristics of machine learning and how those have changed over the last few decades. As a data scientist and machine learning practitioner, I'm constantly looking for ways to better understand the field and keep up with developments. If I could find a way to analyze the recent trajectory of machine learning, I would be in a better place from which to put it to good use.\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9724762908470502,
      "result": {
        "original_header": "The Data",
        "type": "Text_excerpt",
        "value": "To get a sense of the evolution of machine learning, I turned to the research paper hosting website [arXiv.org](https://arxiv.org/) (which is a service of Cornell University), a site which bills itself as \"Open access to 1,605,550 e-prints in the fields of physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics.\" (Note: Thanks to [github.com/niderhoff/nlp-datasets](https://github.com/niderhoff/nlp-datasets) for pointing me in the right direction when I was looking for datasets.) \nArXiv graciously makes the metadata for their whole collection of research papers available through an open API, which meant that I could download descriptions for all 1.6 million papers. For a single API call, the metadata was returned as an XML object with 1000 records, where each record looked like this: \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9944880688082597,
      "result": {
        "original_header": "Pulling the Data",
        "type": "Text_excerpt",
        "value": "The first task was to pull all of the metadata and store it locally for analysis. A quick back-of-the-napkin calculation told me I should expect the full data pull to run about 3 GB in size, so I chose to store each response as a raw XML text file initially for later processing and analysis. In total, I made over 1,600 API calls over the course of 24 hours to pull all of the data.\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.930444214097412,
      "result": {
        "original_header": "Processing the Data",
        "type": "Text_excerpt",
        "value": "While the full dataset was downloading, I wrote scripts to process the XML data into a series of structured CSV files with just the information I wanted: id, url, title, set, subjects, authors, date, and description. I ended up primarily using title, set, subjects, date, and description in my analysis, although I would like to conduct further analysis using authors. \nAfter processing the XML files to individual CSVs, I used another script to combine all of these CSVs into a single CSV that could be directly loaded as a pandas DataFrame. I created another CSV file with just research papers that had the phrase \"machine learning\" in one of their subjects. \nThe final processed CSV file ended up being 1.7 GB, and the machine learning subset CSV file ended up as 81 MB. (This was down from a raw data size of 3.1 GB.)\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9307748394034012,
      "result": {
        "original_header": "Exploratory Data Analysis (EDA)",
        "type": "Text_excerpt",
        "value": "As you can see, the majority of the papers published on arXiv are physics papers\u2014although according to their own analysis, this might be changing with the rise of machine learning. (See https://arxiv.org/help/stats/2018_by_area/index for more.) \nWhat I was really interested in was the research papers specifically related to machine learning. I filtered the data down to just these papers by finding all papers that had \"machine learning\" as part of one of the subjects listed. There were 48,564 such papers, which accounted for about 3% of the total number of papers pulled from the site. The distribution of the subjects listed for these papers is shown below. \nAlthough the two subjects with \"machine learning\" in them are at the top of the list (which we would expect), the subsequent top subjects give us some view into what various subfields of machine learning might be: computer vision, language, optimization, robotics, etc. \nThis subset of over 48,000 paper descriptions was the corpus that I worked with after this point.\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.950638951762954,
      "result": {
        "original_header": "Machine Learning Papers Over Time",
        "type": "Text_excerpt",
        "value": "One of the most startling results from this analysis was also one of the simplest: looking at the number of machine learning papers published over time. \nIt's hard to underscore how significant of a shift this is. Not only has machine learning already had a monumental impact on society, but each year more time and energy is devoted to developing it as a field. It's hard to imagine what the next ten to twenty years are going to bring with this kind of investment being poured into machine learning. \nAnd, fittingly, this underscores the need for applying techniques like machine learning to understand the field of machine learning!\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.974798609024081,
      "result": {
        "original_header": "Topic Modeling",
        "type": "Text_excerpt",
        "value": "The heart of the analysis involving using the natural language processing (NLP) technique of topic modeling to discover latent topics in the corpus of research paper descriptions. (Note: I didn't include the paper titles in my model, although I did display them as part of the analysis of my topics.) By using topic modeling to understand the research papers, I was essentially trying to discover cohesive subfields of machine learning. I used the technique of non-negative matrix factorization (NMF) to accomplish this.\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9170292553325903,
      "result": {
        "original_header": "Text Featurization and Model Hyperparameters",
        "type": "Text_excerpt",
        "value": "I was able to achieve surprisingly good results with very little preprocessing or fine-tuning. The steps shown below comprised my primary analysis pipeline for the majority of the project. \n1. **TfidfVectorizer** \u2014 First, I converted the paper descriptions into tf-idf vectors using scikit-learn's TfidfVectorizer. I removed stop words as part of this process.\n2. **NMF Model** \u2014 Then, I fit an NMF model using the number of topics I was interested in fitting. For the majority of the analysis, I looked at 10 topics and got good results\u2014however, I also looked at 3, 15, 20, and 30 topics, which gave additional insight into the data.\n3. **Topic Modeling Matrices** \u2014 Finally, I used the factored matrices `W` (the document-topic matrix) and `H` (the vocabulary-topic matrix) to look at the relationship between the text and the latent topics I had extracted. \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9638195450889273,
      "result": {
        "original_header": "10 Topics - Relevant Words",
        "type": "Text_excerpt",
        "value": "So what topics was I able to discover? Using an NMF hyperparameter of 10 topics, these were the words most indicative of the topics: \n| Topic Number | Subjective Topic Name | Topic Words |\n| --- | --- | --- |\n| 0 | machine learning / time series | data learning machine time real analysis methods series classification sets |\n| 1 | gradient / optimization / convergence | optimization gradient convex matrix convergence stochastic problems method rank descent |\n| 2 | neural networks / deep learning | neural networks network deep training layer convolutional layers architecture architectures |\n| 3 | reinforcement learning | learning policy reinforcement agent rl agents control policies tasks reward |\n| 4 | variational bayesian | model models inference latent bayesian variational variables distribution gaussian posterior |\n| 5 | graphs / graph ML | graph graphs node nodes embedding structure edges network embeddings spectral |\n| 6 | ML attacks / GANs | adversarial attacks examples attack training robustness perturbations generative gan gans |\n| 7 | image / text / classification | image task classification domain features images tasks model feature text |\n| 8 | clustering | clustering clusters cluster means algorithm spectral data algorithms points mixture |\n| 9 | algorithms / regret / optimization | algorithm regret bounds bound problem optimal sample algorithms complexity lower | \nAlthough a couple of these topics don't seem as clear (e.g. topics 0 and 9), most of these topics are fairly cohesive and give insight into the development of machine learning as a field over the last 20 years. \nAdditionally, there's at least one topic that is a bit mixed or confused: topic #6 appears to be about both vulnerabilities of machine learning algorithms (\"adversarial attacks\") and generative adversarial networks (GANs). Since both of these topics rely heavily on the term \"adversarial\", this topic appears to be a mixture of the two concepts.\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9151714683745645,
      "result": {
        "original_header": "10 Topics - Relevant Papers / Documents",
        "type": "Text_excerpt",
        "value": "For each one of these topics that we've discovered in the corpus of paper descriptions, there are certain papers that \"load\" most heavily on that topic\u2014in other words, they contain words that are strongly related to that topic. \nFor the **Neural Networks and Deep Learning** topic, here are the titles of the papers that load most heavily on that topic: \n| Most Relevant Papers: \"Neural Networks / Deep Learning\" |\n| --- |\n| Deep Neural Network Approximation using Tensor Sketching |\n| A Survey: Time Travel in Deep Learning Space: An Introduction to Deep Learning Models and How Deep Learning Models Evolved from the Initial Ideas |\n| Deep Recurrent Convolutional Neural Network: Improving Performance For Speech Recognition |\n| Deep Adaptive Network: An Efficient Deep Neural Network with Sparse Binary Connections |\n| Deep Fried Convnets | \n| Most Relevant Papers: \"Reinforcement Learning\" |\n| --- |\n| Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning? |\n| Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning |\n| Scalable Centralized Deep Multi-Agent Reinforcement Learning via Policy Gradients |\n| Meta reinforcement learning as task inference |\n| Multi-Task Policy Search | \nAnd for the **Graphs and Graph ML** topic (one of my favorites! See my previous project on graphs: [Finding Patterns in Social Networks Using Graph Data](https://github.com/stevenrouk/social-network-graph-analysis)), here are the titles: \n| Most Relevant Papers: \"Graphs and Graph ML\" |\n| --- |\n| A simple yet effective baseline for non-attributed graph classification |\n| A Unified Framework for Structured Graph Learning via Spectral Constraints |\n| Adaptive Graph Convolutional Neural Networks |\n| Triple2Vec: Learning Triple Embeddings from Knowledge Graphs |\n| Adversarially Regularized Graph Autoencoder for Graph Embedding |\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9646075249688613,
      "result": {
        "original_header": "10 Topics - Document Analyses",
        "type": "Text_excerpt",
        "value": "If we wanted to analyze a specific document according to these topics and the relevant words, we can do that too. Here's an analysis of the \"Graph Convolutional Reinforcement Learning\" paper using these 10 topics: \n<img src=\"images/document_topic_analysis.png\" alt=\"analysis of topic loadings for a single document\"> \n<sub><b></b> Topic Analysis of \"Graph Convolutional Reinforcement Learning\" </sub> \nAnd here's what it looks like when we highlight the words in this paper that are the most relevant to each topic: \n<sub><b></b> Word Analysis of \"Graph Convolutional Reinforcement Learning\" </sub>\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9938011562602609,
      "result": {
        "original_header": "10 Topics - Evolution Over Time",
        "type": "Text_excerpt",
        "value": "And now, we're at a place where we can answer one of the original questions I was wondering about what I started this project: how has the field of machine learning changed over time? Armed with our ten topics and a fairly good understanding of what these topics represent, we can see how these topics have evolved over the last 20 years. \n1. The explosion of interest in neural networks and deep learning. These models have been responsible for much of the news-worthy progress we've seen in the last few years, and these approaches are becoming more feasible with time and computing power increases and we figure out how to leverage GPUs for much faster processing of matrices.\n2. The growth of the GAN / ML attack category. It would take a little more teasing apart to see which of those two areas was most responsible for this growth, although both topics have seen more interest in recent years. (And GANs were only invented in 2014.)\n3. The subtle downturn of topics like optimization, clustering, and a few other topics.\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9188052984930377,
      "result": {
        "original_header": "20 Topics",
        "type": "Text_excerpt",
        "value": "Some other interesting topics show up if we use 20 topics. Here were some of the additional topics that came out of using an NMF model with 20 topics: \nIt's interesting to see which of these topics have exploded in recent years (e.g. anything related to neural nets), and which went through swells and declines (e.g. matrices, kernels).\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9255002936810521,
      "result": {
        "original_header": "Three Topics Per Year",
        "type": "Text_excerpt",
        "value": "If we restrict ourselves to only three latent topics, and we run a model for every year since 2000, some interesting results show up. Following are the words for the three topics for all years between 2000 and 2019. It appears that 2019 is the year of the graph! \n| 2002 | 2003 |\n| --- | --- |\n| keyphrases genex 97 task algorithm document generated experiments c4 set | prediction mu xi nu bayes sequence environments l_ universal based |\n| context problem strategies training domain weather domains set features sensitive | learning information algorithm control chances semantic function problem em data |\n| learning data algorithm bias semantic information algorithms orientation accuracy using | controller neural genetic failure safe free algorithms hybrid pendulum inverted | \n| 2004 | 2005 |\n| --- | --- |\n| learning time algorithm models markov function paper series based method | prediction loss class benchmark bound observations algorithm mdl forecasting space |\n| mdl prediction universal bayes classes convergence class loss sequence countable | distributed models information algorithm network learning classification model neural technique |\n| semantic information distribution similarity mutual level content data applications hylos | experts expert master actions adversary universal specify algorithm problems algorithms | \n| 2006 | 2007 |\n| --- | --- |\n| learning method data algorithm approach model based user using results | model learning algorithm distribution complexity generalization random problem error prediction |\n| stochastic languages rational dees ma automata study class probabilistic multiplicity | technique cover adopted 1a1 1aa land svms mapping classification approaches |\n| prediction strategy loss strategies class universal continuous line measures stationary | data analysis clustering methods method variables parameters number real sparse | \n| 2008 | 2009 |\n| --- | --- |\n| lasso model models selection variables regression estimator sample random dimensional | learning data algorithm algorithms model based method time new problem |\n| data clustering algorithms points network networks quantum results model proposed | regression selection linear kernel variables lasso dimensional models sparse model |\n| learning algorithm method classification kernel problem performance methods information task | matrix rank entries algorithm low problem matrices completion subset collaborative | \n| 2010 | 2011 |\n| --- | --- |\n| method classification data kernel feature sparse features regression based methods | data learning model clustering models based approach method classification kernel |\n| clustering data model models graph inference latent graphical networks clusters | regret policy learning online optimal algorithm problem reward bandit bounds |\n| learning algorithm problem regret online algorithms functions policy function bounds | matrix rank sparse norm lasso low recovery convex problem optimization | \n| 2012 | 2013 |\n| --- | --- |\n| sparse problem algorithm optimization convex matrix problems function algorithms convergence | model models bayesian data inference variables parameters approach latent likelihood |\n| models model bayesian inference variables latent structure network networks data | matrix algorithm problem sparse convex rank optimization bounds algorithms convergence |\n| data learning classification feature based clustering method features methods proposed | learning data classification features training methods feature clustering algorithms method | \n| 2014 | 2015 |\n| --- | --- |\n| algorithm learning algorithms optimization problem function problems stochastic bounds optimal | data model models clustering inference method methods based bayesian approach |\n| model data models network learning networks features classification training deep | deep neural networks learning network training convolutional image model tasks |\n| matrix rank clustering low data norm completion sparse matrices algorithm | algorithm convex optimization problem algorithms matrix problems stochastic learning function | \n| 2016 | 2017 |\n| --- | --- |\n| data model models method based methods learning approach kernel clustering | data model models learning approach method based methods graph features |\n| neural networks network deep learning training convolutional image tasks recurrent | algorithm gradient optimization algorithms stochastic problem convex convergence problems matrix |\n| algorithm optimization convex algorithms gradient convergence problem stochastic problems non | networks neural deep network training learning adversarial convolutional tasks image | \n| 2018 | 2019 |\n| --- | --- |\n| learning data model network deep neural models networks training based | data model models learning training network neural deep networks image |\n| algorithm gradient optimization algorithms problem function stochastic convergence convex problems | algorithm learning algorithms policy optimization problem gradient function optimal problems |\n| adversarial attacks examples attack training networks robustness perturbations gan generative | graph graphs node embedding nodes embeddings network representation structure networks |\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9979398554374195,
      "result": {
        "original_header": "Querying Loadings",
        "type": "Text_excerpt",
        "value": "What if we ask the question, \"which paper is most closely aligned with a certain combination of topics?\" For example, we might be interested in the paper that is most purely about graph ML and not much else. We can query our results by specifying the loading combination we're interested in and seeing which document is closest (via cosine similarity) to that query:\n```\nLoadings:\n\n**************************************************\n>>> [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n**************************************************\n\nMost Similar Papers:\n\n>>> Triple2Vec: Learning Triple Embeddings from Knowledge Graphs\n>>> Topology Based Scalable Graph Kernels\n>>> Fast Haar Transforms for Graph Neural Networks\n>>> Tripartite Heterogeneous Graph Propagation for Large-scale Social Recommendation\n>>> Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8742102915070176,
      "result": {
        "original_header": "Paper Recommender",
        "type": "Text_excerpt",
        "value": "We can also create a simple research paper recommender system based on the papers that people are interested in. Given a research paper that someone enjoyed (or maybe they didn't enjoy it, but they need to learn more about the topic anyway), we can return other similar papers. \nFor example, what are the closest papers to \"Evolving controllers for simulated car racing\"?\n```\nOriginal Paper:\n\n**************************************************\n>>> Evolving controllers for simulated car racing\n**************************************************\n\nMost Similar Papers:\n\n>>> N2N Learning: Network to Network Compression via Policy Gradient Reinforcement Learning\n>>> Neural Network Memory Architectures for Autonomous Robot Navigation\n>>> Learning by Stimulation Avoidance: A Principle to Control Spiking Neural Networks Dynamics\n>>> Efficient Architecture Search by Network Transformation\n>>> Deep Recurrent Q-Learning vs Deep Q-Learning on a simple Partially Observable Markov Decision Process with Minecraft\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9090466176119816,
      "result": {
        "original_header": "Macro Subject Predictor",
        "type": "Text_excerpt",
        "value": "I wanted to get at least a little predictive modeling in here (just for fun), so I trained a Naive Bayes classifier on the \"macro\" subjects for the full 1.6 million paper corpus\u2014in other words, using the description of a paper to predict if that paper was related to physics, math, computer science, statistics, etc. \nWithout much tuning, I got an accuracy of almost 90% on unseen data (89.4%)\u2014pretty good for a first go!\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9292599277304292,
      "result": {
        "original_header": "Differentiating Terms for Macro Subjects",
        "type": "Text_excerpt",
        "value": "| CS, but not Math | Math, but not CS | Statistics, but not Math |\n| --- | --- | --- |\n| algorithm | prove | data |\n| data | space | models |\n| network | group | methods |\n| based | theory | analysis |\n| information | finite | approach |\n| proposed | function | based |\n| channel | give | algorithm |\n| systems | result | distribution |\n| networks | functions | proposed |\n| used | g | regression |\n| algorithms | algebra | used |\n| each | equation | statistical |\n| performance | equations | estimation |\n| approach | class | bayesian |\n| present | x | use |\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.923602080166634,
      "result": {
        "original_header": "K-Means Clustering and t-SNE",
        "type": "Text_excerpt",
        "value": "K-means is considered a \"hard clustering\" algorithm because, unlike topic modeling using NMF, data points can only belong to one cluster. K-means clustering revealed fairly similar cluster topics to NMF. Plotting the two-dimensional t-SNE representation of the clusters (using a randomly sampled subset of the data) reveals that some clusters are much more distinct than others. \n<sub><b></b> t-SNE representation of K-Means Clusters </sub> \nCluster 1 (the orange cluster isolated in the top of the graph) relates to reinforcement learning and robotics, Cluster 0 (dark blue in bottom left) relates to graphs and graph machine learning, and Cluster 9 (light blue in bottom center) relates to clustering. \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9985683505850862,
      "result": {
        "original_header": "Conclusion",
        "type": "Text_excerpt",
        "value": "Not only was this project interesting from a technical perspective, but I was incredibly curious about the results of the analysis as well because of my work in data science and machine learning. Through the application of NLP techniques, I've created a handy tool for myself (and hopefully a useful analysis for others!) to serve as a guide for various topics and subfields of machine learning. (I know I'll be diving more into neural networks in the coming months\u2014something I've been intending to do for a while anyway.) I've also rekindled an interest in keeping up with the latest research papers coming out.\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9207225783623525,
      "result": {
        "original_header": "Future Research",
        "type": "Text_excerpt",
        "value": "Here are some ideas for future research in this area: \n1. Bring in information about citation networks to see which papers are most cited, and how the topics of those papers are related to the overall evolution of the topic distributions I discovered.\n2. Identify which papers were the most ahead of their time (and possibly influential), in that they directly predated a surge of interest in an area.\n3. Analyze the distribution of authors of papers as it relates to topics.\n4. Create a better predictive model. Potentially try to predict sub-categories (such as the \"subjects\" field categories).\n5. Look at which terms differentiate various topics from each other. I did this for macro subjects, but would be interested in applying this technique to subfields of machine learning.\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9393687538750122,
      "result": {
        "original_header": "Appendix #2: Project Organization",
        "type": "Text_excerpt",
        "value": "This was the first project that I tried using the Data Science Cookie Cutter template for, and I think it helped the project organization overall! (Of course my code still exploded in the middle as I dove headlong into the analysis...) \nHere's a rough organization of how the project is setup. (It doesn't follow this template exactly, but it's fairly close.) \n    \u251c\u2500\u2500 LICENSE\n    \u251c\u2500\u2500 Makefile           <- Makefile with commands like `make data` or `make train`\n    \u251c\u2500\u2500 README.md          <- The top-level README for developers using this project.\n    \u251c\u2500\u2500 data\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 external       <- Data from third party sources.\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 interim        <- Intermediate data that has been transformed.\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 processed      <- The final, canonical data sets for modeling.\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 raw            <- The original, immutable data dump.\n    \u2502\n    \u251c\u2500\u2500 docs               <- A default Sphinx project; see sphinx-doc.org for details\n    \u2502\n    \u251c\u2500\u2500 models             <- Trained and serialized models, model predictions, or model summaries\n    \u2502\n    \u251c\u2500\u2500 notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n    \u2502                         the creator's initials, and a short `-` delimited description, e.g.\n    \u2502                         `1.0-jqp-initial-data-exploration`.\n    \u2502\n    \u251c\u2500\u2500 references         <- Data dictionaries, manuals, and all other explanatory materials.\n    \u2502\n    \u251c\u2500\u2500 reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 figures        <- Generated graphics and figures to be used in reporting\n    \u2502\n    \u251c\u2500\u2500 requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n    \u2502                         generated with `pip freeze > requirements.txt`\n    \u2502\n    \u251c\u2500\u2500 setup.py           <- makes project pip installable (pip install -e .) so src can be imported\n    \u251c\u2500\u2500 src                <- Source code for use in this project.\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py    <- Makes src a Python module\n    \u2502   \u2502\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 data           <- Scripts to download or generate data\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 make_dataset.py\n    \u2502   \u2502\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 features       <- Scripts to turn raw data into features for modeling\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 build_features.py\n    \u2502   \u2502\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 models         <- Scripts to train models and then use trained models to make\n    \u2502   \u2502   \u2502                 predictions\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 predict_model.py\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 train_model.py\n    \u2502   \u2502\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 visualization  <- Scripts to create exploratory and results oriented visualizations\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 visualize.py\n    \u2502\n    \u2514\u2500\u2500 tox.ini            <- tox file with settings for running tox; see tox.testrun.org \n<p><small>Project based on the <a target=\"_blank\" href=\"https://drivendata.github.io/cookiecutter-data-science/\">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>\n \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/stevenrouk/evolution-of-machine-learning/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/Presentation%20Materials.ipynb"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/Presentation%20Materials.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/N-Grams.ipynb"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/N-Grams.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/EDA.ipynb"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/EDA.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/Coherence%20Score.ipynb"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/Coherence%20Score.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/Naive%20Bayes%20-%20Predict%20Macro%20Category.ipynb"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/Naive%20Bayes%20-%20Predict%20Macro%20Category.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/Experiments.ipynb"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/Experiments.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/Document%20Analysis%20Report.ipynb"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/Document%20Analysis%20Report.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/K-Means.ipynb"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/K-Means.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/Topic%20Loadings%20Over%20Time.ipynb"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/notebooks/Topic%20Loadings%20Over%20Time.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/stevenrouk/evolution-of-machine-learning/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "stevenrouk/evolution-of-machine-learning"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Steven Rouk - The Evolution of Machine Learning"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/main.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/run_webapp.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/make_boxplots.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/src/database/load_database.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/deep_blue.jpeg"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/homemade_self_driving_car.jpg"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/google_trends_machine_learning.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/arXiv.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/arXiv_XML.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/number_of_papers_per_subject_macro.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/top_twenty_machine_learning_subjects.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/machine_learning_papers_over_time.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/document_topic_analysis.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/word_colors_1.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/word_colors_2.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/word_colors_3.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/word_colors_4.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/test-all-boxplots.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/all-boxplots-20-topics.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/images/kmeans_clustering.png"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9996494917619634,
      "result": {
        "original_header": "Appendix #1: Technologies &amp; Techniques Used",
        "type": "Text_excerpt",
        "value": "Technologies:\n- Python\n- pandas\n- scikit-learn\n- NumPy\n- Jupyter Notebooks\n- matplotlib \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9702290803328004,
      "result": {
        "original_header": "Appendix #2: Project Organization",
        "type": "Text_excerpt",
        "value": "Here's a rough organization of how the project is setup. (It doesn't follow this template exactly, but it's fairly close.) \n    \u251c\u2500\u2500 LICENSE\n    \u251c\u2500\u2500 Makefile           <- Makefile with commands like `make data` or `make train`\n    \u251c\u2500\u2500 README.md          <- The top-level README for developers using this project.\n    \u251c\u2500\u2500 data\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 external       <- Data from third party sources.\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 interim        <- Intermediate data that has been transformed.\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 processed      <- The final, canonical data sets for modeling.\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 raw            <- The original, immutable data dump.\n    \u2502\n    \u251c\u2500\u2500 docs               <- A default Sphinx project; see sphinx-doc.org for details\n    \u2502\n    \u251c\u2500\u2500 models             <- Trained and serialized models, model predictions, or model summaries\n    \u2502\n    \u251c\u2500\u2500 notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n    \u2502                         the creator's initials, and a short `-` delimited description, e.g.\n    \u2502                         `1.0-jqp-initial-data-exploration`.\n    \u2502\n    \u251c\u2500\u2500 references         <- Data dictionaries, manuals, and all other explanatory materials.\n    \u2502\n    \u251c\u2500\u2500 reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 figures        <- Generated graphics and figures to be used in reporting\n    \u2502\n    \u251c\u2500\u2500 requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n    \u2502                         generated with `pip freeze > requirements.txt`\n    \u2502\n    \u251c\u2500\u2500 setup.py           <- makes project pip installable (pip install -e .) so src can be imported\n    \u251c\u2500\u2500 src                <- Source code for use in this project.\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py    <- Makes src a Python module\n    \u2502   \u2502\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 data           <- Scripts to download or generate data\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 make_dataset.py\n    \u2502   \u2502\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 features       <- Scripts to turn raw data into features for modeling\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 build_features.py\n    \u2502   \u2502\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 models         <- Scripts to train models and then use trained models to make\n    \u2502   \u2502   \u2502                 predictions\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 predict_model.py\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 train_model.py\n    \u2502   \u2502\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 visualization  <- Scripts to create exploratory and results oriented visualizations\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 visualize.py\n    \u2502\n    \u2514\u2500\u2500 tox.ini            <- tox file with settings for running tox; see tox.testrun.org \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8537028497997241,
      "result": {
        "original_header": "A Brief History",
        "type": "Text_excerpt",
        "value": "<img src=\"images/deep_blue.jpeg\"> \n<img src=\"images/homemade_self_driving_car.jpg\"> \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8754183990380207,
      "result": {
        "original_header": "The Data",
        "type": "Text_excerpt",
        "value": "<img src=\"images/arXiv_XML.png\" alt=\"arXiv XML example\"> \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9104844843692269,
      "result": {
        "original_header": "10 Topics - Evolution Over Time",
        "type": "Text_excerpt",
        "value": "<img src=\"images/test-all-boxplots.png\"> \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8221066435810881,
      "result": {
        "original_header": "20 Topics",
        "type": "Text_excerpt",
        "value": "<img src=\"images/all-boxplots-20-topics.png\"> \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8700504740251644,
      "result": {
        "original_header": "K-Means Clustering and t-SNE",
        "type": "Text_excerpt",
        "value": "<img src=\"images/kmeans_clustering.png\"> \n"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/stevenrouk/evolution-of-machine-learning/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "evolution-of-machine-learning"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "stevenrouk"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 4164842,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 77739,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "HTML",
        "size": 49830,
        "type": "Programming_language",
        "value": "HTML"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 3207,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "CSS",
        "size": 2309,
        "type": "Programming_language",
        "value": "CSS"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/"
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/help/stats/2018_by_area/index for more."
      },
      "source": "https://raw.githubusercontent.com/stevenrouk/evolution-of-machine-learning/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-04 00:47:33",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1
      },
      "technique": "GitHub_API"
    }
  ]
}