{
  "application_domain": [
    {
      "confidence": 9.89,
      "result": {
        "type": "String",
        "value": "Audio"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "format": "cff",
        "type": "File_dump",
        "value": "# YAML 1.2\n---\ncff-version: \"1.0.4\"\ntitle: \"mexca: Capture emotion expressions from multiple modalities in videos\"\nauthors:\n  - family-names: L\u00fcken\n    given-names: Malte\n    orcid: \"https://orcid.org/0000-0001-7095-203X\"\n  - family-names: Viviani\n    given-names: Eva\n    orcid: \"https://orcid.org/0000-0002-1330-0585\"\n  - family-names: Moodley\n    given-names: Kody\n    orcid: \"https://orcid.org/0000-0001-5666-1658\"\n  - family-names: Pipal\n    given-names: Christian\n    orcid: \"https://orcid.org/0000-0002-5395-2035\"\n  - family-names: Schumacher\n    given-names: Gijs\n    orcid: \"https://orcid.org/0000-0002-6503-4514\"\ndate-released: 2024-05-01\ndoi: 10.5281/zenodo.6976414\nversion: \"1.0.4\"\nrepository-code: \"https://github.com/mexca/mexca\"\nkeywords:\n  - emotion\n  - multimodal\n  - expression\nmessage: \"If you use this software, please cite it using these metadata.\"\nlicense: Apache-2.0\n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/CITATION.cff",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Credits",
        "parent_header": [
          "Multimodal Emotion Expression Capture Amsterdam"
        ],
        "type": "Text_excerpt",
        "value": "Mexca is being developed by the [Netherlands eScience Center](https://www.esciencecenter.nl/) in collaboration with the [Hot Politics Lab](http://www.hotpolitics.eu/) at the University of Amsterdam.\n\nThis package was created with [Cookiecutter](https://github.com/audreyr/cookiecutter) and the [NLeSC/python-template](https://github.com/NLeSC/python-template).\n\n[^1]: We explain the rationale for this setup in the [Docker](https://mexca.readthedocs.io/en/latest/docker.html) section.\n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_of_conduct": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as\ncontributors and maintainers pledge to making participation in our project and\nour community a harassment-free experience for everyone, regardless of age, body\nsize, disability, ethnicity, gender identity and expression, level of experience,\neducation, socio-economic status, nationality, personal appearance, race,\nreligion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment\ninclude:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or\n  advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic\n  address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable\nbehavior and are expected to take appropriate and fair corrective action in\nresponse to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or\nreject comments, commits, code, wiki edits, issues, and other contributions\nthat are not aligned to this Code of Conduct, or to ban temporarily or\npermanently any contributor for other behaviors that they deem inappropriate,\nthreatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces\nwhen an individual is representing the project or its community. Examples of\nrepresenting a project or community include using an official project e-mail\naddress, posting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event. Representation of a project may be\nfurther defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported by contacting the project team at m.luken@esciencecenter.nl. All\ncomplaints will be reviewed and investigated and will result in a response that\nis deemed necessary and appropriate to the circumstances. The project team is\nobligated to maintain confidentiality with regard to the reporter of an incident.\nFurther details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good\nfaith may face temporary or permanent repercussions as determined by other\nmembers of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org), version 1.4,\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/CODE_OF_CONDUCT.md",
      "technique": "file_exploration"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/mexca/mexca"
      },
      "technique": "GitHub_API"
    }
  ],
  "contributing_guidelines": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# Contributing guidelines\n\nWe welcome any kind of contribution to our software, from simple comment or question to a full fledged [pull request](https://help.github.com/articles/about-pull-requests/). Please read and follow our [Code of Conduct](CODE_OF_CONDUCT.md).\n\nA contribution can be one of the following cases:\n\n1. you have a question;\n1. you think you may have found a bug (including unexpected behavior);\n1. you want to make some kind of change to the code base (e.g. to fix a bug, to add a new feature, to update documentation);\n1. you want to make a new release of the code base.\n\nThe sections below outline the steps in each case.\n\n## You have a question\n\n1. use the search functionality [here](https://github.com/mexca/mexca/issues) to see if someone already filed the same issue;\n2. if your issue search did not yield any relevant results, make a new issue;\n3. apply the \"Question\" label; apply other labels when relevant.\n\n## You think you may have found a bug\n\n1. use the search functionality [here](https://github.com/mexca/mexca/issues) to see if someone already filed the same issue;\n1. if your issue search did not yield any relevant results, make a new issue, making sure to provide enough information to the rest of the community to understand the cause and context of the problem. Depending on the issue, you may want to include:\n    - the [SHA hashcode](https://help.github.com/articles/autolinked-references-and-urls/#commit-shas) of the commit that is causing your problem;\n    - some identifying information (name and version number) for dependencies you're using;\n    - information about the operating system;\n1. apply relevant labels to the newly created issue.\n\n## You want to make some kind of change to the code base\n\n1. (**important**) announce your plan to the rest of the community *before you start working*. This announcement should be in the form of a (new) issue;\n1. (**important**) wait until some kind of consensus is reached about your idea being a good idea;\n1. if needed, fork the repository to your own Github profile and create your own feature branch off of the latest master commit. While working on your feature branch, make sure to stay up to date with the master branch by pulling in changes, possibly from the 'upstream' repository (follow the instructions [here](https://help.github.com/articles/configuring-a-remote-for-a-fork/) and [here](https://help.github.com/articles/syncing-a-fork/));\n1. make sure the existing tests still work by running ``pytest``;\n1. add your own tests (if necessary);\n1. update or expand the documentation;\n1. update the `CHANGELOG.md` file with change;\n1. make sure your changes adhere to the style requirements by running ``pre-commit install``. If you commit changes, ``pre-commit`` first checks if the style is correct and corrects it otherwise;\n1. push your feature branch to (your fork of) the mexca repository on GitHub;\n1. create the pull request, e.g. following the instructions [here](https://help.github.com/articles/creating-a-pull-request/).\n\nIn case you feel like you've made a valuable contribution, but you don't know how to write or run tests for it, or how to generate the documentation: don't let this discourage you from making the pull request; we can help you! Just go ahead and submit the pull request, but keep in mind that you might be asked to append additional commits to your pull request.\n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/CONTRIBUTING.md",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Contributing",
        "parent_header": [
          "Multimodal Emotion Expression Capture Amsterdam"
        ],
        "type": "Text_excerpt",
        "value": "If you want to contribute to the development of mexca,\nhave a look at the [contribution guidelines](CONTRIBUTING.md).\n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2022-06-07T11:49:25Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-30T13:08:54Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Multimodal Emotion eXpression Capture Amsterdam. Pipeline for capturing emotion expressions from multiple modalities (video, audio, text) in the wild."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9281153104564704,
      "result": {
        "original_header": "Multimodal Emotion Expression Capture Amsterdam",
        "type": "Text_excerpt",
        "value": "mexca is an open-source Python package which aims to capture human emotion expressions from videos in a single pipeline. \nL\u00fcken, M., Moodley, K., Viviani, E., Pipal, C., & Schumacher, G. (2024, January 18). MEXCA - A simple and robust pipeline for capturing emotion expressions in faces, vocalization, and speech. *PsyArXiv*. https://doi.org/10.31234/osf.io/56svb\n \n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Documentation",
        "parent_header": [
          "Multimodal Emotion Expression Capture Amsterdam"
        ],
        "type": "Text_excerpt",
        "value": "The documentation of mexca can be found on [Read the Docs](https://mexca.readthedocs.io/en/latest/index.html).\n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "https://mexca.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/mexca/mexca/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/mexca/mexca/main/examples/demo.ipynb"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/examples/demo.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/mexca/mexca/main/examples/example_emotion_feature_extraction.ipynb"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/examples/example_emotion_feature_extraction.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/mexca/mexca/main/examples/example_custom_pipeline_components.ipynb"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/examples/example_custom_pipeline_components.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 6
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/mexca/mexca/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "mexca/mexca"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Multimodal Emotion Expression Capture Amsterdam"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_build_file": [
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/mexca/mexca/main/docker/face-extractor/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/docker/face-extractor/Dockerfile",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/mexca/mexca/main/docker/voice-extractor/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/docker/voice-extractor/Dockerfile",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/mexca/mexca/main/docker/speaker-identifier/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/docker/speaker-identifier/Dockerfile",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/mexca/mexca/main/docker/sentiment-extractor/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/docker/sentiment-extractor/Dockerfile",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/mexca/mexca/main/docker/audio-transcriber/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/docker/audio-transcriber/Dockerfile",
      "technique": "file_exploration"
    }
  ],
  "identifier": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://zenodo.org/badge/latestdoi/500818250"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "parent_header": [
          "Multimodal Emotion Expression Capture Amsterdam"
        ],
        "type": "Text_excerpt",
        "value": "mexca can be installed on Windows, macOS and Linux. We recommend Windows 10, macOS 12.6.x, or Ubuntu. The base package can be installed from PyPI via `pip`:\n\n```console\npip install mexca\n```\n\nThe dependencies for the additional components can be installed via:\n\n```console\npip install mexca[vid,spe,voi,tra,sen]\n```\n\nor:\n\n```console\npip install mexca[all]\n```\n\nThe abbreviations indicate:\n\n* `vid`: FaceExtractor\n* `spe`: SpeakerIdentifier\n* `voi`: VoiceExtractor\n* `tra`: AudioTranscriber\n* `sen`: SentimentExtractor\n\nFor details on the requirements and installation procedure, see the [Quick Installation](https://mexca.readthedocs.io/en/latest/quick_installation.html) and [Installation Details](https://mexca.readthedocs.io/en/latest/installation_details.html) sections of our documentation.\n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "invocation": [
    {
      "confidence": 0.832713191331159,
      "result": {
        "original_header": "Multimodal Emotion Expression Capture Amsterdam",
        "type": "Text_excerpt",
        "value": "<div align=\"center\">\n<img src=\"mexca_logo.png\" width=\"400\">\n</div> \n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/mexca/mexca/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "computer-vision, docker, emotion-recognition, python, pytorch, sentiment-analysis, speech-processing, speech-to-text"
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Apache License 2.0",
        "spdx_id": "Apache-2.0",
        "type": "License",
        "url": "https://api.github.com/licenses/apache-2.0",
        "value": "https://api.github.com/licenses/apache-2.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"{}\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n\n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/LICENSE",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License",
        "parent_header": [
          "Multimodal Emotion Expression Capture Amsterdam"
        ],
        "type": "Text_excerpt",
        "value": "The code is licensed under the Apache 2.0 License. This means that mexca can be used, modified and redistributed for free, even for commercial purposes.\n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "mexca"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "mexca"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 279156,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Dockerfile",
        "size": 2011,
        "type": "Programming_language",
        "value": "Dockerfile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 1003,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/mexca/mexca/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maltelueken",
          "type": "User"
        },
        "date_created": "2024-05-01T12:24:59Z",
        "date_published": "2024-05-01T12:26:22Z",
        "description": "Fixes mismatch between PyPI release version and GitHub release version.",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v1.0.4",
        "name": "v1.0.4",
        "release_id": 153658875,
        "tag": "v1.0.4",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v1.0.4",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/153658875",
        "value": "https://api.github.com/repos/mexca/mexca/releases/153658875",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v1.0.4"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maltelueken",
          "type": "User"
        },
        "date_created": "2024-05-01T11:56:53Z",
        "date_published": "2024-05-01T12:00:09Z",
        "description": "Fixes one crucial and one minor bug.\r\n\r\n### Changed\r\n\r\n- Output files in the standard pipeline recipe are save after each video file is processed (instead of saving everything at the end)\r\n- GitHub action workflows are tested on MacOS version 13 because FFMPeg cannot be automatically installed on newest version\r\n- An extra step for freeing disk space is added to GitHub action Docker workflows\r\n\r\n### Fixes\r\n\r\n- A bug in the `FaceExtractor` component where the input for the `MEFARG` model was not sent to the correct device (GPU only)\r\n- A bug in the `SentimentExtractor` compoenent where the tokenizer would raise a run time error in very rare cases (probably for very long sentences). Now, padding is added to avoid the error and exceptions are caught returning `null` sentiment scores.",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v1.0.3",
        "name": "v1.0.3",
        "release_id": 153656535,
        "tag": "v1.0.3",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v1.0.3",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/153656535",
        "value": "https://api.github.com/repos/mexca/mexca/releases/153656535",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v1.0.3"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maltelueken",
          "type": "User"
        },
        "date_created": "2024-04-29T12:51:38Z",
        "date_published": "2024-04-29T12:56:04Z",
        "description": "Changes the pretrained MEFARG model to be downloaded from Hugging Face Hub instead of Google Drive due to problems with the gdown package.",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v1.0.2",
        "name": "v1.0.2",
        "release_id": 153249175,
        "tag": "v1.0.2",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v1.0.2",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/153249175",
        "value": "https://api.github.com/repos/mexca/mexca/releases/153249175",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v1.0.2"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maltelueken",
          "type": "User"
        },
        "date_created": "2024-01-17T15:25:58Z",
        "date_published": "2024-01-17T15:31:29Z",
        "description": "Fixes the PyPI publication. PyPI could not publish the previous version due to a name duplication error.",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v1.0.1",
        "name": "v1.0.1",
        "release_id": 137409204,
        "tag": "v1.0.1",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v1.0.1",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/137409204",
        "value": "https://api.github.com/repos/mexca/mexca/releases/137409204",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v1.0.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maltelueken",
          "type": "User"
        },
        "date_created": "2024-01-17T12:25:06Z",
        "date_published": "2024-01-17T14:54:17Z",
        "description": "Contains some final fixes and adjustments for the first complete release.\r\n\r\n### Changed\r\n\r\n- Upgrades pyannote.audio to version 3.1.1\r\n- Downgrades gdown to version 4.6.0\r\n- Only essential steps are logged on INFO level (i.e., cluster confidence, average embeddings, and removing audio files is now on DEBUG level)\r\n- The error message when the connection to the Docker daemon fails is now more informative\r\n\r\n### Removed\r\n\r\n- onnx-runtime, ruamel.yaml, and torchaudio as requirements for the speaker identifier component due to pyannote.audio upgrade\r\n\r\n### Fixed\r\n\r\n- A bug caused by pyannote.audio version 3.0.0 for short audio clips when frame-wise detected speakers exceeded maximum number of speakers (see #106)\r\n- An issue by gdown when model files hosted on Google Drive could not be accessed anymore (https://github.com/wkentaro/gdown/issues/43)",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v1.0.0",
        "name": "v1.0.0",
        "release_id": 137376778,
        "tag": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v1.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/137376778",
        "value": "https://api.github.com/repos/mexca/mexca/releases/137376778",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v1.0.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maltelueken",
          "type": "User"
        },
        "date_created": "2023-11-21T14:52:04Z",
        "date_published": "2023-11-21T14:53:33Z",
        "description": "Adds average speaker embeddings and improved speaker diarization. Also increases the performance of data processing. Provides an advanced example notebook for extending the standard MEXCA pipeline.\r\n\r\n### Added\r\n\r\n- The `SpeakerAnnoation` class has a new attribute `speaker_average_embeddings` containing the average embeddings for each detected speaker\r\n- The `SpeakerIdentifier` has a new argument to explicitly set the device its run on (by default CPU)\r\n- The `SpeakerIdentifier.apply()` method has a new `show_progress` argument to enable progress bars for detected speech segments and embeddings\r\n- A new notebook on customizing and extending the MEXCA pipeline (`examples/example_custom_pipeline_components.ipynb`)\r\n- Two new recipes for applying the standard MEXCA pipeline and postprocessing the extracted features (`recipes/`)\r\n- The `Pipeline.apply()` method has a new `merge` argument to disable merging features from different modalities; this is useful when customizing a pipeline\r\n- A new logo (thanks to [Ji Qi](https://github.com/jiqicn))\r\n- Documentation on how to use mexca with GPU and CUDA support\r\n- notebook has been added as a dependency for the demo installation\r\n- scikit-learn has been added as an explicit dependency (previously dependency of py-feat)\r\n\r\n### Changed\r\n\r\n- pyannote.audio has been upgraded to version 3.0.0; this required adding the following dependencies:\r\n    - torch >= 2.0.0\r\n    - onnxruntime-gpu on Windows and Linux\r\n    - onnxruntime on MacOS\r\n    - torchaudio on MacOS\r\n- torch has been upgraded to version 2.0.0 for all components requiring it\r\n- The `SpeakerIdentifier` component uses the `pyannote/speaker-diarization-3.0` model by default\r\n- pandas has been replaced by polars; the `Multimodal.features` attribute now stores a `polars.LazyFrame` instead of a `pandas.DataFrame`; this speeds up postprocessing and merging for large data sets\r\n\r\n### Removed\r\n\r\n- py-feat has been removed as a dependency",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v0.7.0-beta",
        "name": "v0.7.0-beta",
        "release_id": 130674591,
        "tag": "v0.7.0-beta",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v0.7.0-beta",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/130674591",
        "value": "https://api.github.com/repos/mexca/mexca/releases/130674591",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v0.7.0-beta"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maltelueken",
          "type": "User"
        },
        "date_created": "2023-08-31T12:33:21Z",
        "date_published": "2023-08-31T12:53:24Z",
        "description": "Adds support for Python 3.10. Refactors data handling and storage using pydantic data structures and validation. Replaces the `audio.features` module with the emvoice package.\r\n\r\n### Added\r\n\r\n- The emvoice package a requirement for the VoiceExtractor component\r\n- Support for Python 3.10\r\n- A `post_min_face_size` argument in the `FaceExtractor` class which allows to filter out faces after detection and before clustering\r\n\r\n### Changed\r\n\r\n- The `BaseFeature` class in the `audio.extraction` module is now an abstract base class and its `requires` method an abstract property\r\n- The `Pipeline.apply()` method can now also take an iterable of filepaths as the `filepath` argument, processing them sequentially\r\n- The container test workflow is refactored and a `pytest.mark.run_env()` decorator added to allow running tests for only one component container in one job; the jobs for different components are completely decoupled\r\n- The flowchart is updated\r\n- Classes in the `data` module are refactored using base classes and pydantic data models\r\n- Classes in the `data` module have methods for JSON (de-) serialization\r\n- The CLIs for all components write output to JSON files with standardized names\r\n- Custom attributes in the `VoiceFeatures` class store `nan` values as `None` for consistency with the facial features\r\n- The `confidence` feature is renamed to `span_confidence` for consistency with the other text features\r\n\r\n### Removed\r\n\r\n- The `audio.features` submodule is removed and its functionality replaced by the emvoice package\r\n- Support for Python 3.7 due to dependency conflicts with the whisper package\r\n\r\n### Fixed\r\n\r\n- A bug in the lazy initialization of the Whisper model\r\n- A bug in loading the a voice feature configuration YAML file from the CLI\r\n- A bug in the calculation of transcription confidence scores for zero length speech segments\r\n- An exception is thrown if a container component fails, propagating the error message to the console",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v0.6.0-beta",
        "name": "v0.6.0-beta",
        "release_id": 119436826,
        "tag": "v0.6.0-beta",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v0.6.0-beta",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/119436826",
        "value": "https://api.github.com/repos/mexca/mexca/releases/119436826",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v0.6.0-beta"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maltelueken",
          "type": "User"
        },
        "date_created": "2023-07-17T15:14:57Z",
        "date_published": "2023-07-17T15:16:31Z",
        "description": "Replaces the methods for predicting facial landmarks and action unit activations. Landmarks are now predicted by facenet-pytorch and action units by the MEFARG model instead of py-feat.\r\n\r\n### Added\r\n\r\n- The `FaceExtractor` component computes average face embeddings via the `compute_avg_embeddings()` method\r\n- The `VideoAnnotation` data class has an additional field `face_average_embeddings`, containing the average face representations for each detected face cluster\r\n- The `AudioTranscriber` component returns the confidence of the transcription (average over each word sequence)\r\n- The `TranscriptionData` data class has an additional field `confidence` for the transcription confidence\r\n- `ruamel.yaml` is added as an explicit dependency for the SpeakerIdentifier component\r\n- `gdown` is added as a dependency for the FaceExtractor component\r\n- Package code adheres to black code style\r\n- Adds pre-commit configuration (enable with `pre-commit install`) in `.pre-commit-config.yaml`\r\n- Adds `black` and `pre-commit` to .\\[dev\\] requirements\r\n\r\n### Changed\r\n\r\n- The `mexca.video` module is split into several submodules: `extraction`, `mefl`, `anfl`, `mefarg`, and `helper_classes`\r\n- Facial landmarks are predicted by `facenet_pytorch.MTCNN` instead of `feat.detector.Detector`\r\n- Facial action units are predicted by `mexca.video.mefarg.MEFARG` instead of `feat.detector.Detector`\r\n- Word-level timestamps are obtained from the native `whisper` package instead of `stable-ts`\r\n\r\n### Removed\r\n\r\n- `py-feat` is removed from the dependencies of the FaceExtractor component\r\n- `stable-ts` is removed from the dependencies of the AudioTranscriber component",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v0.5.0-beta",
        "name": "v0.5.0-beta",
        "release_id": 112512519,
        "tag": "v0.5.0-beta",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v0.5.0-beta",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/112512519",
        "value": "https://api.github.com/repos/mexca/mexca/releases/112512519",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v0.5.0-beta"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maltelueken",
          "type": "User"
        },
        "date_created": "2023-04-26T13:55:48Z",
        "date_published": "2023-04-26T15:05:47Z",
        "description": "Adds voice features, improves the documentation, and updates the example notebooks. Enables GPU support for all pipeline components.\r\n\r\n### Added\r\n\r\n- Classes for extracting voice features in the `mexca.audio.features` module:\r\n    - `FormantAudioSignal` for preemphasized audio signals for formant analysis\r\n    - `AlphaRatioFrames` and `HammarIndexFrames` for calculating alpha ratio and Hammarberg index\r\n    - `SpectralSlopeFrames` for estimating spectral slopes\r\n    - `MelSpecFrames` and `MfccFrames` for computing Mel spectrograms and cepstral coefficients\r\n    - `SpectralFluxFrames` and `RmsEnergyFrames` for calculating spectral flux and RMS energy\r\n- Classes for extracting and interpolating voice features in the `mexca.audio.extraction` module: `FeatureAlphaRatio`, `FeatureHammarIndex`, `FeatureSpectralSlope`, `FeatureHarmonicDifference`, `FeatureMfcc`, `FeatureSpectralFlux`, `FeatureRmsEnergy`\r\n- A `VoiceFeaturesConfig` class for configuring voice feature extraction in `mexca.data`\r\n- The CLI `extract-voice` has a new `--config-filepath` argument for YAML configuration files\r\n- The `FaceExtractor` component has new `max_cluster_frames` argument to set the maximum number of frames for spectral clustering\r\n- The `SentimentExtractor` component has a new `device` argument to run on GPU with 8-bit\r\n- `pyyaml` is added as a requirement for the base package\r\n- `accelerate` and `bisandbytes` are added as requirements for the SentimentExtractor component\r\n\r\n### Changed\r\n\r\n- The set of default voice features that are extracted with `VoiceExtractor` has been expanded\r\n- The default window for STFT is now the Hann window\r\n- Conversion from magnitude/energy to dB is now performed with librosa functions\r\n- The example notebooks have been updated\r\n- `mexca.container.VoiceExtractorContainer` can also handle `VoiceFeaturesConfig` objects\r\n- Required version of `spectralcluster` is set to 0.2.16\r\n- Required version of `transformers` is set to 4.25.1\r\n- Required version of numpy is set to >=1.21.6\r\n\r\n### Fixed\r\n\r\n- Warnings triggered during voice feature extraction\r\n- A bug occuring when using `FaceExtractor` with `device=\"cuda\"` has been fixed",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v0.4.0-beta",
        "name": "More voice features and full GPU support",
        "release_id": 100971899,
        "tag": "v0.4.0-beta",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v0.4.0-beta",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/100971899",
        "value": "https://api.github.com/repos/mexca/mexca/releases/100971899",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v0.4.0-beta"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maltelueken",
          "type": "User"
        },
        "date_created": "2023-04-05T13:28:01Z",
        "date_published": "2023-04-05T14:06:39Z",
        "description": "Improves the audio transcription and sentiment extraction workflows. Refactors the voice feature extraction workflow and adds several new voice features.\r\n\r\n### Added\r\n\r\n- Docker containers are now versioned via tags and the container components automatically fetch the container matching the installed version of mexca; the container with the `:latest` tag can be fetched with the argument `get_latest_tag=True` (#65)\r\n- Classes for extracting voice features (#66):\r\n    - `AudioSignal`, `BaseSignal` for loading and storing signals in the `mexca.audio.features` module\r\n    - `BaseFrames`, `FormantFrames`, `FormantAmplitudeFrames`, `HnrFrames`, `JitterFrames`, `PitchFrames`, `PitchHarmonicsFrames`, `PitchPeriodFrames`, `PitchPulseFrames`, `ShimmerFrames`, `SpecFrames` for computing and storing formant features, glottal pulse features, and pitch features in the `mexca.audio.features` module\r\n    - `BaseFeature`, `FeaturePitchF0`, `FeatureJitter`, `FeatureShimmer`, `FeatureHnr`, `FeatureFormantFreq`, `FeatureFormantBandwidth`, `FeatureFormantAmplitude` for extracting and interpolating voice features in the `mexca.audio.extraction` module\r\n- An `all` extra requirements group which installs the requirements for all of mexca's components (i.e., `pip install mexca[all]`, #64) \r\n\r\n### Changed\r\n\r\n- The `SentimentData` class now has a `text` instead of an `index` attribute, which is used for matching sentiment to transcriptions (#63)\r\n- The sentence sentiment is merged separately from the transcription in `Multimodal._merge_audio_text_features()` (#63)\r\n- librosa (version 0.9) is added as a requirement for the VoiceExtractor component instead of parselmouth; the voice feature extraction now relies on librosa instead of praat (#66)\r\n- stable-ts is required to be version 1.1.5 for compatibility with Python 3.7; in a future version, we might remove stable-ts as a dependency (#67)\r\n- transformers is added as a requirement for the AudioTranscriber component (#67)\r\n- scipy is moved to the general requirements for all components (#66)\r\n- The `VoiceExtractor` class and component is refactored with new default features (#66)\r\n- Tests make better use of fixtures for cleaner and more reusable code (#63)\r\n\r\n### Fixed\r\n- An error in the audio transcription that occurred for extremely short speech segments below the precision of whisper and stable-ts (#63)\r\n\r\n### Removed\r\n- The `toml` extra requirement for the coverage requirement in the `dev` group (#67)",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v0.3.0-beta",
        "name": "mexca v0.3.0-beta",
        "release_id": 98195650,
        "tag": "v0.3.0-beta",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v0.3.0-beta",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/98195650",
        "value": "https://api.github.com/repos/mexca/mexca/releases/98195650",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v0.3.0-beta"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maltelueken",
          "type": "User"
        },
        "date_created": "2023-02-03T12:41:27Z",
        "date_published": "2023-02-03T12:44:44Z",
        "description": "Minor patch that addresses a memory issue and includes some bug and documentation fixes.\r\n\r\n### Added\r\n\r\n- A \"Troubleshooting\" sub section in the \"Installation Details\" section in docs.\r\n- Exception class `AuthenticationError` for failed HuggingFace Hub authentication\r\n- Exception class `NotEnoughFacesError` for too few face detections for clustering\r\n\r\n### Changed\r\n\r\n- Refactored `VideoDataset` class to only load video frames when they are queried. The previous implementation attemped to load the entire video into memory leading to issues. Now, only frames of the current batch are loaded into memory as expected.\r\n\r\n### Fixed\r\n\r\n- Added missing note about HuggingFace Hub authentication to \"Getting Started\" section in docs.\r\n- An exception is triggered if pypiwin32 was not properly installed when initializing a docker client\r\n- An exception is triggered if no HuggingFace Hub token was found when initializing `SpeakerIdentifier` with `use_auth_token=True`\r\n- Correctly passes the HuggingFace Hub token to the Docker build action for the SpeakerIdentifier container",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v0.2.1-beta",
        "name": "mexca v0.2.1-beta",
        "release_id": 91261234,
        "tag": "v0.2.1-beta",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v0.2.1-beta",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/91261234",
        "value": "https://api.github.com/repos/mexca/mexca/releases/91261234",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v0.2.1-beta"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "maltelueken",
          "type": "User"
        },
        "date_created": "2023-01-26T15:29:39Z",
        "date_published": "2023-01-26T15:32:17Z",
        "description": "First beta release. This version is a major overhaul of the first alpha release.\r\n\r\n### Added\r\n\r\n- A component for sentiment extraction\r\n- Data classes as interfaces for component in- and output in the `data` module\r\n- CLIs for all five components, removes general CLI for pipeline\r\n- Interfaces for Docker containers of all five components, removes general Dockerfile\r\n- Functionality to write output to common file formats (JSON, RTTM, SRT)\r\n- Lazy initialization for pretrained models to save memory\r\n- Data loader functionality to the `FaceExtractor` component to allow for batch processing\r\n- Clustering confidence metric to the output of the `FaceExtractor` class\r\n- Logging\r\n- Static type annotations\r\n- Added utils module\r\n- Flowchart to the introduction in docs\r\n- 'Getting Started' section in docs\r\n\r\n### Changed\r\n\r\n- Simplified the structure of the package\r\n- Moved content of core module into separate modules\r\n- Refactors the `Pipeline` class to include five components: `FaceExtractor`, `SpeakerIdentifier`, `VoiceExtractor`, `AudioTranscriber`, `SentimentExtractor`\r\n- Separated the dependencies for all five components: They can all be installed separately from each other\r\n- Whisper for audio transcription instead of fine-tuned wav2vec models via huggingsound\r\n- Adapted the `FaceExtractor` component for the pretrained models used in py-feat v0.5\r\n- Refactors feature merging using `pandas.DataFrame` and `intervaltree.IntervalTree`\r\n- Splits the installation instructions in two parts (quick vs. detailed) in docs\r\n- Updates docker section\r\n- Updates command line section\r\n\r\n### Removed\r\n\r\n- Removed the `AudioIntegrator` and `AudioTextIntegrator` classes, feature merging is done in the `Multimodal` class\r\n- Removed the core module and its submodules\r\n- Removed face-speaker matching (temporarily); might be added again in a future release",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v0.2.0-beta",
        "name": "mexca v0.2.0-beta",
        "release_id": 90313457,
        "tag": "v0.2.0-beta",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v0.2.0-beta",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/90313457",
        "value": "https://api.github.com/repos/mexca/mexca/releases/90313457",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v0.2.0-beta"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "n400peanuts",
          "type": "User"
        },
        "date_created": "2023-01-24T10:51:47Z",
        "date_published": "2022-08-09T09:09:30Z",
        "description": "## What's in this version\r\nThis release contains the first alpha version of mexca. This version is still early development and may contain missing features, bugs, etc.\r\n\r\n## Contributors\r\n* @maltelueken made their first contribution in https://github.com/mexca/mexca/pull/6\r\n* @n400peanuts made their first contribution in https://github.com/mexca/mexca/pull/9\r\n* @dafnevk added __init__.py file\r\n\r\n**Full Changelog**: https://github.com/mexca/mexca/commits/v0.1.0-alpha",
        "html_url": "https://github.com/mexca/mexca/releases/tag/v0.1.0-alpha",
        "name": "First alpha version",
        "release_id": 73968384,
        "tag": "v0.1.0-alpha",
        "tarball_url": "https://api.github.com/repos/mexca/mexca/tarball/v0.1.0-alpha",
        "type": "Release",
        "url": "https://api.github.com/repos/mexca/mexca/releases/73968384",
        "value": "https://api.github.com/repos/mexca/mexca/releases/73968384",
        "zipball_url": "https://api.github.com/repos/mexca/mexca/zipball/v0.1.0-alpha"
      },
      "technique": "GitHub_API"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contact",
    "contributors",
    "faq",
    "support"
  ],
  "somef_provenance": {
    "date": "2024-10-03 23:13:52",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 31
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "How To Use Mexca",
        "parent_header": [
          "Multimodal Emotion Expression Capture Amsterdam"
        ],
        "type": "Text_excerpt",
        "value": "mexca implements the customizable yet easy-to-use Multimodal Emotion eXpression Capture Amsterdam (MEXCA) pipeline for extracting emotion expression features from videos.\nIt contains building blocks that can be used to extract features for individual modalities (i.e., facial expressions, voice, and dialogue/spoken text).\nThe blocks can also be integrated into a single pipeline to extract the features from all modalities at once.\nNext to extracting features, mexca can also identify the speakers shown in the video by clustering speaker and face representations.\nThis allows users to compare emotion expressions across speakers, time, and contexts.\n\nPlease cite mexca if you use it for scientific or commercial purposes.\n\n<div align=\"center\">\n<img src=\"docs/mexca_flowchart.png\" width=\"600\">\n</div>\n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Getting Started",
        "parent_header": [
          "Multimodal Emotion Expression Capture Amsterdam"
        ],
        "type": "Text_excerpt",
        "value": "If you would like to learn how to use mexca, take a look at our [demo](https://github.com/mexca/mexca/tree/main/examples) notebook and the [Getting Started](https://mexca.readthedocs.io/en/latest/getting_started.html) section of our documentation.\n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Examples and Recipes",
        "parent_header": [
          "Multimodal Emotion Expression Capture Amsterdam"
        ],
        "type": "Text_excerpt",
        "value": "In the `examples/` folder, we currently provide two Jupyter notebooks (and a short demo):\n\n- [example_custom_pipeline_components](https://github.com/mexca/mexca/blob/main/examples/example_custom_pipeline_components.ipynb) shows how the standard MEXCA pipeline can be customized and extended\n- [example_emotion_feature_extraction](https://github.com/mexca/mexca/blob/main/examples/example_emotion_feature_extraction.ipynb) shows how to apply the MEXCA pipeline to a video and conduct a basic analysis of the extracted features\n\nThe `recipes/` folder contains two Python scripts that can easily be reused in a new project:\n\n- [recipe_postprocess_features](https://github.com/mexca/mexca/blob/main/recipes/recipe_postprocess_features.py) applies a standard postprocessing routine to extracted features\n- [recipe_standard_pipeline](https://github.com/mexca/mexca/blob/main/recipes/recipe_standard_pipeline.py) applies the standard MEXCA pipeline to a list of videos\n"
      },
      "source": "https://raw.githubusercontent.com/mexca/mexca/main/README.md",
      "technique": "header_analysis"
    }
  ]
}