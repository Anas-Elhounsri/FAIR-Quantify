{
  "application_domain": [
    {
      "confidence": 24.58,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citation",
        "parent_header": [
          "MIMIC-III Benchmarks"
        ],
        "type": "Text_excerpt",
        "value": "If you use this code or these benchmarks in your research, please cite the following publication.\n```\n@article{Harutyunyan2019,\n  author={Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C. and Ver Steeg, Greg and Galstyan, Aram},\n  title={Multitask learning and benchmarking with clinical time series data},\n  journal={Scientific Data},\n  year={2019},\n  volume={6},\n  number={1},\n  pages={96},\n  issn={2052-4463},\n  doi={10.1038/s41597-019-0103-9},\n  url={https://doi.org/10.1038/s41597-019-0103-9}\n}\n```\n**Please be sure also to cite the original [MIMIC-III paper](http://www.nature.com/articles/sdata201635).**\n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C. and Ver Steeg, Greg and Galstyan, Aram",
        "doi": "10.1038/s41597-019-0103-9",
        "format": "bibtex",
        "title": "Multitask learning and benchmarking with clinical time series data",
        "type": "Text_excerpt",
        "url": "https://doi.org/10.1038/s41597-019-0103-9",
        "value": "@article{Harutyunyan2019,\n    url = {https://doi.org/10.1038/s41597-019-0103-9},\n    doi = {10.1038/s41597-019-0103-9},\n    issn = {2052-4463},\n    pages = {96},\n    number = {1},\n    volume = {6},\n    year = {2019},\n    journal = {Scientific Data},\n    title = {Multitask learning and benchmarking with clinical time series data},\n    author = {Harutyunyan, Hrayr and Khachatrian, Hrant and Kale, David C. and Ver Steeg, Greg and Galstyan, Aram},\n}"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/calvin-zcx/SCEHR"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-12-21T04:07:02Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-07-23T08:49:54Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 0.9830034267110096,
      "result": {
        "original_header": "MIMIC-III Benchmarks",
        "type": "Text_excerpt",
        "value": "Python suite to construct benchmark machine learning datasets from the MIMIC-III clinical database. Currently, the benchmark datasets cover four key inpatient clinical prediction tasks that map onto core machine learning problems: prediction of mortality from early admission data (classification), real-time detection of decompensation (time series classification), forecasting length of stay (regression), and phenotype classification (multilabel sequence classification). \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9546068798863188,
      "result": {
        "original_header": "News",
        "type": "Text_excerpt",
        "value": "* 2018 December 28: The second draft of the paper is released on [arXiv](https://arxiv.org/abs/1703.07771).\n* 2017 December 8: This work was presented as a spotlight presentation at NIPS 2017 [Machine Learning for Health Workshop](https://ml4health.github.io/2017/).\n* 2017 March 23: We are pleased to announce the first official release of these benchmarks. We expect to release a revision within the coming months that will add at least ~50 additional input variables. We are likewise pleased to announce that the manuscript associated with these benchmarks is now [available on arXiv](https://arxiv.org/abs/1703.07771).\n \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9898112427147823,
      "result": {
        "original_header": "Motivation",
        "type": "Text_excerpt",
        "value": "Despite rapid growth in research that applies machine learning to clinical data, progress in the field appears far less dramatic than in other applications of machine learning. In image recognition, for example, the winning error rates in the [ImageNet Large Scale Visual Recognition Challenge](http://image-net.org/challenges/LSVRC/) (ILSVRC) plummeted almost 90% from 2010 (0.2819) to 2016 (0.02991).\nThere are many reasonable explanations for this discrepancy: clinical data sets are [inherently noisy and uncertain](http://www-scf.usc.edu/~dkale/papers/marlin-ihi2012-ehr_clustering.pdf) and often small relative to their complexity, and for many problems of interest, [ground truth labels for training and evaluation are unavailable](https://academic.oup.com/jamia/article-abstract/23/6/1166/2399304/Learning-statistical-models-of-phenotypes-using?redirectedFrom=PDF). \nHowever, there is another, simpler explanation: practical progress has been difficult to measure due to the absence of community benchmarks like ImageNet. Such benchmarks play an important role in accelerating progress in machine learning research. For one, they focus the community on specific problems and stoke ongoing debate about what those problems should be. They also reduce the startup overhead for researchers moving into a new area. Finally and perhaps most important, benchmarks facilitate reproducibility and direct comparison of competing ideas. \nHere we present four public benchmarks for machine learning researchers interested in health care, built using data from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database ([paper](http://www.nature.com/articles/sdata201635), [website](http://mimic.physionet.org)). Our four clinical prediction tasks are critical care variants of four opportunities to transform health care using in \"big clinical data\" as described in [Bates, et al, 2014](http://content.healthaffairs.org/content/33/7/1123.abstract): \n* early triage and risk assessment, i.e., mortality prediction\n* prediction of physiologic decompensation\n* identification of high cost patients, i.e. length of stay forecasting\n* characterization of complex, multi-system diseases, i.e., acute care phenotyping \nIn [Harutyunyan, Khachatrian, Kale, and Galstyan 2017](https://arxiv.org/abs/1703.07771), we propose a multitask RNN architecture to solve these four tasks simultaneously and show that this model generally outperforms strong single task baselines.\n \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8377034485097614,
      "result": {
        "original_header": "Structure",
        "type": "Text_excerpt",
        "value": "* Tools for creating the benchmark datasets.  \n* Tools for reading the benchmark datasets.\n* Evaluation scripts.\n* Baseline models and helper tools. \nThe `mimic3benchmark/scripts` directory contains scripts for creating the benchmark datasets.\nThe reading tools are in `mimic3benchmark/readers.py`.\nAll evaluation scripts are stored in the `mimic3benchmark/evaluation` directory.\nThe `mimic3models` directory contains the baselines models along with some helper tools.\nThose tools include discretizers, normalizers and functions for computing metrics. \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9282112435293902,
      "result": {
        "original_header": "Building a benchmark",
        "type": "Text_excerpt",
        "value": "       git clone https://github.com/YerevaNN/mimic3-benchmarks/\n       cd mimic3-benchmarks/\n    \n2. The following command takes MIMIC-III CSVs, generates one directory per `SUBJECT_ID` and writes ICU stay information to `data/{SUBJECT_ID}/stays.csv`, diagnoses to `data/{SUBJECT_ID}/diagnoses.csv`, and events to `data/{SUBJECT_ID}/events.csv`. This step might take around an hour. \n3. The following command attempts to fix some issues (ICU stay ID is missing) and removes the events that have missing information. About 80% of events remain after removing all suspicious rows (more information can be found in [`mimic3benchmark/scripts/more_on_validating_events.md`](mimic3benchmark/scripts/more_on_validating_events.md)). \n4. The next command breaks up per-subject data into separate episodes (pertaining to ICU stays). Time series of events are stored in ```{SUBJECT_ID}/episode{#}_timeseries.csv``` (where # counts distinct episodes) while episode-level information (patient age, gender, ethnicity, height, weight) and outcomes (mortality, length of stay, diagnoses) are stores in BASH2*. This script requires two files, one that maps event ITEMIDs to clinical variables and another that defines valid ranges for clinical variables (for detecting outliers, etc.). **Outlier detection is disabled in the current version**. \n       python -m mimic3benchmark.scripts.create_in_hospital_mortality data/root/ data/in-hospital-mortality/\n       python -m mimic3benchmark.scripts.create_decompensation data/root/ data/decompensation/\n       python -m mimic3benchmark.scripts.create_length_of_stay data/root/ data/length-of-stay/\n       python -m mimic3benchmark.scripts.create_phenotyping data/root/ data/phenotyping/\n       python -m mimic3benchmark.scripts.create_multitask data/root/ data/multitask/ \nAfter the above commands are done, there will be a directory `data/{task}` for each created benchmark task.\nThese directories have two sub-directories: `train` and `test`.\nEach of them contains bunch of ICU stays and one file with name `listfile.csv`, which lists all samples in that particular set.\nEach row of `listfile.csv` has the following form: `icu_stay, period_length, label(s)`.\nA row specifies a sample for which the input is the collection of ICU event of `icu_stay` that occurred in the first `period_length` hours of the stay and the target is/are `label(s)`.\nIn in-hospital mortality prediction task `period_length` is always 48 hours, so it is not listed in corresponding listfiles. \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9534187579232614,
      "result": {
        "original_header": "Readers",
        "type": "Text_excerpt",
        "value": "The `mimic3benchmark/readers.py` contains class `Reader` and five other task-specific classes derived from it.\nThese are designed to simplify reading of benchmark data. The classes require a directory containing ICU stays and a listfile specifying the samples.\nAgain, we encourage to use these readers to avoid mistakes in the reading step (for example using events that happened after the first `period_length` hours).  \nFor more information about using readers view the [`mimic3benchmark/more_on_readers.md`](mimic3benchmark/more_on_readers.md) file. \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9299158664975598,
      "result": {
        "original_header": "Evaluation",
        "type": "Text_excerpt",
        "value": "These scripts receive a `csv` file containing the predictions and produce a `json` file containing the scores and confidence intervals for different metrics.\nWe highly encourage to use these scripts to prevent any mistake in the evaluation step.\nFor details about the usage of the evaluation scripts view the [`mimic3benchmark/evaluation/README.md`](mimic3benchmark/evaluation/README.md) file. \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8879142545578612,
      "result": {
        "original_header": "Baselines",
        "type": "Text_excerpt",
        "value": "The detailed descriptions of the baselines will appear in the next version of the paper. \nLinear models can be found in `mimic3models/{task}/logistic` directories.\nLSTM-based models are in `mimic3models/keras_models` directory. \nPlease note that running linear models can take hours because of extensive grid search and feature extraction.\nYou can change the size of the training data of linear models in the scripts and they will became faster (of course the performance will not be the same).\n \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9695030841013409,
      "result": {
        "original_header": "Train / validation split",
        "type": "Text_excerpt",
        "value": "       python -m mimic3models.split_train_val {dataset-directory}\n       \n`{dataset-directory}` can be either `data/in-hospital-mortality`, `data/decompensation`, `data/length-of-stay`, `data/phenotyping` or `data/multitask`. \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9553899305974808,
      "result": {
        "original_header": "Multitask learning",
        "type": "Text_excerpt",
        "value": "`ihm_C`, `decomp_C`, `los_C` and `ph_C` coefficients control the relative weight of the tasks in the multitask model. Default is `1.0`. Multitask network architectures are stored in `mimic3models/multitask/keras_models`. Here is a sample command for running a multitask model.\n       \n       python -um mimic3models.multitask.main --network mimic3models/keras_models/multitask_lstm.py --dim 512 --timestep 1 --mode train --batch_size 16 --dropout 0.3 --ihm_C 0.2 --decomp_C 1.0 --los_C 1.5 --pheno_C 1.0 --output_dir mimic3models/multitask\n       \n \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9865851694004718,
      "result": {
        "original_header": "General todos:",
        "type": "Text_excerpt",
        "value": "- Improve comments and documentation\n- Add comments about channel-wise LSTMs and deep superivison\n- Add the best state files for each baseline\n- Add https://zenodo.org/ \n- Release 1.0\n- Update citation section with Zenodo DOI\n- Add to MIMIC's derived data repo\n- Refactor, where appropriate, to make code more generally useful\n- Expand coverage of variable map and variable range files.\n- Decide whether we are missing any other high-priority data (CPT codes, inputs, etc.) \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/calvin-zcx/SCEHR/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/mimic3models/experiment_notes/understanding_loss_in_classification.ipynb"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/mimic3models/experiment_notes/understanding_loss_in_classification.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/calvin-zcx/SCEHR/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "calvin-zcx/SCEHR"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "MIMIC-III Benchmarks"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9270808947435963,
      "result": {
        "original_header": "Building a benchmark",
        "type": "Text_excerpt",
        "value": "Here are the required steps to build the benchmark. It assumes that you already have MIMIC-III dataset (lots of CSV files) on the disk.\n1. Clone the repo. \n       git clone https://github.com/YerevaNN/mimic3-benchmarks/\n       cd mimic3-benchmarks/\n    \n2. The following command takes MIMIC-III CSVs, generates one directory per `SUBJECT_ID` and writes ICU stay information to `data/{SUBJECT_ID}/stays.csv`, diagnoses to `data/{SUBJECT_ID}/diagnoses.csv`, and events to `data/{SUBJECT_ID}/events.csv`. This step might take around an hour. \n       python -m mimic3benchmark.scripts.extract_subjects {PATH TO MIMIC-III CSVs} data/root/ \n       python -m mimic3benchmark.scripts.validate_events data/root/ \n       python -m mimic3benchmark.scripts.extract_episodes_from_subjects data/root/ \n       python -m mimic3benchmark.scripts.split_train_and_test data/root/\n\t\n6. The following commands will generate task-specific datasets, which can later be used in models. These commands are independent, if you are going to work only on one benchmark task, you can run only the corresponding command. \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8160972353211665,
      "result": {
        "original_header": "Baselines",
        "type": "Text_excerpt",
        "value": "* Linear/logistic regression\n* Standard LSTM\n* Standard LSTM + deep supervision\n* Channel-wise LSTM\n* Channel-wise LSTM + deep supervision\n* Multitask standard LSTM\n* Multitask channel-wise LSTM \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9516138399478866,
      "result": {
        "original_header": "Train / validation split",
        "type": "Text_excerpt",
        "value": "Use the following command to extract validation set from the training set. This step is required for running the baseline models. Likewise the train/test split, the train/validation split is the same for all tasks. \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.801990189765491,
      "result": {
        "original_header": "Building a benchmark",
        "type": "Text_excerpt",
        "value": "       python -m mimic3benchmark.scripts.validate_events data/root/ \n       python -m mimic3benchmark.scripts.extract_episodes_from_subjects data/root/ \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8299073282578002,
      "result": {
        "original_header": "Train / validation split",
        "type": "Text_excerpt",
        "value": "       python -m mimic3models.split_train_val {dataset-directory}\n       \n`{dataset-directory}` can be either `data/in-hospital-mortality`, `data/decompensation`, `data/length-of-stay`, `data/phenotyping` or `data/multitask`. \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8323341288769373,
      "result": {
        "original_header": "In-hospital mortality prediction",
        "type": "Text_excerpt",
        "value": "Run the following command to train the neural network which gives the best result. We got the best performance on validation set after 28 epochs.\n       \n       python -um mimic3models.in_hospital_mortality.main --network mimic3models/keras_models/lstm.py --dim 16 --timestep 1.0 --depth 2 --dropout 0.3 --mode train --batch_size 8 --output_dir mimic3models/in_hospital_mortality \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8319386776865605,
      "result": {
        "original_header": "Length of stay prediction",
        "type": "Text_excerpt",
        "value": "The best model we got for this task was trained for 19 chunks.\n       \n       python -um mimic3models.length_of_stay.main --network mimic3models/keras_models/lstm.py --dim 64 --timestep 1.0 --depth 1 --dropout 0.3 --mode train --batch_size 8 --partition custom --output_dir mimic3models/length_of_stay \n        python -um mimic3models.length_of_stay.logistic.main --output_dir mimic3models/length_of_stay/logistic \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8461236932588774,
      "result": {
        "original_header": "Phenotype classification",
        "type": "Text_excerpt",
        "value": "The best model we got for this task was trained for 20 epochs.\n       \n       python -um mimic3models.phenotyping.main --network mimic3models/keras_models/lstm.py --dim 256 --timestep 1.0 --depth 1 --dropout 0.3 --mode train --batch_size 8 --output_dir mimic3models/phenotyping \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8001154638097493,
      "result": {
        "original_header": "Multitask learning",
        "type": "Text_excerpt",
        "value": "`ihm_C`, `decomp_C`, `los_C` and `ph_C` coefficients control the relative weight of the tasks in the multitask model. Default is `1.0`. Multitask network architectures are stored in `mimic3models/multitask/keras_models`. Here is a sample command for running a multitask model.\n       \n       python -um mimic3models.multitask.main --network mimic3models/keras_models/multitask_lstm.py --dim 512 --timestep 1 --mode train --batch_size 16 --dropout 0.3 --ihm_C 0.2 --decomp_C 1.0 --los_C 1.5 --pheno_C 1.0 --output_dir mimic3models/multitask\n       \n \n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/calvin-zcx/SCEHR/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "SCEHR"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "calvin-zcx"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 658469,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Batchfile",
        "size": 410844,
        "type": "Programming_language",
        "value": "Batchfile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 1181,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/1703.07771"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Requirements",
        "parent_header": [
          "MIMIC-III Benchmarks"
        ],
        "type": "Text_excerpt",
        "value": "We do not provide the MIMIC-III data itself. You must acquire the data yourself from https://mimic.physionet.org/. Specifically, download the CSVs. Otherwise, generally we make liberal use of the following packages:\n\n- numpy\n- pandas\n\nFor logistic regression  baselines [sklearn](http://scikit-learn.org/) is required. LSTM models use [Keras](https://keras.io/).\n\n"
      },
      "source": "https://raw.githubusercontent.com/calvin-zcx/SCEHR/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "download",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-04 11:39:17",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 6
      },
      "technique": "GitHub_API"
    }
  ]
}