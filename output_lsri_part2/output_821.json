{
  "acknowledgement": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Acknowledgments",
        "type": "Text_excerpt",
        "value": "* This code is inspired by [Voxel2Mesh](https://github.com/cvlab-epfl/voxel2mesh).\n"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "application_domain": [
    {
      "confidence": 45.42,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Reference",
        "type": "Text_excerpt",
        "value": "If you find our work useful in your research or if you use parts of this code or the dataset, please cite the following papers:\n```\n@article{choi2022cirdataset,\n  title={CIRDataset: A large-scale Dataset for Clinically-Interpretable lung nodule Radiomics and malignancy prediction},\n  author={Choi, Wookjin and Dahiya, Navdeep and Nadeem, Saad},\n  journal={International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},\n  year={2022},\n}\n\n@article{choi2021reproducible,\n  title={Reproducible and Interpretable Spiculation Quantification for Lung Cancer Screening},\n  author={Choi, Wookjin and Nadeem, Saad and Alam, Sadegh R and Deasy, Joseph O and Tannenbaum, Allen and Lu, Wei},\n  journal={Computer Methods and Programs in Biomedicine},\n  volume={200},\n  pages={105839},\n  year={2021},\n  publisher={Elsevier}\n}\n```\n"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Choi, Wookjin and Dahiya, Navdeep and Nadeem, Saad",
        "format": "bibtex",
        "title": "CIRDataset: A large-scale Dataset for Clinically-Interpretable lung nodule Radiomics and malignancy prediction",
        "type": "Text_excerpt",
        "value": "@article{choi2022cirdataset,\n    year = {2022},\n    journal = {International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},\n    author = {Choi, Wookjin and Dahiya, Navdeep and Nadeem, Saad},\n    title = {CIRDataset: A large-scale Dataset for Clinically-Interpretable lung nodule Radiomics and malignancy prediction},\n}"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Choi, Wookjin and Nadeem, Saad and Alam, Sadegh R and Deasy, Joseph O and Tannenbaum, Allen and Lu, Wei",
        "format": "bibtex",
        "title": "Reproducible and Interpretable Spiculation Quantification for Lung Cancer Screening",
        "type": "Text_excerpt",
        "value": "@article{choi2021reproducible,\n    publisher = {Elsevier},\n    year = {2021},\n    pages = {105839},\n    volume = {200},\n    journal = {Computer Methods and Programs in Biomedicine},\n    author = {Choi, Wookjin and Nadeem, Saad and Alam, Sadegh R and Deasy, Joseph O and Tannenbaum, Allen and Lu, Wei},\n    title = {Reproducible and Interpretable Spiculation Quantification for Lung Cancer Screening},\n}"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/nadeemlab/CIR"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2022-01-31T16:13:32Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-08-16T01:02:06Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Clinically-Interpretable Radiomics [MICCAI'22, CMPB'21]"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9988552571191754,
      "result": {
        "original_header": "CIRDataset: A large-scale Dataset for Clinically-Interpretable lung nodule Radiomics and malignancy prediction [MICCAI'22]",
        "type": "Text_excerpt",
        "value": "*Spiculations/lobulations, sharp/curved spikes on the surface of lung nodules, are good predictors of lung cancer malignancy and hence, are routinely assessed and reported by radiologists as part of the standardized Lung-RADS clinical scoring criteria. Given the 3D geometry of the nodule and 2D slice-by-slice assessment by radiologists, manual spiculation/lobulation annotation is a tedious task and thus no public datasets exist to date for probing the importance of these clinically-reported features in the SOTA malignancy prediction algorithms. As part of this paper, we release a large-scale Clinically-Interpretable Radiomics Dataset, CIRDataset, containing 956 radiologist QA/QC'ed spiculation/lobulation annotations on segmented lung nodules from two public datasets, LIDC-IDRI (N=883) and LUNGx (N=73). We also present an end-to-end deep learning model based on multi-class Voxel2Mesh extension to segment nodules (while preserving spikes), classify spikes (sharp/spiculation and curved/lobulation), and perform malignancy prediction. Previous methods have performed malignancy prediction for LIDC and LUNGx datasets but without robust attribution to any clinically reported/actionable features (due to known hyperparameter sensitivity issues with general attribution schemes). With the release of this comprehensively-annotated dataset and end-to-end deep learning baseline, we hope that malignancy prediction methods can validate their explanations, benchmark against our baseline, and provide clinically-actionable insights. Dataset, code, pretrained models, and docker containers to reproduce the pipeline as well as the results in the manuscript are available in this repository.*\n \n"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8990799563660065,
      "result": {
        "original_header": "Dataset",
        "type": "Text_excerpt",
        "value": "The first CIR dataset, released [here](https://zenodo.org/record/6762573), contains almost 1000 radiologist QA/QC\u2019ed spiculation/lobulation annotations (computed using our published [LungCancerScreeningRadiomics](https://github.com/choilab-jefferson/LungCancerScreeningRadiomics) library [CMPB'21] and QA/QC'ed by a radiologist) on segmented lung nodules for two public datasets, LIDC (with visual radiologist malignancy RM scores for the entire cohort and pathology-proven malignancy PM labels for a subset) and LUNGx (with pathology-proven size-matched benign/malignant nodules to remove the effect of size on malignancy prediction).  \n![overview_image](./images/samples.png)*Clinically-interpretable spiculation/lobulation annotation dataset samples; the first column - input CT image; the second column - overlaid semi-automated/QA/QC'ed contours and superimposed area distortion maps (for quantifying/classifying spikes, computed from spherical parameterization -- see our [LungCancerScreeninigRadiomics Library](https://github.com/choilab-jefferson/LungCancerScreeningRadiomics)); the third column - 3D mesh model with vertex classifications, red: spiculations, blue: lobulations, white: nodule base.*\n \n"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9883176987205649,
      "result": {
        "original_header": "End-to-End Deep Learning Nodule Segmentation, Spikes' Classification (Spiculation/Lobulation), and Malignancy Prediction Model",
        "type": "Text_excerpt",
        "value": "We also release our multi-class [Voxel2Mesh](https://github.com/cvlab-epfl/voxel2mesh) extension to provide a strong benchmark for end-to-end deep learning lung nodule segmentation, spikes\u2019 classification (lobulation/spiculation), and malignancy prediction; Voxel2Mesh is the only published method to our knowledge that preserves sharp spikes during segmentation and hence its use as our base model. With the release of this comprehensively-annotated dataset, we hope that previous malignancy prediction methods can also validate their explanations/attributions and provide clinically-actionable insights. Users can also generate spiculation/lobulation annotations from scratch for LIDC/LUNGx as well as new datasets using our [LungCancerScreeningRadiomics](https://github.com/choilab-jefferson/LungCancerScreeningRadiomics) library [CMPB'21]. \n![architecure_image](./images/CIR_architecture.png)*Depiction of end-to-end deep learning architecture based on multi-class Voxel2Mesh extension. The standard UNet based voxel encoder/decoder (top) extracts features from the input CT volumes while the mesh decoder deforms an initial spherical mesh into increasing finer resolution meshes matching the target shape. The mesh deformation utilizes feature vectors sampled from the voxel decoder through the Learned Neighborhood (LN) Sampling technique and also performs adaptive unpooling with increased vertex counts in high curvature areas. We extend the architecture by introducing extra mesh decoder layers for spiculation and lobulation classification. We also sample vertices (shape features) from the final mesh unpooling layer as input to Fully Connected malignancy prediction network. We optionally add deep voxel-features from the last voxel encoder layer to the malignancy prediction network.*\n \n"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.998922670323316,
      "result": {
        "type": "Text_excerpt",
        "value": "This library serves as a one-stop solution for analyzing datasets using clinically-interpretable radiomics (CIR) in cancer imaging. The primary motivation for this comes from our collaborators in radiology and radiation oncology inquiring about the importance of clinically-reported features in state-of-the-art deep learning malignancy/recurrence/treatment response prediction algorithms. Previous methods have performed such prediction tasks but without robust attribution to any clinically reported/actionable features (see extensive literature on sensitivity of attribution methods to hyperparameters). This motivated us to curate datasets by annotating clinically-reported features at voxel/vertex-level on public datasets (using our CMPB'21 ) and relating these to prediction tasks (bypassing the \u201cflaky\u201d attribution schemes). With the release of these comprehensively-annotated datasets, we hope that previous malignancy prediction methods can also validate their explanations and provide clinically-actionable insights. We also provide strong end-to-end baselines for extracting these hard-to-compute clinically-reported features and using these in different prediction tasks. \n"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/nadeemlab/CIR/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 6
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/nadeemlab/CIR/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "nadeemlab/CIR"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_build_file": [
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/nadeemlab/CIR/main/docker/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/docker/Dockerfile",
      "technique": "file_exploration"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/nadeemlab/CIR/main/docker/build.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/nadeemlab/CIR/main/./images/spiculation_quantification_video.gif"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/nadeemlab/CIR/main/./images/samples.png"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "type": "Text_excerpt",
        "value": "It is highly recommended to install dependencies in either a python virtual environment or anaconda environment. Instructions for python virtual environment:\n```bash\npython3 -m venv venv\nsource venv/bin/activate\n(venv) pip install torch==1.11.0 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n(venv) pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py38_cu113_pyt1110/download.html\n(venv) pip install wandb sklearn scikit-image ipython ninja pandas opencv-python tqdm\n```\nPlease refer to the this [link](https://github.com/facebookresearch/pytorch3d/blob/main/INSTALL.md#3-install-wheels-for-linux) for the details of pytorch3d installation.\n"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/nadeemlab/CIR/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "cancer-imaging-research, clinical-interpretation-algorithm, computational-imaging, deep-learning, feature-extraction, interpretability, interpretable-deep-learning, medical-imaging, medical-imaging-datasets, python, pytorch-implementation, radiomics, radiomics-feature-extraction, radiomics-features"
      },
      "technique": "GitHub_API"
    }
  ],
  "logo": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/nadeemlab/CIR/main/./images/CIR_architecture.png"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "CIR"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "nadeemlab"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 88438,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Dockerfile",
        "size": 1163,
        "type": "Programming_language",
        "value": "Dockerfile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 37,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1808.08307.pdf\">CMPB'21 Paper</a>\n    |\n    <a href=\"https://zenodo.org/record/6762573\">CIRDataset</a>\n    |\n    <a href=\"https://github.com/choilab-jefferson/LungCancerScreeningRadiomics\">Annotation Pipeline</a>\n    |\n    <a href=\"#installation\">Installation</a>\n    |\n    <a href=\"#usage\">Usage</a>\n    |\n    <a href=\"#docker\">Docker</a>\n    |\n    <a href=\"https://github.com/nadeemlab/CIR/issues\">Issues</a>\n  </p>\n</p>\n\n\nThis library serves as a one-stop solution for analyzing datasets using clinically-interpretable radiomics (CIR"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2206.14903.pdf\">MICCAI'22 Paper</a>\n    |\n    <a href=\"https://arxiv.org/pdf/1808.08307.pdf\">CMPB'21 Paper</a>\n    |\n    <a href=\"https://zenodo.org/record/6762573\">CIRDataset</a>\n    |\n    <a href=\"https://github.com/choilab-jefferson/LungCancerScreeningRadiomics\">Annotation Pipeline</a>\n    |\n    <a href=\"#installation\">Installation</a>\n    |\n    <a href=\"#usage\">Usage</a>\n    |\n    <a href=\"#docker\">Docker</a>\n    |\n    <a href=\"https://github.com/nadeemlab/CIR/issues\">Issues</a>\n  </p>\n</p>\n\n\nThis library serves as a one-stop solution for analyzing datasets using clinically-interpretable radiomics (CIR"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/1808.08307.pdf"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Running Pre-trained Models",
        "parent_header": [
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "1. Mesh Only model is available [here](https://zenodo.org/record/6762573)\n```bash\n    tar xjvf pretrained_model-meshonly.tar.bz2\n    python test.py --model_path experiments/MICCAI2022/Experiment_001/trial_1\n```\n2. Mesh+Encoder model is available [here](https://zenodo.org/record/6762573)\n```bash\n    tar xjvf pretrained_model-mesh+encoder.tar.bz2\n    python test.py --model_path experiments/MICCAI2022/Experiment_002/trial_1\n```\n"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "license",
    "faq",
    "support",
    "identifier",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-04 12:08:50",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 27
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Usage",
        "type": "Text_excerpt",
        "value": "```bash\ngit clone --recursive git@github.com:nadeemlab/CIR.git\n```\nHigh level usage instructions are detailed below. Detailed instructions at each step, including running pre-trained models, are described in following subsections.\n\nStep 1: Update config.py. You may need to set the path to the dataset and also the directory to save the results. All ready to train/test data is available [here](https://zenodo.org/record/6762573).\n\nStep 2: You have to first perform data pre-processing. `python data_preprocess.py`\n\nStep 3: Now execute `python main.py` and this will start training the network.\n\nStep 4: Test the trained model. `python test.py`\n"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Data Pre-processing",
        "parent_header": [
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "Pre-processed data will be saved at the dataset directory.\n\nStep 2.0: Generate nrrd files using LungCancerScreeningRadiomics\n- Lung nodule spiculation data can be generated from the scratch using [LungCancerScreeninigRadiomics](https://github.com/choilab-jefferson/LungCancerScreeningRadiomics) [CMPB'21] for LIDC-IDRI and LUNGx dataset.  \n\n- Pre-processed data is available [here](https://zenodo.org/record/6762573).\n```bash\n   tar xjvf CIRDataset_LCSR.tar.bz2\n```\n\nStep 2.1: Convert isotropic voxel data from LungCancerScreeningRadiomics to 64x64x64 cubic image patch for 3D CNN models (dataset/NoduleDataset.py)\n- Input: Each case consists of four nrrd files (SimpleITK)  \n    LIDC-IDRI-0001_CT_1-all.nrrd                - CT Image  \n    LIDC-IDRI-0001_CT_1-all-ard.nrrd            - Area Distortion Map  \n    LIDC-IDRI-0001_CT_1-all-label.nrrd          - Nodule Segmentation  \n    LIDC-IDRI-0001_CT_1-all-spikes-label.nrrd    - Spike Classification - Spiculation:1, Lobulation: 2, Attachment: 3  \n- Output: Each case consists of four npy files (numpy) - 64x64x64 cubic image patch  \n    LIDC-IDRI-0001_iso0.70_s_0_CT.npy           - CT Image  \n    LIDC-IDRI-0001_iso0.70_s_0_ard.npy          - Area Distortion Map  \n    LIDC-IDRI-0001_iso0.70_s_0_nodule.npy       - Nodule Segmentation  \n    LIDC-IDRI-0001_iso0.70_s_0_spikes.npy        - Spike Classification - Spiculation:1, Lobulation: 2, Attachment: 3  \n\n- Pre-processed data is available [here](https://zenodo.org/record/6762573).\n```bash\n   tar xjvf CIRDataset_npy_for_cnn.tar.bz2\n```\n  \nStep 2.2: Divide datasets into subsets (Training, Validation, Testing), extract surface voxels, and combine voxel data and outcome data (dataset/lidc.py & dataset/lungx.py)\n- Input: Output from the previous step and outcome data  \n  LIDC.csv - Raiological malignancy (RM) only  \n  LIDC72.csv - RM and pathoogical malignancy (PM)  \n  LUNGx.csv - PM only  \n- Output: pickle files for each subset  \n  pre_computed_data_trainig_64_64_64.pickle  \n  pre_computed_data_validation_64_64_64.pickle (LUNGx does not have this)  \n  pre_computed_data_testing_64_64_64.pickle  \n\n- Pre-processed data is available [here](https://zenodo.org/record/6762573).\n```bash\n   tar xjvf CIRDataset_pickle_for_voxel2mesh.tar.bz2\n```\n"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Docker",
        "parent_header": [
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "We provide a Dockerfile that can be used to run the models inside a container.\nFirst, you need to install the [Docker Engine](https://docs.docker.com/engine/install/ubuntu/). For using GPU's you also need to install [NVIDIA container toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker). After installing the Docker, you need to follow these steps:\n\n1. Clone this repository.\n2. To create a docker image from the docker file; from top-level repository directory:\n```\ncd docker; ./build.sh\n```\n* Note: You may need to modify lines 1, 9 and 10 of Dockerfile to match your systems' cuda version.\n3. Upon successful docker image creation:\n* Pre-built docker image including data and pre-trained models is available [here](https://hub.docker.com/r/choilab/cir)\n```\ndocker run --gpus all -it choilab/cir /bin/bash\n```\n4. Then run `python3 test.py --model_path experiments/MICCAI2022/Experiment_001/trial_1` or `python3 test.py --model_path experiments/MICCAI2022/Experiment_002/trial_1` for testing either of the two pre-trained models.\n"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Reproducibility [MICCAI'22]",
        "parent_header": [
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "The following tables show the expected results\u00a0of running the pre-trained 'Mesh Only' and 'Mesh+Encoder' models (as **reported in the paper**).\n\n*Table1. Nodule (Class0), spiculation (Class1), and lobulation (Class2) peak classification metrics*\n<table>\n  <tr>\n    <th colspan=\"7\">Training</th>\n  </tr>\n  <tr>\n    <td rowspan=\"2\"><b>Network</b></td>\n    <td align=\"center\" vetical-align=\"middel\" colspan=\"3\">Chamfer Weighted Symmetric &#8595;</td>\n    <td align=\"center\" vetical-align=\"middel\" colspan=\"3\">Jaccard Index &#8593;</td>\n  </tr>\n  <tr>\n    <td>Class0</td>\n    <td>Class1</td>\n    <td>Class2</td>\n    <td>Class0</td>\n    <td>Class1</td>\n    <td>Class2</td>\n  </tr>\n  <tr>\n    <td><b>Mesh Only</b></td>\n    <td>0.009</td>\n    <td>0.010</td>\n    <td>0.013</td>\n    <td>0.507</td>\n    <td>0.493</td>\n    <td>0.430</td>\n  </tr>\n  <tr>\n    <td><b>Mesh+Encoder</b></td>\n    <td>0.008</td>\n    <td>0.009</td>\n    <td>0.011</td>\n    <td>0.488</td>\n    <td>0.456</td>\n    <td>0.410</td>\n  </tr>\n  <tr>\n    <th colspan=\"7\">Validation</th>\n  </tr>\n  <tr>\n    <td rowspan=\"2\"><b>Network</b></td>\n    <td align=\"center\" vetical-align=\"middel\" colspan=\"3\">Chamfer Weighted Symmetric &#8595;</td>\n    <td align=\"center\" vetical-align=\"middel\" colspan=\"3\">Jaccard Index &#8593;</td>\n  </tr>\n  <tr>\n    <td>Class0</td>\n    <td>Class1</td>\n    <td>Class2</td>\n    <td>Class0</td>\n    <td>Class1</td>\n    <td>Class2</td>\n  </tr>\n  <tr>\n    <td><b>Mesh Only</b></td>\n    <td>0.010</td>\n    <td>0.011</td>\n    <td>0.014</td>\n    <td>0.526</td>\n    <td>0.502</td>\n    <td>0.451</td>\n  </tr>\n  <tr>\n    <td><b>Mesh+Encoder</b></td>\n    <td>0.014</td>\n    <td>0.015</td>\n    <td>0.018</td>\n    <td>0.488</td>\n    <td>0.472</td>\n    <td>0.433</td>\n  </tr>\n  <tr>\n    <th colspan=\"7\">Testing LIDC-PM N=72</th>\n  </tr>\n  <tr>\n    <td rowspan=\"2\"><b>Network</b></td>\n    <td align=\"center\" vetical-align=\"middel\" colspan=\"3\">Chamfer Weighted Symmetric &#8595;</td>\n    <td align=\"center\" vetical-align=\"middel\" colspan=\"3\">Jaccard Index &#8593;</td>\n  </tr>\n  <tr>\n    <td>Class0</td>\n    <td>Class1</td>\n    <td>Class2</td>\n    <td>Class0</td>\n    <td>Class1</td>\n    <td>Class2</td>\n  </tr>\n  <tr>\n    <td><b>Mesh Only</b></td>\n    <td>0.011</td>\n    <td>0.011</td>\n    <td>0.014</td>\n    <td>0.561</td>\n    <td>0.553</td>\n    <td>0.510</td>\n  </tr>\n  <tr>\n    <td><b>Mesh+Encoder</b></td>\n    <td>0.009</td>\n    <td>0.010</td>\n    <td>0.012</td>\n    <td>0.558</td>\n    <td>0.541</td>\n    <td>0.507</td>\n  </tr>\n  <tr>\n    <th colspan=\"7\">Testing LUNGx N=73</th>\n  </tr>\n  <tr>\n    <td rowspan=\"2\"><b>Network</b></td>\n    <td align=\"center\" vetical-align=\"middel\" colspan=\"3\">Chamfer Weighted Symmetric &#8595;</td>\n    <td align=\"center\" vetical-align=\"middel\" colspan=\"3\">Jaccard Index &#8593;</td>\n  </tr>\n  <tr>\n    <td>Class0</td>\n    <td>Class1</td>\n    <td>Class2</td>\n    <td>Class0</td>\n    <td>Class1</td>\n    <td>Class2</td>\n  </tr>\n  <tr>\n    <td><b>Mesh Only</b></td>\n    <td>0.029</td>\n    <td>0.028</td>\n    <td>0.030</td>\n    <td>0.502</td>\n    <td>0.537</td>\n    <td>0.545</td>\n  </tr>\n  <tr>\n    <td><b>Mesh+Encoder</b></td>\n    <td>0.017</td>\n    <td>0.017</td>\n    <td>0.019</td>\n    <td>0.506</td>\n    <td>0.523</td>\n    <td>0.525</td>\n  </tr>\n</table>\n&nbsp;  \n\n*Table 2. Malignancy prediction metrics.*\n<table>\n  <tr>\n    <th colspan=\"6\">Training</th>\n  </tr>\n  <tr>\n    <td><b>Network</b></td>\n    <td>AUC</td>\n    <td>Accuracy</td>\n    <td>Sensitivity</td>\n    <td>Specificity</td>\n    <td>F1</td>\n  </tr>\n  <tr>\n    <td><b>Mesh Only</b></td>\n    <td>0.885</td>\n    <td>80.25</td>\n    <td>54.84</td>\n    <td>93.04</td>\n    <td>65.03</td>\n  </tr>\n  <tr>\n    <td><b>Mesh+Encoder</b></td>\n    <td>0.899</td>\n    <td>80.71</td>\n    <td>55.76</td>\n    <td>93.27</td>\n    <td>65.94</td>\n  </tr>\n  <tr>\n    <th colspan=\"6\">Validation</th>\n  </tr>\n  <tr>\n    <td><b>Network</b></td>\n    <td>AUC</td>\n    <td>Accuracy</td>\n    <td>Sensitivity</td>\n    <td>Specificity</td>\n    <td>F1</td>\n  </tr>\n  <tr>\n    <td><b>Mesh Only</b></td>\n    <td>0.881</td>\n    <td>80.37</td>\n    <td>53.06</td>\n    <td>92.11</td>\n    <td>61.90</td>\n  </tr>\n  <tr>\n    <td><b>Mesh+Encoder</b></td>\n    <td>0.808</td>\n    <td>75.46</td>\n    <td>42.86</td>\n    <td>89.47</td>\n    <td>51.22</td>\n  </tr>\n  <tr>\n    <th colspan=\"6\">Testing LIDC-PM N=72</th>\n  </tr>\n  <tr>\n    <td><b>Network</b></td>\n    <td>AUC</td>\n    <td>Accuracy</td>\n    <td>Sensitivity</td>\n    <td>Specificity</td>\n    <td>F1</td>\n  </tr>\n  <tr>\n    <td><b>Mesh Only</b></td>\n    <td>0.790</td>\n    <td>70.83</td>\n    <td>56.10</td>\n    <td>90.32</td>\n    <td>68.66</td>\n  </tr>\n  <tr>\n    <td><b>Mesh+Encoder</b></td>\n    <td>0.813</td>\n    <td>79.17</td>\n    <td>70.73</td>\n    <td>90.32</td>\n    <td>79.45</td>\n  </tr>\n  <tr>\n    <th colspan=\"6\">Testing LUNGx N=73</th>\n  </tr>\n  <tr>\n    <td><b>Network</b></td>\n    <td>AUC</td>\n    <td>Accuracy</td>\n    <td>Sensitivity</td>\n    <td>Specificity</td>\n    <td>F1</td>\n  </tr>\n  <tr>\n    <td><b>Mesh Only</b></td>\n    <td>0.733</td>\n    <td>68.49</td>\n    <td>80.56</td>\n    <td>56.76</td>\n    <td>71.60</td>\n  </tr>\n  <tr>\n    <td><b>Mesh+Encoder</b></td>\n    <td>0.743</td>\n    <td>65.75</td>\n    <td>86.11</td>\n    <td>45.95</td>\n    <td>71.26</td>\n  </tr>\n</table>\n\n"
      },
      "source": "https://raw.githubusercontent.com/nadeemlab/CIR/main/README.md",
      "technique": "header_analysis"
    }
  ]
}