{
  "application_domain": [
    {
      "confidence": 20.48,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/dhcai21/HaploDMF"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2022-05-31T05:11:34Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-02-08T19:19:47Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "viral haplotyps reconstruction from long reads via Deep Matrix Factorization"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.8881694197526779,
      "result": {
        "original_header": "Output Results",
        "type": "Text_excerpt",
        "value": "All reconstructed haplotypes (Polished by Medaka) are summarized in a file \"*_haplotypes.fasta\". Below is an example of three haplotypes.\n```\n>haplotype_0_length_9181_abundance_0.50_number_of_reads_5000_depth_500\nGGTCTCTCTGGTTAGACCAGATCTGAGCCTGGGAGGTCTCTGGCTAACTAGGGAACC...\n>haplotype_1_length_9178_abundance_0.30_number_of_reads_3000_depth_300\nGTCTCTCTGGTTAGACCAGATCTGAGCCTGGGAGCTCTCTGGCTAACTAGGGAACCC...\n>haplotype_2_length_9180_abundance_0.20_number_of_reads_2000_depth_200\nGGTCTCTCTGGTTAGACCAGATCTGAGCCTGGGAGCTCTCTGGCTAACTAGGGGACC...\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9426246513326807,
      "result": {
        "original_header": "Clusters of Reads",
        "type": "Text_excerpt",
        "value": "For the corresponding read clusters of haplotypes, the alignment file of each cluster can be found in the folder \"clusters\".\n \n"
      },
      "source": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/dhcai21/HaploDMF/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "faq": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Possible problems",
        "parent_header": [
          "HaploDMF",
          "An easiler way to install",
          "An optional way to install"
        ],
        "type": "Text_excerpt",
        "value": "* '../lib/libcrypto.1.0.0.dylib' (no such file) when using samtools\n\n  You can use the command:\n\n  `ln -s your_conda/rvhaplo/lib/libcrypto.your_exisiting_version.dylib your_conda/rvhaplo/lib/libcrypto.1.0.0.dylib`\n\n* AttributeError: module 'distutils' has no attribute 'version'\n\n  You can use the command:\n\n  `pip install setuptools==59.5.0`\n"
      },
      "source": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 0
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/dhcai21/HaploDMF/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "dhcai21/HaploDMF"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "HaploDMF"
      },
      "source": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/haplodmf.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "An easiler way to install",
        "parent_header": [
          "HaploDMF"
        ],
        "type": "Text_excerpt",
        "value": "After cloning this respository, you can use anaconda to install the **haplodmf.yaml**. This will install all packages you need with gpu mode (make sure you have installed cuda on your system to use the gpu version. Othervise, it will run with cpu version). The command is: `conda env create -f haplodmf.yaml -n haplodmf`\n\n* For cpu version pytorch: `conda install pytorch torchvision torchaudio cpuonly -c pytorch`\n* For gpu version pytorch: Search [pytorch](https://pytorch.org/) to find the correct cuda version according to your computer\n"
      },
      "source": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "An optional way to install",
        "parent_header": [
          "HaploDMF",
          "An easiler way to install"
        ],
        "type": "Text_excerpt",
        "value": "```\nconda create -n haplodmf python=3.8.13\n\nconda activate haplodmf\n\nconda install -c bioconda -c conda-forge medaka\n\nconda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia  (Install a correct version of pytorch according to your system)\n\npip install sklearn pandas tqdm\n```\n"
      },
      "source": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9775005713147837,
      "result": {
        "original_header": "Simulation data",
        "type": "Text_excerpt",
        "value": "Simulation datasets can be downloaded at https://portland-my.sharepoint.com/:f:/g/personal/dhcai2-c_my_cityu_edu_hk/Eiey3SkmQXxGlEfWhpgzCVcBmxQ12kbaE6m-djT6c5rWRA?e=Gkz1Di  \n"
      },
      "source": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/dhcai21/HaploDMF/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "GNU General Public License v3.0",
        "spdx_id": "GPL-3.0",
        "type": "License",
        "url": "https://api.github.com/licenses/gpl-3.0",
        "value": "https://api.github.com/licenses/gpl-3.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "HaploDMF"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "dhcai21"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 31301,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 12067,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Dependencies:",
        "parent_header": [
          "HaploDMF"
        ],
        "type": "Text_excerpt",
        "value": "* Conda\n* Python >=3.8.13\n* samtools\n* [Medaka](https://github.com/nanoporetech/medaka) >=1.6.0\n* Pytorch>=1.10.0\n* Required python package: pysam, sklearn, pandas, tqdm, scipy\n"
      },
      "source": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Run HaploDMF on tested data",
        "parent_header": [
          "HaploDMF"
        ],
        "type": "Text_excerpt",
        "value": "`./haplodmf.sh -i test.sam -r reference.fasta -o test_result -p test`<BR/>\n"
      },
      "source": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "download",
    "contact",
    "contributors",
    "documentation",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-04 12:09:57",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 3
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Initialization",
        "parent_header": [
          "HaploDMF",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "`cd HaploDMF`<BR/>\n`chmod +x haplodmf.sh`"
      },
      "source": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Command",
        "parent_header": [
          "HaploDMF",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "`Example:   ./haplodmf.sh -i alignment.sam -r reference.fasta -o result -p prefix`<BR/>\n"
      },
      "source": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Parameters",
        "parent_header": [
          "HaploDMF",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "In HaploDMF, there are several groups of parameters with different purposes: parameters for detecting SNV sites, training the neural network, and clustering. Most of these parameters are provided for other developers who might want to modify our codes for other purposes. Only the following parameters may affect the result:\n\n1. In the training process, we used loss2 to help learn latent features between distant reads. When the sequencing coverage is large, loss2 will not affect the result significantly. Thus, the parameter of controlling the weights between two losses is not sensitive when the sequencing coverage is high. When the coverage is low, loss2\u2019s weight can be set to 0.1-0.2. \n\n2. We used the \u201celbow method\u201d to determine the number of clusters (haplotypes). Empirically, using the default values is suitable for most cases. Stringent values may output a few repetitive haplotypes with low abundance while relaxed values may miss the low-abundant haplotypes. But users can adjust the parameters according to the abundance of haplotypes, number of reads supporting haplotypes, etc., which are provided in the output file.\n\n\n```\nRequired arguments:\n    -i | --input:                     alignment file (sam)\n    -r | --refernece:                 reference genome (fasta)\n\nOptions:\n    -o  | --out:                      Path where to output the results. (default:./result)\n    -p  | --prefix STR:               Prefix of output file. (default: rvhaplo)\n    -is | --input_snv STR:            A file containing a set of SNV sites. (default:None)\n    -mq | --map_qual INT:             Smallest mapping quality in the alignment file. (default:0) \n    -sp | --start_pos INT:            Starting position for reconstructing haplotypes (default: 1)\n    -ep | --end_pos INT:              Ending position for reconstructing haplotypes (default: 1e10)\n    -a  | --abundance FLOAT:          Filter haplotypes with abundance less than a threshold (default: 0)\n    -h  | --help :                    Print help message.\n\n    SNV Detection:\n    -t  | --thread INT:               Number of CPU cores for multiprocessing. (default:8)\n    -e  | --error_rate FLOAT:         Sequencing error rate. (default: 0.1)\n    -s  | --signi_level FLOAT:        Significance level for binomial tests. (default: 0.05)\n    -cp | --cond_pro FLOAT:           A threshold of the maximum conditional probability for SNV sites. (default: 0.65)\n    -f  | --fre_snv FLOAT:            The most dominant base' frequency at a to-be-verified site should >= fre_snv. (default: 0.80)\n    -n1 | --num_read_1 INT:           Minimum # of reads for calculating the conditional probability given one conditional site. (default:10)\n    -n2 | --num_read_2 INT:           Minimum # of reads for calculating the conditional probability given more than one conditional sites. (default: 5)\n    -g  | --gap INT:                  Minimum length of gap between SNV sites for calculating the conditional probability. (default:15)\n    -ss | --smallest_snv INT:         Minimum # of SNV sites for haplotype construction. (default:20)\n\n    DMF model training\n    -w  | --weight FLOAT:             Weight (between 0 and 1) for loss 2. (default:0.2) \n    -lr | --learning_rate FLOAT:      Learning rate for DMF. (default:0.001) \n    -epo| --epoch INT:                Epoch for training. (default:20) \n    -bs | --batch_size INT:           Batch size for training. (default:1024) \n\n    Clustering  and reconstruction   \n    -al | --algorithm STR:            Algorithm for clustering: Hierarchical(ward) or KMeans. (default:ward) \n    -d  | --depth INT:                Depth limitation for consensus sequences generated from clusters. (default:5) \n    -c1 | --cluster_thres1 STR:       Threshold 1 of edit distance with the increasing number of clusters. (default:0.95) \n    -c2 | --cluster_thres2 STR:       Threshold 2 of edit distance with the increasing number of clusters. (default:0.90) \n    -li | --largest_iteration INT:    Largest iteration for clustering. (default:20) \n```\n\n`-mq  | --map_qual`\n\nIf you want to filter some reads with small mapping qualities, you can use this parameter (e.g., 20). When you data have a large number of reads, you can use this parameter to remove some bad-quality reads. This will help accelerate the running.\n\n`-is  | --input_snv`\n\nYou can use other SNV detection tools to obtain a set of SNV sites (1-index on a reference genome). And then input a file containing the SNV sites to HaploDMF for downstream analysis. An example of the file can be found in the folder *tested_data*.\n\n\n`-sp  | --start_pos`\n\nStarting position for reconstructing haplotypes. (1-index)\n\n`-ep  | --end_pos`\n\nEnding position for reconstructing haplotypes. A large default value is for covering the whole genome. (1-index)\n\n`-a  | --abundance `\n\nThis parameter is to filter low-abundance haplotypes from the output file. If you use \"-a 0.05\", then the output file will only contain haplotypes with abundance >= 0.05 (i.e., 5%). The default setting will keep all the haplotypes in the file. And the output file contains information including the number of reads, depth, and abundance for each haplotype. Theoretically, the threshold of abundance is related to the error rate. However, it is hard to define a clear relationship between error rate and abundance. According to our experience, when the sequencing error rate is high (e.g., 18%), the confidence of haplotypes with an abundance of less than 1% is small. Users need to consider other information like the number of reads to determine whether the low-abundant haplotypes are reliable. \n\n`SNV detection`\n\nPlease refer to [RVHaplo](https://github.com/dhcai21/RVHaplo)\n\n`-w  | --weight`\n\nThere are two losses in HaploDMF. Minimizing loss1 learns latent features of reads so that reads sharing more SNVs have similar latent features. Loss2 is to help learn latent features for distant SNV sites. If the distant SNV sites contain SNVs from some haplotypes, latent features of distant reads (without overlap) can be better generated. Because loss1 is the main loss to learn the latent features, we assign a large weight to it and a small weight to loss2. Here, -w/--weight is to assign the weight to loss2. According to our experiments, we suggest selecting a value between (0,0.2]. If the coverage of your data is large and without drop-off in the middle, this parameter will not affect the result significantly. \n\n`-lr  | --learning_rate`\n\nThe learning rate when training the neural network. \n\n`-epo  | --epoch`\n\nThe epoch for training. As the training process can converge fast, we use a small epoch here to reduce the training time. \n\n`-bs | --batch_size`\n\nThe batch size for training. If the memory of your device is small, you can reduce the batch size.\n\n`-al | --algorithm`\n\nThe default algorithm for cluster reads is hierarchical clustering (Ward's method). Because hierarchical clustering is time-consuming when the dataset is large, we provide an alternative clustering algorithm \"KMEAMS\". According to our experiments, the clustering results between these two methods are similar.\n\n`-d | --depth`\n\nThe depth limitation for generating consensus sequences. Because the consensus sequence contains more errors in the ending regions due to the small depth, we set a threshold of depth to remove these regions when calculating the distance between reads and consensus sequences for determining the number of clusters. As the depth is small, removing these regions will not affect the final number of clusters. \n\n`-c1 | --cluster_thres1`\n\nWe use the \"elbow method\" to determine the number of clusters. If the number of different bases between reads and their consensus sequences does not change significantly (e.g., > 0.95) with the increasing number of clusters, the iteration will stop. \n\n`-c2 | --cluster_thres2`\n\nBecause the number of different bases may not decrease significantly at the beginning iteration when the sequencing error rate is high, we use this threshold to double-check the trend of different bases with the increasing number of clusters. For further details, please refer to the supplementary file.\n\n`-li | --largest_iteration`\n\nBecause the high values of \"-c1\" and \"-c2\" may not be able to stop the iteration, we set the largest iteration to avoid the infinite iteration. This value should be large than the number of haplotypes. Usually, the default value (20) is suitable for most datasets.\n"
      },
      "source": "https://raw.githubusercontent.com/dhcai21/HaploDMF/main/README.md",
      "technique": "header_analysis"
    }
  ]
}