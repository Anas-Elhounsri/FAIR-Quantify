{
  "acknowledgement": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Acknowledgement",
        "parent_header": [
          "[CVPR'22] DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis"
        ],
        "type": "Text_excerpt",
        "value": "With the help of Zongwei Zhou, Zuwei Guo started implementing the earlier ideas behind ``United & Unified'', which has branched out into DiRA. We thank them for their feasibility exploration, especially their initial evaluation on TransVW and various training strategies. This research has been supported in part by ASU and Mayo Clinic through a Seed Grant and an Innovation Grant and in part by the NIH under Award Number R01HL128785. The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH. This work  utilized the GPUs provided in part by the ASU Research Computing and in part by the Extreme Science and Engineering Discovery Environment (XSEDE) funded by the National Science Foundation (NSF) under grant number ACI-1548562.  Paper content is covered by patents pending. We build U-Net architecture for segmentation tasks by referring to the released code at [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch). The instance discrimination is based on [MoCo](https://github.com/facebookresearch/moco).\n\n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "application_domain": [
    {
      "confidence": 37.12,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citation",
        "parent_header": [
          "[CVPR'22] DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis"
        ],
        "type": "Text_excerpt",
        "value": "If you use this code or use our pre-trained weights for your research, please cite our paper:\n```\n@misc{haghighi2022dira,\n      title={DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis}, \n      author={Fatemeh Haghighi and Mohammad Reza Hosseinzadeh Taher and Michael B. Gotway and Jianming Liang},\n      year={2022},\n      eprint={2204.10437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Fatemeh Haghighi and Mohammad Reza Hosseinzadeh Taher and Michael B. Gotway and Jianming Liang",
        "format": "bibtex",
        "title": "DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis",
        "type": "Text_excerpt",
        "value": "@misc{haghighi2022dira,\n    primaryclass = {cs.CV},\n    archiveprefix = {arXiv},\n    eprint = {2204.10437},\n    year = {2022},\n    author = {Fatemeh Haghighi and Mohammad Reza Hosseinzadeh Taher and Michael B. Gotway and Jianming Liang},\n    title = {DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis},\n}"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/fhaghighi/DiRA"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2022-04-24T08:25:12Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-18T07:11:37Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Official PyTorch Implementation for DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis - CVPR 2022"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9662059185042031,
      "result": {
        "original_header": "[CVPR'22] DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis",
        "type": "Text_excerpt",
        "value": "This repository provides a PyTorch implementation of the [DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis](https://arxiv.org/pdf/2204.10437.pdf) which is published in [CVPR](https://cvpr2022.thecvf.com/) 2022 (main conference). \nDiscriminative learning, restorative learning, and adversarial learning have proven beneficial for self-supervised learning schemes in computer vision and medical imaging. Existing efforts, however, omit their synergistic effects on each other in a ternary setup, which, we envision, can significantly benefit deep semantic representation learning. To realize this vision, we have developed DiRA, the first framework that unites discriminative, restorative, and adversarial learning in a unified manner to collaboratively glean complementary visual information from unlabeled medical images for fine-grained semantic representation learning.  \n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9087668475378237,
      "result": {
        "original_header": "Publication",
        "type": "Text_excerpt",
        "value": "<b>DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis </b> <br/>\n[Fatemeh Haghighi](https://github.com/fhaghighi)<sup>1*</sup>, [Mohammad Reza Hosseinzadeh Taher](https://github.com/MR-HosseinzadehTaher)<sup>1*</sup>,  [Michael B. Gotway](https://www.mayoclinic.org/biographies/gotway-michael-b-m-d/bio-20055566)<sup>2</sup>, [Jianming Liang](https://chs.asu.edu/jianming-liang)<sup>1</sup><br/>\n<sup>1 </sup>Arizona State University, <sup>2 </sup>Mayo Clinic <br/>\n<sup>*</sup> Equal contributors ordered alphabetically.<br/>\nPublished in: **IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.** \n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9385353159612506,
      "result": {
        "original_header": "Major results from our work",
        "type": "Text_excerpt",
        "value": "2. **DiRA improves robustness to small data regimes.**\n<br/>\n<p align=\"center\"><img width=\"100%\" src=\"images/result1.png\" /></p>\n<br/> \nCredit to [superbar](https://github.com/scottclowe/superbar) by Scott Lowe for Matlab code of superbar.\n \n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9892680632859663,
      "result": {
        "original_header": "2. Pre-training DiRA",
        "type": "Text_excerpt",
        "value": "This implementation only supports multi-gpu, DistributedDataParallel training, which is faster and simpler; single-gpu or DataParallel training is not supported. The instance discrimination setup follows [MoCo](https://github.com/facebookresearch/moco). The checkpoints with the lowest validation loss are used for fine-tuning. We do unsupervised pre-training of a U-Net model with ResNet-50 backbone on ChestX-ray14 using 4 NVIDIA V100 GPUs. \n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9065575556620082,
      "result": {
        "original_header": "Fine-tuning on downstream tasks",
        "type": "Text_excerpt",
        "value": "For downstream tasks, we use the code provided by recent [transfer learning benchmark](https://github.com/MR-HosseinzadehTaher/BenchmarkTransferLearning) in medical imaging.  \nDiRA provides a pre-trained U-Net model, which the encoder can be utilized for the <i>classification</i> and encoder-decoder for the  <i>segmentation</i> downstream tasks.  \nFor classification tasks, a ResNet-50 encoder can be initialized with the pre-trained encoder of DiRA as follows:\n```python\nimport torchvision.models as models\n\nnum_classes = #number of target task classes\nweight = #path to DiRA pre-trained model\nmodel = models.__dict__['resnet50'](num_classes=num_classes)\nstate_dict = torch.load(weight, map_location=\"cpu\")\nif \"state_dict\" in state_dict:\n   state_dict = state_dict[\"state_dict\"]\nstate_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\nstate_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\nstate_dict = {k.replace(\"encoder.\", \"\"): v for k, v in state_dict.items()}\nfor k in list(state_dict.keys()):\n   if k.startswith('fc') or k.startswith('segmentation_head') or k.startswith('decoder') :\n      del state_dict[k]\nmsg = model.load_state_dict(state_dict, strict=False)\nprint(\"=> loaded pre-trained model '{}'\".format(weight))\nprint(\"missing keys:\", msg.missing_keys)\n``` \nFor segmentation tasks, a U-Net can be initialized with the pre-trained encoder and decoder of DiRA as follows:\n```python\nimport segmentation_models_pytorch as smp\n\nbackbone = 'resnet50'\nweight = #path to DiRA pre-trained model\nmodel=smp.Unet(backbone)\nstate_dict = torch.load(weight, map_location=\"cpu\")\nif \"state_dict\" in state_dict:\n   state_dict = state_dict[\"state_dict\"]\nstate_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\nstate_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\nfor k in list(state_dict.keys()):\n   if k.startswith('fc') or k.startswith('segmentation_head'):\n      del state_dict[k]\nmsg = model.load_state_dict(state_dict, strict=False)\nprint(\"=> loaded pre-trained model '{}'\".format(weight))\nprint(\"missing keys:\", msg.missing_keys)\n\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/JLiangLab/DiRA/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 9
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/fhaghighi/DiRA/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "fhaghighi/DiRA"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "[CVPR'22] DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/images/method_idea.png"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/images/w_wo_ReD.png"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/images/result1.png"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/images/CAM.png"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/images/result2.png"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "parent_header": [
          "[CVPR'22] DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis"
        ],
        "type": "Text_excerpt",
        "value": "Clone the repository and install dependencies using the following command:\n```bash\n$ git clone https://github.com/fhaghighi/DiRA.git\n$ cd DiRA/\n$ pip install -r requirements.txt\n```\n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "1. Preparing data",
        "parent_header": [
          "[CVPR'22] DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis",
          "Self-supervised pre-training"
        ],
        "type": "Text_excerpt",
        "value": "We used traing set of ChestX-ray14 dataset for pre-training 2D DiRA models, which can be downloaded from [this link](https://nihcc.app.box.com/v/ChestXray-NIHCC).\n\n- The downloaded ChestX-ray14 should have a directory structure as follows:\n```\nChestX-ray14/\n    |--  images/ \n         |-- 00000012_000.png\n         |-- 00000017_002.png\n         ... \n```\nWe use 10% of training data for validation. We also provide the list of training and validation images in ``dataset/Xray14_train_official.txt`` and ``dataset/Xray14_val_official.txt``, respectively. The training set is based on the officiall split provided by ChestX-ray14 dataset. Training labels are not used during pre-training stage. The path to images folder is required for pre-training stage.\n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.8001073389007474,
      "result": {
        "original_header": "Fine-tuning on downstream tasks",
        "type": "Text_excerpt",
        "value": "For downstream tasks, we use the code provided by recent [transfer learning benchmark](https://github.com/MR-HosseinzadehTaher/BenchmarkTransferLearning) in medical imaging.  \n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8554323936382263,
      "result": {
        "original_header": "[CVPR'22] DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis",
        "type": "Text_excerpt",
        "value": "<br/>\n<p align=\"center\"><img width=\"90%\" src=\"images/method_idea.png\" /></p>\n<br/>\n \n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8015003330323278,
      "result": {
        "original_header": "Fine-tuning on downstream tasks",
        "type": "Text_excerpt",
        "value": "For classification tasks, a ResNet-50 encoder can be initialized with the pre-trained encoder of DiRA as follows:\n```python\nimport torchvision.models as models\n\nnum_classes = #number of target task classes\nweight = #path to DiRA pre-trained model\nmodel = models.__dict__['resnet50'](num_classes=num_classes)\nstate_dict = torch.load(weight, map_location=\"cpu\")\nif \"state_dict\" in state_dict:\n   state_dict = state_dict[\"state_dict\"]\nstate_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\nstate_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\nstate_dict = {k.replace(\"encoder.\", \"\"): v for k, v in state_dict.items()}\nfor k in list(state_dict.keys()):\n   if k.startswith('fc') or k.startswith('segmentation_head') or k.startswith('decoder') :\n      del state_dict[k]\nmsg = model.load_state_dict(state_dict, strict=False)\nprint(\"=> loaded pre-trained model '{}'\".format(weight))\nprint(\"missing keys:\", msg.missing_keys)\n``` \nFor segmentation tasks, a U-Net can be initialized with the pre-trained encoder and decoder of DiRA as follows:\n```python\nimport segmentation_models_pytorch as smp\n\nbackbone = 'resnet50'\nweight = #path to DiRA pre-trained model\nmodel=smp.Unet(backbone)\nstate_dict = torch.load(weight, map_location=\"cpu\")\nif \"state_dict\" in state_dict:\n   state_dict = state_dict[\"state_dict\"]\nstate_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\nstate_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\nfor k in list(state_dict.keys()):\n   if k.startswith('fc') or k.startswith('segmentation_head'):\n      del state_dict[k]\nmsg = model.load_state_dict(state_dict, strict=False)\nprint(\"=> loaded pre-trained model '{}'\".format(weight))\nprint(\"missing keys:\", msg.missing_keys)\n\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/fhaghighi/DiRA/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "collaborative-learning, contrastive-learning, instance-discrimination, medical-imaging, self-supervised-learning, transfer-learning"
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "License Text for ASU GitHub Project\nCopyright 2019 Arizona Board of Regents for and on behalf of Arizona State University, a body corporate \nPatent Pending in the United States of America\nLICENSE\nArizona Board of Regents, for and on behalf of Arizona State University, a body corporate (\u201cLicensor\u201d) offers its \u201cDiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis\u201d project (Tech ID no. M22-158L) (the \u201cWork\u201d) and related Data solely for non-commercial use under the following terms and conditions:\n1. Definitions.\n      \"License\" means the terms and conditions for use, reproduction, and distribution of the Work and the Data set forth in this document.\n      \"You\" (or \"Your\") shall mean an individual or entity exercising permissions granted by this License.\n      \"Source\" form means the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n      \"Object\" form means any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n      \u201cData\u201d means the data compilation(s), data set(s), and database(s) made available by a Contributor under the License.\n      \"Work\" shall mean the work(s) of authorship, whether in Source or Object form, made available under the License, as identified above.\n      \"Derivative Works\" means any work, whether in Source or Object form, that incorporates, is based on, or is otherwise derived from the Work or from any other Derivative Work or Data authorized under this License and whereby such work  constitutes, as a whole, a separate original work of authorship. For the purposes of this License, Derivative Works do not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work, the Data, and Derivative Works thereof.\n      \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner of the submission or by an individual authorized to submit on behalf of the copyright owner.  With respect to Data, Contribution includes data sets, data compilations, and databases intentionally submitted to Licensor for inclusion in or for use in connection with the Work by the owners of the submission or by an individual authorized to submit on behalf of the owner.  For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"\n      \"Contributor\" shall mean Licensor and any individual or entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work, or in the case of Data has been submitted to and accepted by Licensor for inclusion in or for use in connection with the Work.\n2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, revocable (as stated in this section) right to perform any or all of the following actions solely for academic and other non-commercial research purposes: reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.  Any person or entity wishing to make a commercial use of the Work or a Derivative Work must enter into a separate written agreement with the Licensor and, in the case of a Derivative Work, any other applicable copyright owner of that Derivative Work. For the avoidance of doubt, no licenses to use the Work for commercial or for-profit purposes are granted hereunder.  If Licensor determines in its sole discretion that You have exceeded the scope or otherwise breached the terms of the License granted herein, it shall have the right to terminate the License in addition to all other remedies available to it under common law, by statute, and in equity.\n3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, revocable (as stated in this section) right to make, have made, use, and import the Work solely for academic and other non-commercial research purposes; provided that such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work. Any person or entity wishing to make a commercial use of the Work or a Derivative Work must enter into a separate written agreement with the Licensor and, in the case of a Derivative Work, any other applicable patent owner with claims pertaining to that Derivative Work. For the avoidance of doubt, no licenses to use the Work for commercial or for-profit purposes are granted hereunder.  If You institute patent litigation against any individual or entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work or such Contribution shall terminate as of the date such litigation is filed.  If Licensor determines in its sole discretion that You have exceeded the scope or otherwise breached the terms of the License granted herein, it shall have the right to terminate the License in addition to all other remedies available to it under common law, by statute, and in equity.\n4. Grant of Data License.  Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, revocable (as stated in this section) right to use the Data, in whole or in part, solely for academic and other non-commercial research purposes, including the right to copy, modify, or create derivative works of the Data, in whole or in part, in connection with or independent of any use of the Work or any Derivative Work.  Any person or entity wishing to make a commercial use of the Data must enter into a separate written agreement with the Licensor and, in the case of a Derivative Work, any other applicable owner of that Derivative Work.  For the avoidance of doubt, no licenses to use the Data for commercial or for-profit purposes are granted hereunder.  If Licensor determines in its sole discretion that You have exceeded the scope or otherwise breached the terms of the License granted herein, it shall have the right to terminate the License in addition to all other remedies available to it under common law, by statute, and in equity.\n5. Redistribution. You may reproduce and distribute copies of the Work, the Data, or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n      (a) The reproduction and distribution is only for academic or other non-commercial research purposes;\n      (b) You must give any other recipients of the Work, the Data, or Derivative Works a copy of this License;\n      (c) You must cause any modified files to carry prominent notices stating that You changed the files and include in a NOTICE text file a description of Your changes; \n      (d) You must retain, in the Source form of any Derivative Work that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, except for notices that do not pertain to any portion of the Derivative Work;\n      (e) If the Work or Data includes a \"NOTICE\" text file as part of its distribution, then any Derivative Work that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, except for notices that do not pertain to any part of the Derivative Work, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Work; within the Source form or documentation, if provided along with the Derivative Work; or, within a display generated by the Derivative Work, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work and/or Data, provided that such additional attribution notices cannot be construed as modifying the License.  You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work and/or Data otherwise complies with all the terms and conditions stated in this License;\n     (f) With respect to Licensor\u2019s Data, you must cite to Licensor\u2019s research papers \u2013 namely, \u201cTransferable Visual Words: Exploiting the Semantics of Anatomical Patterns for Self-supervised Learning\u201d - whether or not such papers are identified in any NOTICE text file accompanying the Data.\n5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in or use with the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions.  Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate agreement you may have executed or may execute with Licensor regarding such Contributions.\n6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor.    Pursuant to nominative fair use principles, you may merely identify Licensor as the original author of the Work and Data, as applicable.\n7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work and its Data (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND.  LICENSOR HEREBY DISCLAIMS ALL WARRANTIES, WHETHER EXPRESS, IMPLIED, STATUTORY OR OTHER (INCLUDING ALL WARRANTIES ARISING FROM COURSE OF DEALING, USAGE OR TRADE PRACTICE), AND SPECIFICALLY DISCLAIMS ALL IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. WITHOUT LIMITING THE FOREGOING, LICENSOR MAKES NO WARRANTY OF ANY KIND THAT THE WORK, OR ANY OTHER LICENSOR OR THIRD-PARTY GOODS, SERVICES, TECHNOLOGIES OR MATERIALS (INCLUDING ANY SOFTWARE OR HARDWARE), OR ANY PRODUCTS OR RESULTS OF THE USE OF ANY OF THEM, WILL MEET YOUR OR ANY OTHER PERSONS' REQUIREMENTS, OPERATE WITHOUT INTERRUPTION, ACHIEVE ANY INTENDED RESULT, BE COMPATIBLE OR WORK WITH ANY OTHER GOODS, SERVICES, TECHNOLOGIES OR MATERIALS (INCLUDING ANY SOFTWARE, HARDWARE, SYSTEM OR NETWORK), OR BE SECURE, ACCURATE, COMPLETE, FREE OF HARMFUL CODE OR ERROR FREE. ALL OPEN-SOURCE COMPONENTS AND OTHER THIRD-PARTY MATERIALS ARE PROVIDED \"AS IS\" AND ANY REPRESENTATION OR WARRANTY OF OR CONCERNING ANY OF THEM IS STRICTLY BETWEEN YOU AND THE THIRD-PARTY OWNER OR DISTRIBUTOR OF SUCH OPEN-SOURCE COMPONENTS AND THIRD-PARTY MATERIALS. You are solely responsible for determining the appropriateness of using or redistributing the Work and the Data and assume all risks associated with Your exercise of permissions under this License.\n8. Limitation of Liability. UNLESS REQUIRED BY APPLICABLE LAW OR OTHERWISE AGREED IN WRITING, IN NO EVENT WILL LICENSOR OR ANY CONTRIBUTOR BE LIABLE FOR DAMAGES UNDER OR IN CONNECTION WITH THIS LICENSE OR ITS SUBJECT MATTER UNDER ANY LEGAL OR EQUITABLE THEORY, INCLUDING BREACH OF CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY AND OTHERWISE, FOR ANY (a) INCREASED COSTS, DIMINUTION IN VALUE OR LOST BUSINESS, PRODUCTION, REVENUES OR PROFITS, (b) LOSS OF GOODWILL OR REPUTATION, (c) USE, INABILITY TO USE, LOSS, INTERRUPTION, DELAY OR RECOVERY OF ANY LICENSED SOFTWARE[ OR OPEN-SOURCE COMPONENTS OR OTHER THIRD-PARTY MATERIALS], (d) LOSS, DAMAGE, CORRUPTION OR RECOVERY OF DATA, OR BREACH OF DATA OR SYSTEM SECURITY, (e) COST OF REPLACEMENT GOODS OR SERVICES, OR (e) CONSEQUENTIAL, INCIDENTAL, INDIRECT, EXEMPLARY, SPECIAL, ENHANCED OR PUNITIVE DAMAGES, IN EACH CASE REGARDLESS OF WHETHER SUCH PERSONS WERE ADVISED OF THE POSSIBILITY OF SUCH LOSSES OR DAMAGES OR SUCH LOSSES OR DAMAGES WERE OTHERWISE FORESEEABLE, AND NOTWITHSTANDING THE FAILURE OF ANY AGREED OR OTHER REMEDY OF ITS ESSENTIAL PURPOSE\n9.  PyTorch Library.  The Work includes and is implemented on the PyTorch library.  The PyTorch library is offered under the BSD License as follows:\nFrom PyTorch:\n\nCopyright (c) 2016-     Facebook, Inc            (Adam Paszke)\nCopyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\nCopyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\nCopyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\nCopyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\nCopyright (c) 2011-2013 NYU                      (Clement Farabet)\nCopyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\nCopyright (c) 2006      Idiap Research Institute (Samy Bengio)\nCopyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n\nFrom Caffe2:\n\nCopyright (c) 2016-present, Facebook Inc. All rights reserved.\n\nAll contributions by Facebook:\nCopyright (c) 2016 Facebook Inc.\n \nAll contributions by Google:\nCopyright (c) 2015 Google Inc.\nAll rights reserved.\n \nAll contributions by Yangqing Jia:\nCopyright (c) 2015 Yangqing Jia\nAll rights reserved.\n \nAll contributions from Caffe:\nCopyright(c) 2013, 2014, 2015, the respective contributors\nAll rights reserved.\n \nAll other contributions:\nCopyright(c) 2015, 2016 the respective contributors\nAll rights reserved.\n \nCaffe2 uses a copyright model similar to Caffe: each contributor holds\ncopyright over their contributions to Caffe2. The project versioning records\nall such contribution and copyright details. If a contributor wants to further\nmark their specific copyright on a particular contribution, they should\nindicate their copyright solely in the commit message of the change when it is\ncommitted.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\n3. Neither the names of Facebook, Deepmind Technologies, NYU, NEC Laboratories America\n   and IDIAP Research Institute nor the names of its contributors may be\n   used to endorse or promote products derived from this software without\n   specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\nSUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\nCONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\nARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\nPOSSIBILITY OF SUCH DAMAGE.\n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/LICENSE",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "License",
        "parent_header": [
          "[CVPR'22] DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis"
        ],
        "type": "Text_excerpt",
        "value": "Released under the [ASU GitHub Project License](./LICENSE).\n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "DiRA"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "fhaghighi"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 296510,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/pdf/2204.10437.pdf"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Requirements",
        "parent_header": [
          "[CVPR'22] DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis"
        ],
        "type": "Text_excerpt",
        "value": "+ Linux\n+ Python\n+ Install PyTorch ([pytorch.org](http://pytorch.org))\n"
      },
      "source": "https://raw.githubusercontent.com/JLiangLab/DiRA/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "run",
    "download",
    "contact",
    "contributors",
    "documentation",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-04 12:04:21",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 99
      },
      "technique": "GitHub_API"
    }
  ]
}