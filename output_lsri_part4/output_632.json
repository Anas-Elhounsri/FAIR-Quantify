{
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/SchulzLab/TFAnalysis"
      },
      "technique": "GitHub_API"
    }
  ],
  "contact": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Contact",
        "parent_header": [
          "TFAnalysis"
        ],
        "type": "Text_excerpt",
        "value": "Please contact *fbejhati[at]mmci.uni-saarland.de* or *fschmidt[at]mmci.uni-saarland.de* in case of questions.\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2016-10-10T10:28:42Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-10-11T12:06:21Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "processing and analysis of transcription factor binding sites"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9555002564220157,
      "result": {
        "original_header": "TFAnalysis",
        "type": "Text_excerpt",
        "value": "This project contains our code used for generating the leaderboard, conference, and final round submissions to the _ENCODE DREAM in vivo Transcription Factor Binding Site Prediction Challenge_.\nThe master branch reflects the code we used for the final-round submissions. Tree 53508fd4b0 reflects the state of the code for the conference-round.\nThe following description  is valid for the final-round submission only.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9868296392996989,
      "result": {
        "original_header": "Required software",
        "type": "Text_excerpt",
        "value": "Note that _TEPIC_  has additional dependencies. A link to the respective repository is included in this project.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9624773437104698,
      "result": {
        "original_header": "Required data",
        "type": "Text_excerpt",
        "value": "In addition, the human reference genome in fasta format, version *hg19*, must be available. \nPosition Frequency Matrices (PFMs), obtained from Jaspar, Hocomoco, and Uniprobe are already included in the _TEPIC_ repository.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/SchulzLab/TFAnalysis/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 2
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/SchulzLab/TFAnalysis/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "SchulzLab/TFAnalysis"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "TFAnalysis"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Step by step guide",
        "parent_header": [
          "TFAnalysis"
        ],
        "type": "Text_excerpt",
        "value": "In the following sections, the usage of our pipeline is described step by step.\nPlease add a **/** after foldernames in the command line arguments.\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Processing TF ChIP-seq label tsv data",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide",
          "Data preprocessing"
        ],
        "type": "Text_excerpt",
        "value": "The provided TF ChIP-seq label tsv files are separated by TF and tissue. Further, the training data is balanced by randomly choosing\njust as many samples from the unbound class as there are for the bound class. \nUse the script `Preprocessing/Split_and_Balance_ChIP-seq_TSV_files.py` to perform these tasks. \n\nIn the Preprocessing folder, the command line is:\n```\npython Split_and_Balance_ChIP-seq_TSV_files.py <Folder containing the TF ChIP-seq label tsv files> <Target directory>\n```\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Computing DNase coverage in Bins",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide"
        ],
        "type": "Text_excerpt",
        "value": "We compute the DNase coverage in all bins used for training, testing, and the leaderboard data using the python script `Preprocessing/Compute_DNase_Coverage.py`\nTo compute the coverage, execute the following command in the Preprocessing folder.\n```\npython Compute_DNase_Coverage.py <Path to the DNase bam files> <Path to the directory containing the balanced ChIP-seq labels> <File containing the leaderboard regions> <File containing the test regions>  <Target directory>\n```\n\nWe use the _bedtools coverage_ tool to compute the DNase coverage in the bins from the DNase bam files. \nIn case that several DNase replicates are available for one tissue, the median coverage over all replicates will be computed. \nThis is done by the RScript `Preprocessing/computeMedianCoverage.R`.\nIn addition to the actual bins, we compute the DNase coverage for their left and right neighbouring bins. To simplify the merging process later on,\nwe generate additional files that contain the coordinates of the right and the left bins with respect to the original file and compute the coverage for those bins too.\n\nNote that this computation can take several hours; it also requires at least 500GB of main memory, as some DNase bam files are very large.\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Computing Transcription Factor affinities using TEPIC",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide"
        ],
        "type": "Text_excerpt",
        "value": "Transcription factor binding affinities are calculated using the [TEPIC](https://github.com/SchulzLab/TEPIC) method. \nThese affinities will be later used in a random forest model as features to predict the binding of a distinct TF.\n*TEPIC* has to be started manually on all labelled ChIP-seq bed files as well as on the leaderboard and test data bins.\nPlease check the TEPIC repository for details on the method. \n\nStarting TEPIC as follows produces all files for one TF which are necessary for its further processing:\n```\nbash TEPIC.sh -g <Reference genome> -b <Bed file> -o <Prefix of the output files (including the path)> -p <Position frequency matrices> -c <Number of cores>\n```\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Merging Transcription Factor affinities and DNase data for Model training.",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide"
        ],
        "type": "Text_excerpt",
        "value": "We provide a Python script to combine the TEPIC annotations with the DNase coverage data:\n`Preprocessing/IntegrateTraining.py`\n\nTo integrate the Test data, use the following command in the Preprocessing folder:\n```\npython IntegrateTraining.py <Path to the TEPIC annotations of the training data> <Path to the DNase coverage data for the middle bins> <Path to the DNase coverage data for the left bins> <Path to the DNase coverage data for the right bins> <Target directory>\n```\nThe files `Preprocessing/headerC.txt`, `Preprocessing/headerC_TL.txt`,`Preprocessing/headerL.txt`, and `Preprocessing/headerR.txt` are required to generate the correct headers while merging the data. \nBoth leaderboard data and test data will be integrated later.\n\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 1.1 Generating RData files",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide",
          "Predicting Transcription Factor binding in bins using the full feature set"
        ],
        "type": "Text_excerpt",
        "value": "Before the random forest models can be trained, the training data files need to be reformatted. \nTo shorten the time required for loading the data, the reformatted data is stored as a RData file.\nThis is done by the script `Preprocessing/Dump_Training_Data_As_RData.R`.\n\nThe command to run the script is:\n```\nRscript Dump_Training_As_RData.R <Folder holding the subfolders with the training data for all TFs> <Target directory for the RData files>\n```"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 1.2 Training Random Forests",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide",
          "Predicting Transcription Factor binding in bins using the full feature set"
        ],
        "type": "Text_excerpt",
        "value": "To train the random forests, the script `Classification/Train_Random_Forest_Classifiers_Full_Feature_Space.py` can be used.\n\nWe learn 4,500 trees and use the default values for cross validation. \nWe had to reduce the amount of training data to 30,000 bound and unbound samples of each class to make the learning feasible in terms of memory usage and\nFortran memory limitations.\n\nThe command is:\n```\npython Train_Random_Forest_Classifiers_Full_Feature_Space.py  <Folder containing the RData files containing features with the full feature space, produced in Step 1.1> <Target directory to store the output>\n```\nDue to space constraints, we can not use the full feature space for predictions on leaderboard and test data. \nTherefore, we use the feature importance of the learned models to determine which features should be used subsequently.\nFor each tissue that is avaiable as a training data set, we consider the top 20 features. The union of those will be used later to learn a model for that particular TF.\nThis script learns the models on all RData files that are present in the given directory.\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Determine top features",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide"
        ],
        "type": "Text_excerpt",
        "value": "To determine the top features, use the script `Classification/Get_Feature_Importance_From_Full_Models.py`.\nThe features are extracted from the RFs trained in Step 1.2.\n\nThe command is:\n```\npython Get_Feature_Importance_From_Full_Models.py <Target directory> <Path to the integrated files (txt) on full feature space> <Path to the RData file that should be processed> <Name of the TF that should be analysed>\n```\nNote that this must be called individually on all RData files and TFs for which the reduced feature space should be produced.\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Shrink the feature space",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide"
        ],
        "type": "Text_excerpt",
        "value": "We use the files containing the top TFs to generate the final TF features for our models. We have three scripts to extract the suitable\ndata from training, leaderboard and test files: `Preprocessing/CutTrainingData.py`, `Preprocessing/CutLeaderboardData.py` , `Preprocessing/CutTestData.py`\n\nTo shrink the training data run the following command in the Preprocessing folder:\n```\npython CutTrainingData.py <Path to the complete TF annotations used for training> <Path to the files containing the TFs that should be kept> < Target directory>\n```\n\nTo generate the TF data for the leaderboard round run the following command:\n```\npython CutLeaderboardData.py <Path containing the complete TF annotation of the leaderboard data> <Path to the files containing the TFs that should be kept> < Target directory>\n```\n\nTo generate the TF data for the final round run the following command:\n```\npython CutTestData.py <Path containing the complete TF annotation of the test data> <Path to the files containing the TFs that should be kept> < Target directory>\n```\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Merge TF annotations and DNase data for Training data, Leaderboard data, and Test data",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide"
        ],
        "type": "Text_excerpt",
        "value": "Before we can retrain the models and apply them to the Leadeboard and the Test data, we have to merge the TF affinities and the DNase data again.\nWe provide two individual Python scripts to combine the TEPIC annotations with the DNase coverage data:\n`Preprocessing/IntegrateLeaderboard.py`, `Preprocessing/IntegrateTest.py`\n\n\nTo integrate the Leaderboard data, use the following command in the Preprocessing folder:\n```\npython IntegrateLeaderboard.py <Path to the reduced TEPIC annotations of the leaderboard data> <Path to the DNase coverage data for the middle bins computed in the leaderboard regions> <Path to the DNase coverage data for the left bins computed in the leaderboard regions> <Path to the DNase coverage data for the right bins computed in the leaderboard regions> <Target directory>\n```\n\nTo integrate the Test data, use the following command in the Preprocessing folder:\n```\npython IntegrateTest.py <Path to the reduced TEPIC annotations of the test data> <Path to the DNase coverage data for the middle bins computed in the test regions> <Path to the DNase coverage data for the left bins computed in the training regions> <Path to the DNase coverage data for the right bins computed in the test regions> <Target directory>\n```\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Computing maximisied TF features",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide"
        ],
        "type": "Text_excerpt",
        "value": "In addition to shrink the feature space, we found that the performance of the random forests are improved when one considers the maximum affinity value for a TF in all adjacent bound training samples instead of the original values.\nThis transformation is performed by the script `Preprocessing/ConvertTrainingDataToMaxAffinityFormat.py`.\n\nThe command line to run this script is:\n```\npython ConvertTrainingDataToMaxAffinityFormat.py <Path to the shrunk, integrated, Training data sets> <Target directory>\n```\n\nSimilar to the maximum affinity transformation we perform on the Training data, we also reprocessed the Leaderboard and Test data. \nHere, for a center bin _i_ we determine a new value by searching for the maximum value in the 2 upstream bins _i-2_, _i-1_, in the downstream bins _i+1_, and  _i+2_ as well as in bin \n_i_ itself.\n\nThis is done using a first in first out queue in the script `Preprocessing/ConvertMaxLeaderboardTest.py`\n```\npython ConvertMaxLeaderboardTest.py <Path to either the shrunk, integrated, Test or Leaderboard Files> <Target directory>\n```\nNote that this script runs about 14 hours on the test data.\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 2.1 Generating RData files",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide",
          "Retrain the models"
        ],
        "type": "Text_excerpt",
        "value": "As above, before the random forest models can be trained, the training data files need to be reformatted and RData files are created.\nAgain, this is done by the script `Preprocessing/Dump_Training_Data_As_RData.R`.\n\nThe command to run the script is:\n```\nRscript Dump_Training_As_RData.R <Folder holding the subfolders with the shrunk training data for all TFs> <Target directory for the RData files>\n```\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 2.2 Learn models",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide",
          "Retrain the models"
        ],
        "type": "Text_excerpt",
        "value": "To train the random forests, the script `Classification/Train_Random_Forest_Classifiers_Reduced_Feature_Space.py` can be used.\nWe use the same parameteres as above.\n\nThe command is:\n```\npython Train_Random_Forest_Classifiers_Reduced_Feature_Space.py <Folder containing the RData files produced in Step 2.1> <Target directory to store the learned models>\n```\nThis learns models for all RData files that are present in the given directory.\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Apply them to Leaderboard data and Test data",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide"
        ],
        "type": "Text_excerpt",
        "value": "To make predictions on the leaderboard and test data sets, the script `Classification/Predict_TF_Binding.py` can be used. \nThis script has to be started manually for all files that should be classified.\n\nThe command to run the script for one such file is:\n```\npython Predict_TF_Binding.py <File to be classified> <Folder containing the trained random forest models from Step 2.2> <Name of the TF for which binding should be predicted> <Target directory to store the predictions> \n```\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Preparing data for submission",
        "parent_header": [
          "TFAnalysis",
          "Step by step guide"
        ],
        "type": "Text_excerpt",
        "value": "In order to reformat the data such that it fullfills the requirements of the challenge, use the script \n`Postprocessing/Submission_Format.bash`.\nHere, the data is sorted, renamed, and stored according to the challenge conventions.\n\nThe command to run the script is:\n```\nbash Submission_Format.bash <TF name> <Tissue name> <File to reformat> <F for Final round submission, L for Leaderboard submission>\n```"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9999962246647768,
      "result": {
        "original_header": "Required software",
        "type": "Text_excerpt",
        "value": "In order to operate our code on a linux system, the following softwares must be installed:\n- [bedtools](https://github.com/arq5x/bedtools2) (minimum version 2.25.0)\n- R (minimum version 3.x.x)\n- The _randomForest_ R-package\n- Python (minimum version 2.7)\n- [TEPIC](https://github.com/SchulzLab/TEPIC) \n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9635827546958458,
      "result": {
        "original_header": "Required data",
        "type": "Text_excerpt",
        "value": "To run our scripts, the following data from Synapse must be available in decompressed form:\n- The file *training_data.ChIPseq.tar*\n- The file *training_data.annotations.tar*\n- All DNase bam files stored in the [DNase bams folder at synapse](https://www.synapse.org/#!Synapse:syn6176232) \nIn addition, the human reference genome in fasta format, version *hg19*, must be available. \nPosition Frequency Matrices (PFMs), obtained from Jaspar, Hocomoco, and Uniprobe are already included in the _TEPIC_ repository.\n \n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/SchulzLab/TFAnalysis/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "MIT License\n\nCopyright (c) 2016 \n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/LICENSE",
      "technique": "file_exploration"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "TFAnalysis"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "SchulzLab"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 48666,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "R",
        "size": 24932,
        "type": "Programming_language",
        "value": "R"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 1022,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/SchulzLab/TFAnalysis/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "Florian411",
          "type": "User"
        },
        "date_created": "2017-09-01T09:27:57Z",
        "date_published": "2018-09-05T14:28:00Z",
        "description": "This is the release of our final contribution to the ENCODE DREAM in vivo Transcription Factor binding site prediction challenge.",
        "html_url": "https://github.com/SchulzLab/TFAnalysis/releases/tag/2.0",
        "name": "Release for F1000 article",
        "release_id": 12743482,
        "tag": "2.0",
        "tarball_url": "https://api.github.com/repos/SchulzLab/TFAnalysis/tarball/2.0",
        "type": "Release",
        "url": "https://api.github.com/repos/SchulzLab/TFAnalysis/releases/12743482",
        "value": "https://api.github.com/repos/SchulzLab/TFAnalysis/releases/12743482",
        "zipball_url": "https://api.github.com/repos/SchulzLab/TFAnalysis/zipball/2.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "requirements",
    "contributors",
    "documentation",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-11-04 04:13:25",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ]
}