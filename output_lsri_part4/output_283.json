{
  "application_domain": [
    {
      "confidence": 5.38,
      "result": {
        "type": "String",
        "value": "Audio"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/pachterlab/SBP_2019"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-08-23T03:20:52Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-04-11T09:58:25Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Code for producing the analysis in the \"Quantifying the tradeoff between sequencing depth and cell number in single-cell RNA-seq\" manuscript"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.8819285035538164,
      "result": {
        "original_header": "1) FASTQ Subsampling and processing with kallisto bus",
        "type": "Text_excerpt",
        "value": "Starting from the raw FASTQ files, subsample and process with kallisto bus to generate genecount matrices. This is done with the snakemake file `1) cell-depth-tradeoff-subsampling.py`. It can be called  by doing `snakemake -j 10 -s 1) cell-depth-tradeoff-subsampling.py`. `-j 10` runs 10 parallel jobs. It uses the `.tsv` file `cell-depth-tradeoff-metadata.tsv` to provide information for each run, described below. \nOnce downloaded, for each dataset the multiple `.fastq.gz` files should be concatenated into a single file for R1 and a single file for R2 for each dataset. For example:\n```\ncat pbmc_10k_v3_S1_L001_R1_001.fastq.gz pbmc_10k_v3_S1_L002_R1_001.fastq.gz > pbmc_10k_v3_R1_concat_1.fastq.gz\n```\nIt is necessary to have a single file for each read so that seqtk can subsample from it. \nThe output files produced for this step in the paper in CaltechDATA are in the tar file `snakemake_subsampling_output.tar.gz`\n \n"
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9048629379998403,
      "result": {
        "original_header": "Metadata parameters for the subsampling with `cell-depth-tradeoff-metadata.tsv`",
        "type": "Text_excerpt",
        "value": "The path to each subsampled file should be provided in the `dataset_sample_path` column of the `cell-depth-tradeoff-metadata.tsv`. The name of the R1 file should be in the `concat_read1_file` column, and the same for R2 and the `concat_read2_file` column.  \nFinally, the depths to be subsampled should be in the `subsampling_depths` column, in a single cell, and separated by commas (this is why it's a tsv file instead of csv, to avoid confusion). \n"
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.970147537322479,
      "result": {
        "original_header": "2) Create `.H5AD` files with subsampled count matrices",
        "type": "Text_excerpt",
        "value": "For each dataset, we need to parse all gene count matrices produces (outputted by bustools in .mtx format) into an h5 file using anndata. The short notebook `2) create_H5AD.ipynb` does this and provides a quick check that it worked.  \nThe output files produced for this step in the paper in CaltechDATA are `pbmc10k_subsamples.h5ad`, `heart10k_subsamples.h5ad` and `neurons10k_subsamples.h5ad` \n"
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8484345152379608,
      "result": {
        "original_header": "4) Process scVI results and generate figure",
        "type": "Text_excerpt",
        "value": "The notebook `4) make_all_plots.ipynb` will use the `.csv` and `.h5ad` files to perform analysis and figure creation. Further comments are provided in the notebook itself.\n \n"
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9712259557850689,
      "result": {
        "type": "Text_excerpt",
        "value": "Code for producing the analysis in the \"Quantifying the tradeoff between sequencing depth and cell number in single-cell RNA-seq\" by Valentine Svensson, Eduardo Beltrame and Lior Pachter \n"
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/pachterlab/SBP_2019/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/4%29%20make_all_plots.ipynb"
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/4%29%20make_all_plots.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/2%29%20create_H5AD.ipynb"
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/2%29%20create_H5AD.ipynb",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/Example_analysis.ipynb"
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/Example_analysis.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/pachterlab/SBP_2019/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "pachterlab/SBP_2019"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9442231269165594,
      "result": {
        "original_header": "1) FASTQ Subsampling and processing with kallisto bus",
        "type": "Text_excerpt",
        "value": "To run the snakemake file you need to have `seqkt` v1.3 or greater, `kallisto` v0.46 or greater and `bustools` v0.39.2 or greater. All of them can be installed with conda and the bioconda channel:\n```\nconda install -c bioconda seqtk\nconda install -c bioconda kallisto\nconda install -c bioconda bustools\n``` \nThe `FASTQ` files for the 3 datasets used in the paper can be downloaded from the 10x website:\n```\nhttps://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/pbmc_10k_v3\nhttps://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/heart_10k_v3\nhttps://support.10xgenomics.com/single-cell-gene-expression/datasets/3.0.0/neuron_10k_v3\n``` \nThe output files produced for this step in the paper in CaltechDATA are in the tar file `snakemake_subsampling_output.tar.gz`\n \n"
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8039189220031483,
      "result": {
        "type": "Text_excerpt",
        "value": "The workflow has 4 steps. The output data after each step can be downloaded from CaltechDATA at https://data.caltech.edu/records/1276 \n"
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8389684135572981,
      "result": {
        "original_header": "1) FASTQ Subsampling and processing with kallisto bus",
        "type": "Text_excerpt",
        "value": "Starting from the raw FASTQ files, subsample and process with kallisto bus to generate genecount matrices. This is done with the snakemake file `1) cell-depth-tradeoff-subsampling.py`. It can be called  by doing `snakemake -j 10 -s 1) cell-depth-tradeoff-subsampling.py`. `-j 10` runs 10 parallel jobs. It uses the `.tsv` file `cell-depth-tradeoff-metadata.tsv` to provide information for each run, described below. \n"
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/pachterlab/SBP_2019/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "SBP_2019"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "pachterlab"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 9178130,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 18852,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "3) Run scVI",
        "type": "Text_excerpt",
        "value": "The script `3) run_scvi.py` will process the `.h5ad` files defined in the `adata_files` dictionary. Note that even though each sampled point is relatively quick to train in scVI (10-15 minutes), because we sampled each dataset across multiple depths and numbers of cells, we end up with ~200 points to train per dataset. This can take a day or two to finish, since the script does not run parallel jobs by default. \n\n\nThe default files used to reproduce the workflow are:\n```\nadata_files= {\n'neurons10k' :'./neurons10k_subsamples.h5ad',     \n'heart10k' :'./heart10k_subsamples.h5ad',\n'pbmc10k' :'./pbmc10k_subsamples.h5ad',    \n}\n```\n\nWe keep 15% of the cells in the validation set and use 85% for training. This can be adjusted by changing the fraction of `total_cells` used for `n_retained_cells`.\n```\n    total_cells = adata.n_obs\n    n_retained_cells = int(0.85*total_cells)\n```\n\nThe list `cells_sizes` stores the numbers of cells to sample. In our workflow we started at 250 cells, and sampled upwards geometrically, multiplying by a factor of square root of 2 (1.41) until reaching the size of the training set.   \n\n```\n    sampling_size = 250\n    while sampling_size < n_retained_cells:\n        cells_sizes.append(sampling_size)\n        sampling_size = int(sampling_size*np.sqrt(2))\n```\nAfter scVI training, a t-SNE is computed in the embedding space and the coordinates are also saved. \nThe results of each trained point are saved in a csv file named `scvi_output_{ds}<dataset>_c<number of sampled cells>_d<subsampled depth>.csv`, for example `heart10k_c1409_d200000.csv`. This naming convention is used by the final analysis notebook to parse the csv files and perform plotting.\n\nThe output files produced for this step in the paper in CaltechDATA are in the tar file `scvi_output_all_datasets.tar.gz`\n"
      },
      "source": "https://raw.githubusercontent.com/pachterlab/SBP_2019/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-11-04 02:45:21",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 15
      },
      "technique": "GitHub_API"
    }
  ]
}