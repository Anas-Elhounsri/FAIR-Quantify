{
  "application_domain": [
    {
      "confidence": 0.9366368888909875,
      "result": {
        "type": "String",
        "value": "Semantic web"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/robinparky/prolucidComPIL"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2018-08-30T22:46:45Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-10-29T22:53:29Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 0.8542033392533671,
      "result": {
        "original_header": "Output file format",
        "type": "Text_excerpt",
        "value": "ProLuCIDComPIL outputs search results in the SQT file format, which contains unfiltered proteomic scoring information, including the best scoring peptide matches for each scan, parent proteins for each matched peptide, and other search-related information.\n```\nSQT Format\nS       10210   [information for scan #10210]\nM       1       [best scoring peptide match]\nL       [parent protein for peptide match 1]\nL       [parent protein for peptide match 1]\nL       [parent protein for peptide match 1]\nM       2       [second-best scoring peptide match]\nL       [parent protein for peptide match 2]\nL       [parent protein for peptide match 2]\n...\n```\n----\n### Running locally\n \n### Configuration Of Build_compil:\nDownload [build_compil](https://bitbucket.org/sulab/metaproteomics)\n1. go to directory where build_compil is downloaded\n2. edit ex/python/multiprocess_JSON_import.py\n    * Change \"HOST\" and \"PORT\" variables to match your mongodb configuration \n3. edit create_compil\n    1. Change variable \"FASTADB\"  so that it is assigned to \"${ORIGFASTADB%.*}\"_renumbered.\"${ORIGFASTADB##*.}\"\n    2. Change \"MONGO_HOST\" and \"MONGO_PORT\" to match your mongodb configuration\n        * Examples\n            * \"HOST = localhost\"\n            * \"PORT = 27017\" \n4. edit blazmass.params\n    * Change \"mongoDB_URI\" parameter to match your mongodb configuration\n        * Example: \"mongodb://localhost:27017\" \n###  Run Search\nDownload ProLuCIDCompil [here](http://fields.scripps.edu/prolucid_compil/download/prolucid_compil.jar).\n1. Run \"java -Xmx10G -jar prolucid_compil.jar example.ms2 search.xml [num_threads]\"\n\t- search.xml - edit search.xml as described in \"Configure Search Parameters\"\n\t- [num threads] - number of threads to assign to search; in general assigning more threads to search increased performance but increased strain on mongodb server and memory usage on local node. The optimum number of threads assigned per node would heavily depend on node specification, network configuration, and mongodb sharding configuration. For our 8 shard mongodb set up, I assigned 2 threads per node and had no more than 60 threads access the mongodb server.\n\t- Example:\n\t\t- java -Xmx10G -jar prolucid_compil.jar example.ms2 search.xml 4\n\t\t\n### Running DTASelect\n* Download DTASelect [here](http://fields.scripps.edu/yates/wp/?page_id=17)\n* When running DTASelect, add \"-noDB\" option to DTASelect.params or command line arguments. When the fasta file reaches sizes greater than 1 GB, DTASelect runs very slowly when it attempts to load the fasta file.  \"-noDB\" stops DTASelect from reading database files, and allows the program run the rest of analysis without issue. \n\t\n\t \n"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8207568107954769,
      "result": {
        "original_header": "Configuration Of Build_compil:",
        "type": "Text_excerpt",
        "value": "Download [build_compil](https://bitbucket.org/sulab/metaproteomics)\n1. go to directory where build_compil is downloaded\n2. edit ex/python/multiprocess_JSON_import.py\n    * Change \"HOST\" and \"PORT\" variables to match your mongodb configuration \n"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/robinparky/prolucidComPIL/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/robinparky/prolucidComPIL/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "robinparky/prolucidComPIL"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "ProLuCIDComPIL"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9768651693808184,
      "result": {
        "original_header": "Output file format",
        "type": "Text_excerpt",
        "value": "*tested on CentOS 7 \n**Requirements** \n* Java 1.8 (Oracle or OpenJDK)\n* [MongoDB 3.0+](http://www.mongodb.org/)\n    * MongoDB databases can be running locally (`localhost`), remotely as a single node (typically using TCP port 27017), or sharded behind a `mongos` process (typically port 27018)\n* Databases\n    * see metaproteomics repository for [build_compil](https://bitbucket.org/sulab/metaproteomics)\n    * Download Compil 2.0 here: ftp://massive.ucsd.edu/MSV000082943/updates/2018-12-26_titusj_78e282d8/sequence/combined_reverse.fasta\n    \n# Instructions \n### Configuration Of Build_compil:\nDownload [build_compil](https://bitbucket.org/sulab/metaproteomics)\n1. go to directory where build_compil is downloaded\n2. edit ex/python/multiprocess_JSON_import.py\n    * Change \"HOST\" and \"PORT\" variables to match your mongodb configuration \n3. edit create_compil\n    1. Change variable \"FASTADB\"  so that it is assigned to \"${ORIGFASTADB%.*}\"_renumbered.\"${ORIGFASTADB##*.}\"\n    2. Change \"MONGO_HOST\" and \"MONGO_PORT\" to match your mongodb configuration\n        * Examples\n            * \"HOST = localhost\"\n            * \"PORT = 27017\" \n4. edit blazmass.params\n    * Change \"mongoDB_URI\" parameter to match your mongodb configuration\n        * Example: \"mongodb://localhost:27017\" \n### Upload Fasta file to MongoDB\n* create_compil is current located in build_compil\n1. Go to directory where build_compil is installed\n2. Update the blazmass.params if needed\t\n3. run \"create_compil path/to/fasta/file database_name database_name\"\n\t* the fasta file should not have any reverse proteins on it\n\t* Example: \n\t\t* \"create_compil ~/testFasta/test.fasta testDB testDB\" \n### Configure Search Parameters\nDownload sample search.xml [here](http://fields.scripps.edu/prolucid_compil/download/search.xml).\n1. Edit \" <database_name>\\[database path]</database_name>\" line and replace \"[database path]\" with path to fasta file\n\t* Example:\n\t\t* <database_name>/home/yateslab/project_data/prolucid_compil/2610search/example.fasta</database_name>\n2. Edit \"<mongo_db_name>\\[insert database_name]</mongo_db_name>\" line and replace \\[insert database name] with database name\n\t* Example:\n\t\t* <mongo_db_name>testDB</mongo_db_name>\n\t* Database name should be the same as \"database_name\" used in step 3 in \"Upload Fasta File to MongoDB\" process\n3. Edit \" <mongo_uri>\\[insert database url]</mongo_uri>\" and replace \\[insert database url] with mongodb url\n\t* Example:\n\t\t* <mongo_uri>mongodb://localhost:27017</mongo_uri>\n4. Edit other parameters as necessary. The other parameters would match those of a regular prolucid search.xml \n###  Run Search\nDownload ProLuCIDCompil [here](http://fields.scripps.edu/prolucid_compil/download/prolucid_compil.jar).\n1. Run \"java -Xmx10G -jar prolucid_compil.jar example.ms2 search.xml [num_threads]\"\n\t- search.xml - edit search.xml as described in \"Configure Search Parameters\"\n\t- [num threads] - number of threads to assign to search; in general assigning more threads to search increased performance but increased strain on mongodb server and memory usage on local node. The optimum number of threads assigned per node would heavily depend on node specification, network configuration, and mongodb sharding configuration. For our 8 shard mongodb set up, I assigned 2 threads per node and had no more than 60 threads access the mongodb server.\n\t- Example:\n\t\t- java -Xmx10G -jar prolucid_compil.jar example.ms2 search.xml 4\n\t\t\n### Running DTASelect\n* Download DTASelect [here](http://fields.scripps.edu/yates/wp/?page_id=17)\n* When running DTASelect, add \"-noDB\" option to DTASelect.params or command line arguments. When the fasta file reaches sizes greater than 1 GB, DTASelect runs very slowly when it attempts to load the fasta file.  \"-noDB\" stops DTASelect from reading database files, and allows the program run the rest of analysis without issue. \n\t\n\t \n"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9998625426550348,
      "result": {
        "original_header": "Configuration Of Build_compil:",
        "type": "Text_excerpt",
        "value": "Download [build_compil](https://bitbucket.org/sulab/metaproteomics)\n1. go to directory where build_compil is downloaded\n2. edit ex/python/multiprocess_JSON_import.py\n    * Change \"HOST\" and \"PORT\" variables to match your mongodb configuration \n"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9991314438548482,
      "result": {
        "original_header": "Upload Fasta file to MongoDB",
        "type": "Text_excerpt",
        "value": "* create_compil is current located in build_compil\n1. Go to directory where build_compil is installed\n2. Update the blazmass.params if needed\t\n3. run \"create_compil path/to/fasta/file database_name database_name\"\n\t* the fasta file should not have any reverse proteins on it\n\t* Example: \n\t\t* \"create_compil ~/testFasta/test.fasta testDB testDB\" \n"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9904756246797398,
      "result": {
        "original_header": "Configure Search Parameters",
        "type": "Text_excerpt",
        "value": "Download sample search.xml [here](http://fields.scripps.edu/prolucid_compil/download/search.xml).\n1. Edit \" <database_name>\\[database path]</database_name>\" line and replace \"[database path]\" with path to fasta file\n\t* Example:\n\t\t* <database_name>/home/yateslab/project_data/prolucid_compil/2610search/example.fasta</database_name>\n2. Edit \"<mongo_db_name>\\[insert database_name]</mongo_db_name>\" line and replace \\[insert database name] with database name\n\t* Example:\n\t\t* <mongo_db_name>testDB</mongo_db_name>\n\t* Database name should be the same as \"database_name\" used in step 3 in \"Upload Fasta File to MongoDB\" process\n3. Edit \" <mongo_uri>\\[insert database url]</mongo_uri>\" and replace \\[insert database url] with mongodb url\n\t* Example:\n\t\t* <mongo_uri>mongodb://localhost:27017</mongo_uri>\n4. Edit other parameters as necessary. The other parameters would match those of a regular prolucid search.xml\n \n"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8441070879562126,
      "result": {
        "original_header": "We modified original Prolucid search engine to be compatible to ComPIL metaproteomics data analysis.",
        "type": "Text_excerpt",
        "value": "ProLuCIDCompil can be download here: [ProLuCIDComPIL.jar](http://fields.scripps.edu/prolucid_compil/download/prolucid_compil.jar).\n \n"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8019863879413272,
      "result": {
        "original_header": "Output file format",
        "type": "Text_excerpt",
        "value": "### Configure Search Parameters\nDownload sample search.xml [here](http://fields.scripps.edu/prolucid_compil/download/search.xml).\n1. Edit \" <database_name>\\[database path]</database_name>\" line and replace \"[database path]\" with path to fasta file\n\t* Example:\n\t\t* <database_name>/home/yateslab/project_data/prolucid_compil/2610search/example.fasta</database_name>\n2. Edit \"<mongo_db_name>\\[insert database_name]</mongo_db_name>\" line and replace \\[insert database name] with database name\n\t* Example:\n\t\t* <mongo_db_name>testDB</mongo_db_name>\n\t* Database name should be the same as \"database_name\" used in step 3 in \"Upload Fasta File to MongoDB\" process\n3. Edit \" <mongo_uri>\\[insert database url]</mongo_uri>\" and replace \\[insert database url] with mongodb url\n\t* Example:\n\t\t* <mongo_uri>mongodb://localhost:27017</mongo_uri>\n4. Edit other parameters as necessary. The other parameters would match those of a regular prolucid search.xml \n"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8297872597938422,
      "result": {
        "original_header": "Configuration Of Build_compil:",
        "type": "Text_excerpt",
        "value": "Download [build_compil](https://bitbucket.org/sulab/metaproteomics)\n1. go to directory where build_compil is downloaded\n2. edit ex/python/multiprocess_JSON_import.py\n    * Change \"HOST\" and \"PORT\" variables to match your mongodb configuration \n"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8067890829938806,
      "result": {
        "original_header": "Configure Search Parameters",
        "type": "Text_excerpt",
        "value": "Download sample search.xml [here](http://fields.scripps.edu/prolucid_compil/download/search.xml).\n1. Edit \" <database_name>\\[database path]</database_name>\" line and replace \"[database path]\" with path to fasta file\n\t* Example:\n\t\t* <database_name>/home/yateslab/project_data/prolucid_compil/2610search/example.fasta</database_name>\n2. Edit \"<mongo_db_name>\\[insert database_name]</mongo_db_name>\" line and replace \\[insert database name] with database name\n\t* Example:\n\t\t* <mongo_db_name>testDB</mongo_db_name>\n\t* Database name should be the same as \"database_name\" used in step 3 in \"Upload Fasta File to MongoDB\" process\n3. Edit \" <mongo_uri>\\[insert database url]</mongo_uri>\" and replace \\[insert database url] with mongodb url\n\t* Example:\n\t\t* <mongo_uri>mongodb://localhost:27017</mongo_uri>\n4. Edit other parameters as necessary. The other parameters would match those of a regular prolucid search.xml\n \n"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/robinparky/prolucidComPIL/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "prolucidComPIL"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "robinparky"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Running locally",
        "parent_header": [
          "ProLuCIDComPIL",
          "BASH2*"
        ],
        "type": "Text_excerpt",
        "value": "*tested on CentOS 7\n\n**Requirements**\n\n* Java 1.8 (Oracle or OpenJDK)\n* [MongoDB 3.0+](http://www.mongodb.org/)\n    * MongoDB databases can be running locally (`localhost`), remotely as a single node (typically using TCP port 27017), or sharded behind a `mongos` process (typically port 27018)\n* Databases\n    * see metaproteomics repository for [build_compil](https://bitbucket.org/sulab/metaproteomics)\n    * Download Compil 2.0 here: ftp://massive.ucsd.edu/MSV000082943/updates/2018-12-26_titusj_78e282d8/sequence/combined_reverse.fasta\n    "
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Run Search",
        "parent_header": [
          "Instructions"
        ],
        "type": "Text_excerpt",
        "value": "Download ProLuCIDCompil [here](http://fields.scripps.edu/prolucid_compil/download/prolucid_compil.jar).\n1. Run \"java -Xmx10G -jar prolucid_compil.jar example.ms2 search.xml [num_threads]\"\n\t- search.xml - edit search.xml as described in \"Configure Search Parameters\"\n\t- [num threads] - number of threads to assign to search; in general assigning more threads to search increased performance but increased strain on mongodb server and memory usage on local node. The optimum number of threads assigned per node would heavily depend on node specification, network configuration, and mongodb sharding configuration. For our 8 shard mongodb set up, I assigned 2 threads per node and had no more than 60 threads access the mongodb server.\n\t- Example:\n\t\t- java -Xmx10G -jar prolucid_compil.jar example.ms2 search.xml 4\n\t\t"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Running DTASelect",
        "parent_header": [
          "Instructions"
        ],
        "type": "Text_excerpt",
        "value": "* Download DTASelect [here](http://fields.scripps.edu/yates/wp/?page_id=17)\n* When running DTASelect, add \"-noDB\" option to DTASelect.params or command line arguments. When the fasta file reaches sizes greater than 1 GB, DTASelect runs very slowly when it attempts to load the fasta file.  \"-noDB\" stops DTASelect from reading database files, and allows the program run the rest of analysis without issue. \n\t\n\t\n\n\n"
      },
      "source": "https://raw.githubusercontent.com/robinparky/prolucidComPIL/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "download",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-11-04 05:25:37",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 2
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ]
}