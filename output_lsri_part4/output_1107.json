{
  "code_of_conduct": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment include:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n* Other conduct which could reasonably be considered inappropriate in a professional setting\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n## Scope\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at <gatk-support@broadinstitute.zendesk.com>. The project team will review and investigate all complaints, and will respond in a way that it deems appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\n\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4, available at [http://contributor-covenant.org/version/1/4][version]\n\n[homepage]: http://contributor-covenant.org\n[version]: http://contributor-covenant.org/version/1/4/\n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/CODE_OF_CONDUCT.md",
      "technique": "file_exploration"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/broadinstitute/gatk"
      },
      "technique": "GitHub_API"
    }
  ],
  "contributing_guidelines": [
    {
      "confidence": 1,
      "result": {
        "original_header": "<a name=\"contribute\">How to contribute to GATK</a>",
        "type": "Text_excerpt",
        "value": "(Note: section inspired by, and some text copied from, [Apache Parquet](https://github.com/apache/parquet-mr))\n \nWe welcome all contributions to the GATK project. The contribution can be a [issue report]( https://github.com/broadinstitute/gatk/issues) \nor a [pull request](https://github.com/broadinstitute/gatk/pulls). If you're not a committer, you will \nneed to [make a fork](https://help.github.com/articles/fork-a-repo/) of the gatk repository \nand [issue a pull request](https://help.github.com/articles/be-social/) from your fork.\n\nFor ideas on what to contribute, check issues labeled [\"Help wanted (Community)\"](https://github.com/broadinstitute/gatk/issues?q=is%3Aopen+is%3Aissue+label%3A%22Help+Wanted+%28Community%29%22). Comment on the issue to indicate you're interested in contibuting code and for sharing your questions and ideas.\n\nTo contribute a patch:\n* Break your work into small, single-purpose patches if possible. It\u2019s much harder to merge in a large change with a lot of disjoint features.\n* Submit the patch as a GitHub pull request against the master branch. For a tutorial, see the GitHub guides on [forking a repo](https://help.github.com/articles/fork-a-repo/) and [sending a pull request](https://help.github.com/articles/be-social/). If applicable, include the issue number in the pull request name.\n* Make sure that your code passes all our tests. You can run the tests with `./gradlew test` in the root directory.\n* Add tests for all new code you've written. We prefer unit tests but high quality integration tests that use small amounts of data are acceptable.\n* Follow the [**General guidelines for GATK4 developers**](https://github.com/broadinstitute/gatk#general-guidelines-for-gatk4-developers).\n\nWe tend to do fairly close readings of pull requests, and you may get a lot of comments. Some things to consider:\n* Write tests for all new code.\n* Document all classes and public methods.\n* For all public methods, check validity of the arguments and throw `IllegalArgumentException` if invalid.\n* Use braces for control constructs, `if`, `for` etc.\n* Make classes, variables, parameters etc `final` unless there is a strong reason not to.\n* Give your operators some room. Not `a+b` but `a + b` and not `foo(int a,int b)` but `foo(int a, int b)`.\n* Generally speaking, stick to the [Google Java Style guide](https://google.github.io/styleguide/javaguide.html)\n\nThank you for getting involved!\n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2014-12-02T20:50:36Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-11-04T01:02:30Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Official code repository for GATK versions 4 and up"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9726344329850645,
      "result": {
        "original_header": "GATK 4",
        "type": "Text_excerpt",
        "value": "This repository contains the next generation of the Genome Analysis Toolkit (GATK). The contents\nof this repository are 100% open source and released under the Apache 2.0 license (see [LICENSE.TXT](https://github.com/broadinstitute/gatk/blob/master/LICENSE.TXT)). \nGATK4 aims to bring together well-established tools from the [GATK](http://www.broadinstitute.org/gatk) and\n[Picard](http://broadinstitute.github.io/picard/) codebases under a streamlined framework,\nand to enable selected tools to be run in a massively parallel way on local clusters or in the cloud using\n[Apache Spark](http://spark.apache.org/). It also contains many newly developed tools not present in earlier\nreleases of the toolkit.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9161969756869984,
      "result": {
        "original_header": "<a name=\"downloading\">Downloading GATK4</a>",
        "type": "Text_excerpt",
        "value": "* A zip archive with everything you need to run GATK4 can be downloaded for each release from the [github releases page](https://github.com/broadinstitute/gatk/releases). We also host unstable archives generated nightly in the Google bucket gs://gatk-nightly-builds. \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9472874527680264,
      "result": {
        "original_header": "<a name=\"dockerSoftware\">Tools Included in Docker Image</a>",
        "type": "Text_excerpt",
        "value": "We also include an installation of R (3.6.2) with the following popular packages included:\n* data.table\n* dplyr\n* ggplot2 \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9545841820386085,
      "result": {
        "original_header": "<a name=\"building\">Building GATK4</a>",
        "type": "Text_excerpt",
        "value": "        ./gradlew bundle\n        \n  Equivalently, you can just type:\n  \n        ./gradlew\n        \n    * This creates a zip archive in the `build/` directory with a name like `gatk-VERSION.zip` containing a complete standalone GATK distribution, including our launcher `gatk`, both the local and spark jars, and this README.    \n    * You can also run GATK commands directly from the root of your git clone after running this command.\n    * Note that you *must* have a full git clone in order to build GATK, including the git-lfs files in `src/main/resources/large`. The zipped source code alone is not buildable.\n    * The large files under `src/main/resources/large/` are required to build GATK, since they are packaged inside the GATK jar and used by tools at runtime. These include things like ML models and native C/C++ libraries used for acceleration of certain tools.\n    * The large files under `src/test/resources/large/`, on the other hand, are only required by the test suite when running tests, and are not required to build GATK. \n* **Other ways to build:**\n    * `./gradlew installDist`  \n        * Does a *fast* build that only lets you run GATK tools from inside your git clone, and locally only (not on a cluster). Good for developers! \n    * `./gradlew installAll`\n        * Does a *semi-fast* build that only lets you run GATK tools from inside your git clone, but works both locally and on a cluster. Good for developers!\n    * `./gradlew localJar`\n        * Builds *only* the GATK jar used for running tools locally (not on a Spark cluster). The resulting jar will be in `build/libs` with a name like `gatk-package-VERSION-local.jar`, and can be used outside of your git clone.\n    * `./gradlew sparkJar`\n        * Builds *only* the GATK jar used for running tools on a Spark cluster (rather than locally). The resulting jar will be in `build/libs` with a name like `gatk-package-VERSION-spark.jar`, and can be used outside of your git clone. \n        * This jar will not include Spark and Hadoop libraries, in order to allow the versions of Spark and Hadoop installed on your cluster to be used. \n* For faster gradle operations, add `org.gradle.daemon=true` to your `~/.gradle/gradle.properties` file.\n  This will keep a gradle daemon running in the background and avoid the ~6s gradle start up time on every command. \n* Gradle keeps a cache of dependencies used to build GATK.  By default this goes in `~/.gradle`.  If there is insufficient free space in your home directory, you can change the location of the cache by setting the `GRADLE_USER_HOME` environment variable. \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9443382454914929,
      "result": {
        "original_header": "<a name=\"running\">Running GATK4</a>",
        "type": "Text_excerpt",
        "value": "* The standard way to run GATK4 tools is via the **`gatk`** wrapper script located in the root directory of a clone of this repository.\n    * Requires Python 2.6 or greater (this includes Python 3.x)\n    * You need to have built the GATK as described in the [Building GATK4](#building) section above before running this script.\n    * There are several ways `gatk` can be run:\n        * Directly from the root of your git clone after building\n        * By extracting the zip archive produced by `./gradlew bundle` to a directory, and running `gatk` from there\n        * Manually putting the `gatk` script within the same directory as fully-packaged GATK jars produced by `./gradlew localJar` and/or `./gradlew sparkJar`\n        * Defining the environment variables `GATK_LOCAL_JAR` and `GATK_SPARK_JAR`, and setting them to the paths to the GATK jars produced by `./gradlew localJar` and/or `./gradlew sparkJar` \n    * `gatk` can run non-Spark tools as well as Spark tools, and can run Spark tools locally, on a Spark cluster, or on Google Cloud Dataproc.\n    * ***Note:*** running with `java -jar` directly and bypassing `gatk` causes several important system properties to not get set, including htsjdk compression level!\n    \n* For help on using `gatk` itself, run **`./gatk --help`** \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9323305941107062,
      "result": {
        "original_header": "<a name=\"dataproc\">Running GATK4 Spark tools on Google Cloud Dataproc:</a>",
        "type": "Text_excerpt",
        "value": "  * You must have a [Google cloud services](https://cloud.google.com/) account, and have spun up a Dataproc cluster\n    in the [Google Developer's console](https://console.developers.google.com). You may need to have the \"Allow API access to all Google Cloud services in the same project\" option enabled (settable when you create a cluster).\n  * You need to have installed the Google Cloud SDK from [here](https://cloud.google.com/sdk/), since\n    `gatk` invokes the `gcloud` tool behind-the-scenes. As part of the installation, be sure\n      that you follow the `gcloud` setup instructions [here](https://cloud.google.com/sdk/gcloud/). As this library is frequently updated by Google, we recommend updating your copy regularly to avoid any version-related difficulties.\n  * Your inputs to the GATK when running on dataproc are typically in Google Cloud Storage buckets, and should be specified on\n    your GATK command line using the syntax `gs://my-gcs-bucket/path/to/my-file`\n  * You can run GATK4 jobs on Dataproc from your local computer or from the VM (master node) on the cloud. \n      ```\n      ./gatk PrintReadsSpark \\\n          -I gs://my-gcs-bucket/path/to/input.bam \\\n          -O gs://my-gcs-bucket/path/to/output.bam \\\n          -- \\\n          --spark-runner GCS --cluster myGCSCluster \\\n          --num-executors 5 --executor-cores 2 --executor-memory 4g \\\n          --conf spark.yarn.executor.memoryOverhead=600\n      ```\n  * When using Dataproc you can access the web interfaces for YARN, Hadoop and HDFS by opening an SSH tunnel and connecting with your browser.  This can be done easily using included `gcs-cluster-ui` script.\n  \n    BASH3*\n    Or see these [these instructions](https://cloud.google.com/dataproc/cluster-web-interfaces) for more details.\n  * Note that the spark-specific arguments are separated from the tool-specific arguments by a `--`.\n  * If you want to avoid uploading the GATK jar to GCS on every run, set the `GATK_GCS_STAGING`\n    environment variable to a bucket you have write access to (eg., `export GATK_GCS_STAGING=gs://<my_bucket>/`)\n  * Dataproc Spark clusters are configured with [dynamic allocation](https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation) so you can omit the \"--num-executors\" argument and let YARN handle it automatically.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9419065091411563,
      "result": {
        "original_header": "<a name=\"R\">Using R to generate plots</a>",
        "type": "Text_excerpt",
        "value": "Certain GATK tools may optionally generate plots using the R installation provided within the conda environment.  If you are uninterested in plotting, R is still required by several of the unit tests.  Plotting is currently untested and should be viewed as a convenience rather than a primary output.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9539232764007763,
      "result": {
        "original_header": "<a name=\"tab_completion\">Bash Command-line Tab Completion (BETA)</a>",
        "type": "Text_excerpt",
        "value": "* This tab completion functionality has only been tested in the bash shell, and is released as a beta feature. \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9348204125767657,
      "result": {
        "original_header": "<a name=\"dev_guidelines\">General guidelines for GATK4 developers</a>",
        "type": "Text_excerpt",
        "value": "* **Try to keep datafiles under 100kb in size.** Larger test files should go into `src/test/resources/large` (and subdirectories) so that they'll be stored and tracked by git-lfs as described [above](#lfs). \n* GATK4 is Apache 2.0 licensed.  The license is in the top level LICENSE.TXT file.  Do not add any additional license text or accept files with a license included in them. \n* Don't issue or accept pull requests that significantly decrease coverage (less than 1% decrease is sort of tolerable).  \n* Don't use `toString()` for anything other than human consumption (ie. don't base the logic of your code on results of `toString()`.) \n* Don't override `clone()` unless you really know what you're doing. If you do override it, document thoroughly. Otherwise, prefer other means of making copies of objects. \n* Git: Rebase and squash commits when merging. \n* If you push to master or mess up the commit history, you owe us 1 growler or tasty snacks at happy hour. If you break the master build, you owe 3 growlers (or lots of tasty snacks). Beer may be replaced by wine (in the color and vintage of buyer's choosing) in proportions of 1 growler = 1 bottle. \n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9765019398569011,
      "result": {
        "original_header": "<a name=\"testing\">Testing GATK</a>",
        "type": "Text_excerpt",
        "value": "* We use [Broad Jenkins](https://gatk-jenkins.broadinstitute.org/view/Performance/) for our long-running tests and performance tests.\n    * To add a performance test (requires Broad-ID), you need to make a \"new item\" in Jenkins and make it a \"copy\" instead of a blank project. You need to base it on either the \"-spark-\" jobs or the other kind of jobs and alter the commandline.  \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8985674202960549,
      "result": {
        "original_header": "<a name=\"intellij\">Creating a GATK project in the IntelliJ IDE (last tested with version 2016.2.4):</a>",
        "type": "Text_excerpt",
        "value": "* In IntelliJ, click on \"Import Project\" in the home screen or go to File -> New... -> Project From Existing Sources... \n* Ensure that \"Gradle project\" points to the build.gradle file in the root of your GATK clone \n* After downloading project dependencies, IntelliJ should open a new window with your GATK project \n* Make sure that the Java version is set correctly by going to File -> \"Project Structure\" -> \"Project\". Check that the \"Project SDK\" is set to your Java 17 JDK, and \"Project language level\" to 17 (you may need to add your Java 17 JDK under \"Platform Settings\" -> SDKs if it isn't there already). Then click \"Apply\"/\"Ok\".\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9240029724558991,
      "result": {
        "original_header": "<a name=\"debugging\">Setting up debugging in IntelliJ</a>",
        "type": "Text_excerpt",
        "value": "* Follow the instructions above for creating an IntelliJ project for GATK \n* Ensure that \"Use classpath of module:\" is set to use the \"gatk\" module's classpath \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9244071583833948,
      "result": {
        "original_header": "<a name=\"jprofiler\">Setting up profiling using JProfiler</a>",
        "type": "Text_excerpt",
        "value": "   * Running JProfiler standalone:\n       * Build a full GATK4 jar using `./gradlew localJar`\n       * In the \"Session Settings\" window, select the GATK4 jar, eg. `~/gatk/build/libs/gatk-package-4.alpha-196-gb542813-SNAPSHOT-local.jar` for \"Main class or executable JAR\" and enter the right \"Arguments\"\n       * Under \"Profiling Settings\", select \"sampling\" as the \"Method call recording\" method. \n   * Running JProfiler from within IntelliJ:\n       * JProfiler has great integration with IntelliJ (we're using IntelliJ Ultimate edition) so the setup is trivial.   \n       * Follow the instructions [above](#intellij) for creating an IntelliJ project for GATK  \n       * Right click on a test method/class/package and select \"Profile\" \n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8979034975730201,
      "result": {
        "original_header": "<a name=\"sonatype\">Uploading Archives to Sonatype (to make them available via maven central)</a>",
        "type": "Text_excerpt",
        "value": "* You must have a registered account on the sonatype JIRA (and be approved as a gatk uploader)\n* You need to configure several additional properties in your `/~.gradle/gradle.properties` file \n* If you want to upload a release instead of a snapshot you will additionally need to have access to the gatk signing key and password\n```\n#needed for snapshot upload\nsonatypeUsername=<your sonatype username>\nsonatypePassword=<your sonatype password>\n\n#needed for signing a release\nsigning.keyId=<gatk key id>\nsigning.password=<gatk key password>\nsigning.secretKeyRingFile=/Users/<username>/.gnupg/secring.gpg\n```\n \nBuilds are considered snapshots by default.  You can mark a build as a release build by setting `-Drelease=true`.  \nThe archive name is based off of `git describe`.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8962895975329404,
      "result": {
        "original_header": "<a name=\"docker_building\">Building GATK4 Docker images</a>",
        "type": "Text_excerpt",
        "value": "Please see the [the Docker README](scripts/docker/README.md) in ``scripts/docker``.  This has instructions for the Dockerfile in the root directory.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9024017266709875,
      "result": {
        "original_header": "<a name=\"gatkwdlgen\">Generating GATK4 WDL Wrappers</a>",
        "type": "Text_excerpt",
        "value": "* A WDL wrapper can be generated for any GATK4 tool that is annotated for WDL generation (see the wiki article\n[How to Prepare a GATK tool for WDL Auto Generation](https://github.com/broadinstitute/gatk/wiki/How-to-Prepare-a-GATK-tool-for-WDL-Auto-Generation))\nto learn more about WDL annotations. \n* To generate WDL Wrappers and validate the resulting outputs, run `./gradlew gatkWDLGenValidation`.\nRunning this task requires a local [cromwell](https://github.com/broadinstitute/cromwell) installation, and environment\nvariables `CROMWELL_JAR` and `WOMTOOL_JAR` to be set to the full pathnames of the `cromwell` and `womtool` jar files.\nIf no local install is available, this task will run automatically on github actions in a separate job whenever a PR is submitted. \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9453079510528709,
      "result": {
        "original_header": "<a name=\"zenhub\">Using Zenhub to track github issues</a>",
        "type": "Text_excerpt",
        "value": "* Zenhub allows the GATK development team to assign time estimates to issues, and to mark issues as Triaged/In Progress/In Review/Blocked/etc.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9405933250421986,
      "result": {
        "original_header": "<a name=\"spark_further_reading\">Further Reading on Spark</a>",
        "type": "Text_excerpt",
        "value": "[Apache Spark](https://spark.apache.org/) is a fast and general engine for large-scale data processing.\nGATK4 can run on any Spark cluster, such as an on-premise Hadoop cluster with HDFS storage and the Spark\nruntime, as well as on the cloud using Google Dataproc. \nIn a cluster scenario, your input and output files reside on HDFS, and Spark will run in a distributed fashion on the cluster.\nThe Spark documentation has a good [overview of the architecture](https://spark.apache.org/docs/latest/cluster-overview.html). \nYou can find more information about tuning Spark and choosing good values for important settings such as the number\nof executors and memory settings at the following: \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9606116563210487,
      "result": {
        "original_header": "<a name=\"discussions\">Discussions</a>",
        "type": "Text_excerpt",
        "value": "* [GATK forum](https://gatk.broadinstitute.org/hc/en-us/community/topics) for general discussions on how to use the GATK and support questions.\n* [Issue tracker](https://github.com/broadinstitute/gatk/issues) to report errors and enhancement ideas. \n* Discussions also take place in [GATK pull requests](https://github.com/broadinstitute/gatk/pulls)\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.949212992697491,
      "result": {
        "original_header": "<a name=\"citing\">Citing GATK</a>",
        "type": "Text_excerpt",
        "value": "If you use GATK in your research, please see [this article](https://gatk.broadinstitute.org/hc/en-us/articles/360035530852-How-should-I-cite-GATK-in-my-own-publications) for details on how to properly cite GATK.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "documentation": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/broadinstitute/gatk/tree/master/docs"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "wiki",
        "type": "Url",
        "value": "https://github.com/broadinstitute/gatk/wiki"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "download": [
    {
      "confidence": 1,
      "result": {
        "original_header": "<a name=\"lfs\">Using Git LFS to download and track large test data</a>",
        "parent_header": [
          "<a name=\"developers\">For GATK Developers</a>"
        ],
        "type": "Text_excerpt",
        "value": "We use [git-lfs](https://git-lfs.github.com/) to version and distribute test data that is too large to check into our repository directly. You must install and configure it in order to be able to run our test suite.\n\n* After installing [git-lfs](https://git-lfs.github.com/), run `git lfs install`\n    * This adds hooks to your git configuration that will cause git-lfs files to be checked out for you automatically in the future.\n    \n* To manually retrieve the large test data, run `git lfs pull` from the root of your GATK git clone.\n    * The download size is approximately 5 gigabytes.\n    \n* To add a new large file to be tracked by git-lfs, simply:\n    * Put the new file(s) in `src/test/resources/large` (or a subdirectory)\n    * `git add` the file(s), then `git commit -a`\n    * That's it! Do ***not*** run `git lfs track` on the files manually: all files in `src/test/resources/large` are tracked by git-lfs automatically. \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/broadinstitute/gatk/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/docs/CNV/archived/allele-fraction-model-approximation.ipynb"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/docs/CNV/archived/allele-fraction-model-approximation.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 591
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/broadinstitute/gatk/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "broadinstitute/gatk"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_build_file": [
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/Dockerfile",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/mitochondria_m2_wdl/Haplochecker/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/mitochondria_m2_wdl/Haplochecker/Dockerfile",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/docker/gatkbase/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/docker/gatkbase/Dockerfile",
      "technique": "file_exploration"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/build_docker_remote.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/build_docker.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/src/main/java/org/broadinstitute/hellbender/tools/validation/validate-reads-spark-pipeline.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/publish_gatk_tool_wdls.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/binary_search.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/prepareJitpackEnvironment.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/copy_small_to_hdfs_on_gcs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/exome_reads-pipeline_gcs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/exome_md-bqsr-hc_hdfs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/genome_count-reads_gcs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/test_case_2.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/copy_exome_to_hdfs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/genome_reads-pipeline_gcs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/run_gcs_cluster.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/small_reads-pipeline_hdfs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/test_case_4.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/genome_reads-pipeline_hdfs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/small_md-bqsr-hc_hdfs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/small_md-bqsr-hc_gcs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/exome_reads-pipeline_hdfs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/utils.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/copy_small_to_hdfs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/genome_md-bqsr-hc_hdfs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/copy_genome_to_hdfs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/copy_exome_to_hdfs_on_gcs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/test_case_5.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/copy_genome_to_hdfs_on_gcs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_eval/small_reads-pipeline_gcs.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/sv/run_whole_pipeline.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/sv/create_cluster.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/sv/delete_cluster.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/sv/sanity_checks.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/sv/default_init.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/sv/manage_sv_pipeline.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/sv/copy_sv_results.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/sv/stepByStep/svDiscover.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/sv/stepByStep/discoverVariants.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/sv/stepByStep/scanBam.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/docker/delete_all_untagged_images.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/docker/release_prebuilt_docker_image.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/docker/gatkbase/build_docker_base_locally.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/docker/gatkbase/release_prebuilt_base_image.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/docker/gatkbase/build_docker_base_cloud.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/getDbSNP.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/downloadHgncDataSource.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/getAllTxMappings.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/getGencodeXRefseq.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/finalizeFuncotatorReleaseDirectory.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/getGencode.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/getGencodeXHGNC.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/createDataSourcesFromBedFiles.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/createLiftoverForHg38ToB37.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/createLiftoverForB37ToHg38.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/cosmic/createTestDataForCosmic.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/cosmic/createSqliteCosmicDb.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/cosmic/getCosmicDataSources.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "installation": [
    {
      "confidence": 0.9924030852638411,
      "result": {
        "original_header": "<a name=\"downloading\">Downloading GATK4</a>",
        "type": "Text_excerpt",
        "value": "You can download and run pre-built versions of GATK4 from the following places: \n* A zip archive with everything you need to run GATK4 can be downloaded for each release from the [github releases page](https://github.com/broadinstitute/gatk/releases). We also host unstable archives generated nightly in the Google bucket gs://gatk-nightly-builds. \n* You can download a GATK4 docker image from [our dockerhub repository](https://hub.docker.com/r/broadinstitute/gatk/). We also host unstable nightly development builds on [this dockerhub repository](https://hub.docker.com/r/broadinstitute/gatk-nightly/).\n    * Within the docker image, run gatk commands as usual from the default startup directory (/gatk).\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9976208640984211,
      "result": {
        "original_header": "<a name=\"dockerSoftware\">Tools Included in Docker Image</a>",
        "type": "Text_excerpt",
        "value": "We also include an installation of Python3 (3.6.10) with the following popular packages included:\n* numpy\n* scipy\n* tensorflow\n* pymc3\n* keras\n* scikit-learn\n* matplotlib\n* pandas\n* biopython\n* pyvcf\n* pysam \nWe also include an installation of R (3.6.2) with the following popular packages included:\n* data.table\n* dplyr\n* ggplot2 \nFor more details on system packages, see the GATK [Base Dockerfile](scripts/docker/gatkbase/Dockerfile) and for more details on the Python3/R packages, see the [Conda environment setup file](scripts/gatkcondaenv.yml.template). Versions for the Python3/R packages can be found there.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9770739467286873,
      "result": {
        "original_header": "<a name=\"building\">Building GATK4</a>",
        "type": "Text_excerpt",
        "value": "* **To do a full build of GATK4, first clone the GATK repository using \"git clone\", then run:** \n        ./gradlew bundle\n        \n  Equivalently, you can just type:\n  \n        ./gradlew\n        \n    * This creates a zip archive in the `build/` directory with a name like `gatk-VERSION.zip` containing a complete standalone GATK distribution, including our launcher `gatk`, both the local and spark jars, and this README.    \n    * You can also run GATK commands directly from the root of your git clone after running this command.\n    * Note that you *must* have a full git clone in order to build GATK, including the git-lfs files in `src/main/resources/large`. The zipped source code alone is not buildable.\n    * The large files under `src/main/resources/large/` are required to build GATK, since they are packaged inside the GATK jar and used by tools at runtime. These include things like ML models and native C/C++ libraries used for acceleration of certain tools.\n    * The large files under `src/test/resources/large/`, on the other hand, are only required by the test suite when running tests, and are not required to build GATK. \n* **Other ways to build:**\n    * `./gradlew installDist`  \n        * Does a *fast* build that only lets you run GATK tools from inside your git clone, and locally only (not on a cluster). Good for developers! \n    * `./gradlew installAll`\n        * Does a *semi-fast* build that only lets you run GATK tools from inside your git clone, but works both locally and on a cluster. Good for developers!\n    * `./gradlew localJar`\n        * Builds *only* the GATK jar used for running tools locally (not on a Spark cluster). The resulting jar will be in `build/libs` with a name like `gatk-package-VERSION-local.jar`, and can be used outside of your git clone.\n    * `./gradlew sparkJar`\n        * Builds *only* the GATK jar used for running tools on a Spark cluster (rather than locally). The resulting jar will be in `build/libs` with a name like `gatk-package-VERSION-spark.jar`, and can be used outside of your git clone. \n        * This jar will not include Spark and Hadoop libraries, in order to allow the versions of Spark and Hadoop installed on your cluster to be used. \n* **To remove previous builds, run:**  \n* Gradle keeps a cache of dependencies used to build GATK.  By default this goes in `~/.gradle`.  If there is insufficient free space in your home directory, you can change the location of the cache by setting the `GRADLE_USER_HOME` environment variable. \n* The version number is automatically derived from the git history using `git describe`, you can override it by setting the `versionOverride` property.\n  ( `./gradlew -DversionOverride=my_weird_version printVersion` )\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9696723511633473,
      "result": {
        "original_header": "<a name=\"running\">Running GATK4</a>",
        "type": "Text_excerpt",
        "value": "* The standard way to run GATK4 tools is via the **`gatk`** wrapper script located in the root directory of a clone of this repository.\n    * Requires Python 2.6 or greater (this includes Python 3.x)\n    * You need to have built the GATK as described in the [Building GATK4](#building) section above before running this script.\n    * There are several ways `gatk` can be run:\n        * Directly from the root of your git clone after building\n        * By extracting the zip archive produced by `./gradlew bundle` to a directory, and running `gatk` from there\n        * Manually putting the `gatk` script within the same directory as fully-packaged GATK jars produced by `./gradlew localJar` and/or `./gradlew sparkJar`\n        * Defining the environment variables `GATK_LOCAL_JAR` and `GATK_SPARK_JAR`, and setting them to the paths to the GATK jars produced by `./gradlew localJar` and/or `./gradlew sparkJar` \n    * `gatk` can run non-Spark tools as well as Spark tools, and can run Spark tools locally, on a Spark cluster, or on Google Cloud Dataproc.\n    * ***Note:*** running with `java -jar` directly and bypassing `gatk` causes several important system properties to not get set, including htsjdk compression level!\n    \n* For help on using `gatk` itself, run **`./gatk --help`** \n* To run a non-Spark tool, or to run a Spark tool locally, the syntax is: **`./gatk ToolName toolArguments`**. \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9570089182934037,
      "result": {
        "original_header": "<a name=\"jvmoptions\">Passing JVM options to gatk</a>",
        "type": "Text_excerpt",
        "value": "* To pass JVM arguments to GATK, run `gatk` with the `--java-options` argument:  \n    ```\n    ./gatk --java-options \"-Xmx4G\" <rest of command>\n     \n    ./gatk --java-options \"-Xmx4G -XX:+PrintGCDetails\" <rest of command>\n    ``` \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9862689403501574,
      "result": {
        "original_header": "<a name=\"configFileOptions\">Passing a configuration file to gatk</a>",
        "type": "Text_excerpt",
        "value": "* To pass a configuration file to GATK, run `gatk` with the `--gatk-config-file` argument:  \n\t```\n\t./gatk --gatk-config-file GATKProperties.config <rest of command>\n\t``` \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9518038343133672,
      "result": {
        "original_header": "<a name=\"sparklocal\">Running GATK4 Spark tools locally:</a>",
        "type": "Text_excerpt",
        "value": "* GATK4 Spark tools can be run in local mode (without a cluster). In this mode, Spark will run the tool\n  in multiple parallel execution threads using the cores in your CPU. You can control how many threads\n  Spark will use via the `--spark-master` argument.\n  \n* Examples: \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9088601129997215,
      "result": {
        "original_header": "<a name=\"sparkcluster\">Running GATK4 Spark tools on a Spark cluster:</a>",
        "type": "Text_excerpt",
        "value": "**`./gatk ToolName toolArguments -- --spark-runner SPARK --spark-master <master_url> additionalSparkArguments`**\n* Examples: \n  ```\n  ./gatk PrintReadsSpark -I hdfs://path/to/input.bam -O hdfs://path/to/output.bam \\\n      -- \\\n      --spark-runner SPARK --spark-master <master_url>\n  ``` \n* You can also omit the \"--num-executors\" argument to enable [dynamic allocation](https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation) if you configure the cluster properly (see the Spark website for instructions).\n* Note that the Spark-specific arguments are separated from the tool-specific arguments by a `--`.\n* Running a Spark tool on a cluster requires Spark to have been installed from http://spark.apache.org/, since\n   `gatk` invokes the `spark-submit` tool behind-the-scenes.\n* Note that the examples above use YARN but we have successfully run GATK4 on Mesos as well.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9978497359260707,
      "result": {
        "original_header": "<a name=\"dataproc\">Running GATK4 Spark tools on Google Cloud Dataproc:</a>",
        "type": "Text_excerpt",
        "value": "  * You must have a [Google cloud services](https://cloud.google.com/) account, and have spun up a Dataproc cluster\n    in the [Google Developer's console](https://console.developers.google.com). You may need to have the \"Allow API access to all Google Cloud services in the same project\" option enabled (settable when you create a cluster).\n  * You need to have installed the Google Cloud SDK from [here](https://cloud.google.com/sdk/), since\n    `gatk` invokes the `gcloud` tool behind-the-scenes. As part of the installation, be sure\n      that you follow the `gcloud` setup instructions [here](https://cloud.google.com/sdk/gcloud/). As this library is frequently updated by Google, we recommend updating your copy regularly to avoid any version-related difficulties.\n  * Your inputs to the GATK when running on dataproc are typically in Google Cloud Storage buckets, and should be specified on\n    your GATK command line using the syntax `gs://my-gcs-bucket/path/to/my-file`\n  * You can run GATK4 jobs on Dataproc from your local computer or from the VM (master node) on the cloud. \n  Once you're set up, you can run a Spark tool on your Dataproc cluster using a command of the form: \n      ```\n      ./gatk PrintReadsSpark \\\n          -I gs://my-gcs-bucket/path/to/input.bam \\\n          -O gs://my-gcs-bucket/path/to/output.bam \\\n          -- \\\n          --spark-runner GCS --cluster myGCSCluster \\\n          --num-executors 5 --executor-cores 2 --executor-memory 4g \\\n          --conf spark.yarn.executor.memoryOverhead=600\n      ```\n  * When using Dataproc you can access the web interfaces for YARN, Hadoop and HDFS by opening an SSH tunnel and connecting with your browser.  This can be done easily using included `gcs-cluster-ui` script.\n  \n    BASH3*\n    Or see these [these instructions](https://cloud.google.com/dataproc/cluster-web-interfaces) for more details.\n  * Note that the spark-specific arguments are separated from the tool-specific arguments by a `--`.\n  * If you want to avoid uploading the GATK jar to GCS on every run, set the `GATK_GCS_STAGING`\n    environment variable to a bucket you have write access to (eg., `export GATK_GCS_STAGING=gs://<my_bucket>/`)\n  * Dataproc Spark clusters are configured with [dynamic allocation](https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation) so you can omit the \"--num-executors\" argument and let YARN handle it automatically.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9431896778661867,
      "result": {
        "original_header": "<a name=\"R\">Using R to generate plots</a>",
        "type": "Text_excerpt",
        "value": "Certain GATK tools may optionally generate plots using the R installation provided within the conda environment.  If you are uninterested in plotting, R is still required by several of the unit tests.  Plotting is currently untested and should be viewed as a convenience rather than a primary output.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.988038059531133,
      "result": {
        "original_header": "<a name=\"tab_completion\">Bash Command-line Tab Completion (BETA)</a>",
        "type": "Text_excerpt",
        "value": "* A tab completion bootstrap file for the bash shell is now included in releases.  This file allows the command-line shell to complete GATK run options in a manner equivalent to built-in command-line tools (e.g. grep).   \n* This tab completion functionality has only been tested in the bash shell, and is released as a beta feature. \n* To enable tab completion for the GATK, open a terminal window and source the included tab completion script:\n```\nsource gatk-completion.sh\n```\n \n* Sourcing this file will allow you to press the tab key twice to get a list of options available to add to your current GATK command.  By default you will have to source this file once in each command-line session, then for the rest of the session the GATK tab completion functionality will be available.  GATK tab completion will be available in that current command-line session only. \n* Note that you must have already started typing an invocation of the GATK (using gatk) for tab completion to initiate:\n```\n./gatk <TAB><TAB>\n```\n* We recommend adding a line to your bash settings file (i.e. your ~/.bashrc file) that sources the tab completion script.  To add this line to your bash settings / bashrc file you can use the following command:\n```\necho \"source <PATH_TO>/gatk-completion.sh\" >> ~/.bashrc\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9639321508604973,
      "result": {
        "original_header": "<a name=\"dev_guidelines\">General guidelines for GATK4 developers</a>",
        "type": "Text_excerpt",
        "value": "* All pull requests must be reviewed before merging to master (even documentation changes). \n* Don't issue or accept pull requests that introduce warnings. Warnings must be addressed or suppressed. \n* Don't issue or accept pull requests that significantly decrease coverage (less than 1% decrease is sort of tolerable).  \n* Git: Don't push directly to master - make a pull request instead.  \n* Git: Rebase and squash commits when merging. \n* If you push to master or mess up the commit history, you owe us 1 growler or tasty snacks at happy hour. If you break the master build, you owe 3 growlers (or lots of tasty snacks). Beer may be replaced by wine (in the color and vintage of buyer's choosing) in proportions of 1 growler = 1 bottle. \n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9936268574046565,
      "result": {
        "original_header": "<a name=\"testing\">Testing GATK</a>",
        "type": "Text_excerpt",
        "value": "* Before running the test suite, be sure that you've installed `git lfs` and downloaded the large test data, following the [git lfs setup instructions](#lfs) \n* To run the test suite, run **`./gradlew test`**.\n    * Test report is in `build/reports/tests/test/index.html`.\n    * What will happen depends on the value of the `TEST_TYPE` environment variable: \n       * unset or any other value         : run non-cloud unit and integration tests, this is the default\n       * `cloud`, `unit`, `integration`, `conda`, `spark`   : run only the cloud, unit, integration, conda (python + R), or Spark tests\n       * `all`                            : run the entire test suite\n    * Cloud tests require being logged into `gcloud` and authenticated with a project that has access\n      to the cloud test data.  They also require setting several certain environment variables.\n      * `HELLBENDER_JSON_SERVICE_ACCOUNT_KEY` : path to a local JSON file with [service account credentials](https://cloud.google.com/storage/docs/authentication#service_accounts) \n      * `HELLBENDER_TEST_PROJECT` : your google cloud project \n      * `HELLBENDER_TEST_STAGING` : a gs:// path to a writable location\n      * `HELLBENDER_TEST_INPUTS` : path to cloud test data, ex: gs://hellbender/test/resources/ \n    * Setting the environment variable `TEST_VERBOSITY=minimal` will produce much less output from the test suite  \n* To run tests and compute coverage reports, run **`./gradlew jacocoTestReport`**. The report is then in `build/reports/jacoco/test/html/index.html`.\n  (IntelliJ has a good coverage tool that is preferable for development). \n    * Before merging any branch make sure that all required tests pass on Github.\n    * Every Actions build will upload the test results to our GATK Google Cloud Storage bucket and a zipped artifact upload.\n      A link to the uploaded report will appear at the very bottom of the github actions log.\n      Look for the line that says `See the test report at`.\n      Test github actions test artifacts will not show up on the webpage until the entire test has concluded.\n      If TestNG itself crashes there will be no report generated. \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9589589299129933,
      "result": {
        "original_header": "<a name=\"intellij\">Creating a GATK project in the IntelliJ IDE (last tested with version 2016.2.4):</a>",
        "type": "Text_excerpt",
        "value": "* Ensure that you have `gradle` and the Java 17 JDK installed \n* You may need to install the TestNG and Gradle plugins (in preferences) \n* Clone the GATK repository using git \n* In IntelliJ, click on \"Import Project\" in the home screen or go to File -> New... -> Project From Existing Sources... \n* Select the root directory of your GATK clone, then click on \"OK\" \n* Ensure that \"Gradle project\" points to the build.gradle file in the root of your GATK clone \n* Make sure the Gradle JVM points to Java 17. You may need to set this manually after creating the project, to do so find the gradle settings by clicking the wrench icon in the gradle tab on the right bar, from there edit \"Gradle JVM\" argument to point to Java 17. \n* After downloading project dependencies, IntelliJ should open a new window with your GATK project \n* Make sure that the Java version is set correctly by going to File -> \"Project Structure\" -> \"Project\". Check that the \"Project SDK\" is set to your Java 17 JDK, and \"Project language level\" to 17 (you may need to add your Java 17 JDK under \"Platform Settings\" -> SDKs if it isn't there already). Then click \"Apply\"/\"Ok\".\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9441665195286668,
      "result": {
        "original_header": "<a name=\"debugging\">Setting up debugging in IntelliJ</a>",
        "type": "Text_excerpt",
        "value": "* Follow the instructions above for creating an IntelliJ project for GATK \n* Go to Run -> \"Edit Configurations\", then click \"+\" and add a new \"Application\" configuration \n* Enter the arguments for the command you want to debug in \"Program Arguments\" \n* Click \"Apply\"/\"Ok\" \n* Set breakpoints, etc., as desired, then select \"Run\" -> \"Debug\" -> \"GATK debug\" to start your debugging session \n* In future debugging sessions, you can simply adjust the \"Program Arguments\" in the \"GATK debug\" configuration as needed\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9984614692196309,
      "result": {
        "original_header": "<a name=\"jprofiler\">Setting up profiling using JProfiler</a>",
        "type": "Text_excerpt",
        "value": "   * Running JProfiler from within IntelliJ:\n       * JProfiler has great integration with IntelliJ (we're using IntelliJ Ultimate edition) so the setup is trivial.   \n       * Follow the instructions [above](#intellij) for creating an IntelliJ project for GATK  \n       * Right click on a test method/class/package and select \"Profile\" \n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9699196511498055,
      "result": {
        "original_header": "<a name=\"sonatype\">Uploading Archives to Sonatype (to make them available via maven central)</a>",
        "type": "Text_excerpt",
        "value": "To upload snapshots to Sonatype you'll need the following: \n* You must have a registered account on the sonatype JIRA (and be approved as a gatk uploader)\n* You need to configure several additional properties in your `/~.gradle/gradle.properties` file \n* If you want to upload a release instead of a snapshot you will additionally need to have access to the gatk signing key and password\n```\n#needed for snapshot upload\nsonatypeUsername=<your sonatype username>\nsonatypePassword=<your sonatype password>\n\n#needed for signing a release\nsigning.keyId=<gatk key id>\nsigning.password=<gatk key password>\nsigning.secretKeyRingFile=/Users/<username>/.gnupg/secring.gpg\n```\n \nBuilds are considered snapshots by default.  You can mark a build as a release build by setting `-Drelease=true`.  \nThe archive name is based off of `git describe`.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9301464973872038,
      "result": {
        "original_header": "<a name=\"docker_building\">Building GATK4 Docker images</a>",
        "type": "Text_excerpt",
        "value": "Please see the [the Docker README](scripts/docker/README.md) in ``scripts/docker``.  This has instructions for the Dockerfile in the root directory.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9432290822840598,
      "result": {
        "original_header": "<a name=\"releasing_gatk\">Releasing GATK4</a>",
        "type": "Text_excerpt",
        "value": "Please see the [How to release GATK4](https://github.com/broadinstitute/gatk/wiki/How-to-release-GATK4) wiki article for instructions on releasing GATK4.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9909115769095097,
      "result": {
        "original_header": "<a name=\"gatkdocs\">Generating GATK4 documentation</a>",
        "type": "Text_excerpt",
        "value": "* Generated docs will be in the `build/docs/gatkdoc` directory.\n \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9997576917382978,
      "result": {
        "original_header": "<a name=\"gatkwdlgen\">Generating GATK4 WDL Wrappers</a>",
        "type": "Text_excerpt",
        "value": "* To generate WDL Wrappers and validate the resulting outputs, run `./gradlew gatkWDLGenValidation`.\nRunning this task requires a local [cromwell](https://github.com/broadinstitute/cromwell) installation, and environment\nvariables `CROMWELL_JAR` and `WOMTOOL_JAR` to be set to the full pathnames of the `cromwell` and `womtool` jar files.\nIf no local install is available, this task will run automatically on github actions in a separate job whenever a PR is submitted. \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9536865033582211,
      "result": {
        "original_header": "<a name=\"zenhub\">Using Zenhub to track github issues</a>",
        "type": "Text_excerpt",
        "value": "We use [Zenhub](https://www.zenhub.com/) to organize and track github issues. \n* To add Zenhub to github, go to the [Zenhub home page](https://www.zenhub.com/) while logged in to github, and click \"Add Zenhub to Github\" \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9839123104884854,
      "result": {
        "original_header": "<a name=\"spark_further_reading\">Further Reading on Spark</a>",
        "type": "Text_excerpt",
        "value": "Note that if you don't have a dedicated cluster you can run Spark in\n[standalone mode](https://spark.apache.org/docs/latest/spark-standalone.html) on a single machine, which exercises\nthe distributed code paths, albeit on a single node. \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9691258015777328,
      "result": {
        "type": "Text_excerpt",
        "value": "***Please see the , where you can download a precompiled executable, read documentation, ask questions, and receive technical support. For GitHub basics, see .*** \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8565317517320893,
      "result": {
        "original_header": "<a name=\"dataproc\">Running GATK4 Spark tools on Google Cloud Dataproc:</a>",
        "type": "Text_excerpt",
        "value": "  * Examples: \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8349281440998516,
      "result": {
        "original_header": "<a name=\"configFileOptions\">Passing a configuration file to gatk</a>",
        "type": "Text_excerpt",
        "value": "* To pass a configuration file to GATK, run `gatk` with the `--gatk-config-file` argument:  \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8532236262584965,
      "result": {
        "original_header": "<a name=\"testing\">Testing GATK</a>",
        "type": "Text_excerpt",
        "value": "* To run the test suite, run **`./gradlew test`**.\n    * Test report is in `build/reports/tests/test/index.html`.\n    * What will happen depends on the value of the `TEST_TYPE` environment variable: \n       * unset or any other value         : run non-cloud unit and integration tests, this is the default\n       * `cloud`, `unit`, `integration`, `conda`, `spark`   : run only the cloud, unit, integration, conda (python + R), or Spark tests\n       * `all`                            : run the entire test suite\n    * Cloud tests require being logged into `gcloud` and authenticated with a project that has access\n      to the cloud test data.  They also require setting several certain environment variables.\n      * `HELLBENDER_JSON_SERVICE_ACCOUNT_KEY` : path to a local JSON file with [service account credentials](https://cloud.google.com/storage/docs/authentication#service_accounts) \n      * `HELLBENDER_TEST_PROJECT` : your google cloud project \n      * `HELLBENDER_TEST_STAGING` : a gs:// path to a writable location\n      * `HELLBENDER_TEST_INPUTS` : path to cloud test data, ex: gs://hellbender/test/resources/ \n    * Setting the environment variable `TEST_VERBOSITY=minimal` will produce much less output from the test suite  \n* To run a subset of tests, use gradle's test filtering (see [gradle doc](https://docs.gradle.org/current/userguide/java_plugin.html)):\n    * You can use `--tests` with a wildcard to run a specific test class, method, or to select multiple test classes:\n        * `./gradlew test --tests *SomeSpecificTestClass`\n        * `./gradlew test --tests *SomeTest.someSpecificTestMethod`\n        * `./gradlew test --tests all.in.specific.package*` \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8003402995134077,
      "result": {
        "original_header": "<a name=\"intellij\">Creating a GATK project in the IntelliJ IDE (last tested with version 2016.2.4):</a>",
        "type": "Text_excerpt",
        "value": "* Select \"Import project from external model\", then \"Gradle\", then click on \"Next\" \n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/broadinstitute/gatk/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "bioinformatics, dna, gatk, genome, genomics, ngs, science, sequencing, spark"
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "gatk"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "broadinstitute"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Java",
        "size": 24995315,
        "type": "Programming_language",
        "value": "Java"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 568634,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "WDL",
        "size": 544831,
        "type": "Programming_language",
        "value": "WDL"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 256357,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "FreeMarker",
        "size": 70594,
        "type": "Programming_language",
        "value": "FreeMarker"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "R",
        "size": 39426,
        "type": "Programming_language",
        "value": "R"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "HTML",
        "size": 20805,
        "type": "Programming_language",
        "value": "HTML"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "JavaScript",
        "size": 13808,
        "type": "Programming_language",
        "value": "JavaScript"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Dockerfile",
        "size": 7317,
        "type": "Programming_language",
        "value": "Dockerfile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "CSS",
        "size": 2963,
        "type": "Programming_language",
        "value": "CSS"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Roff",
        "size": 266,
        "type": "Programming_language",
        "value": "Roff"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "lbergelson",
          "type": "User"
        },
        "date_created": "2024-10-24T15:47:47Z",
        "date_published": "2024-10-30T16:11:13Z",
        "description": "**Download release:** [gatk-4.6.1.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.6.1.0/gatk-4.6.1.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.6.1.0 release:**\r\n-------------------------------------- \r\n* Modernize the aging Conda environment with up to date python dependencies.  All the python tools have been updated appropriately.  This will enable easier integration of new machine learning tools. \r\n  * ***If you use python tools outside of the docker, you must rebuild your conda environment for this release***\r\n*  `CNNScoreVariants` has been replaced by `NVScoreVariants`, a rewritten and modernized version.  The python code for this tool was written by members of NVIDIA Genomics Research.  \r\n   * Thank you Babak Zamirai, Ankit Sethia, Mehrzad Samadi, George Vacek and the whole NVIDIA genomics team!\r\n   *  This [ GATK blog post ](https://gatk.broadinstitute.org/hc/en-us/articles/10064202674971-Introducing-NVIDIA-s-NVScoreVariants-a-new-deep-learning-tool-for-filtering-variants) has more of the story from when we first made the tool available for testing.\r\n*  New `Funcotator` argument `--prefer-mane-transcripts` which improves transcript selection and lays groundwork for upcoming improvements.\r\n*  New argument `--variant-output-filtering` which lets you restrict output variants based on the input intervals.  This replaces and imrpoves on `--only-output-calls-starting-in-interval` and works with `SelectVariants` and other VariantWalkers.  This is useful to prevent duplicating variants when splitting an input VCF into multiple shards.\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n* **CNNScoreVariants -> NVScoreVariants** (https://github.com/broadinstitute/gatk/pull/8004, https://github.com/broadinstitute/gatk/pull/9010, https://github.com/broadinstitute/gatk/pull/9009)\r\n    * CNNScore variants has been replaced by NVScoreVariants, scripts that use it should be updated to use NVScoreVariants instead.\r\n    * The training tools (CNNVariantTrain, CNNVariantWriteTensors)have been removed.  If you need to retrain the model for your data type you should continue to use GATK 4.6.0.0.   New training tools are in development to work alongside NVScoreVariants and will be added in subsequent releases.   \r\n  \r\n* **New Tools** \r\n    * New tool `GtfToBed` to convert Gencode GTF files to BED files (#7159, https://github.com/broadinstitute/gatk/pull/8942)\r\n    * New tool for internal use  `VcfComparator` (https://github.com/broadinstitute/gatk/pull/8933, https://github.com/broadinstitute/gatk/pull/8973) \r\n\r\n* **Joint Calling GVS**\r\n    *  Adds QD and AS_QD emission from VariantAnnotator on GVS input (https://github.com/broadinstitute/gatk/pull/8978)\r\n\r\n* **GenomicsDB**\r\n    * Switch to logging a warning instead of an exception for intervals in query that were not part of GenomicsDBImport (https://github.com/broadinstitute/gatk/pull/8987)\r\n    \r\n* **Funcotator**\r\n    *  Added a '--prefer-mane-transcripts' mode that enforces MANE_Select tagged Gencode transcripts where possible  )(https://github.com/broadinstitute/gatk/pull/9012)\r\n        \r\n* **SV Calling** \r\n    * Handle CTX_PP/QQ and CTX_PQ/QP CPX_TYPE values inSVConcordance (https://github.com/broadinstitute/gatk/pull/8885)\r\n    * Complex SV intervals support by @mwalker174  (https://github.com/broadinstitute/gatk/pull/8521)\r\n    * Require both overlap and breakend proximity for depth-only SV clustering (https://github.com/broadinstitute/gatk/pull/8962)\r\n\r\n* **Flow Based Calling**\r\n    * Modified HaplotypeBasedVariantRecaller to support non-flow reads (https://github.com/broadinstitute/gatk/pull/8896)\r\n    * FlowFeatureMapper: X_FILTERED_COUNT semantics adjusted and documented more accurately  (https://github.com/broadinstitute/gatk/pull/8894)\r\n    * Changes to flow arguments in haplotype caller from Picard (see [Picard release notes](https://github.com/broadinstitute/picard/releases/tag/3.3.0)\r\n\r\n* **Miscellaneous Features**\r\n    * Added a check for whether files can be created and executed within the configured tmp-dir (https://github.com/broadinstitute/gatk/pull/8951)\r\n\r\n* **Documentation**\r\n    *  Clarify in the README which git lfs files are required to build GATK (https://github.com/broadinstitute/gatk/pull/8914)\r\n    * Add docs about citing GATK  (https://github.com/broadinstitute/gatk/pull/8947)\r\n    * Update Mutect2.java Documentation (https://github.com/broadinstitute/gatk/pull/8999)\r\n    * Add more detailed conda setup instructions to the GATK README (https://github.com/broadinstitute/gatk/pull/9001)\r\n    * Adding small warning messages to not to feed any GVCF files to these tools (https://github.com/broadinstitute/gatk/pull/9008)\r\n\r\n* **Refactoring**\r\n   * Swapped mito mode in Mutect to use the mode argument utils (https://github.com/broadinstitute/gatk/pull/8986)\r\n\r\n* **Tests**\r\n   * Adding a test to capture an expected edge case in Reblocking (https://github.com/broadinstitute/gatk/pull/8928)\r\n   * Update the large CRAM files to v3.0 (https://github.com/broadinstitute/gatk/pull/8832)\r\n   * Update CRAM detector output files (https://github.com/broadinstitute/gatk/pull/8971)\r\n   * Add dependency submission workflow so we can monitor vulnerabilities (https://github.com/broadinstitute/gatk/pull/9002)\r\n\r\n* **Dependencies**\r\n   Updating dependencies to make use of modern frameworks with fewer vulnerabilities was a focus of this release.  \r\n    *  Updated Python and PyMC, removed TensorFlow, and added PyTorch in conda environment. (https://github.com/broadinstitute/gatk/pull/8561)\r\n    \r\n    *  Rebuild gatk-base docker image (3.3.1) in order to pull in recent patches (https://github.com/broadinstitute/gatk/pull/9005)\r\n   * Updates to java build and dependencies  (https://github.com/broadinstitute/gatk/pull/8998,  https://github.com/broadinstitute/gatk/pull/9006, https://github.com/broadinstitute/gatk/pull/9016)\r\n     *  Update to the Gralde 8.10.2\r\n     *  Improvements to `build.gradle` to use of features like consuming publishes Bills of Materials (BOMs) \r\n     * Update many direct and transitive java dependencies to fix security vulnerabilities.\r\n     * Update [Htsjdk 4.1.1 to 4.1.3](https://github.com/samtools/htsjdk/compare/4.1.1...4.1.3) \r\n     * Update [Picard 3.2.0 to 3.3.0](https://github.com/broadinstitute/picard/releases/tag/3.3.0) \r\n     * Update hdf5-java-bindings to version 1.2.0-hdf5_2.11.0 (https://github.com/broadinstitute/gatk/pull/8908)\r\n      \r\n\r\n    \r\n   ",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.6.1.0",
        "name": "4.6.1.0",
        "release_id": 181544992,
        "tag": "4.6.1.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.6.1.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/181544992",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/181544992",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.6.1.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2024-06-29T23:12:41Z",
        "date_published": "2024-06-29T23:24:15Z",
        "description": "**Download release:** [gatk-4.6.0.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.6.0.0/gatk-4.6.0.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.6.0.0 release:**\r\n--------------------------------------\r\n\r\n* We've fixed a serious CRAM writing bug that affects GATK versions 4.3 through 4.5 and Picard versions 2.27.3 through 3.1.1. This bug can, in limited cases, lead to reads with an incorrect base sequence being written. See [this comment to GATK issue 8768](https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2198315437) and the full release notes below for more details on what conditions trigger the bug. \r\n    * ***To help users detect whether their CRAM files are affected, we've released a CRAM scanning tool called `CRAMIssue8768Detector` that can detect whether a particular CRAM file is affected by this bug. If you suspect that some of your CRAM files may have been affected, please run this tool on them for confirmation!***\r\n\r\n* By overwhelming popular demand, we've switched back to using the standard `./.` representation for no-calls in `GenotypeGVCFs` and `GenomicsDB` instead of `0/0` with `DP=0`. This reverts the change described in our article [GenotypeGVCFs and the death of the dot](https://gatk.broadinstitute.org/hc/en-us/articles/6012243429531-GenotypeGVCFs-and-the-death-of-the-dot).\r\n    * We intend to publish a new article shortly to replace that older article with further details on this change. When we do so, we'll link to it from here.\r\n\r\n* The `Mutect2` germline resource can now have split multiallelic format\r\n\r\n* Added an `--inverted-read-filter` argument to allow for selecting reads that fail read filters from the command line easily\r\n\r\n* We've fixed a number of issues with HTTP support, mainly affecting the loading of side inputs such as indices over HTTP \r\n\r\n* Reduced the number of layers in the GATK docker image to help users running into docker quota issues\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **Important CRAM writing bug fix and detection tool**\r\n    * We've updated to `HTSJDK` 4.1.1 and `Picard` 3.2.0 (#8900), which fix a serious bug in the CRAM writing code first reported in [GATK issue 8768](https://github.com/broadinstitute/gatk/issues/8768#issuecomment-2198315437)\r\n    * This issue affects GATK versions 4.3.0.0 through 4.5.0.0, and is fixed in GATK 4.6.0.0.\r\n    * This issue also affects Picard versions 2.27.3 through 3.1.1, and is fixed in Picard 3.2.0.\r\n    * The bug is triggered when writing a CRAM file using one of the affected GATK/Picard versions, and both of the following conditions are met:\r\n        * At least one read is mapped to the very first base of a reference contig\r\n        * The file contains more than one CRAM container (10,000 reads) with reads mapped to that same reference contig\r\n    * When both of these conditions are met, the resulting CRAM file may have corrupt containers associated with that contig containing reads with an incorrect sequence.\r\n    * Since many common references such as hg38 have N's at the very beginning of the autosomes and X/Y, many pipelines will not be affected by this bug. However, users of a telomere-to-telomere reference, users doing mitochondrial calling, and users with reads aligned to the alt sequences will want to scan their CRAM files for possible corruption.\r\n    * The other mitigating circumstance is that when a CRAM is affected, the signal will be overwhelmingly obvious, with the mismatch rate typically jumping from sub-1% to 80-90% for the affected regions, making it likely to be caught by standard QC processes.\r\n    * ***We've released a CRAM scanning tool called `CRAMIssue8768Detector` (#8819) that can detect whether a particular CRAM file is affected by this bug. If you suspect that some of your CRAM files may have been affected, please run this tool on them for confirmation!***\r\n\r\n* **Joint Calling**\r\n    * We've switched back to using the standard `./.` representation for no-calls in `GenotypeGVCFs` and `GenomicsDB` instead of `0/0` with `DP=0` (#8715) (#8741) (#8759)\r\n        * This reverts the change described in our article [GenotypeGVCFs and the death of the dot](https://gatk.broadinstitute.org/hc/en-us/articles/6012243429531-GenotypeGVCFs-and-the-death-of-the-dot)\r\n    * Fix for `GenotypeGVCFs` with mixed ploidy sites (#8862)\r\n    * Fix for `GnarlyGenotyper` when PLs are null (#8878)\r\n    * Fixed bug in `ReblockGVCF` when removing annotations (#8870)\r\n    * Enable `ReblockGVCF` to subset AS annotations that aren't \"raw\" (pipe-delimited) (#8771)\r\n    * Remove header lines in `ReblockGVCF` when we remove FORMAT annotations (#8895)\r\n    * `ReblockGVCF`: Add malaria spanning deletion exception regression test with fix (#8802)\r\n    * Restore some `GnarlyGenotyper` tests (#8893)\r\n\r\n* **HaplotypeCaller**\r\n    * Fix to long deletions that overhang into the assembly window causing exceptions in `HaplotypeCaller` (#8731)\r\n    \r\n* **Mutect2**\r\n    * The `Mutect2` germline resource can now have split multiallelic format (#8837)\r\n    * Make the `Mutect2` haplotype and clustered events filters smarter about germline events (#8717)\r\n    * Added the DragSTR model to the Mutect2 WDL (#8716)\r\n    * Improvements to `Mutect2`'s `Permutect` training data mode (#8663)\r\n    * Bigger `Permutect` tensors and `Permutect` test datasets can be annotated with truth VCF (#8836)\r\n    * `Mutect2` WDL and GetSampleName can handle multiple sample names in BAM headers (#8859)\r\n    * `Permutect` dataset engine outputs contig and read group indices, not names (#8860)\r\n    * Normal artifact LOD is now defined without the extra minus sign (#8668)\r\n    \r\n* **CNV Calling**\r\n    * Fixed the GT header in `PostprocessGermlineCNVCalls`'s `--output-genotyped-intervals` output (#8621)\r\n    \r\n* **SV Calling**\r\n    * Reduced `SVConcordance` memory footprint (#8623)\r\n    * Rewrote complex SV functional annotation in `SVAnnotate` (#8516)\r\n    * We now handle the `CTX_INV` subtype in `SVAnnotate` (#8693)\r\n\r\n* **Flow-based Calling**\r\n    * SNVQ recalibration tool added for flow-based reads (#8697)\r\n    * Bug fix in flow-based allele filtering  (#8775)\r\n    * Fixed a bug in flow-based `AlleleFiltering` that ignored more than a single sample (#8841)\r\n    * Fixed an edge case in flow-based variant annotation  (#8810)\r\n\r\n* **Notable Enhancements**\r\n    * Added an `--inverted-read-filter` argument to allow for selecting reads that fail read filters from the command line easily (#8724)\r\n    * Inverted `SoftClippedReadFilter` to conform to the standard filtering logic (#8888)\r\n    * Reduced the number of docker layers in the GATK image from 44 to 16 (#8808)\r\n    * `VariantFiltration`: added a `--mask-description` argument to write custom mask filter description in VCF header (#8831)\r\n    * `GatherVcfsCloud` is no longer beta (#8680)\r\n\r\n* **Miscellaneous Changes**\r\n    * `GetPileupSummaries` now uses the standard `MappingQualityReadFilter` instead of a custom `--min-mapping-quality` argument (#8781)\r\n    * `Funcotator`: suppress a log message about b37 contigs when not doing b37/hg19 conversion (#8758)\r\n    * Output the new image name at the end of a successful cloud docker build (#8627)\r\n    * Exclude the test folder from code coverage calculations (#8744)\r\n    * Removed deprecated genomes in the cloud docker image that was causing CNN WDL test failures (#8891)\r\n    * Re-commit large test files as lfs stubs (#8769)\r\n    * Standardize test results directory between normal/docker tests (#8718)\r\n    * Improve failure message in `VariantContextTestUtils` (#8725)\r\n    * Update the `setup_cloud` github action (#8651)\r\n    * Parameterize the logging frequency for ProgressLogger in `GatherVcfsCloud` (#8662)\r\n    \r\n* **Documentation**\r\n    * Updated the README to include list of popular software included in docker image (#8745)\r\n    \r\n* **Dependencies**\r\n    * Updated `HTSJDK` to 4.1.1, which fixes the CRAM writing bug described above (#8900)\r\n    * Updated `Picard` to 3.2.0, which fixes the CRAM writing bug described above (#8900)\r\n    * Updated `GenomicsDB` to 1.5.3, which supports M1 Macs and switches no-call representation back to `./.` (#8710) (#8759)\r\n    * Updated `http-nio` to 1.1.1, which fixes several URL-handling bugs with HTTP support (#8889)\r\n    * Updated several miscellaneous dependencies to fix security vulnerabilities (#8898)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.6.0.0",
        "name": "4.6.0.0",
        "release_id": 163161691,
        "tag": "4.6.0.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.6.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/163161691",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/163161691",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.6.0.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2023-12-13T22:42:16Z",
        "date_published": "2023-12-13T22:53:56Z",
        "description": "**Download release:** [gatk-4.5.0.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.5.0.0/gatk-4.5.0.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.5.0.0 release:**\r\n--------------------------------------\r\n\r\n* `HaplotypeCaller` now supports custom ploidy regions that can be specified via a new `--ploidy-regions` argument, overriding the global `-ploidy` setting\r\n\r\n* The default `SmithWaterman` implementation for `HaplotypeCaller` and `Mutect2` is now the hardware-accelerated version, resulting in a significant speedup\r\n\r\n* `Funcotator` has a new datasource release that brings in the latest version of `Gencode` and several other key data sources\r\n\r\n* We've updated our dependencies and our docker environment to greatly cut down on known security vulnerabilities\r\n\r\n* We've greatly improved support for `http`/`https` inputs in GATK-native tools (though most Picard tools bundled with GATK do not yet support it)\r\n\r\n* We've ported some additional DRAGEN features to `HaplotypeCaller` that bring us closer to functional equivalence with DRAGEN v3.7.8\r\n\r\n* `GenomicsDBImport` now has support for Azure storage `az://` URIs\r\n\r\n* `GnarlyGenotyper` now has haploid support\r\n\r\n* Lots of important bug fixes, including a fix for a bug in the Intel GKL that could cause output files to intermittently fail to be compressed properly\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **HaplotypeCaller**\r\n  * HaplotypeCaller now supports custom ploidy regions (#8609)\r\n    * Added a new argument to `HaplotypeCaller` called `--ploidy-regions` which allows the user to input a `.bed` or `.interval_list` with the \"name\" column equal to a positive integer for the ploidy to use when calling variants in that region \r\n    * The main use case is for calling haploid variants outside the PAR for XY individuals as required by the VCF spec, but this provides a much more flexible interface for other similar niche applications, like genotyping individuals with other known aneuploidies\r\n    * The global `-ploidy` flag will still provide the background default (or the built-in ploidy of 2 for humans), but the user-supplied values will supersede these in overlapping regions\r\n  * Changed the `SmithWaterman` implementation to default to `FASTEST_AVAILABLE` (#8485)\r\n  * Fixed a bug in pileup calling mode relating to the number of haplotypes (#8489)\r\n  * Huge simplication of genotyping likelihoods calculations -- no change in output (#6351)\r\n  * Be explicit about when variants are biallelic (#8332)\r\n  * Fixed debug log severity for read threading assembler messages (#8419)\r\n  * Fixed issue with visibility of the `--dont-use-softclipped-bases` argument (#8271)\r\n\r\n* **Mutect2**\r\n  * Added a `--base-qual-correction-factor` to allow a scale factor to be provided to modify the base qualities reported by the sequencer and used in the `Mutect2` substitution error model (#8447)\r\n    * Set to zero to turn off the error model changes introduced in GATK 4.1.9.0\r\n  * Fixed a bug in `FilterMutectCalls` for GVCFs (#8458)\r\n    * When using GVCFs with `Mutect2` (for example with the Mitochondria mode), in the filtering step ADs for symbolic alleles are set to 0 so it doesn't contribute to overall AD. There was an off-by-one error that removed the alt allele AD rather than the `<NON_REF>` allele AD. This led to NaNs and errors when a site had no ref reads (for example a GT of `[ref,alt,<NON_REF>]` and AD of `[0,300,0]` would accidentally be changed to an AD of `[0,0,0]` if the alt index was removed instead of the `<NON_REF>` index).\r\n\r\n* **DRAGEN-GATK**\r\n  * Added implementations of the \"columnwise detection\" and \"PDHMM\" (partially-determined HMM) features from DRAGEN to bring us much closer to functional equivalence with DRAGEN v3.7.8 (#8083)\r\n  * Development work to prepare the way for the final missing DRAGEN 3.7.8 feature, \"joint detection\":\r\n    * Graph method for PDHMM event groups that unifies finding/merging and overlap/mutual exclusion (#8366)\r\n    * Rewrote haplotype construction methods in `PartiallyDeterminedHaplotypeComputationEngine` (#8367)\r\n    * More refactoring in `PartiallyDeterminedHaplotypeComputationEngine` and preparing for joint detection (#8492)\r\n    * Innocuous housekeeping changes in the partially-determined haplotypes code (#8361)\r\n    * Clarify cryptic bitwise operations in the partially-determined haplotype `EventGroup` subclass (#8400)\r\n    \r\n* **Joint Calling**\r\n  * Added haploid support to `GnarlyGenotyper` (#7750)\r\n  * Fix to allow `GenotypeGVCFs` to properly handle events not in minimal representation (#8567)\r\n  * `ReblockGVCF`: added a `--keep-site-filters` argument to keep site-level filters (#8304) (#8308)\r\n  * `ReblockGVCF`: added a `--add-site-filters-to-genotype` argument to move site-level filters to genotype-level filters (#8484)\r\n  * `ReblockGVCF`: added a `--format-annotations-to-remove` argument to specify format-level annotations to remove from all genotypes in final GVCF (#8411)\r\n  * `ReblockGVCF`: added a check to make sure the input VCF is a GVCF rather than a single sample VCF (#8411)\r\n  * Improved an error message in `GnarlyGenotyper` (#8270)\r\n  * Added a `mergeWithRemapping()` method in `ReferenceConfidenceVariantContextMerger` to perform allele remapping prior to genotyping (#8318)\r\n  * GVS (Genomic Variant Store) development:\r\n    * Incorporated changes from the GVS branch to existing files (#8256)\r\n    * Incorporated build changes from the GVS branch (#8249)\r\n    * Merged non-GVS bits required by the GVS branch [VS-971] (#8362)\r\n\r\n* **GenomicsDB**\r\n  * Allow `GenomicsDBImport` to accept Azure `az://` URIs as input (#8438)\r\n  * Updated to a newer `GenomicsDB` release with Java 17 support, improved error messages/logging, and generally improved performance (#8358)\r\n\r\n* **Funcotator**\r\n  * New data source release V1.8 (#8512)\r\n    * Updated `Gencode` to version 43, and also updated `COSMIC`, `Clinvar`, and several other datasources to their latest versions\r\n    * The data sources are now split by reference into separate hg19 and hg38 bundles to cut down on size\r\n  * Fixed support for newer `Gencode` GTF versions by making the `GencodeGTFField` parsing more permissive (#8351)\r\n  * Fixed `Funcotator` VCF output renderer to correctly preserve B37 contig names on output for B37 aligned files (#8539)\r\n  * Fix bug in VCF comparison code that causes `Funcotator` to crash with certain datasources (#8445)\r\n  * Connected the splice site window size to CLI parameters (#8463)\r\n  * Allow `LocatableXsvFuncotationFactory` to read gzipped files (#8363)\r\n\r\n* **CNV Calling**\r\n  * Matched gCNV pipeline arguments to those that were shown to have good performance in running large exome cohorts (#8234)\r\n  * Added resource usage section to the `GermlineCNVCaller` java doc (#8064)\r\n    \r\n* **SV Calling**\r\n  * Added support for breakend replacement alleles in `SVCluster` (#8408)\r\n    * Implements allele collapsing for \"breakend replacement\" BND alleles, as described in section 5.4 of the VCFv4.2 spec\r\n  * Size similarity linkage and bug fixes for SV matching tools (#8257)\r\n    * Added size similarity criterion to the `SVConcordance` and `SVCluster` tools. This is particularly useful for accurately matching smaller SVs that have a high degree of breakpoint uncertainty, in which case reciprocal overlap does not work well. PESR/mixed variant types must have size similarity, reciprocal overlap, and breakend window criteria met. Depth-only variants may have either size similarity + reciprocal overlap OR breakend window criteria met (or both).\r\n  * Updated SV split-read strand validation and clustering (#8378)\r\n    * Adds some flexibility to the allowed split-read strand annotations on SV records:\r\n      * Allow INS -+ strands\r\n      * Allow INV null strands\r\n      * When clustering, only require that strands match for INV/BND records\r\n  * Sample set and annotation improvements for `SVConcordance` (#8211)\r\n\r\n* **Mitochondrial pipeline** \r\n  * Added a variable for the user to specify the java heap size in Picard in the MT pipeline (#8406)\r\n  * Exposed runtime attributes as arguments in the MT pipeline (#8413) (#8417)\r\n\r\n* **Flow-based Calling**\r\n  * New/updated flow-based read tools (#8579)\r\n     * Added a new `GroundTruthScorer` tool to score reads against a reference/ground truth\r\n     * Updated `FlowFeatureMapper`\r\n  * Created an `AddFlowBaseQuality` tool that writes reads from flow-based SAM/BAM/CRAM files that pass criteria to a new file while adding a base-quality attribute (BQ) (#8235)\r\n  * Added an experimental tool `FlowPairHMMAlignReadsToHaplotypes` that aligns flow-based reads to set of haplotypes / templates (#8305)\r\n  * Fixed an issue with reads that contain the tp tag sometimes being incorrectly identified as flow-based  (#8337)\r\n  * Minor changes and fixes to flow-based annotations (#8442)\r\n  * Removed a line in `FlowBasedAnnotation` that contained a bug and thus was meaningless (#8421)\r\n  * Additional annotation in FeatureMap (#8347)\r\n  * Removed unnecessary flow-based argument and option (#8342)\r\n  * `GroundTruthScorer` doc update (#8597)\r\n  * Removed unnecessary and buggy validation check (#8580)\r\n\r\n* **Notable Enhancements**\r\n  * Major security fixes in our dependencies and docker environment\r\n    * Updated the GATK base docker image to Ubuntu 22.04 for security fixes and newer versions of genomics packages like `samtools` and `bcftools` (#8610)\r\n    * Updated GATK dependencies to address known security vulnerabilities, and added a vulnerability scanner to `build.gradle` (#8607)\r\n  * Greatly improved HTTP support (#8611)\r\n    * Updated the `http-nio` library and made tweaks to HTSJDK to make it available in more places.  The new version of `http-nio` should provide much more reliable access to http(s) file paths.  This is supported by all methods accessing Paths, and includes SAM/BAM/CRAM and VCF/Feature files.  It includes a new retry mechanism which retries after transient errors.  It also includes bug fixes and various other minor improvements, such as making encoded Path handling more consistent.\r\n  * Added a new `PrintFileDiagnostics` tool that can output the internal metadata of `CRAM`, `CRAI` and `BAI` files for diagnostic purposes (#8577)\r\n  * Added a new `TransmittedSingleton` annotation and added quality threshold arguments to the `PossibleDenovo` annotation (#8329)\r\n  * Support multiple read name inputs in `ReadNameReadFilter` (#8405)\r\n  * Added a native GATK implementation for `2bit` references, and removed the dependency on the ADAM library (#8606)\r\n\r\n* **Bug Fixes**\r\n  * Fixed a major bug in the Intel GKL that could cause output files to intermittently fail to be compressed properly (#8409)\r\n\r\n* **Miscellaneous Changes**\r\n  * `CNNVariantTrain`: exposed more CNN training parameters as arguments (#8483)\r\n  * Support underscores in bucket names on Google Cloud (#8439)\r\n  * Performed some refactoring on the new annotation-based filtering tools (#8131)\r\n  * Added tags to `dockstore.yaml` (#8323)\r\n  * Added the ability to specify the RELEASE arg to the cloud-based docker build, and added a new docker release script (#8247)\r\n  * Added an option to `AnalyzeSaturationMutagenesis` to keep disjoint mates (#8557)\r\n  * Exit with code 137 when we get an `OutOfMemoryError` (#8277)\r\n  * Updates to reduce size of docker image (#8259)\r\n  * Free up space on Github Actions runners for all jobs (#8386) (#8371) (#8373)\r\n  * Fixed warnings in Github Actions (#8241)\r\n  * Disabled line-by-line codecov comments (#8613)\r\n  * Fixed a bug in the GATK download metrics script (#8418)\r\n  * Updated the Spark version in the GATK jar manifest, and hooked up the Spark version constant in build.gradle (#8625)\r\n  * Fixed a warning in Gradle (#8431)\r\n  * Pinned joblib to v1.1.1 in the python environment (#8391)\r\n  * Updated the Ubuntu version for the Carrot github action because github dropped support for 18.04 (#8299)\r\n    \r\n* **Documentation**\r\n  * Major update to documentation generation for Metrics classes (#7749)\r\n  * Updated some dead links to the GATK forums in the docs (#8273)\r\n   \r\n* **Dependencies**\r\n  * Updated `Picard` to 3.1.1 (#8585)\r\n  * Updated `HTSJDK` 4.1.0 (#8620)\r\n  * Updated the `Intel GKL` to 0.8.11 (#8409)\r\n  * Updated `Apache Spark` to 3.5.0 (#8607)\r\n  * Updated `Hadoop` to 3.3.6 (#8607)\r\n  * Updated `google-cloud-nio` to 0.127.8\r\n  * Updated `http-nio` to 1.1.0 (#8626)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.5.0.0",
        "name": "4.5.0.0",
        "release_id": 133825179,
        "tag": "4.5.0.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.5.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/133825179",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/133825179",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.5.0.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2023-03-16T18:55:06Z",
        "date_published": "2023-03-16T19:08:54Z",
        "description": "**Download release:** [gatk-4.4.0.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.4.0.0/gatk-4.4.0.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.4.0.0 release:**\r\n--------------------------------------\r\n\r\n* We've moved to Java 17, the latest long-term support (LTS) Java release, for building and running GATK! Previously we required Java 8, which is now end-of-life. \r\n    * Newer non-LTS Java releases such as Java 18 or Java 19 may work as well, but since they are untested by us we only officially support running with Java 17.\r\n\r\n* Significant enhancements to `SelectVariants`, including arguments to enable `GVCF` filtering support and to work with genotype fields more easily.\r\n\r\n* A new tool `SVConcordance`, that calculates SV genotype concordance between an \"evaluation\" VCF and a \"truth\" VCF\r\n\r\n* Bug fixes and enhancements to the support for the Ultima Genomics flow-based sequencing platform introduced in GATK 4.3.0.0\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **Flow-based Variant Calling**\r\n    * `FlowFeatureMapper`: added surrounding-median-quality-size feature (#8222)\r\n    * Removed hardcoded limit on max homopolymer call (#8088)\r\n    * Fixed bug in dynamic read disqualification (#8171)\r\n    * Fixed a bug in the parsing of the T0 tag (#8185)\r\n    * Updated flow-based calling `Mutect2` parameters to make them consistent with the `HaplotypeCaller` parameters (#8186)\r\n        \r\n* **SelectVariants**\r\n    * Enabled GVCF type filtering support in `SelectVariants` (#7193)\r\n        * Added an optional argument `--ignore-non-ref-in-types` to support correct handling of VariantContexts that contain a NON_REF allele. This is necessary because every variant in a GVCF file would otherwise be assigned the type MIXED, which makes it impossible to filter for e.g. SNPs.\r\n        * Note that this only enables correct handling of GVCF input. The filtered output files are VCF (not GVCF) files, since reference blocks are not extended when a variant is filtered out.\r\n    * `SelectVariants`: added new arguments for controlling genotype JEXL filtering (#8092)\r\n        * `-select-genotype`: with this new genotype-specific JEXL argument, we support easily filtering by genotype fields with expressions like 'GQ > 0', where the behavior in the multi-sample case is 'GQ > 0' in at least one sample. It's still possible to manually access genotype fields using the old `-select` argument and expressions such as `vc.getGenotype('NA12878').getGQ() > 0`.\r\n        * `--apply-jexl-filters-first`: This flag is provided to allow the user to do JEXL filtering before subsetting the format fields, in particular the case where the filtering is done on INFO fields only, which may improve speed when working with a large cohort VCF that contains genotypes for thousands of samples.\r\n\r\n* **SV Calling**\r\n    * Added a new tool `SVConcordance`, that calculates SV genotype concordance between an \"evaluation\" VCF and a \"truth\" VCF (#7977)\r\n    * Recognize MEI DELs with ALT format <DEL:ME> in `SVAnnotate` (#8125)\r\n    * Don't sort rejected reads output from `AnalyzeSaturationMutagenesis` (#8053)\r\n\r\n* **Notable Enhancements**\r\n    * `GenotypeGVCFs`: added an `--keep-specific-combined-raw-annotation` argument to keep specified raw annotations (#7996)\r\n    * `VariantAnnotator` now warns instead of fails when the variant contains too many alleles (#8075)\r\n    * Read filters now output total reads processed in addition to the number of reads filtered (#7947)\r\n    * Added `GenomicsDB` arguments to the `CreateSomaticPanelOfNormals` tool (#6746)\r\n    * Added a `DeprecatedFeature` annotation and a process for officially marking GATK tools as deprecated (#8100)\r\n    * Prevent tool `close()` methods from hiding underlying errors (#7764)\r\n\r\n* **Bug Fixes**\r\n    * Fixed issue causing `VariantRecalibrator` to sometimes fail if user provided duplicate -an options (#8227)\r\n    * `ReblockGVCF`: remove A,R, and G length attributes when `ReblockGVCF` subsets an allele (#8209)\r\n        * Previously if an input gVCF had allele length, reference length, or genotype length annotations in the FORMAT field, `ReblockGVCF` would not remove all of them at sites where an allele was dropped. This makes the output gVCF invalid since the annotation length no longer matches the length described in the header at those sites. Now we fix up F1R2, F2R1, and AF annotations and remove any other annotations that are not already handled that are defined as A, R, or G length in the header.\r\n    * Fixed a `gCNV` bug that breaks the inference when only 2 intervals are provided (#8180)\r\n    * Fixed NPE from unintialized logger in `GenotypingEngine` (#8159)\r\n    * Fixed asynchronous Python exception propagation in `StreamingPythonExecutor`/`CNNScoreVariants` (#7402)\r\n    * Fixed issue in `ShiftFasta` where the interval list output was never written (#8070)\r\n    * Bugfix for the type of some output files in the somatic CNV WDL (#6735) (#8130)\r\n    * `MergeAnnotatedRegions` now requires a reference as asserted in its documentation (#8067)\r\n\r\n* **Miscellaneous Changes**\r\n    * Deprecated an untested `VariantRecalibrator` argument and an old `ReblockGVCF` argument that produced invalid GVCFs (#8140)\r\n    * Removed old `GnarlyGenotyper` code with a diploid assumption to prepare for adding haploid support to `GnarlyGenotyper` (#8140)\r\n    * `ReblockGVCF`: add error message for when tree-score-threshold is set but the TREE_SCORE annotation is not present (#8218)\r\n    * `TransferReadTags`: allow empty unaligned bams as input (#8198)\r\n    * Refactored `JointVcfFiltering` WDL and expanded tests. (#8074)\r\n    * Updated the carrot github action workflow to the most recent version, which supports using `#carrot_pr` to trigger branch vs master comparison runs (#8084)\r\n    * Replaced uses of `File.createTempFile()` with `IOUtils.createTempFile()` to ensure that temp files are deleted on shutdown (#6780)\r\n    * Don't require python just to instantiate the `CNNScoreVariants` tool classes. (#8128)\r\n    * Made several `Funcotator` methods and fields protected so it is easier to extend the tool (#8124) (#8166)\r\n    * Test for presence of ack result message and simplify `ProcessControllerAckResult` API (#7816)\r\n    * Fixed the path reported by the gatkbot when there are test failures (#8069)\r\n    * Fixed incorrect boolean value in `DirichletAlleleDepthAndFractionIntegrationTest` (#7963)\r\n    * Removed two ancient and unused `HaplotypeCaller` test files that are no longer needed (#7634)\r\n    * Added scattered gCNV case WDL to dockstore file (#8217)\r\n    \r\n* **Documentation**\r\n    * Updated instructions for installing Java in the README (#8089)\r\n    * Added documentation on `OMP_NUM_THREADS` and `MKL_NUM_THREADS` to `GermlineCNVCaller` and `DetermineGermlineContigPloidy` (#8223)\r\n    * Improvements to `PileupDetectionArgumentCollection` documentation (#8050)\r\n    * Fixed typo in documentation for `VariantAnnotator` (#8145)\r\n    \r\n* **Dependencies**\r\n    * Moved to `Java 17`, the latest LTS Java release, for building/running GATK (#8035)\r\n    * Updated `Gradle` to 7.5.1 (#8098)\r\n    * Updated the GATK base docker image to 3.0.0 (#8228)\r\n    * Updated `HTSJDK` to 3.0.5 (#8035)\r\n    * Updated `Picard` to 3.0.0 (#8035)\r\n    * Updated `Barclay` to 5.0.0 (#8035)\r\n    * Updated `GenomicsDB` to 1.4.4 (#7978)\r\n    * Updated `Spark` to 3.3.1 (#8035)\r\n    * Updated `Hadoop` to 3.3.1. (#8102)\r\n    * Require `commons-text` 1.10.0 to fix a security vulnerability (#8071)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.4.0.0",
        "name": "4.4.0.0",
        "release_id": 95892670,
        "tag": "4.4.0.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.4.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/95892670",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/95892670",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.4.0.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2022-10-13T01:04:06Z",
        "date_published": "2022-10-13T01:13:54Z",
        "description": "**Download release:** [gatk-4.3.0.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.3.0.0/gatk-4.3.0.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.3.0.0 release:**\r\n--------------------------------------\r\n\r\n* Support for the Ultima Genomics flow-based sequencing platform\r\n\r\n* A next-generation suite of tools for variant filtration based on site-level annotation, intended to eventually supersede the older `VariantRecalibrator` workflow\r\n\r\n* `CompareReferences` and `CheckReferenceCompatibility`: new tools for comparing and checking compatibility with genomic references\r\n\r\n* Support in `HaplotypeCaller`/`Mutect2` for supplementing the variants discovered in local assembly with variants discovered via a pileup-based approach\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **Support for the Ultima Genomics flow-based sequencing platform** (#7876)\r\n    * Added a new `--flow-mode` argument to `HaplotypeCaller` which better supports flow-based calling\r\n        * Added a new Haplotype Filtering step after assembly which removes suspicious haplotypes from the genotyper\r\n        * Added two new likelihoods models, `FlowBasedHMM` and the `FlowBasedAlignmentLkelihoodEngine`\r\n    * Added a new `--flow-mode` argument to `Mutect2` which better supports flow-based calling\r\n    * Added support for uncertain read end-positions in `MarkDuplicatesSpark`\r\n    * Added a new tool `FlowFeatureMapper` for quick heuristic calling of bams for diagnostics\r\n    * Added a new tool `GroundTruthReadsBuilder` to generate ground truth files for Basecalling\r\n    * Added a new diagnostic tool `HaplotypeBasedVariantRecaller` for recalling VCF files using the `HaplotypeCallerEngine`\r\n    * Added a new tool breaking up CRAM files by their blocks, `SplitCram`\r\n    * Added a new read interface called `FlowBasedRead` that manages the new features for FlowBased data\r\n    * Added a number of flow-specific read filters\r\n    * Added a number of flow-specific variant annotations\r\n    * Added support for read annotation-clipping as part of clipreads and GATKRead\r\n    * Added a new `PartialReadsWalker` that supports terminating before traversal is finished\r\n\r\n* **Next-generation suite of tools for variant filtration based on site-level annotations** (#7954) (#8049)\r\n    * This tool suite is intended to eventually supersede the older `VariantRecalibrator` workflow\r\n    * The new tools include:\r\n        * `ExtractVariantAnnotations`: extracts site-level variant annotations, labels, and other metadata from a VCF file to HDF5 files\r\n        * `TrainVariantAnnotationsModel`: trains a model for scoring variant calls based on site-level annotations\r\n        * `ScoreVariantAnnotations`: scores variant calls in a VCF file based on site-level annotations using a previously trained model\r\n        \r\n* **New Reference Comparison Tools**\r\n    * `CompareReferences`: a new tool for analyzing the differences between references at both the dictionary and the base level (#7930) (#7987) (#7973)  \r\n        * In its default mode, this tool uses the reference dictionaries to generate an MD5-keyed table comparing the specified references, and does an analysis to summarize the differences between the references provided. \r\n        * Comparisons are made against a \"primary\" reference, specified with the `-R` argument. Subsequent references to be compared may be specified using the ``--references-to-compare` argument.\r\n        * A supplementary table keyed by sequence name can be displayed using the `--display-sequences-by-name argument`; to display only sequence names for which the references are not consistent, run with the `--display-only-differing-sequences` argument as well.\r\n        * MD5s can be recalculated from the actual sequence when missing from the dictionary\r\n        * When run with `--base-comparison FULL_ALIGNMENT`, the tool performs full-sequence alignment on the differing reference sequences to produce a VCF with SNPs and Indels. However, this mode ignores IUPAC / N bases.\r\n        * Running with `--base-comparison FIND_SNPS_ONLY` finds single-base differences between differing reference sequences of the same length. This mode can handle IUPAC / N bases correctly, but not indels.\r\n        * To perform the full-sequence alignment, GATK now packages a distribution of `MUMmer` for x86_64 Mac and Linux, which can be invoked from within the GATK using the new `MummerExecutor` class.\r\n    * `CheckReferenceCompatibility`: a new tool to check a BAM/CRAM/VCF for compatibility against a set of references (#7959) (#7973)\r\n        * This tool generates a table analyzing the compatibility of a BAM/CRAM/VCF input file against provided references.\r\n        * The tool works to compare BAM/CRAMs (specified using the -I argument) as well as VCFs (specified using the -V argument) against provided reference(s), specified using the `--references-to-compare` argument.\r\n        * When MD5s are present, the tool decides compatibility based on all sequence information (MD5, name, length); when MD5s are missing, the tool makes compatibility calls based only on sequence name and length.\r\n        \r\n* **HaplotypeCaller/Mutect2**\r\n    * Added an optional \"Pileup Detection\" step to `Mutect2` and `HaplotypeCaller` before assembly that supplements the variants from local assembly with variants that show up in the pileups (#7432)\r\n    * Fixed a `Mutect2` `IndexOutOfBoundException` with germline resource (#7979)\r\n    * `Mutect3` dataset enhancements: optional truth VCF for labels, seq error likelihood annotation (#7975)\r\n    * Added `Mutect3` dataset generation to the `Mutect2` WDL (#7992)\r\n    * `GetPileupSummaries` now streams its output rather than storing it in memory (#7664)\r\n    * Fixed a rare edge case in the `AdaptiveChainPruner` where the `JavaPriorityQueue` is undefined for tied elements (#7851)\r\n\r\n* **SV Calling**\r\n    * `CondenseDepthEvidence`: a new tool that combines adjacent intervals in DepthEvidence files (#7926)\r\n    * `LocusDepthtoBAF`: a new tool that merges locus-sorted LocusDepth evidence files, calculates the bi-allelic frequency (baf) for each sample and site, and writes these values as a BafEvidence output file (#7776)\r\n    * `PrintReadCounts`: a new tool that prints (and optionally subsets) an read depth (DepthEvidence) file or a counts file as one or more (for multi-sample DepthEvidence files) counts files for CNV determination (#8015)\r\n    * `CollectSVEvidence`: fixed a bug where trailing SNP sites and depth intervals without read coverage were being omitted from the output (#8045)\r\n    * `CollectSVEvidence`: added read depth generation and raw-counts output (#8015)\r\n    * Improved `PrintSVEvidence` performance by tweaking the `MultiFeatureWalker` traversal (#7869)\r\n    * Fixes related to `BafEvidence` (biallelic-frequency of a sample at some locus) (#7861)\r\n    * Fixed a bug where the end coordinate was being incorrectly compared when sorting discordant read pair evidence (#7835)\r\n    * Sort output from `SVClusterEngine` (#7779)\r\n    * Remove abandoned SV filtering project and unneeded build dependency (#7950)\r\n\r\n* **CNV Calling**\r\n    * Fix a no-call genotype ploidy bug in `JointGermlineCNVSegmentation` (#7779)\r\n    * Added numerical-stability tests and updated test data for all `ModelSegments` single-sample and multiple-sample modes (#7652)\r\n    * Added a gCNV integration test to detect numerical differences in the outputs (#7889)\r\n\r\n* **GenomicsDB**\r\n    * `GenomicsDBImport`: added the ability to specify explicit index locations via the sample name map file (#7967)\r\n        * Each line in the sample name map file may now optionally contain a third column with the path/URI to the index. This is useful when the index is not in the same location as the corresponding GVCF.\r\n    \r\n* **Bug Fixes**\r\n    * Fixed an issue where we weren't properly merging AD values when combining GVCFs and no PLs were present (#7836)\r\n    * Fixed a bug in `ReblockGVCF` that could cause the first position on a contig to be dropped (#8028)\r\n    * Fixed an allele-ordering issue in the allele-specific annotation code (#7585)\r\n    * `VariantRecalibrator`: type change int -> long to prevent tranche novel variant count overflow (#7864)\r\n    * Fixed an issue with tabix index generation (#7858)\r\n    * Fixed a bug in `SiteDepthCodec` (#7910)\r\n\r\n* **Miscellaneous Changes**\r\n    * `VariantsToTable` now includes all fields when none are specified (#7911)\r\n    * `SelectVariants` now warns the user about poor performance when the sample names in the VCF header are unsorted (#7887)\r\n    * `VariantRecalibrator` now has a `--dont-run-rscript` argument to disable execution of its R script but still output the actual R script file (#7900)\r\n    * Added some generic read tag/expression filters for use on numeric tags (#7746)\r\n    * Replaced Travis CI with Github Actions for our continuous testing (#7754)\r\n    * Switched over to Github Actions for building our nightly docker image (#7775)\r\n    * Created a new `build_docker_remote.sh` script for building the docker image remotely with Google Cloud Build (#7951)\r\n    * Added an argument mode manager for group arguments and a demonstration of how it might be used in `HaplotypeCaller` `--dragen-mode` (#7745)\r\n    * Added unit tests for the `Utils.concat()` methods (#7918)\r\n    * Added a test to validate WDLs in the scripts directory. (#7826)\r\n    * Added a `use_allele_specific_annotation` arg and fixed task with empty input in the `JointVcfFiltering` WDL (#8027)\r\n    * Fixed an issue in the GATK stats script in which the first day's downloads on a new release were set to 0 (#7794)\r\n    * Fixed a typo in the Dockerfile that broke git lfs pull (#7806)\r\n    * Removed unused code in the `utils.solver` package (#7922)\r\n    * Corrected the time for GATK nightly build cron jobs (#7784)\r\n    * Disabled the red \"X\" from failing `CodeCov` builds and delaying the posting of coverage information to complete test (#7817)\r\n    * Some minor misc engine changes (#7744)\r\n\r\n* **Documentation**\r\n    * Marked `JointGermlineCNVSegmentation` as a DocumentedFeature (#7871)\r\n    * Marked `SVAnnotate` as a DocumentedFeature (#7833)\r\n    * Marked `CollectSVEvidence` as a DocumentedFeature (#8041)\r\n    * Docs clarification in `GenotypeGVCFs` for some reblocking-related funkiness (#7846)\r\n    * Updated the GATK Readme to reflect the switch from Travis CI to Github Actions (#7808)\r\n \r\n* **Dependencies**\r\n    * Updated `HTSJDK` to 3.0.1 (#8025)\r\n    * Updated `Picard` to 2.27.5 (#8025)\r\n    * Updated `protobuf` to 3.21.6 (#8036)\r\n    * Updated `gsalib` to 2.2.1 (#8048)\r\n    * Pinned `typing_extensions` Python package to `4.1.1` in the GATK conda environment (#7802)\r\n    ",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.3.0.0",
        "name": "4.3.0.0",
        "release_id": 79706865,
        "tag": "4.3.0.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.3.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/79706865",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/79706865",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.3.0.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2022-04-13T19:10:59Z",
        "date_published": "2022-04-13T19:24:07Z",
        "description": "**Download release:** [gatk-4.2.6.1.zip](https://github.com/broadinstitute/gatk/releases/download/4.2.6.1/gatk-4.2.6.1.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.2.6.1 release:**\r\n--------------------------------------\r\n\r\nThis release contains a single bug fix for `GenotypeGVCFs` to fix an erroneous `IllegalStateException` (\"No likelihood sum exceeded zero -- method was called for variant data with no variant information.\") in the edge case where unnormalized PLs are present at monomorphic sites.\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.2.6.1",
        "name": "4.2.6.1",
        "release_id": 64395311,
        "tag": "4.2.6.1",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.2.6.1",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/64395311",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/64395311",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.2.6.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2022-04-08T19:15:10Z",
        "date_published": "2022-04-08T19:27:07Z",
        "description": "**Download release:** [gatk-4.2.6.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.2.6.0/gatk-4.2.6.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.2.6.0 release:**\r\n--------------------------------------\r\n\r\n* Important bug fixes for the joint calling tools (GenotypeGVCFs / GenomicsDB)\r\n    * GATK 4.2.5.0 contained two joint genotyping bugs that are now fixed in GATK 4.2.6.0:\r\n        * `GenotypeGVCFs` can throw NullPointerExceptions in some cases with many alternate alleles.  \r\n        * The expectation-maximization component of the QUAL calculation was disabled, leading to false positive, low quality alleles at some multi-allelic sites.\r\n    * **If you are running these tools in 4.2.5.0 we strongly recommend updating to 4.2.6.0**\r\n\r\n* Fixed a \"Bucket is a requester pays bucket but no user project provided\" error that occurred when accessing requester pays buckets in Google Cloud Storage even when the `--gcs-project-for-requester-pays` argument was specified\r\n   * **If you continue to encounter problems accessing requester pays Google Cloud Storage buckets in 4.2.6.0, please let us know by filing a Github issue!**\r\n\r\n* Two new tools for the Structural Variation calling pipeline: `SVAnnotate` and `PrintSVEvidence`\r\n\r\n* Some fixes to genotype-given-alleles mode in `HaplotypeCaller` and `Mutect2`\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **Joint Calling (GenotypeGVCFs / GenomicsDB)**\r\n    * GATK 4.2.5.0 contained two joint genotyping bugs which are now fixed in 4.2.6.0:\r\n        * `GenotypeGVCFs` can throw NullPointerExceptions in some cases with many alternate alleles.\r\n            * Fixed in:\r\n                * Fix for `NullPointerException` when GenomicsDB has more ALT alleles than specified maximum and many GQ0 hom-ref genotypes allow variants to pass the QUAL filter (#7738)  \r\n        * The expectation-maximization component of the QUAL calculation was disabled, leading to false positive, low quality alleles at some multi-allelic sites.\r\n            * Fixed in: \r\n                * Fix multi-allelic QUAL calculation and restore some missing ALT annotation data in `ReblockGVCFs` (#7670)\r\n    * Mention acceptable compressed VCF file extensions in `GenomicsDBImport` error message (#7692)\r\n\r\n* **SV Calling**\r\n    * Added a new tool `SVAnnotate` (#7431)\r\n        * `SVAnnotate` adds functional annotations for SVs called by `GATK-SV` (#7431)\r\n    * Added a new tool `PrintSVEvidence` (#7695)\r\n        * `PrintSVEvidence` is a tool that can merge any number of files containing one of five types of evidence of structural variation.  It's also capable of subsetting regions or samples.  It's used to merge evidence from a cohort in the `GATK-SV` pipeline.\r\n    * Added start/end coordinate validation to `SVCallRecord` (#7714)\r\n\r\n* **HaplotypeCaller / Mutect2**\r\n    * Fixed an edge case in `HaplotypeCaller` where filtered alleles in the vicinity of forced-calling alleles could result in empty calls (#7740)\r\n        * This affects users who run genotype given alleles mode in non-GVCF mode\r\n    * Fixed a bug in `HaplotypeCaller` and `Mutect2` where force-calling alleles were lost upon trimming by placing allele injection after trimming (#7679)\r\n    * Added a debug ``--pair-hmm-results-file` argument that dumps the the exact inputs/outputs of the PairHMM to a file (#7660)\r\n    * Some changes to `Mutect2` to support the future `Mutect3` (#7663)\r\n        * Added training data for the Mutect3 normal artifact filter \r\n        * Output tensors for Mutect3 as plain text rather than VCF\r\n\r\n* **RNA Tools**\r\n    * `TransferReadTags`: a new tool that transfers a read tag from an unaligned bam to the matching aligned bam (#7739).\r\n        * This tool allows us to retrieve read tags that get lost when converting a SAM file to fastqs, then back to SAM (which is necessary if e.g. running fastp to clip adapter bases before alignment).\r\n    * `PostProcessReadsForRSEM`:  a new tool that re-orders and filters reads before running RSEM, which has stringent requirements on the input SAM (https://github.com/deweylab/RSEM) (#7752).\r\n\r\n* **Funcotator**\r\n    * Added custom `VariantClassification` severity ordering. (#7673)\r\n        * Users can now customize the severity ratings of the various `VariantClassifications` using the new `--custom-variant-classification-order` argument\r\n    * Added logging statements to the b37 conversion process explaining why the automatic b37 conversion does or does not take place on their VCFs (#7760)\r\n\r\n* **VariantRecalibrator**\r\n    * Added regularization to covariance in GMM maximization step to fix convergence issues in `VariantRecalibrator` (#7709)\r\n        * This makes the tool more robust in cases where annotations are highly correlated\r\n\r\n* **Bug Fixes**\r\n    * Fixed a \"Bucket is a requester pays bucket but no user project provided\" error that occurred when accessing requester pays buckets in Google Cloud Storage even when `--gcs-project-for-requester-pays` was specified (#7700) (#7730)\r\n    * Fix for the `PossibleDeNovo` annotation to work without Genotype Likelihoods (#7662)\r\n        * `PossibleDeNovo` checks each trio's genotype (including parent hom ref genotypes) for likelihoods even though it doesn't actually use the PLs. The PLs can get dropped if GVCFs are reblocked which means this annotation no longer works as expected. This changes the check to look for GQs instead of PLs as the GQs are used as part of the annotation.\r\n    * Fixed a bug with the `--mate-too-distant-length` in `MateDistantReadFilter` not being configurable (#7701)\r\n\r\n* **GATK Engine**\r\n    * Added a new `MultiFeatureWalker` traversal to the GATK engine (#7695)\r\n    * Removed an ancient, unused option to track unique reads in a `LocusIteratorByState` (#6410)\r\n        \r\n* **Miscellaneous Changes**\r\n    * Added back the `jcenter` repository resolver to our gradle build, fixing a \"Could not find biz.k11i:xgboost-predictor:0.3.0\" error when building GATK from source (#7665)\r\n    * We now properly update the `latest` tag in the `broadinstitute/gatk-nightly` Dockerhub repo (#7703)\r\n    * The docker build now only does a `git lfs pull` on `src/main/resources/large` (#7727)\r\n    * Install git lfs with --force in the `Dockerfile` (#7682)\r\n    * Fix WDL generation for `MultiVariantWalkers` by adding a companion index to the `MultiVariantWalker` input variant arg (#7689)\r\n    * Added google apps script to automatically update GATK release stats. (#7637)\r\n    * Updated the GATK stats script to be more universally usable (#7759)\r\n    * Added `JointCallExomeCNVs` to `.dockstore.yml` and included a note in the WDL (#7719)\r\n\r\n* **Documentation**\r\n    * Corrected the docs for the `--heterozygosity` argument in the `GenotypeCalculationArgumentCollection` (#7661)\r\n    \r\n* **Dependencies**\r\n    * Updated `Picard` to `2.27.1` (#7766)\r\n    * Updated `google-cloud-nio` to `0.123.25` (#7730)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.2.6.0",
        "name": "4.2.6.0",
        "release_id": 64000854,
        "tag": "4.2.6.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.2.6.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/64000854",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/64000854",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.2.6.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2022-02-04T22:14:05Z",
        "date_published": "2022-02-04T22:26:23Z",
        "description": "**Download release:** [gatk-4.2.5.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.2.5.0/gatk-4.2.5.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.2.5.0 release:**\r\n--------------------------------------\r\n\r\n* Fixed a `GenotypeGVCFs` `IllegalStateException` error reported by multiple users in https://github.com/broadinstitute/gatk/issues/7639\r\n\r\n* Added a new tool `SVCluster` that clusters structural variants based on coordinates, event type, and supporting algorithms.\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **Joint Calling (GenotypeGVCFs / GenomicsDB)**\r\n    * Fixed an `IllegalStateException` in `GenotypeGVCFs` arising from GenomicsDB output with too many alts and no likelihoods, and also added a `--genomicsdb-max-alternate-alleles` argument that is separate from the `--max-alternate-alleles` argument used by `GenotypeGVCFs` (#7655)\r\n        * This fixes the `GenotypeGVCFs` error reported in https://github.com/broadinstitute/gatk/issues/7639\r\n        * The new `--genomicsdb-max-alternate-alleles` argument is required to be at least one greater than the `--max-alternate-alleles` argument, to account for the NON_REF allele.\r\n    * `ReblockGVCF`: fixed an edge case where hom-ref \"variant\" records with no data had wrong-sized PLs and didn't merge with adjacent blocks (#7644)\r\n\r\n* **SV Calling**\r\n    * Added a new tool `SVCluster` that clusters structural variants based on coordinates, event type, and supporting algorithms. (#7541) \r\n        * Primary use cases include:\r\n            * Clustering SVs produced by multiple callers, based on interval overlap, breakpoint proximity, and sample overlap.\r\n            * Merging multiple SV VCFs with disjoint sets of samples and/or variants.\r\n            * Defragmentation of copy number variants produced with depth-based callers.\r\n     \r\n* **Mutect2**\r\n    * The palindrome ITR artifact transformer now skips reads whose contigs are not in sequence dictionary (#6968)\r\n        * This fixes a NullPointerException error in `Mutect2` reported in #6851\r\n    \r\n* **GATK Engine**\r\n    * Added a new read filter, `ExcessiveEndClippedReadFilter` (#7638)\r\n        * This filter will keep reads that have fewer than the specified number of clipped bases on either end. \r\n        * Designed with long reads in mind, and as a result has a default value of 1000.\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.2.5.0",
        "name": "4.2.5.0",
        "release_id": 58730486,
        "tag": "4.2.5.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.2.5.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/58730486",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/58730486",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.2.5.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "lbergelson",
          "type": "User"
        },
        "date_created": "2022-01-04T15:58:22Z",
        "date_published": "2022-01-04T22:10:04Z",
        "description": "**Download release:** [gatk-4.2.4.1.zip](https://github.com/broadinstitute/gatk/releases/download/4.2.4.1/gatk-4.2.4.1.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.2.4.1 release:**\r\n-------------------------------------\r\n* Fix more newly discovered log4j2 vulnerabilities.  Now that people are paying attention they are finding all sorts of things. \r\n\r\n\r\n**Full list of changes:**\r\n--------------------------\r\n\r\n* **Build System**\r\n   * Upgrade our build from Gradle 5.6 to the newest 7.3.2  (#7609)\r\n   * This fixes some gradle bugs which were blocking development\r\n\r\n* **GenomicsDB**\r\n    * Update to genomicsdb 1.4.3 (#7613) which fixes #7598\r\n    *  Fix bug which caused --max_alternate_alleles to be ignored when using GenomicsDB (#7576)\r\n\r\n* **Miscellaneous Changes**\r\n    * Update .dockstore.yml (#7595)\r\n    * Fix developer doc in AS_RMSMappingQuality (#7607)\r\n\r\n* **Dependencies**\r\n   * Update log4j to 2.17.1 (#7624)(#7615)\r\n   * Upgrade to Barclay 4.0.2. (#7602)\r\n   * Update to genomicsdb 1.4.3 (#7613) ",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.2.4.1",
        "name": "4.2.4.1 the log4j strikes back",
        "release_id": 56414386,
        "tag": "4.2.4.1",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.2.4.1",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/56414386",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/56414386",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.2.4.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "lbergelson",
          "type": "User"
        },
        "date_created": "2021-12-15T17:44:28Z",
        "date_published": "2021-12-15T19:33:06Z",
        "description": "**Download release:** [gatk-4.2.4.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.2.4.0/gatk-4.2.4.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.2.4.0 release:**\r\n--------------------------------------\r\n\r\n* Fix a major security bug due to log4j vulnerability.  (CVE-2021-44228)\r\n* Improvement to calculation of ExcessHet in joint genotyping. (GenotypeGVCFs, GnarlyGenotyper, ExcessHet).\r\n\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **Funcotator**\r\n    * Aligned the Funcotator checkIfAlreadyAnnotated test with the Funcotator engine code. (#7555)\r\n\r\n* **GenotypeGVCFs** / **ExcessHet**\r\n    * Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding tests. We now report the same value as ExcHet in bcftools. Note that previous values of 3.0103 (corresponding to mid-p values of 0.5) will now be 0.0000.  (#7394)\r\n    * Updated expected ExcessHet values in integration test resources and added an update toggle to GnarlyGenotyperIntegrationTest.\r\n    * Updated ExcessHet documentation.\r\n\r\n* **Miscellaneous Changes**\r\n    * Delete an unused .gitattributes file which was unintentionally stored in git-lfs and caused an error message to appear sometimes when checking out the repository. (#7594)\r\n    * Remove trailing tab in VariantsToTable output header (#7559)\r\n\r\n* **Documentation**\r\n    * Updated AUTHORS file to remove a contributor's name at their request. (#7580)\r\n    * Remove outdated javadoc line in AssemblyBasedCallerUtils (#7554)\r\n\r\n* **Dependencies**\r\n    * Updated log4j to version 2.13.1 -> 2.16.0 to patch CVE-2021-44228 (#7605)",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.2.4.0",
        "name": "4.2.4.0 the log4shell edition",
        "release_id": 55389060,
        "tag": "4.2.4.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.2.4.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/55389060",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/55389060",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.2.4.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2021-11-02T21:46:48Z",
        "date_published": "2021-11-02T22:08:25Z",
        "description": "**Download release:** [gatk-4.2.3.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.2.3.0/gatk-4.2.3.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.2.3.0 release:**\r\n--------------------------------------\r\n\r\n* Notable bug fixes for `Mutect2` and `Funcotator`\r\n\r\n* Support in `CombineGVCFs` and `GenotypeGVCFs` for \"reblocked\" GVCFs as produced by the `ReblockGVCF` tool. Reblocked GVCFs have a significantly reduced storage footprint.\r\n\r\n* More control over the Smith-Waterman parameters in `HaplotypeCaller` and `Mutect2`\r\n\r\n* A new Fragment Allele Depth (`FAD`) variant annotation similar to the `AD` annotation except that allele support is considered per read pair, not per individual read\r\n\r\n* GenomicsDB bug fixes and enhancements\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **HaplotypeCaller/Mutect2**\r\n    * Fixed a bug where `Mutect2` failed to filter germline variants with alternate representations (#7103)\r\n        * This caused variants with alternative representations in gnomAD to not be recognized as being the same as called variants in some cases. This resulted in variants that were called and not filtered, but they should have been filtered by \"germline\".\r\n    * Exposed Smith-Waterman parameters as tool arguments in `HaplotypeCaller`, `Mutect2`, and `FilterAlignmentArtifacts`. (#6885)\r\n        * Enables use of alternative parameters for different event representation (e.g. three consecutive SNPs instead of two small indels)\r\n    * Can now specify the Smith-Waterman implementation in `FilterAlignmentArtifacts` (#7105)\r\n    * Added a `--debug-assembly-variants-out` diagnostic option to output a side VCF with variants detected by assembly for `HaplotypeCaller` and `Mutect2` (#7384)\r\n    * `Mutect2`: the `--genotype-germline-sites` argument is no longer marked as experimental (#7533)\r\n\r\n* **GenotypeGVCFs / CombineGVCFs**\r\n    * Updated `CombineGVCFs` and `GenotypeGVCFs` to handle \"reblocked\" GVCFs with diploid data that are potentially missing hom-ref genotype PLs (#7223)\r\n    * Homozygous reference genotypes with no PLs and zero depth are now output as no-calls by `GenotypeGVCFs` (#7471)\r\n    * Bug fixes for `GenotypeGVCFs`/`GnarlyGenotyper` when allele-specific annotations have empty values due to lack of informative reads or no depth (#7491) (#7186)\r\n\r\n* **GenomicsDB**\r\n    * Added a new `--call-genotypes` GenomicsDB argument, enabling output of called genotypes (i.e. not ./.) when tools like `CombineGVCFs` and `SelectVariants` read from a GenomicsDB workspace (#7223)\r\n    * Added a `--bypass-feature-reader` argument to `GenomicsDBImport` to allow the C-based htslib VCF reader implementation to be used instead of the Java implementation (#7393)\r\n        * Using this option will reduce memory usage and potentially speed up the import process\r\n    * Updated to GenomicsDB 1.4.2 (#7520)\r\n        * This release fixes a commonly-encountered bookkeeping issue with GenomicsDB array fragments. Should fix errors of the type: \"Error: Cannot read from buffer; Error: cannot load book-keeping\" as reported in https://github.com/broadinstitute/gatk/issues/7012\r\n        * Full release notes are here: https://github.com/GenomicsDB/GenomicsDB/releases/tag/v1.4.2\r\n\r\n* **Funcotator**\r\n    * Fixed a `StringIndexOutOfBoundsException` in the protein change prediction code that could be triggered by certain indels. The fix avoids the crash by adding additional bounds checking. (#7513)\r\n    * Allow `FilterFuncotations` to process multi-transcript genes (#7506)\r\n\r\n* **CNV Calling**\r\n    * CNV WDLs now handle BAM/CRAM index paths explicitly, as for cases where the index is not in the same path as its file (#7518)\r\n    * gCNV in the CASE mode now fills in all hidden DenoisingModelConfig and CopyNumberCallingConfig arguments from the input model configuration (#7464)\r\n    * Exposed number of samples used for estimating denoised copy ratios in gCNV via a new `--num-samples-copy-ratio-approx` argument (#7450)\r\n\r\n* **SV Calling**\r\n    * `JointGermlineCNVSegmentation`: bug fixes and refactoring (#7243)\r\n        * A number of bugs, particularly with max-clique clustering, have been fixed, as well as a parameter swap bug in `JointGermlineCNVSegmentation`\r\n        * Reworks classes used by `JointGermlineCNVSegmentation` for SV clustering and defragmentation. The design of `SVClusterEngine` has been overhauled to enable the implementation of `CNVDefragmenter` and `BinnedCNVDefragmenter` subclasses. Logic for producing representative records from a collection of clustered SVs has been separated into an `SVCollapser` class, which provides enhanced functionality for handling genotypes for SVs more generally.\r\n\r\n* **Notable Enhancements**\r\n    * Added a new Fragment Allele Depth (`FAD`) variant annotation (#7511)\r\n        * This annotation is identical to the `AD` annotation except that allele support is considered per read pair, not per individual read\r\n\r\n* **Miscellaneous Changes**\r\n    * `SplitIntervals`: added new tool arguments to control output file naming (#7488)\r\n    * Fixed an issue that caused the Travis CI test suite reports to fail to be uploaded (#7525)\r\n    * Updated Travis CI authentication information (#7521)\r\n\r\n* **Documentation**\r\n    * Updated `StrandBiasBySample` documentation (#7283)\r\n    * Updated `MarkDuplicatesSpark` documentation (#7191) (#7535)\r\n    * Added a comment to ``.travis.yml` about the checkout depth (#7421)\r\n\r\n* **Dependencies**\r\n    * Updated to `GenomicsDB` 1.4.2 (#7520)\r\n    * Updated `sqlite-jdbc` library to a newer version to support M1 Macs (#7519)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.2.3.0",
        "name": "4.2.3.0",
        "release_id": 52555877,
        "tag": "4.2.3.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.2.3.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/52555877",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/52555877",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.2.3.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2021-08-18T23:57:47Z",
        "date_published": "2021-08-19T00:08:46Z",
        "description": "**Download release:** [gatk-4.2.2.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.2.2.0/gatk-4.2.2.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.2.2.0 release:**\r\n--------------------------------------\r\n\r\n* The `ReblockGVCF` tool is now out of beta with several important improvements. This tool can be used to postprocess `HaplotypeCaller` GVCFs to decrease filesize.\r\n\r\n* `FilterMutectCalls` now has a `--microbial-mode` argument that sets filters to defaults appropriate for microbial calling\r\n\r\n* Important bug fixes to `CalibrateDragstrModel` and `Funcotator`\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **New Tools**\r\n    * `ShiftFasta`: create a fasta with the bases shifted by an offset (#6694)\r\n\r\n* **ReblockGVCF**\r\n    * `ReblockGVCF` is now out of beta (#7419)\r\n    * Improved `ReblockGVCF` output to eliminate overlapping reference blocks and reference gaps following trimmed deletions (#7122)\r\n    * Fixed bugs associated with input no-call genotypes and fixed an off-by-one error at contig starts (#7404)\r\n    * Fixed an error on ref blocks with missing DPs (if `--floor-blocks` arg is not provided); fixed rare cases where spanning deletion (*) allele is incorrectly modified (#7400)\r\n\r\n* **Mutect2**\r\n    * `FilterMutectCalls`: added a `--microbial-mode` argument that sets filters to defaults appropriate for microbial calling (#6694)\r\n\r\n* **ValidateVariants**\r\n    * Added an optional argument to check for GVCF reference blocks overlapping variants or other reference blocks (#7405)\r\n\r\n* **DRAGEN-GATK**\r\n    * Fixed a thread safety issue in `CalibrateDragstrModel` that could cause intermittent `ArrayIndexOutOfBoundsExceptions` (#7417)\r\n    * Added documentation for `ComposeSTRTableFile` (#7409)\r\n\r\n* **Funcotator**\r\n    * Fixed an issue where the `Match_Norm_Seq_Allele1` and `Match_Norm_Seq_Allele2` fields were not being populated in MAF output (#7422)\r\n\r\n* **Mitochondrial pipeline** \r\n    * Removed calls to `FilterNuMTs` and `FilterLowHetSites`, which are no longer being used (#7325)\r\n\r\n* **CNV Calling**\r\n    * Fixed a bug resulting from prefix strings of less than 3 characters when creating temporary files in `GermlineCNVCaller` and improved documentation of corresponding utility methods. (#7411)\r\n\r\n* **Documentation**\r\n    * Fixed an argument name typo in the `CombineGVCFs` docs (#7413)\r\n    * Fixed the wording of a comment in `MultiVariantDataSource` (#7388)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.2.2.0",
        "name": "4.2.2.0",
        "release_id": 48079787,
        "tag": "4.2.2.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.2.2.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/48079787",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/48079787",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.2.2.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2021-07-30T21:13:12Z",
        "date_published": "2021-07-30T21:27:16Z",
        "description": "**Download release:** [gatk-4.2.1.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.2.1.0/gatk-4.2.1.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.2.1.0 release:**\r\n--------------------------------------\r\n\r\n* Several important fixes to HaplotypeCaller and the new DRAGEN-GATK code introduced in GATK 4.2.0.0\r\n\r\n* Started laying the groundwork in `Mutect2` for `Mutect3`, which will be more machine learning focused\r\n\r\n* `LocalAssembler`: a new tool that performs local assembly of small regions to discover structural variants (#6989)\r\n\r\n* Support for multi-sample segmentation in `ModelSegments`\r\n\r\n* Major speed improvements and several important fixes to `Funcotator`\r\n\r\n* A new version of the Intel Genomics Kernel Library (GKL), with many important fixes and improvements\r\n\r\n* A new version of GenomicsDB, with improved cloud support\r\n\r\n* A GATK-wide option to shard VCFs on output, which is often useful for pipelining\r\n\r\n* GATK support for block compressed interval (`.bci`) files, which is useful when working with extremely large interval lists\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **New Tools**\r\n    * `LocalAssembler`: a new tool that performs local assembly of small regions to discover structural variants (#6989)\r\n\r\n* **HaplotypeCaller**\r\n    * Fixed a rare edge case in DRAGEN mode that could result in negative GQs when `USE_POSTERIOR_PROBABILITIES` is set (#7120) \r\n    * Fixed a rare edge case (mainly affecting DRAGEN mode) that could cause the PL arrays to be deleted when genotyping in `HaplotypeCaller` (#7148)\r\n    * Fixed a bug in the `AlleleLikelihoods` that could result in new evidence X being assigned arbitrary likelihoods left over from previous evidence (#7154)\r\n    * Fixed a \"Padded span must contain active span\" error caused by invalid feature file intervals that weren't being checked for validity against the sequence dictionary (#7295)\r\n    * Do not add the artificial haplotype read group to the bamout file when `--bam-writer-type NO_HAPLOTYPES` is specified (#7141)\r\n    * Suppressed excessive log output related to `JumboAnnotation` warnings in `HaplotypeCaller` (#7358)\r\n\r\n* **DRAGEN-GATK**\r\n    * `CalibrateDragstrModel`: fixed a sporadic out-of-memory error (#7212)\r\n    * `CalibrateDragstrModel`: fixed an \"IllegalArgumentException: Start cannot exceed end\" error (#7212)\r\n\r\n* **Mutect2**\r\n    * Added a training data mode (`--training-data-mode`) to `Mutect2` to prepare for `Mutect3` (#7109)\r\n        * Training data mode collects data on variant- and artifact-supporting read sets for fitting a deep learning filtering model\r\n    * Better error bars for samples with small contamination in `CalculateContamination` (#7003)\r\n    \r\n* **Funcotator**\r\n    * Greatly improved `Funcotator` performance by optimizing the VCF sanitization code (#7370)\r\n        * In our tests, this change appears to speed up the tool by roughly 2x\r\n    * Updated the Gencode GTF Codec to be more permissive with transcript and gene types (#7166)\r\n        * Now the Gencode GTF Codec no longer restricts `transcriptType` and `geneType` to a limited set of values. These fields are now each stored as a String. This allows for arbitrary values in these fields and will help to future-proof (and species-proof) the GTF parser.\r\n        * Fixes \"IndexFeatureFile Error to Run Funcotator with Mouse Ensembl GTF\" (#7054)\r\n    * Now can decode codons containing IUPAC bases into amino acids. (#7188)\r\n    * Updated the tool to allow for protein changes with N / IUPAC bases. (#6778)\r\n        * Added the ability to have IUPAC bases in either the ref/alt alleles OR in the reference when calculating the amino acid sequence. In this case, the code will no longer throw a user exception, but will log a warning and will produce ? amino acids in the case that they cannot be decoded from the amino acid table. Currently this will happen any time an N or IUPAC base is in the region to be coded into amino acids.\r\n        * Added AminoAcid.UNDECODABLE as a placeholder for any unknown / undecodable amino acid (such as in the case of an ambiguous IUPAC base).\r\n    * `Funcotator` now checks whether the input has already been annotated, and by default throws an error in that case. \r\n        * We also added a `--reannotate-vcf` override argument to explicitly allow reannotation (#7349)\r\n\r\n* **CNV Calling**\r\n    * Enabled multi-sample segmentation in `ModelSegments` (#6499)\r\n    * Removed mapping error rate from estimate of denoised copy ratios output by gCNV, and updated sklearn. (#7261)\r\n    * Moved gCNV sample QA check into the Postprocessing task in the WDL (#7150)\r\n    \r\n* **SV Calling**\r\n    * Added `LocalAssembler`, a new tool that performs local assembly of small regions to discover structural variants (#6989)\r\n\r\n* **The Genomics Kernel Library (GKL)**\r\n    * Updated to GKL version 0.8.8, and remove the FPGA PairHMM as an option (#7203)\r\n         * This is a significant update to the GKL that comes with many fixes and improvements:\r\n             * Update ISAL and OTC Zlib libraries to latest version (Q1 2021)\r\n             * Fixed 3 reproducible issues and retested out of 4 more in GKL\r\n             * Updated build for Centos 7 and Current Mac.\r\n             * Ran valgrind on limited C unit tests (passed)\r\n             * Major improvements to input validation\r\n             * Major updates to Error handling and propagation.\r\n             * Added Negative space unit testing coverage\r\n             * Regular Static Code Scanning\r\n             * Good overall quality of life improvement for the software\r\n             \r\n* **GenomicsDB**\r\n    * Moved to GenomicsDB 1.4.1, and add a toggle between the GCS Connector and native GCS support (#7224)\r\n        * This release allows for the direct use of the native GCS C++ client instead of the GCS Cloud Connector via HDFS. The GCS Cloud Connector can still be used with GenomicsDB via the ``--genomicsdb-use-gcs-hdfs-connector option`\r\n        * Using the native client with GCS allows for GenomicsDB to use the standard paradigms to help with authentication, retries with exponential backoff, configuring credentials, etc., and also helps with performance issues with GCS. See #7070.\r\n    * Allow specifying S3 and Azure blob storage uri's to GenomicsDB in addition to GCS and HDFS (#7271)\r\n    * Fixes related to the GenomicsDB upgrade (#7257)\r\n        * Fixed an issue where the combine operation for certain fields needs to take care to not remap missing fields to NON_REF\r\n        * Fixes \"Regression in GenomicsDBImport progress meter\" #7222\r\n        * Adds tests for \"GenomicsDBImport Creating Workspace Where REF is Inappropriately N?\" #7089\r\n    * Improved the error message in `GenomicsDBImport` when failing to open a `FeatureReader` (#7375)\r\n\r\n* **Mitochondrial pipeline** \r\n    * Added median coverage metric to the mitochondrial pipeline (#7253)\r\n    \r\n* **Notable Enhancements**\r\n    * Added a GATK-wide option (`--max-variants-per-shard`) to shard VCFs on output (#6959)\r\n        * Sharded output is often extremely useful for pipelining\r\n    * Added GATK support for block compressed interval (`.bci`) files (#7142)\r\n    * Added an `AlleleDepthPseudoCounts` (DD) genotype annotation. (#7303)\r\n        * Similar to AD, the new annotation (DD) captures the depth of each allele's supporting evidence or reads, however it does so by following a variational Bayes approach looking into the likelihoods rather than applying a fixed threshold. This turns out to be more robust in some instances.\r\n        * To get the new non-standard annotation in `HaplotypeCaller` you need to add `-A AllelePseudoDepth`\r\n    * We now track the source of variants in `MultiVariantWalkers`, which is important for some tools such as `VariantEval` (#7219)\r\n\r\n* **Bug Fixes**\r\n    * Fixed key ordering bugs in the implementations of `Histogram.median()` and `CompressedDataList.iterator()` (#7131)\r\n         * These bugs could result in incorrect RankSumTest annotations in some cases\r\n    * Fixed the `DepthPerSampleHC` and `StrandBiasBySample` annotations to not spam the logs with \"Annotation will not be calculated\" warnings (#7357)\r\n    * `VariantEval`: fixed contig stratification to defer to user-defined intervals (#7238)\r\n\r\n* **Miscellaneous Changes**\r\n    * The `ProgressMeter` can now be completely disabled for all tools / traversals by overriding `GATKTool.disableProgressMeter()` (#7354)\r\n    * We now authenticate with Dockerhub in our Travis builds, to help avoid tests failing due to quota issues (#7204) (#7256)\r\n    * Migrated `VariantEval` to be a `MultiVariantWalkerGroupedOnStart` (#6973)\r\n    * `VariantEval`: added an argument to specify the `PedigreeValidationType` (#7240)\r\n    * Converted `InfoFieldAnnotation`/`GenotypeAnnotation` into interfaces. (#7041)\r\n    * Allow `MultiVariantWalkerGroupedOnStart` subclasses to view/set `ignoreIntervalsOutsideStart` (#7301)\r\n    * `PedigreeAnnotation`: consolidate code, provide getters, and allow `PedigreeValidationType` to be set (#7277)\r\n    * `ASEReadCounter`: added a warning for variants lacking GT fields (#7326)\r\n    * Added filters to `dockstore.yml` so that only the master branch and the releases get synced to Dockstore (#7217)\r\n    * Fixed a compatibility issue between Java 11 and `log4j2` (#7339)\r\n    * We now update the gcloud package signing key at the start of every docker build (#7180)\r\n    * Updated our Artifactory key (#7208)\r\n    * Disabled some Spark dataproc tests because of dependency issues. (#7170)\r\n    * Removed some embedded licenses from scripts (#7340)\r\n \r\n* **Documentation**\r\n    * Variant annotation documentation: removed broken links to related annotations from the tool docs (#7307)\r\n    * Updated the link to an article on Jexl expressions (#7317)\r\n    * Fixed several broken links in docs for the CNV tools (#7309)\r\n    * Fixed broken links in the docs for `Funcotator`, `VariantRecalbrator`, and `ASEReadCounter` (#7270)\r\n    * Fixed typos in the tool documentation for `HaplotypeCaller` and `LeftAlignAndTrimVariants` (#6440)\r\n    * Clarify pipeline inputs in documentation for `GnarlyGenotyper` (#7231)\r\n    \r\n* **Dependencies**\r\n    * Updated `HTSJDK` to version `2.24.1` (#7149)\r\n    * Updated `Picard` to version `2.25.4` (#7255)\r\n    * Updated `GenomicsDB` to version `1.4.1` (#7224)\r\n    * Updated the `Genomics Kernel Library (GKL)` to version `0.8.8` (#7203)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.2.1.0",
        "name": "4.2.1.0",
        "release_id": 47074264,
        "tag": "4.2.1.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.2.1.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/47074264",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/47074264",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.2.1.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2021-02-19T21:07:43Z",
        "date_published": "2021-02-19T21:26:49Z",
        "description": "**Download release:** [gatk-4.2.0.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.2.0.0/gatk-4.2.0.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.2.0.0 release:**\r\n--------------------------------------\r\n\r\n* We've worked closely with Illumina to port a number of significant innovations for germline short variant calling from their DRAGEN pipeline to GATK. These improvements will form the basis of the upcoming open-source implementation of the DRAGEN pipeline which we're calling [DRAGEN-GATK](https://gatk.broadinstitute.org/hc/en-us/articles/360039984151-DRAGEN-GATK-Update-Let-s-get-more-specific)\r\n\r\n* A number of other fixes and improvements to `HaplotypeCaller` to improve the phasing of variant calls and to fix edge cases with indels and spanning deletions\r\n\r\n* A new pipeline for gCNV exome joint calling\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **DRAGEN-GATK** (#6634) (#7063)\r\n    * With this release we've worked closely with Illumina to make improvements to the GATK `HaplotypeCaller` to allow it to output germline short variant calls that are functionally equivalent to the calls made by their DRAGEN 3.4.12 pipeline. See [our blog post on DRAGEN-GATK](https://gatk.broadinstitute.org/hc/en-us/articles/360039984151-DRAGEN-GATK-Update-Let-s-get-more-specific) for more details on these improvements. A full `DRAGEN-GATK` pipeline that leverages these new features will be released in the near future as a WDL workflow script in the [WARP](https://github.com/broadinstitute/warp) repo on GitHub as well as a featured workspace in [Terra](https://terra.bio/). \r\n    * Below is a summary of the improvements we've ported from DRAGEN in this release. We recommend that most users wait until the complete `DRAGEN-GATK` pipeline is released as a WDL workflow before evaluating these features, though advanced users comfortable with building their own pipelines are welcome to try them out now:\r\n        * **DragSTR**: a port of DRAGEN's model for STRs (Short Tandem Repeats) that adjusts HMM indel priors based on empirical reference contexts for better indel calling.\r\n            * Using DragSTR involves running two new tools prior to the `HaplotypeCaller`:    \r\n                * `ComposeSTRTableFile`: scans a reference for STR sites and outputs a table file with a subsample of the available STR sites across the genome.     \r\n                * `CalibrateDragstrModel`: given the STR table for a reference produced by `ComposeSTRTableFile` and the reads for a specific sample, generates a model for potential sequencing errors for STR sites of various sizes for that sample.\r\n            * After running these tools, you then run `HaplotypeCaller` with the **`--dragstr-params-path`** argument to pass it the DragSTR model generated by `CalibrateDragstrModel`.\r\n        * **BQD (Base Quality Dropout)** and **FRD (Foreign Read Detection)**: two new genotyper error models ported from DRAGEN\r\n            * The `Base Quality Dropout (BQD)` model penalizes variants with low average base quality scores and high average sequencing cycle counts among genotyped reads and reads that were otherwise excluded from the genotyper to model read-context dependent sequencing errors.\r\n            * The `Foreign Read Detection (FRD)` model uses an adjusted mapping quality score as well as read strandedness information to penalize reads that are likely to have originated from somewhere else on the genome or from contamination.\r\n            * To activate the BQD and FRD models, run `HaplotypeCaller` with the **`--dragen-mode`** argument.\r\n        * Added a new variant QUAL score model that reports the variant QUAL score as the posterior of the reference genotype based on the sample-dependent DRAGEN STR and flat SNP priors.\r\n\r\n* **HaplotypeCaller**\r\n    * We now add physical phasing information (PGT/PID/PS attributes) to genotypes with spanning deletion alleles (#6937)\r\n    * Fixed two phasing bugs (#7019)\r\n        * Fixed \"HaplotypeCaller emitting incorrect phasing when genotyping hom-het-het\" (https://github.com/broadinstitute/gatk/issues/6463)\r\n        * Fixed \"Phased variants do not have the same phase set identifier\" (https://github.com/broadinstitute/gatk/issues/6845)\r\n    * Fixed quality score calculation for sites with spanning deletions (#6859)\r\n        * This fixes a bug in the AlleleFrequencyCalculator that was causing quality to be overestimated for sites with * alleles representing spanning deletions.\r\n    * Added the ability for indels to be recovered from dangling heads in the assembly graph, and a new `--num-matching-bases-in-dangling-end-to-recover` argument for filtering dangling ends (#6113) (#7086)\r\n    * Improved handling of indels/spanning deletions in the cigar base quality adjustment code. (#6886)\r\n        * This aims to better handle the edge cases that come up when mates have mismatching numbers of bases at the start or end of the reads relative to each-other.  \r\n    * Fixed a bug where overlapping reads in subsequent assembly regions could have invalid base qualities (#6943)\r\n    * Convert non-ACGT IUPAC bases to N in HaplotypeCaller prior to assembly to prevent a crash (#6868)\r\n    * Renamed the `--mapping-quality-threshold` argument to `--mapping-quality-threshold-for-genotyping`, and updated its documentation to be less confusing (#7036)\r\n    * Added an option for `HaplotypeCaller` and `Mutect2` to produce a bamout without artificial haplotypes (#6991)\r\n    * Updated the `--debug-graph-transformations` argument to emit the assembly graph both before and after chain pruning (#7049)\r\n\r\n* **Mutect2** \r\n    * Fixed the `--dont-use-soft-clipped-bases` argument in `Mutect2` to actually work as intended (#6823)\r\n        * Due to a bug, this option did nothing because a copy of the original reads was modified. By deleting the unnecessary mapping quality filtering (this is totally redundant with the M2 read filter), we finalize (and thereby discard soft clips if requested) an assembly region made from the original reads, not a copy.\r\n    * Fixed a bug in the `Mutect2` engine active region code that could affect the ability to call tumor alts when the normal has a different alt at the same site (#6908)\r\n    * Removed an obsolete cram to bam conversion step in the `Mutect2` WDL (#6970)\r\n    * Updated the `Mutect2` whitepaper in `docs/mutect/mutect.pdf` to accurately reflect current filter names, and updated the section on `FilterAlignmentArtifacts` (#6967)\r\n\r\n* **CNV Calling**\r\n    * A new pipeline for gCNV exome joint calling (#6554)\r\n        * Added a new tool (`JointGermlineCNVSegmentation`) and associated workflow (`scripts/cnv_wdl/germline/joint_call_exome_cnvs.wdl`) to combine gCNV segments and calls across samples\r\n        * `JointGermlineCNVSegmentation` segments and genotypes CNV calls from the germline CNV pipeline jointly across multiple samples.  \r\n        * The workflow in `scripts/cnv_wdl/germline/joint_call_exome_cnvs.wdl` produces a joint, multi-sample genotyped VCF.  \r\n        * For whole genomes, we recommend CNVs as part of a full SV callset with https://github.com/broadinstitute/gatk-sv (soon to be added to Terra)\r\n    * `GermlineCNVCaller` now restarts inference once with a new random seed when inference diverges. Also added a new entry point to PythonScriptExecutor that returnes ProcessOutput. (#6866)\r\n        * This is intended to alleviate transient issues with GermlineCNVCaller inference in which the ELBO converges to a NaN value, by calling the python gCNV code with an updated random seed input.\r\n    * `CreateReadCountPanelOfNormals`: fixed a bug in the logic for filtering zero-coverage samples and intervals (#6624)\r\n    * `FilterIntervals`: fixed a bug in the tool logic when filtering on annotations and -XL is used to exclude intervals (#7046)\r\n\r\n* **SV Calling**\r\n    * `PrintSVEvidence`: a new tool that prints any of the Structural Variation evidence file types: read count (RD), discordant pair (PE), split-read (SR), or B-allele frequency (BAF) (#7026)\r\n        * This tool is used frequently in the GATK-SV pipeline for retrieving subsets of evidence records from a bucket over specific intervals. Evidence file formats comply with the current specifications in the existing GATK-SV pipeline.\r\n\r\n* **GenomicsDB**\r\n    * Introduced a new feature for `GenomicsDBImport` that allows merging multiple contigs into fewer GenomicsDB partitions (#6681)\r\n        * Controlled via the new `--merge-contigs-into-num-partitions` argument to `GenomicsDBImport` \r\n        * This should produce a huge performance boost in cases where users have a very large number of contigs. Prior to this change, GenomicsDB would create a separate folder/partition for each contig, which slowed down import to a crawl when there were many contigs.\r\n        \r\n* **Funcotator**\r\n    * Added sorting by strand order for transcript subcomponents (#7065)\r\n        * This fixes an issue where the coding sequence, protein prediction, and other annotations could be incorrect for the hg19 version of Gencode, due to the individual elements of each transcript appearing in numerical order, rather than the order in which they appear in the transcript at transcription time.\r\n    * Updated the Funcotator tutorial link in the tool documentation. (#6920) (#6925)\r\n\r\n* **Mitochondrial pipeline** \r\n    * Simplified the max_reads_per_alignment_start argument in mitochondria_m2_wdl/AlignAndCall.wdl (#6904)\r\n    * Remove the unused \"autosomal_coverage\" parameter from the Filter task in mitochondria_m2_wdl/AlignAndCall.wdl (#6888)\r\n\r\n* **Notable Enhancements**\r\n    * Add a `-O` option to save the output to a file in the following tools: `FlagStat`, `CountBases`, `CountReads`, `CountVariants`, and `CountBasesInReference` (#7072)\r\n    * `DepthOfCoverage`: added a new gene_statistics output file (#7025)\r\n    * `ReblockGVCF`: allow reblocking with no PLs (#6757)\r\n\r\n* **Bug Fixes**\r\n    * Fixed a `ClosedChannelException` error when doing multiple queries on remote CRAM files, and added a test to verify proper stream management (#7066)\r\n    * `SelectVariants`: Fixed an issue where SelectVariants could generate duplicate VCF header lines in some circumstances, resulting in an invalid VCF (#7069)\r\n    * `VariantAnnotator`: fixed a NullPointerException by adding a validation check that all samples in the input bam are present in the provided vcf before running (#6944)\r\n    * `SplitNCigarReads`: fixed an error where the read mate key was not sufficiently strict about read names, causing cigar errors (#6909)\r\n    * `CalculateGenotypePosteriors`: ensure that resources have the same sequence dictionary as the input VCF (#6430)\r\n    * `MarkDuplicatesSpark`: fixed a NullPointerException when a null ReadNameRegex was provided (#7002)\r\n    * `GnarlyGenotyper`: bugfix for the QUALapprox calculation, tolerate missing VarDP, and support AS_QUALapprox if QUALapprox is missing (#7061)\r\n    * Fixed the GATK version number in the docker image when doing releases to not end in \"-SNAPSHOT\" (#6883)\r\n\r\n* **Miscellaneous Changes**\r\n    * Switched GATK to the Apache 2.0 license (#7079)\r\n    * We now print the current Spark version on GATK startup (#7028)\r\n    * Added a log warning message when the total size of the PL arrays for a variant will likely exceed 100,000 (#6334)\r\n    * Added a script to publish GATK tool WDLs for each release (#6980)\r\n    * Migrated the `GATKPath` base class to `HtsPath` (#6763)\r\n    * Migrate additional tools to `GATKPath` (#6718)\r\n    * Made `BaseUtils.convertIUPACtoN()` and `BaseUtils.simpleBaseToBaseIndex()` methods more robust to handle all possible byte values (#7010)\r\n    * Enabled CARROT integration for triggering test runs from PR comments (#6917) (#6986)\r\n    * Added loci information to several annotation warnings (#6891)\r\n    * `VariantRecalibrator`: added locus information to a ref allele mismatch error message (#6964)\r\n    * `ReferenceConfidenceVariantContextMerger`: corrected AS annotation warning message to use GATK4 annotation names (#6985)\r\n    * Made the `CNNScoreVariants` task in `cnn_variant_wdl/cnn_variant_common_tasks.wdl` robust to the reads and index being in different locations. (#6900)\r\n    * Updated gcloud docker commands in `build_docker.sh` (#7078)\r\n    * Added version number to the dockstore yml file (#6905)\r\n    * Switched travis gcloud installation to use noninteractive mode (#6974)\r\n    * Deleted the obsolete tool `FixCallSetSampleOrdering` (#7022)\r\n    * Echo the log file after a failed travis run. (#7020)\r\n    * Temporarily disable the PairHMMUnitTest on Java 11. (#7044)\r\n    * Pin our h5py version to 2.10.0. (#6955)\r\n  \r\n* **Documentation**\r\n    * Added a link to the new `gatk-tool-wdls` repository to the README (#6982)\r\n    * Updated JEXL documentation website link in `SelectVariants` and `VariantFiltration` (#7029)\r\n    * Updated the `ApplyVQSR` docs to consistently use the GATK4 tool name: ApplyRecalibration -> ApplyVQSR\r\n    * Modified the README to reflect the current download size for Git LFS files (#6933)\r\n    * Fixed a typo in the conda environment YML documentation. (#6935)\r\n    * Removed reference to -Dtest.single from the README (#6914)\r\n    * Fixed a typo in a javadoc comment in `HaplotypeCallerEngine` (#7033)\r\n\r\n* **Dependencies**\r\n    * Updated HTSJDK to 2.24.0 (#7073)\r\n    * Updated Picard to 2.25.0 (#7075)\r\n    ",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.2.0.0",
        "name": "4.2.0.0",
        "release_id": 38337224,
        "tag": "4.2.0.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.2.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/38337224",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/38337224",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.2.0.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2020-10-09T22:23:51Z",
        "date_published": "2020-10-09T22:35:14Z",
        "description": "**Download release:** [gatk-4.1.9.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.1.9.0/gatk-4.1.9.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.1.9.0 release:**\r\n--------------------------------------\r\n\r\n* A major update to `Funcotator`, bringing in the latest Gencode release, fixing compatibility issues with dbSNP, and more!\r\n\r\n* Two new tools, `GeneExpressionEvaluation` and `ReferenceBlockConcordance`\r\n\r\n* Significant performance improvements to `DepthOfCoverage` and `SelectVariants`\r\n\r\n* Some important bug fixes:\r\n    * Fixed a bug in `HaplotypeCaller` and `Mutect2` where we were losing insertion events that immediately followed a deletion\r\n    * A fix for the \"CreateSomaticPanelOfNormals output PoN has much less variants in 4.1.8.0 than before\" issue reported in https://github.com/broadinstitute/gatk/issues/6744\r\n    * A fix for a frequently-encountered `NullPointerException` in the `AS_StrandBiasTest` annotation when running `CombineGVCFs` reported in https://github.com/broadinstitute/gatk/issues/6766 \r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **New Tools**\r\n    * `GeneExpressionEvaluation`: a tool for evaluating gene expression from RNA-seq reads aligned to whole genome (#6602)\r\n        * This tool counts fragments to evaluate gene expression from RNA-seq reads aligned to the genome.  Features to evaluate expression over are defined in an input annotation file in gff3 fomat. Output is a tsv listing sense and antisense expression for all stranded grouping features, and expression (labeled as sense) for all unstranded grouping features.\r\n \r\n    * `ReferenceBlockConcordance`: a new tool to evaluate concordance of reference blocks in GVCF files (#6802)\r\n        * This tool compares the reference blocks of two GVCF files against each other and produces three histograms:\r\n            * *Truth block histogram*: Indicates the number of occurrences of reference blocks with a given confidence score and length in the truth GVCF\r\n            * *Eval block histogram*: Indicates the number of occurrences of reference blocks with a given confidence score and length in the eval GVCF\r\n            * *Confidence concordance histogram*: Reflects the confidence scores of bases in reference blocks in the truth and eval VCF, respectively. An entry of 10 at bin \"80,90\" means that there are 10 bases which simultaneously have a reference confidence of 80 in the truth GVCF and a reference confidence of 90 in the eval GVCF.\r\n    \r\n* **HaplotypeCaller/Mutect2**\r\n    * Fixed a bug in `HaplotypeCaller` and `Mutect2` where we were losing insertion events that immediately followed a deletion (#6696)\r\n    * Added a workaround for an issue with multiallelics in the `CreateSomaticPanelOfNormals` pipeline (#6871)\r\n        * This fixes the \"CreateSomaticPanelOfNormals output PoN has much less variants in 4.1.8.0 than before\" issue reported in https://github.com/broadinstitute/gatk/issues/6744\r\n    * Made improvements to the `Mutect2` active region detection code that resulted in recovering some low-AF calls that we were missing (#6821)\r\n    * Made the `HaplotypeCaller`/`Mutect2` adaptive pruner smarter in complex graphs, resulting in modest improvements to indel sensitivity when using the adaptive pruning option (#6520)    \r\n    * Fixed a bug in variation event detection code that could sometimes lead to mistreating indel assembly windows as SNP assembly windows (#6661)\r\n    * Fixed a bug in `FragmentUtils` where insertion quals were used instead of deletion quals when adjusting base qualities for two overlapping reads from the same fragment (#6815)\r\n    * Fixed a concurrent modification exception error for local runs of `HaplotypeCallerSpark` (#6741)\r\n    * Marked the `--linked-de-bruijn-graph` argument as Advanced rather than Hidden (#6737)\r\n    * Made a small tweak to `Mutect2`'s callable sites count (#6791)\r\n    * Added a \"requester pays\" option to `Mutect2` WDL tasks that access bams for use with Google Cloud \"requester pays\" buckets (#6879)\r\n\r\n* **Funcotator**\r\n    * A major set of updates to `Funcotator` (#6660)\r\n        * Updated to the latest Gencode release\r\n        * Fixed the contig naming compatibility issue with dbSNP reported in https://github.com/broadinstitute/gatk/issues/6564 (\"hg38 dbSNP has incorrect contig names\")\r\n        * Now both hg19 and hg38 have the contig names translated to \"chr__\"\r\n        * Added 'lncRNA' to GeneTranscriptType.\r\n        * Added \"TAGENE\" gene tag.\r\n        * Added the MANE_SELECT tag to FeatureTag.\r\n        * Added the STOP_CODON_READTHROUGH tag to FeatureTag.\r\n        * Updated the GTF versions that are parseable.\r\n        * Fixed a parsing error with new versions of gencode and the remap positions (for liftover files).\r\n        * Added test for indexing new lifted over gencode GTF.\r\n        * Added Gencode_34 entries to MAF output map.\r\n        * Pointed data source downloader at new data sources URL.\r\n        * Minor updates to workflows to point at new data sources.\r\n        * Updated retrieval scripts for dbSNP and Gencode.\r\n        * Added required field to gencode config file generation.\r\n        * Now gencode retrieval script enforces double hash comments at top of gencode GTF files.\r\n        * Fixed an erroneous trailing tab in MAF file output reported in https://github.com/broadinstitute/gatk/issues/6693    \r\n    * Added a maximum version number for data sources in `Funcotator` (#6807)\r\n    * Added a \"requester pays\" option to the `Funcotator` WDL for use with Google Cloud \"requester pays\" buckets (#6874)\r\n    * `FuncotateSegments`: fixed an issue with the default value of --alias-to-key-mapping being set to an immutable value (#6700)\r\n\r\n* **GenomicsDB**\r\n    * Updated to GenomicsDB Version 1.3.2, which brings better propagation of errors messages from the GenomicsDB library (#6852)\r\n        * Using the GATK option GATK_STACKTRACE_ON_USER_EXCEPTION will now also output a limited C/C++ stacktrace\r\n        \r\n* **CNV Tools**\r\n    * Fixed a bug in the `KernelSegmenter`: the minimal data to calculate the segmentation cost should be `2 * windowSize`, rather than `windowSize` (#6835)\r\n    * Germline CNV WDL improvements for WGS (#6607)\r\n        * Modified gCNV WDLs to improve Cromwell performance when running on a large number of intervals, as in WGS\r\n        * Added optional disabled_read_filters input to CollectCounts\r\n        * Enabled GCS streaming for CollectCounts and CollectAllelicCounts\r\n    * Added a \"requester pays\" option to the germline and somatic CNV WDLs for use with Google Cloud \"requester pays\" buckets (#6870)\r\n\r\n* **Mitochondrial Pipeline**\r\n    * Fix to correctly handle spaces in sample names in the Mitochondria WDL (#6773)\r\n    * Exposed a `max_reads_per_alignment_start` argument in the Mitochondria WDL (#6739)\r\n    * Updated the `HaploChecker` Dockerfile to reflect the correct haplocheck CLI (#6867)\r\n\r\n* **Notable Enhancements**\r\n    * Significantly improved the performance of `DepthOfCoverage` by removing slow string formatting calls (#6740)\r\n        * In a test run with default arguments locally the runtime for a WGS full chr15 drops from ~8.9 minutes to ~4.7 minutes after this patch\r\n    * Significantly improved the performance of `SelectVariants` with large numbers of samples by changing an operation to scale linearly instead of quadratically with the number of samples (#6729)\r\n        * On one example with several thousand samples there was a speed up from ~5 minutes to 0.1 minutes\r\n    * WDL generation: made several improvements to automatic WDL generation, annotated additional tools for WDL generation, and added a section to the README with instructions on generating WDLs for GATK tools (#6800)     \r\n    * Added a suite of utility methods for working with Google BigQuery: `BigQueryUtils` (#6759) (#6861)    \r\n    * The GATK docker image can now be built with a simple `docker build .` command (no extra arguments needed) (#6764) (#6842) (#6782)\r\n    * Added a Dockstore yml file with workflow descriptions for the WDLs in the GATK repo, to facilitate automatic publication to Dockstore (#6770)\r\n\r\n* **Bug Fixes**\r\n    * Fixed a `NullPointerException` in the `AS_StrandBiasTest` annotation reported in https://github.com/broadinstitute/gatk/issues/6766 (#6847)\r\n    * Fixed a bug with soft clips in `LeftAlignIndels` (#6792)\r\n    * `VariantRecalibrator`: uniquify annotations to fix the error reported in https://github.com/broadinstitute/gatk/issues/2221 (#6723)\r\n    * Fixed an issue where `ContextCovariate` in `BaseRecalibrator` mistakenly assumed that all non-ACGT bases in the read are N (#6625)\r\n    * Fixed a crash in `CountBasesSpark` when using the `-L` option (#6767)\r\n\r\n* **Miscellaneous Changes**\r\n    * Significant refactoring of the SV discovery classes (#6652)\r\n    * `FilterVariantTranches`: report more info when the ref alleles don't match (#6723)  \r\n    * We now report the target url in exceptions thrown by `HtsgetReader` (#6799)\r\n    * Added more information to error messages in `AssemblyRegion` for contigs not in the reference dictionary (#6781)\r\n    * Improved an error message in `GATKRead.setMatePosition()` (#6779)\r\n    * Updated the Barclay WDL template for compatibility with the Debian distribution (#6841)\r\n    * Temporarily disabled `HtsgetReader` tests to work around issues caused by a server-side upgrade. (#6804)\r\n    * Re-enabled an `IndexFeatureFile` test for uncompressed BCF. (#6716)\r\n\r\n* **Documentation**\r\n    * Marked `LearnReadOrientationModel` as a `DocumentedFeature` (#6726)\r\n    * Added a gentle warning about loss of True Positives with the default `FilterIntervals` params (#6751)\r\n    * Updated the README to mention that the conda environment is not officially supported on macOS at this time. (#6788)\r\n    * Fixed a typo in the example command for `SplitIntervals` (#6869)\r\n    * Fixed a typo in the `--tmp-dir` argument in the `GenomicsDBImport` docs (#6785)\r\n    * Fixed a typo in the `--tmp-dir` argument in the `GenotypeGVCFs` docs (#6784)\r\n    * Removed outdated argument references from the `DepthOfCoverage` documentation. (#6810)\r\n    * Fixed a typo with \"-genelist\" argument to \"-gene-list\" in the `DepthOfCoverage` documentation. (#6880)\r\n    * Fixed a typo in the docs for the `Mutect2` --pcr-indel-qual argument (#6840)\r\n\r\n* **Dependencies**\r\n    * Upgraded `Picard` to 2.23.3 (#6717)\r\n    * Upgraded `Barclay` to 4.0.1. (#6864)\r\n    * Updated `GenomicsDB` to 1.3.2 (#6852)\r\n    * Added a new dependency on `Google BigQuery` 1.117.1 (#6759)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.1.9.0",
        "name": "4.1.9.0",
        "release_id": 32392743,
        "tag": "4.1.9.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.1.9.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/32392743",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/32392743",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.1.9.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2020-07-20T21:04:51Z",
        "date_published": "2020-07-20T21:14:47Z",
        "description": "**Download release:** [gatk-4.1.8.1.zip](https://github.com/broadinstitute/gatk/releases/download/4.1.8.1/gatk-4.1.8.1.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.1.8.1 release:**\r\n--------------------------------------\r\n\r\n* This is a minor point release intended primarily to push out a needed enhancement to the `Mutect2` pipeline.\r\n\r\n* This release also introduces a new framework for the auto-generation of WDLs for GATK/Picard tools. Over the next several GATK releases, we intend to hook GATK/Picard tools up to the new WDL generator, with the ultimate goal of having WDLs automatically published for all tools with each release.\r\n        \r\n**Full list of changes:**\r\n-------------------------\r\n    \r\n* **Mutect2**\r\n    * We now allow for the passing of additional arguments to `GetPileupSummaries` from the `Mutect2` WDL (#6713)\r\n\r\n* **GATK Engine**\r\n    * Added a new framework for the auto-generation of WDLs for GATK/Picard tools (#6504)\r\n        * Over the next several GATK releases, we intend to hook GATK/Picard tools up to the new WDL generator, with the ultimate goal of having WDLs automatically published for all tools with each release\r\n        \r\n* **Bug Fixes**\r\n    * Fixed an error (reported in https://github.com/broadinstitute/gatk/issues/6664) when trying to read `.vcf`/`.tbi` files located in a path that contains spaces in the name (#6702)\r\n    \r\n* **Miscellaneous Changes**\r\n    * Removed a few GATK classes that are redundant with Picard classes. (#6678)\r\n\r\n* **Documentation**\r\n    * Added instructions for running Spark tools in LOCAL mode to the README (#6682)\r\n    * Removed documentation reference to a GATK 3.x annotation that no longer exists (#6679)\r\n\r\n* **Dependencies**\r\n    * Updated HTSJDK to `2.23.0` (#6702)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.1.8.1",
        "name": "4.1.8.1",
        "release_id": 28756749,
        "tag": "4.1.8.1",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.1.8.1",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/28756749",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/28756749",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.1.8.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2020-06-26T19:05:41Z",
        "date_published": "2020-06-26T19:16:50Z",
        "description": "**Download release:** [gatk-4.1.8.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.1.8.0/gatk-4.1.8.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.1.8.0 release:**\r\n--------------------------------------\r\n\r\n* A major new release of `GenomicsDB` (1.3.0), with enhanced support for shared filesystems such as NFS and Lustre, support for MNVs, and better compression leading to a roughly 50% reduction in workspace size in our tests. This also includes a fix for an error in `GenotypeGVCFs` that several users were encountering when reading from GenomicsDB.\r\n\r\n* A major overhaul of the `PathSeq` microbial detection pipeline containing many improvements\r\n\r\n* Initial/prototype support for reading from HTSGET services in GATK\r\n    * *Over the next several releases, we intend for HTSGET support to propagate to more tools in the GATK*\r\n\r\n* Fixes for a couple of frequently-reported errors in `HaplotypeCaller` and `Mutect2`  (https://github.com/broadinstitute/gatk/issues/6586 and https://github.com/broadinstitute/gatk/issues/6516) \r\n\r\n* Significant updates to our Python/R library dependencies and Docker image\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **New Tools**\r\n    * `HtsgetReader`: an experimental tool to localize files from an HTSGET service (#6611)\r\n        * *Over the next several releases, we intend for HTSGET support to propagate to more tools in the GATK*   \r\n    * `ReadAnonymizer`: a tool to anonymize reads with information from the reference (#6653)  \r\n        * This tool is useful in the case where you want to use data for analysis, but cannot publish the data without anonymizing the sequence information.  \r\n           \r\n* **HaplotypeCaller/Mutect2**\r\n    * Fixed an \"evidence provided is not in sample\" error in `HaplotypeCaller` when performing contamination downsampling (#6593)\r\n        * This fixes the issue reported in https://github.com/broadinstitute/gatk/issues/6586 \r\n    * Fixed a \"String index out of range\" error in the `TandemRepeat` annotation with `HaplotypeCaller` and `Mutect2` (#6583)\r\n        * This addresses an edge case reported in https://github.com/broadinstitute/gatk/issues/6516 where an alt haplotype starts with an indel, and hence the variant start is one base before the assembly region due to padding a leading matching base\r\n    * Better documentation for `FilterAlignmentArtifacts` (#6638)\r\n    * Updated the `CreateSomaticPanelOfNormals` documentation (#6584)\r\n    * Improved the tests for `NuMTFilterTool` (#6569)\r\n\r\n* **PathSeq**\r\n    * Major overhaul of the PathSeq WDLs (#6536)\r\n        * This new PathSeq WDL redesigns the workflow for improved performance in the cloud. \r\n        * Downsampling can be applied to BAMs with high microbial content (ie >10M reads) that normally cause performance issues.\r\n        * Removed microbial fasta input, as only the sequence dictionary is needed.\r\n        * Broke pipeline down to into smaller tasks. This helps reduce costs by a) provisioning fewer resources at the filter and score phases of the pipeline and b) reducing job wall time to minimize the likelihood of VM preemption.\r\n        * Filter-only option, which can be used to cheaply estimate the number of microbial reads in the sample.\r\n        * Metrics are now parsed so they can be fed as output to the Terra data model.\r\n        * CRAM-to-BAM capability\r\n        * Updated WDL readme\r\n        * Deleted unneeded WDL json configuration, as the configuration can be provided in Terra\r\n    * Added an `--ignore-alignment-contigs` argument to `PathSeq` filtering that lets users specify any contigs that should be ignored. (#6537)\r\n        * *This is useful for BAMs aligned to hg38, which contains the Epstein-Barr virus (chrEBV)*\r\n\r\n* **GenomicsDB**\r\n    * Upgraded to `GenomicsDB` version 1.3.0 (#6654)\r\n        * Added a new argument `--genomicsdb-shared-posixfs-optimizations` to help with shared POSIX filesystems like NFS and Lustre. This turns on disable file locking and for GenomicsDB import it minimizes writes to disks. The performance on some of the gatk datasets for the import of about 10 samples went from 23.72m to 6.34m on NFS which was comparable to importing to a local filesystem. Hopefully this helps with Issue #6487 and #6627. Also, fixes Issue #6519.\r\n        * This version of GenomicsDB also uses pre-compression filters for offset and compression files for new workspaces and genomicsdb arrays. The total sizes for a GenomicsDB workspace using the same dataset as above and the 10 samples went from 313MB to 170MB with no change in import and query times. Smaller GenomicsDB arrays also help with performance on distributed and cloud file systems.\r\n        * This version has added support to handle MNVs similar to deletions as described in Issue #6500.\r\n        * There is added support in `GenomicsDBImport` to have multiple contigs in the same GenomicsDB partition/array. This will hopefully help import times in cases where users have many thousands of contigs. Changes are still needed from the GATK side to make use of this support.\r\n        * Logging has been improved somewhat with the native C/C++ code using spdlog and fmt and the Java layer using apache log4j and log4j.properties provided by the application. Also, info messages like No valid combination operation found for INFO field AA - the field will NOT be part of INFO fields in the generated VCF records will only be output once for the operation.\r\n    * Made `VCFCodec` the default for query streams from `GenomicsDB` (#6675)\r\n        * This fixes the frequently-reported `NullPointerException` in `GenotypeGVCFs` when reading from GenomicsDB (see https://github.com/broadinstitute/gatk/issues/6667)\r\n        * Added a `--genomicsdb-use-bcf-codec` argument to opt back in to using the BCFCodec, which is faster but prone to the above error on certain datasets\r\n\r\n* **CNV Tools**\r\n    * `DetermineGermlineContigPloidy` can now process interval lists with a single contig (#6613)\r\n    * `FilterIntervals` now filters out any singleton intervals (#6559)\r\n    * Fixed an inaccurate error message in `SVDDenoisingUtils` (#6608)\r\n\r\n* **Docker/Conda Overhaul** (#5026)\r\n    * Our docker image is now built off of Ubuntu 18.04 instead of 16.04\r\n        * This brings in newer versions of several important packages such as `samtools`\r\n    * Updated many of the Python libraries installed via our conda environment and included in our Docker image to newer versions, resolving several outstanding issues in the process\r\n    * R dependencies are now installed via conda in our Docker build instead of the now-removed `install_R_packages.R` script\r\n        * Due to this change, we recommend that tools that use R packages (e.g., to create plots) should now be run using the GATK docker image or the conda environment.\r\n    * *NOTE:* significant updates and changes to the Ubuntu version, native packages, and R/python packages may result in corresponding numerical changes in results.\r\n\r\n* **Mitochondrial Pipeline**\r\n    * Minor updates to the mitochondrial pipeline WDLs (#6597)\r\n\r\n* **Notable Enhancements**\r\n    * `RevertSamSpark` now supports CRAMs (#6641)\r\n    * Fixed a `VariantAnnotator` performance issue that could cause the tool to run very slowly on certain inputs (#6672)\r\n    * More flexible matching of dbSNP variants during variant annotation (#6626)\r\n        * Add all dbsnp id's which match a particular variant to the variant's id, instead of just the first one found in the dbsnp vcf.\r\n        * Be less brittle to variant normalization issues, and match differing variant representations of the same underlying variant. This is implemented by splitting and trimming multiallelics before checking for a match, which I suspect are the predominant cause of these types of matching failures.\r\n    * Added a `--min-num-bases-for-segment-funcotation` argument to `FuncotateSegments` (#6577)\r\n        * This will allow for segments of length less than 150 bases to be annotated if given at run time (defaults to 150 bases to preserve the previous behavior).\r\n    * `SplitIntervals` can now handle more than 10,000 shards (#6587)\r\n\r\n* **Bug Fixes**\r\n    * Fixed interval summary files being empty in `DepthOfCoverage` (#6609)\r\n    * Fixed a crash in the BQSR R script with newer versions of R (#6677)\r\n    * Fix crash when reporting error when trying to build GATK with a JRE (#6676)\r\n    * Fixed an issue where `ReadsSourceSpark.getHeader()` wasn't propagating the reference at all when a CRAM file input resides on GCS, so it always resulted in a \"no reference was provided\" error, even when a reference was provided. (#6517)\r\n    * Fixed an issue where `ReadsSourceSpark.checkCramReference()` always tried to create a Hadoop Path object for the reference no matter what file system it lives on, which fails when using a reference on GCS. (#6517)\r\n    * Fixed an issue where the tab completion integration tests weren't emitting any output (#6647)\r\n      \r\n* **Miscellaneous Changes**\r\n    * Created a new `ReadsDataSource` interface (#6633)\r\n    * Migrated read arguments and downstream code to `GATKPath` (#6561)\r\n    * Renamed `GATKPathSpecifier` to `GATKPath`. (#6632)\r\n    * Add a read/write roundtrip Spark integration test for a CRAM and reference on HDFS. (#6618)   \r\n    * Deleted redundant methods in `SVCigarUtils`, and rewrote and moved the rest to `CigarUtils` (#6481) \r\n    * Re-enabled tests for HTSGET now that the reference server is back to a stable version (#6668)\r\n    * Disabled `SortSamSparkIntegrationTest.testSortBAMsSharded()` (#6635)\r\n    * Fixed a typo in a `SortSamSpark` log message. (#6636)\r\n    * Removed incorrect logger from `DepthOfCoverage`. (#6622)\r\n\r\n* **Documentation**\r\n    * Fixed annotation equation rendering in the tool docs. (#6606)\r\n    * Adding a note as to how to filter on MappingQuality in `DepthOfCoverage` (#6619)\r\n    * Clarified the docs for the `--gcs-project-for-requester-pays` argument to mention the need for `storage.buckets.get` permission on the bucket being accessed (#6594)\r\n    * Fixed a dead forum link in the `SelectVariants` documentation (#6595)\r\n\r\n* **Dependencies**\r\n    * Updated HTSJDK to 2.22.0 (#6637)\r\n    * Updated Picard to 2.22.8 (#6637)\r\n    * Updated Barclay to 3.0.0 (#4523)\r\n    * Updated Spark to 2.4.5 (#6637)\r\n    * Updated Disq to 0.3.6 (#6637)\r\n    * Updated the version of Cromwell used on Travis to v51 (#6628)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.1.8.0",
        "name": "4.1.8.0",
        "release_id": 27971612,
        "tag": "4.1.8.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.1.8.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/27971612",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/27971612",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.1.8.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2020-04-23T23:06:49Z",
        "date_published": "2020-04-23T23:16:52Z",
        "description": "**Download release:** [gatk-4.1.7.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.1.7.0/gatk-4.1.7.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.1.7.0 release:**\r\n--------------------------------------\r\n\r\n* Added allele-specific filtering to the mitochondrial pipeline. \r\n    * *Allele-specific filtering is important for mitochondrial calling because there are many more multi-allelic sites than in the germline autosome.*\r\n\r\n* A fix for the frequently-encountered \"Smith-Waterman alignment failure\" error in `HaplotypeCaller` and `Mutect2`\r\n\r\n* Initial support for http(s) paths for BAM inputs, including signed urls\r\n\r\n* A new tool, `DownsampleByDuplicateSet`, to randomly sample a fraction of duplicate sets from an input bam sorted by UMI\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **New Tools**\r\n\r\n    * `DownsampleByDuplicateSet`: a new tool to randomly sample a fraction of an input bam sorted by UMI. (#6512)\r\n        * Given a bam grouped by unique molecular identifier (UMI), this tool drops a specified fraction of duplicate sets and returns a new bam.\r\n        * A duplicate set refers to a group of reads whose fragments start and end at the same genomic coordinate _and_ share the same UMI.\r\n        * The input bam must first be sorted by UMI using [FGBio GroupReadsByUmi](http://fulcrumgenomics.github.io/fgbio/tools/latest/GroupReadsByUmi.html).\r\n        * Use this tool to create, for instance, an insilico mixture of duplex-sequenced samples to simulate tumor subclones.\r\n\r\n* **HaplotypeCaller**/**Mutect2**\r\n\r\n    * Fixed a regression in `HaplotypeCaller` and `Mutect2` where alt haplotypes with a deletion at the end of the padded region caused exceptions (#6544)\r\n        * This bug produced error messages like the following: \"*Smith-Waterman alignment failure. Cigar = 275M with reference length 275 but expecting reference length of 303*\"\r\n    * Fixed an `ArrayIndexOutOfBoundsException` in `GenotypeUtils.computeDiploidGenotypeCounts()` caused by mistakenly assuming ploidy two for no-calls (#6563)\r\n    * Added more control over scattering in the `Mutect2` PON WDL to allow arbitrarily fine scattering, reducing the memory required for downstream runs of `GenomicsDBImport` (#6527)\r\n    * Invert `--correct-overlapping-quality` argument in `HaplotypeCaller` to `--do-not-correct-overlapping-quality` (#6528)\r\n\r\n* **Mitochondrial Pipeline**\r\n\r\n    * Added allele-specific filtering to the mitochondrial pipeline (#6399)\r\n        * Allele-specific filtering is important for mitochondria because there are many more multi-allelic sites than in the germline autosome and therefore, downstream tools have access to more of the good allele data.\r\n        * These Mutect2 filters used in the MT pipeline are now allele-specific: `weak_evidence`, `base_qual`, `map_qual`, `duplicate`, `strand_bias`, `strand_artifact`, `position`, `contamination`, and `low_allele_frac`. \r\n        * They are added to the `AS_FilterStatus` annotation in the INFO field.\r\n        * The `numt_chimera` and `numt_novel` filters have been replaced by the `possible_numt` filter.\r\n        * Two new filtering tools have been added: `NuMTFilterTool` for the `possible_numt` filter and `MTLowHeteroplasmyFilterTool` for the `mt_many_low_hets` filter, both of which are allele-specific.\r\n        * The `--split-multi-allelics` option of the `LeftAlignAndTrimVariants` tool now splits the annotations in the FORMAT and INFO fields that are of type A and R (allele-specific, and allele-specific with reference).\r\n        * The `VariantFiltration` tool now has an `--apply-allele-specific-filters` option that will apply masks at the allele level. Before this addition, sites that should not be masked, but had deletions that spanned a masked site would have been masked. Now, if this option is specified, only the alleles spanning the masked site will be masked.\r\n\r\n* **GATK Engine**\r\n    * Added initial support for http(s) paths for BAM inputs, including signed urls (#6526)\r\n      \r\n* **Miscellaneous Changes**\r\n    * Exposed maximum copy ratio and point size for CNV plotting tools (#6482)\r\n    * Decreased an epsilon value in `VariantRecalibrator` so that our production exome joint genotyping tests pass (#6534)\r\n    * Migrated reference arguments and downstream code to `GATKPathSpecifier` (#6524)\r\n    * Removed obsolete `isCompatibleWithSparkBroadcast()` method. (#6523)\r\n\r\n* **Documentation**\r\n    * Cleaned up the handling of some missing values in auto-generated GATK tool documentation (#6565)\r\n        * Now docs won't include null, \"\", or [] in the default value list.\r\n    * Added a README for the CNN variant scoring workflow, and added an input JSON for `Mutect2` workflow files located in GCS buckets (#6542)\r\n    * Fixed a typo in a ploidy prior example in the docs for `DetermineGermlineContigPloidy` (#6531)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.1.7.0",
        "name": "4.1.7.0",
        "release_id": 25830697,
        "tag": "4.1.7.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.1.7.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/25830697",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/25830697",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.1.7.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2020-03-25T16:19:42Z",
        "date_published": "2020-03-25T16:35:22Z",
        "description": "**Download release:** [gatk-4.1.6.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.1.6.0/gatk-4.1.6.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.1.6.0 release:**\r\n--------------------------------------\r\n\r\n* `Funcotator` now supports ENSEMBL GTF files (and non-human species)\r\n\r\n* A beta port of the GATK3 tool `DepthOfCoverage`, a tool to assess sequence coverage by a wide array of metrics, partitioned by sample, read group, library, or gene (#5913)\r\n\r\n* Several important bug fixes and enhancements to `HaplotypeCaller` and `Mutect2`, *including*:\r\n    * A fix for an often-reported issue where `HaplotypeCaller` could produce reads starting with deletions during the realignment step and error out.\r\n    * A fix for another often-reported issue where `Mutect2` could emit MNPs despite `--max-mnp-distance` being 0, causing downstream errors in `GenomicsDB` about MNPs not being supported.\r\n    \r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **New Tools**\r\n    * A beta port of the GATK3 tool `DepthOfCoverage`, a tool to assess sequence coverage by a wide array of metrics, partitioned by sample, read group, library, or gene (#5913)\r\n        * This port fixes several bugs and changes some behavior present in the GATK3 version:\r\n            * Fixed a longstanding bug in GATK3 DepthOfCoverage where using multiple partition types results in column header and body lines having mismatching ordering causing incorrect output. \r\n            * The old version used to merge adjacent and overlapping intervals when generating interval summary files. This is no longer the case as in GATK4 adjacent and overlapping intervals are tabulated as separate lines in the output (This also applies to gene lists which would previously have been merged as well). \r\n            * Changed the behavior of gene list coverage to no longer count introns when generating interval summaries for gene lists. \r\n            * Added support for RefSeqGeneList files as optional gene list input.\r\n\r\n* **HaplotypeCaller**\r\n    * Fixed a bug where single-base intervals led to no calls (#6507)\r\n        * This fixes the issue reported in https://github.com/broadinstitute/gatk/issues/6495 \"HaplotypeCaller doesn't detect alternate alleles with 1 bp intervals\"\r\n    * Clean leading deletions from reads realigned to best haplotypes (#6498)\r\n        * This fixes the issue reported in https://github.com/broadinstitute/gatk/issues/6490 \"HaplotypeCaller might be producing bogus reads with deletions at their alignment start during realignment to best haplotype step\"\r\n    * Fixed an edge case when haplotypes have leading insertion after trimming (#6518)\r\n\r\n* **Mutect2**\r\n    * `Mutect2` can now filter MNVs with orientation bias (#6486)\r\n    * Added an experimental pileup-based read error corrector, which in our evaluations reduces false positives and improves speed at no cost to sensitivity (#6470)\r\n    * Switched CigarBuilder's order for adjacent indels to be deletion first (#6510)\r\n        * Fixes https://github.com/broadinstitute/gatk/issues/6473 \"Mutect2 (GATK 4.1.5.0) emitting MNPs despite max-mnp-distance 0\"\r\n        * This also resolves downstream errors in `GenomicsDB` about not supporting MNPs\r\n    * Fixed several bugs involving `getReadCoordinateForReferenceCoordinate()` (#6485)\r\n        * Fixes https://github.com/broadinstitute/gatk/issues/6342 \"Mutect2 occasionally writes nonsense / invalid values for MPOS info tag\"\r\n        * Fixes https://github.com/broadinstitute/gatk/issues/6314 \"GATK4.1.3.0 Mutect2 enable-all-annotations option error\"\r\n        * Fixes https://github.com/broadinstitute/gatk/issues/6294 \"ReadPosRankSumTest with leading insertions\"\r\n        * Fixes https://github.com/broadinstitute/gatk/issues/5492 \"ReadPosRankSumTest doesn't work for two deletions with one base in between\"\r\n\r\n* **Funcotator**\r\n    * `Funcotator` now supports ENSEMBL GTF files (and non-human species) (#6477) (#6492)\r\n        * Users can now create datasources for any species for which ENSEMBL has an annotated GTF file and the corresponding coding region FASTA file\r\n        * When creating new data sources, the user must still use `gencode` as the parent folder for the GTF data source subfolders.  For example, for E. coli MG1655:\r\n            * DATASOURCES\r\n                * gencode\r\n                    * ASM584v2\r\n                        * Escherichia_coli_str_k_12_substr_mg1655.ASM584v2.44.gtf\r\n                        * Escherichia_coli_str_k_12_substr_mg1655.ASM584v2.cds.all.fa\r\n                        * \u2026\r\n                        * gencode.config\r\n        * For more information on creating data sources see the Funcotator tutorial on the GATK Forums.\r\n        * An example datasource for E. coli MG1655 can be found in the large test files for Funcotator\r\n        * For ENSEMBL datasources for vertebrates: ftp://ftp.ensembl.org/pub/\r\n        * For ENSEMBL datasources for other species: ftp://ftp.ensemblgenomes.org/pub/\r\n   \r\n* **CNV Calling**\r\n    * Upgrade CNV WDLs to 1.0 spec (#6506)\r\n    * Fixed an off-by-one segmentation argument in `ModelSegments`. (#6497)\r\n    \r\n* **Miscellaneous Changes**\r\n    * Simplified cigar and clipping code; added tests and fixed a few bugs including https://github.com/broadinstitute/gatk/issues/6130 (#6403)\r\n    * Refactored and enhanced ArgumentsBuilder (#6474)\r\n    * Allow all GATKSparkTools to set the SBI index granularity (#6458)\r\n    * Delete NioBam and related classes (#6479)\r\n    * Clean up old interval code (#6465)\r\n    * Remove duplicate copy of the NIO prefetching code (#6464)\r\n    * Fix ignored test in GATKReadAdaptersUnitTest (#6471)\r\n    * Fix alternate spellings of De Bruijn in the codebase (#6472)\r\n\r\n* **Documentation**\r\n    * Fix a broken set of javadoc references in FeatureDataSource (#6478)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.1.6.0",
        "name": "4.1.6.0",
        "release_id": 24857841,
        "tag": "4.1.6.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.1.6.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/24857841",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/24857841",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.1.6.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2020-02-28T22:55:27Z",
        "date_published": "2020-02-28T23:01:51Z",
        "description": "**Download release:** [gatk-4.1.5.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.1.5.0/gatk-4.1.5.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.1.5.0 release:**\r\n--------------------------------------\r\n\r\n* A new, improved version of the `--linked-de-bruijn-graph` mode for `HaplotypeCaller` and `Mutect2` that has better sensitivity compared to the previous linked DeBruijn graph implementation (#6394)\r\n\r\n* A new version of `GenomicsDB` that fixes many frequently-reported issues\r\n\r\n* `LeftAlignIndels` now works for multiple indels\r\n\r\n* `VariantAnnotator` and `Concordance` are now out of beta\r\n\r\n* A significant number of bug fixes to major tools like `GenotypeGVCFs` and `SelectVariants` \r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **HaplotypeCaller**\r\n    * New, improved version of the `--linked-de-bruijn-graph` mode for `HaplotypeCaller` and `Mutect2` that has better sensitivity compared to the previous linked DeBruijn graph implementation (#6394)\r\n        * Running `HaplotypeCaller` in this mode will reduce the number of erroneous haplotypes discovered which can improve genotyping, phasing, and runtime.\r\n        * Changed the haplotype recovery step to check that it covers all paths through the graph even if there are poorly supported paths in the JunctionTrees. Added the argument `--disable-artificial-haplotype-recovery` to disable this behavior.\r\n        * Added the ability to expand graph kmer size after haplotype recovery in the event that there was a failure due to overcomplicated assembly graphs.     \r\n        * Added code to squeeze extra sensitivity out of the junction trees by tolerating SNP errors when threading the junction trees themselves\r\n    * Realigning to best haplotype handles indels better (#6461)\r\n    * Fixed issue #5434 on inconsistent selection of reads for the PL, AD, and DP calculations. (#6055)\r\n    * Fixed bug where SNP and indel pseudocounts were swapped in the `AlleleFrequencyCalculator` (#6401)\r\n    * The qual used in `HaplotypeCaller`'s `isActive()` method now matches that of `GenotypeGVCFs`. That is, they both now use the new qual. (#6343)\r\n    * Skip non-nucleotide alleles in force-calling mode, fixing bug (#6405)\r\n    * Fixed the hidden/experimental `--error-correct-reads` argument to actually correct the bases and qualities (#6366)\r\n    * Removed the deprecated and obsolete `--use-new-qual-calculator` argument (#6398)\r\n    * Refactored code related to windows and padding for assembly and genotyping, with slight changes to HMM padding for indels (#6358)\r\n\r\n* **Mutect2**\r\n    * Improved `SomaticClusteringModel` (#6337)\r\n    * Sped up Mutect2 reference confidence model with fast likelihoods model (#6457)\r\n    * Modified Fragment creation for Mutect2 to not fail for supplementary reads (#6327)\r\n    * Uniqify PG IDs in `FilterAlignmentArtifacts` (#6304)\r\n    * Fixed error in RealignmentEngine due to converting from exclusive to inclusive interval ends (#6404)\r\n    * Added an error message for no callable sites in Mutect2 (#6445)\r\n    * Changed filter reporting in Mutect2 (#6288)\r\n    * Fixed force-calling mode in M2 mito WDL (#6359)\r\n    * Pass the reference to the realignment filter in the Mutect2 WDL (#6360)\r\n    * Deleted the old orientation bias filter (#6408)\r\n    * Made callable sites a Long to avoid integer overflow (#6303)\r\n\r\n* **GenomicsDB**\r\n    * Move to `GenomicsDB` 1.2.0 (#6305)\r\n        * Fixes an issue with `GenomicsDBImport` erroring out due to duplicate fields in the Info, Format, and/or Filter fields. (https://github.com/broadinstitute/gatk/issues/6158)\r\n        * Fixes an issue with `GenomicsDBImport` not completing for mixed ploidy samples (https://github.com/broadinstitute/gatk/issues/6275)\r\n        * This version uses a 64-bit htslib to workaround overflow issues when computed annotation sizes exceed the 32-bit integer space\r\n          \r\n* **Joint Calling**\r\n    * `GenotypeGVCFs`: improved checking for upstream deletions in the `GenotypingEngine` (#6429)\r\n        * Fixes rare cases where `GenotypeGVCFs` could emit a variant with a spanned allele (*), and a genotype that references the spanned allele, but fail to emit the upstream spanning variant.\r\n    * `GenotypeGVCFs`: Don't call the NON_REF allele in genotypes or ADs (#6437)\r\n    * Parse combined `AS_QUALapprox` values from older reblocked GVCFs properly (#6442)\r\n    * Added a force output sites argument to `GenotypeGVCFs` (#6263)\r\n    * Remove extraneous alleles in GenotypeGVCFs force-output mode (#6406)\r\n    \r\n* **CNV Calling**\r\n    * Copy temporary files early in gcnvkernel to avoid inadvertent temporary directory cleanup. (#6297)\r\n    * Enabled streaming of counts.tsv/counts.tsv.gz files in gCNV CLIs. (#6266)\r\n    * Fixed shard index in PostprocessGermlineCNVCalls log message. (#6313)\r\n    * gCNV vcf cleanup (#6352)\r\n    * Index output VCFs for GCNV postprocessing (#6330)\r\n\r\n* **Notable Enhancements**\r\n    * `VariantAnnotator` is now out of beta (#6402)\r\n    * `Concordance` is out of beta (#6397)\r\n    * `LeftAlignIndels` now works for multiple indels (#6427)\r\n    * `FilterVariantTranches` can now handle cases where there are only SNPs or only indels, and not both (#6411)\r\n    * Added new read filters for `NotProperlyPaired` and for `MateDistant` (#6295)\r\n    * Made the `.git` directory optional during build (#6450)\r\n\r\n* **Bug Fixes**\r\n    * Handle zero-weight Gaussians correctly in `VariantRecalibrator` (#6425)\r\n    * Fixed the `--invalidate-previous-filters` argument in `VariantFiltration` to work as intended (ie., roll back all variants to unfiltered status) (#6412)\r\n    * Fixed a bug where `SelectVariants` takes forever on many-allelic somatic samples (#6446)\r\n    * Make sure `SelectVariants` outputs variants in correct order (assuming input vcf is correctly sorted) (#6444)\r\n    * Fixed a NPE crash in `VariantEval` when run with no intervals/reference (#6283)\r\n    * Fixed a NPE crash in `FastaReferenceMaker` (#6435)\r\n    * Fixed an out-of-bounds error in `CountNs` annotation (#6355)\r\n    * Fixed a bug in hardClipCigar function that caused incorrect cigar calculation (#6280)\r\n    * `AnalyzeSaturationMutagenesis`: fixed bug in codon calling for in-frame inserts (#6332)\r\n\r\n* **Miscellaneous Changes**\r\n    * Collect split read and paired end evidence files for GATK-SV pipeline (#6356)\r\n    * Add \"PASS\" filter line for `ApplyVQSR` and `FilterMutectCalls` (#6436)\r\n    * Added engine functionality for accessing the user defined intervals without merging them (#5887)\r\n    * Trim intervals loaded from interval files. (#6375)\r\n    * Propagate read group filters in `ReadGroupBlackListReadFilter`. (#6300)\r\n    * Modified ANDed read filter output message for readability (#6315)\r\n    * Clearly label the number of reads processed in the `BaseRecalibrator` log output (#6447)\r\n    * Clearly label the `CountReads` tool output (#6449)\r\n    * Improved the error messages for missing contigs in the reference (#6469)\r\n    * Avoid a copy and reverse operation in `CigarUtils.isGood()` (#6439)\r\n    * Fixed `GenotypeAlleleCount`'s toString() method (#6376)\r\n    * Minor Funcotator WDL updates. (#6326)\r\n    * Added a `getPairOrientation()` method to `GATKRead` (#6420)\r\n    * Merged `GATKProtectedVariantContextUtils` methods into other classes (#6409)\r\n    * Deleted a lot of unused VCF constants (#6361)\r\n    * Deleted some unused genotyping code (#6354)\r\n    * Fixed incoherent unit test cases in allele subsetting utils (#6448)\r\n    * Add Python script executor error message for SIGKILL exit code 137. (#6414)\r\n    * Pip install pinned numpy. (#6413)\r\n    * Do not install R on travis, and only run the R tests on the Docker. (#6454)\r\n    * Fixes for `IndexFeatureFile` error reporting. (#6367)\r\n    * Temporarily remove dead Berkeley mirror to unblock builds. (#6422)\r\n    * Disable CNNVariantPipelineTest.testTrainingReadModel until failures are resolved. (#6331)\r\n    * Delete unused JsonSerializer (#6415)\r\n    * Delete empty file SparkToggleCommandLineProgram.java. (#6311)\r\n    \r\n* **Documentation**\r\n    * Clarify the definition of the `NON_REF` allele (#6431)\r\n    * Clarify behavior of `SplitIntervals` for lists of adjacent intervals (#6423)\r\n    * Update docs to reflect the fact that `TandemRepeat` works with `HaplotypeCaller` (#5943)\r\n    * Update LeftAlignIndels documentation (#6177)     \r\n    * Update hyperlink to new GATK forum page in the README (#6381)\r\n    * Add minValue/minRecommended value to ApplyBQSRArgumentCollection (#6438)\r\n    * Small README fixes (#6451)\r\n    * Fix some GATK doc issues (#6318)\r\n    * Update copyright date in LICENSE.TXT (#6383)\r\n\r\n* **Dependencies**\r\n    * Updated `HTSJDK` to 2.21.2 (#6462)\r\n    * Updated `Picard` to 2.21.9 (#6462)\r\n    * Updated `Disq` to 0.3.5 (#6323)\r\n    * Updated `GenomicsDB` to 1.2.0 (#6305)\r\n    * Updated `TestNG` to 7.0.0 (#5787)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.1.5.0",
        "name": "4.1.5.0",
        "release_id": 24091652,
        "tag": "4.1.5.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.1.5.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/24091652",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/24091652",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.1.5.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "lbergelson",
          "type": "User"
        },
        "date_created": "2019-11-27T17:31:39Z",
        "date_published": "2019-11-27T19:50:45Z",
        "description": "**Download release:** [gatk-4.1.4.1.zip](https://github.com/broadinstitute/gatk/releases/download/4.1.4.1/gatk-4.1.4.1.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.1.4.1 release:**\r\n--------------------------------------\r\n*  New **experimental** `HaplotypeCaller` assembly mode which improves phasing, reduces false positives, improves calling at complex sites, and has 15-20% speedup vs the current assembler.  It is enabled with option ` --linked-de-bruijn-graph`.  This mode is still experimental and not recommended for production use yet.\r\n* `IndexFeatureFile` improvements:\r\n    * now cloud enabled\r\n    * changed controversial `F` argument to `I` instead.\r\n*  Bug fixes and improvements in `GenomicsDB`, `Mutect2`, variant annotation, and more!\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n* **New Tools**\r\n    *  `PrintReadsHeader`: a new tool to print a BAM/SAM/CRAM header to a file (#6153)\r\n\r\n* **HaplotypeCaller**\r\n    *  Experimental prototype of JunctionTree based haplotype finding. (#6034) #5925\r\n    *  Fix a genotyping bug were reference/alt likelihoods were capped differently. (#6196)\r\n    \r\n* **Mutect2**\r\n    *  `Mutect2` now warns but does not fail when three or more reads have the same name. (#6240)\r\n    *  Fixed the random seed at the beginning of `FilterMutectCalls` (#6208)\r\n    *  `GetSampleName` and `GetPileupSummaries` in the M2 pipeline are no longer beta. (#6215)\r\n    *  Increase number of iterations in `CalculateContamination` to 30. (#6282)\r\n    *  Handled an edge case with high scatter count in M2 WDL. (#6216)\r\n    *  Use ArgumentsBuilder in M2 tests. (#6219)\r\n    \r\n* **Joint Calling**\r\n    *  Allele-specific VQSR convergence fix. (#6262)\r\n    *  Fix to Allele Fraction annotation bug in multisample vcfs. (#6251)\r\n    *  Fix RAW_MQ header inconsistencies after reblocking. (#6276)\r\n    *  Mark SNP/indel mode argument in `GatherTranches` as required so tranches are named properly. (#6273)\r\n\r\n* **CNV Calling**\r\n    *  Fixed model parameter assignment typo in gCNV ploidy model (#6285)\r\n    *  Added docker option to the gcnv QC tasks. (#6185)\r\n    *  Added epsilons to overdispersion in gCNV models to avoid NaNs. (#6245) #4824 #6226 #6227\r\n    *  Assert that ELBO did not become NaN during each step of inference of gCNV. (#6186)\r\n    *  Added ability to override `THEANO_FLAGS` environment variable in gCNV tools. (#6244) #6235\r\n    *  Removed erroneous short argument names in R scripts for CNV plotting. (#6197)\r\n\r\n* **GenomicsDB**\r\n    * Allow GATK to configure annotation processing instead of hardcoding values in GenomicsDB [GDB-39](https://github.com/GenomicsDB/GenomicsDB/pull/39)\r\n    * High ploidy sites with many genotypes no longer causes an overflow error. [GDB-54](https://github.com/GenomicsDB/GenomicsDB/pull/54)\r\n    * Add missing libcurl in the native GenomicsDB library. #6122 [GDB-66](https://github.com/GenomicsDB/GenomicsDB/pull/66)\r\n    * No longer crashes when vcfbufferstream from htslib appears to be invalid. [GDB-67](https://github.com/GenomicsDB/GenomicsDB/pull/67)\r\n    * Propagated native GenomicsDB exceptions as java IOExceptions. [GDB-68](https://github.com/GenomicsDB/GenomicsDB/pull/68)\r\n    * Fix issue when using vid protobuf interface and there is more than 1 config. [GDB-70](https://github.com/GenomicsDB/GenomicsDB/pull/70)\r\n   * Cleanup GenomicsDB vid combine protobuf mapping overrides. #6190\r\n    \r\n* **Miscellaneous Changes**\r\n    *  Cloud-enable `IndexFeatureFile` and change input arg name from -F to -I. (#6246) #6161\r\n    *  WDL to run `ReadsPipelineSpark` on a multicore machine (#6213)\r\n    *  Replace `TwoPassReadWalker` with more general `MultiplePassReadWalker`. (#6154)\r\n    *  Abolish unfilled likelihoods and revamp `VariantAnnotator`. (#6172)\r\n    *  Improve exception message in `ValidateVariants`. (#6076)\r\n    *  Fix Syntax Warning when running GATK with python 3.8 (#6231)\r\n\r\n* **Developer / Testing**\r\n    *  Report errors logs in github comment (#6247) 6234\r\n    *  Add .java-version to gitignore to support jenv users. (#6232)\r\n    *  Restart test JVM after every 100 test classes  do reduce out of memory failures. (#6093)\r\n    *  Running the cloud tests on java 11 on travis. (#6210)\r\n\r\n* **Documentation**\r\n    *  Clarify definition of PGT in VCF header (#6221)\r\n    *  docs for paired reads in Mutect2 somatic genotyping (#6264)\r\n    *  Fix some typos in the allele subsetting code. (#6265)\r\n\r\n* **Dependencies**\r\n    *  Update picard to 2.21.2 (#6253)\r\n    *  Update disq to 0.3.4 (#6252)\r\n    *  update htsjdk to 2.21.0 (#6250)\r\n    *  Update to Genomicsdb 1.1.2.2 (#6206) (#6188)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.1.4.1",
        "release_id": 21780156,
        "tag": "4.1.4.1",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.1.4.1",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/21780156",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/21780156",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.1.4.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2019-10-08T19:30:38Z",
        "date_published": "2019-10-08T19:40:46Z",
        "description": "**Download release:** [gatk-4.1.4.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.1.4.0/gatk-4.1.4.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.1.4.0 release:**\r\n--------------------------------------\r\n\r\n* Major improvements and fixes to `Mutect2`, including more intelligent handling of paired reads during genotyping and better filtering.\r\n\r\n* Important bug fixes to `HaplotypeCaller`, the joint calling pipeline, and `Funcotator`\r\n\r\n* Beta support for building/testing on Java 11 (#6119) (#6145)\r\n    * *We encourage you to try this out and give us feedback!*\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **New Tools**\r\n    * `AlleleFrequencyQC`: a QC tool that uses `VariantEval` to bin variants in 1000 Genomes by allele frequency. For each bin, we compare the expected allele frequency from 1000 Genomes with the observed allele frequency in the input VCF. This was designed with arrays in mind, as a way to discover potential bugs in our pipeline. #6039)\r\n\r\n* **Mutect2**\r\n    * `Mutect2` genotyping now forces paired reads to support the same haplotype (#5831)\r\n    * New `FilterAlignmentArtifacts` now realigns a locally-assembled unitig of all variant read pairs (#6143)\r\n    * Fixed a `Mutect2` bug that overfiltered by one variant (#6101)\r\n    * Fixed a small gene panel edge case for `CalculateContamination` (#6137)\r\n    * Fixed a small gene panel edge case in orientation bias filter (#6141)\r\n    * Unified the NIO and non-NIO M2 WDLs (call-caching will now work on Terra) (#6108)\r\n    * Updated `Mutect2` pon WDL to WDL 1.0 (#6187)\r\n    * Removed `Oncotator` from the M2 WDL (`Funcotator` is still there) (#6144)\r\n    * Fixed an issue in the M2 WDL that could cause the Funcotate task to be ignored by tools such as dxWDL (#6077)\r\n    * Some miscellaneous code refactoring/improvements (#6184) (#6136) (#6107) (#6159)\r\n\r\n* **HaplotypeCaller**\r\n    * `HaplotypeCaller` now force-calls like `Mutect2`: the `-genotyping-mode GENOTYPE_GIVEN_ALLELES` argument is gone (now you only need to specify `--alleles force-calls.vcf`) and alleles are now force-called *in addition* to any other alleles (#6090)\r\n    * Renamed `--output-mode EMIT_ALL_SITES` to `--output-mode EMIT_ALL_ACTIVE_SITES`, and clarified the documentation for the argument (#6181)\r\n    * Fixed a rare bug in the genotyping engine where it could emit untrimmed alleles for SNP sites (#6044)\r\n    * Fixed some sources of non-determinism in the `HaplotypeCaller` that in rare cases could cause the output to vary slightly given the same inputs (#6195) (#6104)\r\n    * Deleted the old exact AF calculation model (#6099)\r\n\r\n* **Joint Calling**\r\n    * Fixed a regression in GATK 4.1.3.0 that caused us to not emit the `AS_QD` annotation when running a joint calling pipeline with `CombineGVCFs` (`GenomicsDB` was unaffected) (#6168)\r\n    * Fixed allele-specific annotation array length issues when alleles are subset in tools such as `GenotypeGVCFs` (#6079)\r\n    * Changed `AS_RankSum` outputs to \".\" for missing values rather than \"nul\" (#6079)\r\n\r\n* **Funcotator**\r\n    * Fixed a bug that caused `Funcotator` to outputs fields in wrong order in some cases when writing a VCF (#6178)\r\n        * Specifically, `Funcotator` would output functation fields in the wrong order when there was more than 1 site in a VCF data source with the exact same position and alleles and it matched one of the variants being annotated\r\n\r\n* **Mitochondrial pipeline** \r\n    * Renamed the output vcf with the name of the sample and supplied a default value for `autosomal_median_coverage` (meaning you'll now get the `NuMT` filter even if you don't provide the actual autosomal coverage) (#6160)\r\n\r\n* **Miscellaneous Changes**\r\n    * Beta support for building/testing on Java 11 (#6119) (#6145)\r\n    * `UpdateVCFSequenceDictionary` now supports replacing an invalid sequence dictionary in a VCF (#6140)\r\n    * `CountFalsePositives` now requires an intervals file (#6120)\r\n    * `AnalyzeSaturationMutagenesis`: use supplementary alignments to identify large deletions (#6092)\r\n    * `AnalyzeSaturationMutagenesis`: an insert at the start codon is not in the ORF (#6121)\r\n    * Added a check for null sequence dictionaries in the dictionary validation code (#6147)\r\n    * Update SV Spark pipeline example shell scripts saving results to GCS (#6114)\r\n    * Update public key for installing R in docker (#6116)\r\n    * Log exceptions during deletion on JVM exit instead of throwing (#6125)\r\n    * Don't fail the build if we're in a git worktree folder (#6169)\r\n    * Free a bit of memory fir the test suite by disabling mysql and postgress on travis (#6085)\r\n    * Delete bogus index files for queryname sorted CRAMs. (#6149)\r\n    * Cleanup GenomicsDB debugging test output (#6089)\r\n\r\n* **Documentation**\r\n    * Fixed mitochondria mode documentation in `FilterMutectCalls` (#6174)\r\n\r\n* **Dependencies**\r\n    * Updated `HTSJDK` to 2.20.3 (#6126)\r\n    * Updated `Picard` to 2.21.1 (#6205)\r\n    * Updated `google-cloud-nio` to 0.107.0 (#6042)\r\n    * Updated `Gradle` to 5.6 (#6106)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.1.4.0",
        "name": "4.1.4.0",
        "release_id": 20558867,
        "tag": "4.1.4.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.1.4.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/20558867",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/20558867",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.1.4.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2019-08-09T19:14:24Z",
        "date_published": "2019-08-09T19:20:25Z",
        "description": "**Download release:** [gatk-4.1.3.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.1.3.0/gatk-4.1.3.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.1.3.0 release:**\r\n--------------------------------------\r\n\r\n* `GnarlyGenotyper`, a new beta joint genotyping tool which, along with `ReblockGVCF`, forms part of a forthcoming more scalable version of our joint genotyping pipeline that we call the \"GATK Biggest Practices\" pipeline\r\n* `FuncotateSegments`, a new beta companion tool to `Funcotator` that performs functional annotation on a segment file (`.seg`) rather than a VCF\r\n* `GenomicsDBImport` now has the ability to incrementally update an existing GenomicsDB workspace\r\n* Several important bug fixes to `HaplotypeCaller` and `Mutect2`\r\n\r\n**Compatibility notes:**\r\n--------------------------------------\r\n * `GermlineCNVCaller` models built  in cohort mode with previous releases are no longer compatible.  Users should rebuild these models with this release before running `GermlineCNVCaller` in case mode.  See the **CNV Tools** section below for more details.\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **New Tools**\r\n\r\n    * **GnarlyGenotyper** (beta tool) (#4947) (#6075)\r\n        * The `GnarlyGenotyper` is designed to perform joint genotyping on cohorts of at least tens of thousands of samples called with `HaplotypeCaller` and post-processed with `ReblockGVCF` to produce a multi-sample callset in a super highly scalable manner.\r\n        * Caveats:\r\n            * `GnarlyGenotyper` is intended to be used with GVCFs for which low quality variants have already been removed, derived from post-processing `HaplotypeCaller` GVCFs with `ReblockGVCF`.  See the \"Biggest Practices\" usage example in the `ReblockGVCF` docs for details.\r\n            * `GnarlyGenotyper` does not subset alternate alleles and can return some highly multi-allelic sites.  PLs will not be output for sites with more than 6 alts to save space.\r\n            * `GnarlyGenotyper` assumes all diploid genotypes\r\n        * Annotations:\r\n            * To generate all the annotations necessary for VQSR, input variants to the `GnarlyGenotyper` must include the `QUALapprox` and `VarDP` annotations along with the latest `RAW_MQandDP` annotation.\r\n            * If allele-specific annotations are present, they will be used appropriately and a new `AS_AltDP` annotation giving the total depth across samples for each alternate allele will be added.\r\n        * A GATK \"Biggest Practices\" pipeline including the `GnarlyGenotyper` is forthcoming pending some fixes improving on the above caveats.\r\n\r\n    * **FuncotateSegments** (beta tool) (#5941)\r\n        * A companion tool to `Funcotator` that performs functional annotation on a segment file (`.seg`) rather than a VCF\r\n        * The Somatic CNV pipeline can optionally run this tool for functional annotation\r\n\r\n* **HaplotypeCaller**/**Mutect2**\r\n    * Fixed a regression in `HaplotypeCaller`/`Mutect2` that caused some variants to be lost at sites with high complexity (#5952)\r\n    * Fixed a GGA (GENOTYPE_GIVEN_ALLELES) mode bug in `HaplotypeCaller`/`Mutect2` where added alleles' cigars could have soft clips (#6047)\r\n        * This bug would manifest as a \"Cigar cannot be null\" error\r\n    * Fixed a bug where cached indel informativeness values could be incorrectly applied to the wrong sites in `HaplotypeCaller`/`Mutect2` (#5911)\r\n    * Fixed an edge case in `HaplotypeCaller`/`Mutect2` where dangling end merging creates cycles (#5960)\r\n    * Added hidden arguments to the assembly engine to track found haplotype counts and kmers used (#6049)\r\n    * Fixed a bug in `CalculateContamination` when contamination is indistinguishable from zero (#5971)\r\n    * Fixed a bug where normal p value argument in `FilterMutectCalls` was declared static (#5982)\r\n\r\n* **CNV Tools**\r\n    * Added `FuncotateSegments` as an option to the Somatic CNV WDL (#5967)\r\n    * Added QC metrics to the Germline CNV workflow (#6017)\r\n    * Enabled GC-bias correction by default in CNV workflows (#5966)\r\n    * Added denoised coverage file concatenation output to gCNV postprocessor (#5823) *Note:* The addition of this feature breaks compatibility with gCNV cohort-mode models built with previous releases.\r\n    * Changed cr.igv.seg output of ModelSegments to give log2 Segment_Mean. (#5976)\r\n    * Fixed CNV plotting script to allow spaces in input filenames. (#5983)\r\n\r\n* **GenomicsDBImport**\r\n    * Added support for making incremental updates to existing workspaces (#5970)\r\n        * This can be done using the new `--genomicsdb-update-workspace-path` argument\r\n    * Fixed a crash in `GenomicsDBImport` on queries at positions inside deletions (#5899)\r\n    * Treat AS_QUALapprox and AS_VarDP strings as array of int vectors (#5933)\r\n\r\n* **Mitochondrial Calling Pipeline**\r\n    * Added NIO support and updated to WDL 1.0 (#6074)\r\n\r\n* **Spark Tools**\r\n    * Removed the beta label from many simple Spark tools (#5991)\r\n    * Bug fix for reading references from GCS on Spark (#6070)\r\n    * Eliminated an unnecessary sort step in `HaplotypeCallerSpark` (#5909)\r\n    * Fixed `BaseRecalibratorSpark` failure on a cluster due to system classloader issue (#5979)\r\n    * Added a WDL for `ReadsPipelineSpark` (#5904)\r\n    * Added a command-line argument to toggle using NIO on reading for Spark (#6010)\r\n    * Added advanced arguments to `MarkDuplicatesSpark` to allow non-queryname sorted inputs when specifying multiple input bams and to treat unsorted inputs as queryGroup-sorted (#5974)\r\n    * Clarified the behavior of `MarkDuplicatesSpark` when given multiple input bams, and improved the sorting behavior if given a mix of queryname-sorted and query-grouped bams (#5901)\r\n    * Changed `spark.yarn.executor.memoryOverhead` to `spark.executor.memoryOverhead` as promoted by Spark 2.3 (#6032)\r\n    * Handle newly-added arguments in `ApplyBQSRUniqueArgumentCollection` (#5949)\r\n\r\n* **Miscellaneous Changes**\r\n    * Added a new `BaseQualityHistogram` variant annotation to generate base quality histograms (#5986)\r\n    * Added a new `SoftClippedReadFilter` that can filter out reads where the ratio of soft-clipped bases to total bases exceeds some given value (#5995)\r\n    * Fixed a serious bug in `ValidateVariants` where the tool would silently do no validation in the default case when a DBSNP file was not provided (#5984)\r\n    * Fixed a \"Record covers a position previously traversed\" error in `ValidateVariants` for GVCFS with multiple contigs (#6028)\r\n    * The `RMSMappingQuality` annotation now requires the `--allow-old-rms-mapping-quality-annotation-data` argument to run with GVCFs created by older versions of the GATK (#6060)\r\n    * Added a simple TSV/CSV/XSV writer with cloud write support as an alternative to TableWriter (#5930)\r\n    * `Funcotator`: added Funcotator stand-alone WDL to supported area (#5999)\r\n    * Extracted the `GenotypeGVCFs` engine into publicly accessible class/function (#6004)\r\n    * Refactored `VariantEval` methods to allow subclasses to override (#5998)\r\n    * `AnalyzeSaturationMutagenesis`: arbitrarily choose 1 read for disjoint pairs, dump rejected reads, and various other improvements (#5926) (#6043)\r\n    * Normalized some AssemblyRegion args in `HaplotypeCallerSpark` (#5977)\r\n    * Don't redundantly delete temporary directories in `RSCriptExecutor` (#5894)\r\n    * Treat all source files as UTF-8 for java, javadoc (#5946)\r\n    * Updated an out-of-date argument name in an error message for the `CycleCovariate`\r\n    * Changed an error about \"duplicate feature inputs\" to be a UserException (#5951)\r\n    * Got rid of `ExpandingArrayList` in favor of `ArrayList` (#6069)\r\n    * Disabled Codecov for now on travis due to spurious errors (#6052)\r\n    * Lowered the Xms value in the test JVM (#6087)\r\n    * Updated the travis installed R version to 3.2.5, matching our base docker image (#6073)\r\n    * Fixed an erroneous warning about GCS test configuration (#5987)\r\n    * Added a code of conduct (#6036)\r\n\r\n* **Documentation**\r\n    * `FilterVariantTranches` documentation fix and improvement (#5837)\r\n    * Updated `FilterMutectCalls` usage examples (#5890)\r\n    * Added `--max-mnp-distance 0` to usage example in `CreateSomaticPanelOfNormals` docs (#5972)\r\n    * Updated the `MarkDuplicatesSpark` documentation to no longer contain a misleading usage example (#5938)\r\n    * Added a clarification to the README to warn users to set their Gradle JVM properly in Intellij after setup (#6066)\r\n    * Added links to download Java 8 to the README (#6025)\r\n    * Remove non-ascii chars from javadoc (#5936)\r\n\r\n* **Dependencies**\r\n    * Updated HTSJDK to 2.20.1 (#6083)\r\n    * Updated Picard to 2.20.5 (#6083)\r\n    * Updated Disq to 0.3.3 (#6083)\r\n    * Updated Spark to 2.4.3 (#5990)\r\n    * Updated Gradle to 5.4.1 (#6007)\r\n    * Updated GenomicsDB to 1.1.0.1 (#5970)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.1.3.0",
        "name": "4.1.3.0",
        "release_id": 19200643,
        "tag": "4.1.3.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.1.3.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/19200643",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/19200643",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.1.3.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2019-04-23T17:27:31Z",
        "date_published": "2019-04-23T17:33:34Z",
        "description": "**Download release:** [gatk-4.1.2.0.zip](https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip)\r\n**Docker image:** [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n**Highlights of the 4.1.2.0 release:**\r\n--------------------------------------\r\n\r\n* Two new tools, `MethylationTypeCaller` and `AnalyzeSaturationMutagenesis` (see below for descriptions)\r\n* Significant improvements to `GENOTYPE_GIVEN_ALLELES` mode in `Mutect2` and `HaplotypeCaller`\r\n* Fixed a serious bug in `Funcotator` that could cause END positions to be wrong for some deletions in MAF output\r\n* Significant updates to the mitochondrial calling pipeline\r\n\r\n**Full list of changes:**\r\n-------------------------\r\n\r\n* **New Tools**\r\n    * **MethylationTypeCaller** (#5762)\r\n        * Identifies methylated bases from bisulfite sequencing data. Given a bisulfite sequenced, methylation-aware aligned BAM and a reference, it outputs methylation-site coverage to a specified output vcf file.\r\n    * **AnalyzeSaturationMutagenesis** (#5803)(#5883)\r\n        * Processes reads from a saturation mutagenesis experiment, an experiment that systematically perturbs a mini-gene to ascertain which amino-acid variations are tolerable at each codon of the open reading frame. Its main job is to discover variations from wild-type sequence among the reads, and to summarize the variations observed.\r\n\r\n* **Mutect2**\r\n    * Made significant improvements to `GENOTYPE_GIVEN_ALLELES` mode in `Mutect2` and `HaplotypeCaller` (#5874). These improvements are described in more detail in https://github.com/broadinstitute/gatk/issues/5857\r\n    * `CalculateContamination` now works much better for very small gene panels (#5873)\r\n    * We now correctly handle inputs with 100% contamination in `Mutect2` filtering (#5853)\r\n    * `Mutect2` now uses natural logarithms internally (#5858). This does not change any outputs.\r\n    * Minor update to the `Mutect2` PON WDL (#5859)\r\n\r\n* **Funcotator**\r\n    * Fixed a serious bug that could cause END positions to be wrong for some deletions in MAF output (#5876)\r\n    * The tool now throws a user error for an AD field with only 1 value in MAF mode (#5860)\r\n    * Added a new filter to `FilterFuncotations`. For two autosomal recessive genes, MUTYH and ATP7B, homozygous variants and compound heterozygous variants will be tagged and added to the output vcf. (#5843)\r\n\r\n* **Mitochondrial Calling Pipeline**\r\n    * Updated the pipeline for the new `Mutect2` filtering scheme and pulled filtering after the liftover and recombining of the VCF. (#5847)\r\n    * Made the subsetting of the WGS bam fast by using `PrintReads` over just chrM instead of traversing the whole bam for NuMT mates. (#5847)\r\n    * Moved polymorphic NuMTs based on autosomal coverage to a filter (it was an annotation before) (#5847)\r\n    * Added an option to hard filter by VAF (#5847)\r\n    * Bug fix for large input files to the mitochondrial pipeline (we now include the size of the input BAM/CRAM when calculating disk size, when necessary) (#5861)\r\n\r\n* **Structural Variation Calling Pipeline**\r\n    * Bug fix to `QNameFinder` to handle reads with negative unclipped starts (#5864)\r\n\r\n* **Miscellaneous Changes**\r\n    * Added a `--min-fragment-length` argument to the `FragmentLengthReadFilter` (#5886)\r\n    * Added a `--spark-verbosity` argument to control verbosity of Spark-generated logs (#5825)\r\n    * Added a new `WalkerBase` abstract class to be used for all built-in walkers (#4964)\r\n    * Exposed transient attributes in the `GATKRead` API (#5664)\r\n    * Convert more code to use `GATKPathSpecifier` (#5870) (#5832). This also fixes an `InvalidPathException` on Windows machines. \r\n    * Fixes to the test suite related to the recent introduction of a codec for Picard interval lists (#5879)\r\n    * Eliminated an error message during the Docker build in Travis logs by creating a directory before copying to it. (#5878)\r\n\r\n* **Documentation**\r\n    * Updated the `Mutect2` WDL README with `Funcotator` information (#5892)\r\n    * Updated a usage example for `CreateHadoopBamSplittingIndex` (#5898)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.1.2.0",
        "name": "4.1.2.0",
        "release_id": 16927834,
        "tag": "4.1.2.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.1.2.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/16927834",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/16927834",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.1.2.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2019-03-28T22:52:13Z",
        "date_published": "2019-03-28T23:06:55Z",
        "description": "**Highlights of the 4.1.1.0 release:**\r\n-------------------------------------\r\n   * A substantial (~33%) speedup to the `HaplotypeCaller` in GVCF mode (`-ERC GVCF`)\r\n   * Major updates to `Mutect2`, including completely overhauled filtering and smarter handling of overlapping read pairs.\r\n   * A tensorflow update for `CNNScoreVariants` that speeds up the tool by roughly ~2X when using the 2D model.\r\n   * Important updates to the mitochondrial calling pipeline, and improved memory usage in the CNV pipeline.\r\n   * Important bug fixes to `Funcotator`, `VariantEval`, `GenomicsDBImport`, and other tools, as well as to the `--pedigree` argument for annotations.\r\n\r\n**Docker image:** https://hub.docker.com/r/broadinstitute/gatk/\r\n\r\nFull list of changes:\r\n-------------------\r\n\r\n* **HaplotypeCaller**\r\n    * Greatly improved the performance of the ReferenceConfidenceModel using dynamic programming and caching (#5607)\r\n        * This speeds up whole-genome GVCF mode calling (`-ERC GVCF`) by ~33% in our tests!\r\n    * Optimized some additional performance hotspots in the ReferenceConfidenceModel (#5616) (#5469) (#5652)\r\n    * Can now write VCF outputs to Google Cloud Storage (GCS) (#5378)\r\n    * Don't output variants with no ALT allele if the * (spanning deletion) allele gets dropped (#5844)\r\n    * Added a `--force-active` argument that marks all regions as active. Useful for debugging/diagnostics. (#5635)\r\n    * `HaplotypeCallerSpark`: made performance improvements to allow the tool to run on WGS in strict mode (#5721)\r\n    * Fixed rare infinite recursion bug in `KBestHaplotypeFinder` (also affects `Mutect2`)(#5786)\r\n\r\n* **Mutect2**\r\n    * Overhaul of `FilterMutectCalls`, which now applies a single threshold to an overall error probability (#5688)  \r\n        * `FilterMutectCalls` automatically determines the optimal threshold.  \r\n        * The new somatic clustering model learns tumors' allele fraction spectra and overall SNV and indel mutation rates in order to improve filtering.\r\n        * Includes a rewrite of `Mutect2` documentation -- better organization and now includes command line examples in addition to math.\r\n    * `Mutect2` now modifies base and indel qualities of overlapping paired reads to account for PCR error rather than discarding reads (#5794)  \r\n        * This especially improves indel sensitivity.\r\n    * Optimized `Mutect2` read orientation filtering by collecting F1R2 counts from within Mutect2 itself, greatly reducing wall-clock and CPU time (#5840)\r\n    * New `Mutect2` panel of normals workflow using `GenomicsDB` for scalability (#5675)  \r\n        * Panel of normals removes germline variants in order to contain only technical artifacts, and contains information about artifact prevalence \r\n    * Rewrote `Mutect2` active region likelihood as special case of full somatic likelihoods model, which reduces runtime by 5% (#5814)\r\n    * `Funcotator` updates in `Mutect2` WDL (#5742) (#5735)\r\n    * Prune assemby graph before checking for cycles (#5562)\r\n    * Refactor `Mutect2` inheritance so that it doesn't have inactive arguments (#5758)\r\n    * Added CRAM support to the `Mutect2` WDL (#5668)\r\n    * Split MNPs in `Mutect2` PON WDL, fixing a potential bug (#5706)\r\n    * Handle negative infinity log likelihoods from PairHMM in `Mutect2` (#5736)\r\n    * Fixed overfiltering in `Mutect2` in GGA alleles mode with no reads (#5743)\r\n    * Correct some `Mutect2` VCF header lines (#5792)\r\n    * Handle unmarked duplicates with mate MQ = 0 in `Mutect2` (#5734)\r\n    * Output sample names in `Mutect2` PON header (#5733)\r\n    * Avoid error due to finite precision error in `Mutect2` PON creation (#5797)\r\n    * Update `Mutect2` javadoc to reflect v4.1 changes. (#5769)\r\n    * Renamed the `OxoGReadCounts` annotation to `OrientationBiasReadCounts` (#5840)\r\n\r\n* **CNNScoreVariants**\r\n    * We now use the latest Intel-optimized tensorflow (#5725)\r\n        * This speeds up the 2D CNN by roughly 2X in our tests!\r\n    * `FilterVariantTranches` is out of beta (#5628)\r\n    * Fixed `CNNScoreVariants` hanging when the conda environment is not set up (#5819)\r\n        * We now make sure that the GATK tool Python package is present before executing streaming Python commands.\r\n    * Extensive updates to the CNN WDLs (#5251) \r\n\r\n* **Mitochondrial Calling Pipeline**\r\n    * Added an option to recover all dangling branches, on by default for MT calling (#5693)\r\n        * Fixes a large number of missed calls\r\n    * Use adaptive pruning in the mitochondria pipeline (#5669)\r\n    * Changed defaults in mitochondria mode in response to `Mutect2` filtering overhaul (#5827)\r\n    * Allowed the MT pipeline to work on bams with a mix of single and paired-end reads (#5818)\r\n    * Added a hard filter to M2 for polymorphic NuMTs and low VAF sites (#5842)\r\n    * Updated the `haplochecker` version to `0.1.2` to fix a bug with flipping the major and minor hg headers in its output (#5760)\r\n    * Added the rest of the mitochondria joint-calling pipeline (#5673)\r\n        * Merging and genotyping \"somatic\" GVCFs from `Mutect2`\r\n    * Added a read filter for unmapped reads and their mates (#5826)\r\n    * Refactored the MT WDL to make validations easier (#5708)\r\n    * Updated a variable name in MT WDL to match gatk-workflows version (#5694)\r\n\r\n* **GenotypeGVCFs**\r\n    * Added an option to merge intervals for better `GenotypeGVCFs` performance on `GenomicsDB` exome input (#5741)\r\n    * Trim per-allele FORMAT annotations and optionally retain raw AS annotations (#5833)\r\n        * `GenotypeGVCFs` now uses the header info to determine if FORMAT lists need to be subset when alleles are dropped\r\n        * Fixes \"F1R2 and F2R2 annotations not updated by GenotypeGvcfs\" (https://github.com/broadinstitute/gatk/issues/5704)\r\n\r\n* **Funcotator**\r\n    * Non-locatable data sources can create funcotations again (#5774)\r\n        * Fixes a bug where `Funcotator` was not adding funcotations from non-locatable data sources\r\n    * Fixed handling of symbollic alleles when determining best transcript for `GencodeFuncotation` creation. (#5834)\r\n    * `FilterFuncotations`: support for multi-allelic variants (#5588)\r\n    * `FilterFuncotations`: support for gnomAD for allele frequency in `ClinVarFilter` and `LofFilter`, with a new argument telling it which dataset of gnomAD or ExAC to use (#5691)\r\n    * Added `#` as a character to be sanitized by `VCFOutputRenderer` (#5817)\r\n    * Added in Markdown files for Funcotator forum posts (#5630)\r\n    * Updated `Funcotator` documentation with a FAQ section to respond to user comments (#5755)\r\n\r\n* **CNV Tools**\r\n    * Improved memory usage in gCNV (#5781)\r\n    * Improved memory requirements of `CollectReadCounts` (#5715)\r\n    * Added some fixes for minor CNV issues (#5699)\r\n    * Added io_commons.read_csv to address issues with formatting of sample names in gCNV (#5811)\r\n    * Added gCNV PROBPROG 2018 extended abstract, archived notes on CNV methods, and deleted some legacy documentation (#5732)\r\n\r\n* **Miscellaneous Changes**\r\n    * `SelectVariants` can now write VCF outputs to Google Cloud Storage (GCS) (#5378)\r\n    * `VariantEval` bug fix: don't require the output file to already exist (#5681)\r\n    * Fixed the `--pedigree` argument in the `PossibleDeNovo` annotation (#5663)\r\n    * `GenomicsDBImport`: fixed a core dump when querying overlapping deletions (#5799)\r\n    * `GatherPileupSummaries`: a new tool that combines the output of `GetPileupSummaries` from disjoint scatter jobs (#5599)\r\n    * `VariantsToTable`: add splitting for allele-specific annotations and ADs (#5697)\r\n    * `CalculateGenotypePosteriors`: fix reported bug where no-call genotypes with no reads get genotype posterior probabilities and calls (#5667)\r\n    * Added a new argument to Spark tools enabling the user to control whether to sort the reads on output (#4874)\r\n    * `ReadsPipelineSpark`: fixed an \"Interval not within the bounds of a contig\" error (#5645)\r\n    * `Concordance`: fixed the tool to allow for no variation alleles in the truth data. (#5718)\r\n    * `ReblockGVCF`: fix sites with zero AD to actually use SITE-level DP value as intended in (#5835)\r\n    * Change `UpdateVCFSequenceDictionary` to use the specified dictionary uniformly (#5093)\r\n    * Fixed gatk-nightly Docker builds (https://hub.docker.com/r/broadinstitute/gatk-nightly/) (#5759)\r\n    * Print the Picard/HTSJDK versions in addition to the GATK version when running with `--version` (#5757)\r\n    * `IndexFeatureFile`: fixed a crash on VCFs with 0 records (#5795)\r\n    * `PrintBGZFBlockInformation`: removed the file extension check so that we can accept bams (#5801)\r\n    * Added a new read filter: `IntervalOverlapReadFilter` (#5656)\r\n    * Add NIO Path support to `TableReader` and `TableWriter` (#5785)\r\n    * Replaced `IntervalsSkipList` with `OverlapDetector` (#4154)\r\n    * Removed some unused arguments in VCF merging code (#5745)\r\n    * Kebab-case some arguments in `LocusWalker` and `LocusWalkerSpark` (#5770)\r\n    * Removed an unnecessary IllegalArgumentException in `PairHMM` (#5705)\r\n    * Removed accidental uses of log4j v1 (#5682)\r\n    * Improvements to Spark evaluation scripts (#5815)\r\n    * Extract tests from `PrintReadsIntegrationTest` to share with the Spark version. (#5689)\r\n\r\n* **Documentation**\r\n    * Improved the documentation for the `StrandOddsRatio` annotation (#5703)\r\n    * Fixed the descriptions of some `HaplotypeCaller` arguments (#5658)\r\n    * Update `VariantRecalibrator` example code to reflect new tagged argument syntax (#5710)\r\n    * Corrected javadoc for the `InbreedingCoeff` annotation (#5768)\r\n    * `CalculateGenotypePosteriors`: minor updates to javadoc and logger type (#5601)\r\n    * Added and Updated javadoc for `SortSamSpark` and `MarkDuplicatesSpark` (#5672)\r\n    * Added a link to a \"GitHub basics for researchers\" article at top of the GATK README (#5643)\r\n    * Updated the main GATK README to remove outdated references to the Intel conda environment (#5753)\r\n    * Trimmed overly-long tool one-line summaries to shorten --list display width. (#5551)\r\n\r\n* **Dependencies**\r\n    * Updated `HTSJDK` to 2.19.0 (#5812)\r\n    * Updated `Picard` to 2.19.0 (#5812)\r\n    * Updated `Disq` to 0.3.0 (#5812)\r\n    * Updated `google-cloud-nio` to 0.81.0 (#5752)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.1.1.0",
        "name": "4.1.1.0",
        "release_id": 16424010,
        "tag": "4.1.1.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.1.1.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/16424010",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/16424010",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.1.1.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2019-01-30T03:19:48Z",
        "date_published": "2019-01-30T03:38:18Z",
        "description": "It's been a year since the GATK `4.0.0.0` release in January 2018, and we decided that it was time to package up the past year's worth of GATK improvements into a new major release, which we're calling version `4.1.0.0`!\r\n\r\nTo commemorate this milestone, we'll be publishing a series of in-depth technical articles and blog posts covering the major new features in version `4.1.0.0` on the [official GATK blog](https://software.broadinstitute.org/gatk/blog).\r\n\r\nBelow we've compiled the highlights of the new features added between versions `4.0.0.0` and `4.1.0.0`. If you're interested in seeing *only* the changes between the last release (`4.0.12.0`) and this release (`4.1.0.0`), click [here](#previous-version-diff) instead.\r\n\r\nOfficial docker image is here: [https://hub.docker.com/r/broadinstitute/gatk/](https://hub.docker.com/r/broadinstitute/gatk/)\r\n\r\n## Major changes between versions 4.0.0.0 and 4.1.0.0 (January 2018 to January 2019):\r\n------\r\n\r\n* **Next-Gen VQSR Replacement For Single-Sample**\r\n    * New suite of tools `CNNScoreVariants`, `CNNVariantTrain`, `CNNVariantWriteTensors`, and `FilterVariantTranches`\r\n    * `CNNScoreVariants` is now out of beta and ready for production use\r\n    * Performs variant training and scoring using a convolutional neural network. \r\n    * Single-sample only\r\n    * Produces better results than the legacy `VariantRecalibrator` (VQSR) and comparable or better results to third-party tools like `DeepVariant`\r\n    * Sophisticated 2D model that uses the reads\r\n\r\n* **Major HaplotypeCaller Improvements**\r\n    * Now genotypes and outputs spanning deletions\r\n    * Now outputs VCF spec-compliant phased variants\r\n    * Can emit MNPs via a new `--max-mnp-distance` argument\r\n    * Important fix to the reference confidence calculation upstream of indels\r\n    * New `HaplotypeCaller` priors for variants sites and homRef blocks\r\n        * Added new `--population-callset` argument allowing an external panel of variants to be specified to inform the frequency distribution underlying the genotype priors\r\n        * Added new `--num-reference-samples-if-no-call` argument to control whether to infer (and with what effective strength) that only reference alleles were observed at sites not seen in any panel\r\n\r\n* **Major Mutect2 Improvements**\r\n    * `Mutect2` is now out of beta\r\n    * Support for multi-sample calling\r\n    * Lots of support for high-depth calling such as cfDNA, UMIs, mitochondria, including a new active region likelihood, probabilistic assembly graph pruning that adjusts to the local depth, a new mitochondria mode, and new filters for blood biopsy and mitochondria\r\n    * Now outputs VCF spec-compliant phased variants\r\n    * Can emit MNPs via a new `--max-mnp-distance` argument\r\n    * Added a genotype given alleles (GGA) mode\r\n    * New STR indel error model that improves sensitivity and precision in STR (short-tandem repeat) contexts\r\n    * Many new/improved filters to reduce false positives (eg., `FilterAlignmentArtifacts`) \r\n    * Mutect2 now automatically recognizes and removes end repair artifacts in regions with inverted tandem repeats.  This is extremely important for some FFPE samples.\r\n    * New probabilistic orientation bias tool\r\n    * Got rid of many questionable indels showing up in bamout of Mutect2 and the HaplotypeCaller\r\n    * Big improvements to CalculateContamination, especially when tumor has lots of CNVs\r\n    * NIO support in Mutect2 WDL\r\n    * Significant speed improvements\r\n    * Improved allele fraction estimation\r\n    * Initial GVCF output support\r\n\r\n* **Mitochondrial Calling** \r\n    * Added `--mitochondria-mode` to `Mutect2` and `FilterMutectCalls`. This increases sensitivity and only applies filters that are optimized for mitochondria. \r\n\r\n* **New allele frequency / qual score model**\r\n    * Is now the default in `HaplotypeCaller` and `GenotypeGVCFs`\r\n    * Optimized for greater speed, should resolve many `GenotypeGVCFs` memory issues\r\n    * Rare numerical finite precision issues in the allele-specific qual have been resolved\r\n\r\n* **Major Improvements to the CNV (Copy Number Variation) tools**\r\n    * The CNV tools are now out of beta. \r\n        * This includes the tools: `AnnotateIntervals`, `CallCopyRatioSegments`, `CollectAllelicCounts`, `CollectReadCounts`, `CreateReadCountPanelOfNormals`, `DenoiseReadCounts`, `DetermineGermlineContigPloidy`, `FilterIntervals`, `GermlineCNVCaller`, `ModelSegments`, `PostprocessGermlineCNVCalls`, `PreprocessIntervals`, `PlotDenoisedCopyRatios`, and `PlotModeledSegments`\r\n    * Completed the `GermlineCNVCaller` (gCNV) pipeline and made various performance/runtime improvements to both the methods and WDLs.\r\n    * Major changes include the addition of new tools (`PostprocessGermlineCNVCalls`, `FilterIntervals`, and `CollectReadCounts`, which replaces `CollectFragmentCounts`), as well as improvements to existing tools (notably, `AnnotateIntervals`).\r\n    * Improved support for various formats, namely VCF output in the gCNV pipeline, IGV-compatible .seg output in the `ModelSegments` somatic CNV pipeline, and CRAM support for all CNV WDLs.\r\n    * Developed tools and WDLs for tagging and filtering of germline events in the `ModelSegments` somatic CNV pipeline.\r\n\r\n* **Funcotator Official Release**\r\n    * Funcotator is now out of beta\r\n    * Huge number of bug fixes and accuracy improvements. Output for several fields is now more correct than other well-known functional annotation tools.\r\n    * Some new features include:\r\n        * MAF output support\r\n        * NIO support for datasources\r\n        * gnomAD support\r\n        * dbsnp support\r\n        * Support for Mitochondrial amino acid sequence/protein change strings\r\n        * 5'/3' flank support\r\n        * Major performance improvements due to added caching\r\n        * Added ALL mode for transcript selection (`--transcript-selection-mode ALL`) which will output full annotation fields for all transcripts\r\n    * Created a new `FuncotatorDataSourceDownloader` tool to download data sources\r\n    * Added an experimental `FilterFuncotations` tool\r\n\r\n* **MarkDuplicatesSpark is now a Validated, Scalable Replacement for MarkDuplicates**\r\n    * MarkDuplicatesSpark is now out of beta\r\n    * Rewritten version of the tool matches Picard `MarkDuplicates` output and has greatly improved performance and scalability\r\n    * Supports multiple BAM inputs\r\n    * Indexes BAM outputs on-the-fly in parallel on a cluster\r\n\r\n* **Additional Tools Ported from GATK3**\r\n    * Ported `VariantAnnotator`\r\n    * Ported `VariantEval`\r\n    * Ported `FastaAlternateReferenceMaker` and `FastaReferenceMaker`\r\n    * Ported `LeftAlignAndTrimVariants`\r\n    * Restored `GenotypeGVCFs` `--include-non-variant-sites` argument\r\n\r\n* **Major Improvements to the SV (Structural Variation) Tools**\r\n    * Improvements to collection and calling of events based on discordant read pair evidence.\r\n    * A new scaffolding algorithm greatly improves the contiguity of local assemblies, increasing sensitivity.\r\n    * Regions of excessive sequencing depth are excluded from evidence collection and assembly, improving runtime performance.\r\n    * A major overhaul of our algorithm for calling events based on local assemblies improves accuracy and allows for the accurate reporting of small complex SVs.\r\n    * A machine learning (xgBoost) based classifier for SV evidence improves runtime and increases accuracy by determining which regions should be fed into the local assembly workflow.\r\n\r\n* **Spark Improvements**\r\n    * New [Disq](https://github.com/disq-bio/disq) Spark library allows faster and more accurate loading of formats like BAM and VCF\r\n    * `HaplotypeCallerSpark` now has a \"strict mode\" that closely matches the regular `HaplotypeCaller`\r\n    * Created `RevertSamSpark`, a parallelized Spark version of Picard's `RevertSam` tool\r\n    * Migrated most Spark tools that take a reference and/or VCF to use Spark's intrinsic file copying mechanism instead of broadcast to distribute the reference and VCFs to worker nodes -- a big performance win!\r\n\r\n* **GenomicsDB Improvements**\r\n    * Allele-specific annotation support\r\n    * Multi-interval support (with some performance caveats)\r\n    * Support for sites-only queries\r\n    * Support for returning the GT field in queries\r\n    * New protobuf-based API to allow configuration without editing JSON files\r\n    * Added in machinery to allow per-annotation combine operations to be specified\r\n    * Allow for hdfs and gcs URI's to be passed to GenomicsDB\r\n    * Migrated from `com.intel.genomicsdb` to `org.genomicsdb`\r\n\r\n* **\"Goodies\" Worth Mentioning**\r\n    * Added fasta.gz support to the `-R/--reference` argument in walker tools\r\n    * `SelectVariants` can now drop specific annotation fields from the output vcf\r\n    * `CalculateGenotypePosteriors` now supports indels\r\n    * New tool `ReblockGVCF` to merge reference blocks in single-sample GVCFs for smaller filesizes\r\n    * Improved MQ calculation accuracy, especially at sites with many uninformative reads; concomitant with new annotation tag and format\r\n    * The `-L` argument now supports GCS (Google Cloud Storage) for interval list files / bed / vcf files in walker tools\r\n    * Added support for \"Requester Pays\" GCS (Google Cloud Storage) buckets via new `--gcs-project-for-requester-pays` argument\r\n    * Added GCS (Google Cloud Storage) output (-O) support to more tools\r\n    * Improved Python integration (eliminated timeouts and reliance on prompt synchronization) means fewer glitches during runs of ML-based tools\r\n    * A significantly (~33%) smaller GATK docker image\r\n    * Changed argument tagging syntax from \"--arg tag:value\" to \"--arg:tag value\"\r\n        * Affects command-line interface for `VariantRecalibrator`, `VariantEval`, `VariantFiltration`, and `VariantAnnotator`\r\n\r\n\r\n## <a id=\"previous-version-diff\">Changes between versions 4.0.12.0 and 4.1.0.0 *only*:</a>\r\n------\r\n\r\n* Many tools are now out of beta and ready for production use!\r\n    * `CNNScoreVariants` is out of beta (#5548)\r\n    * `Funcotator` and `FuncotatorDataSourceDownloader` are out of beta (#5621)\r\n    * `MarkDuplicatesSpark` is out of beta (#5603)\r\n    * CNV tools are out of beta (#5596). This includes: `AnnotateIntervals`, `CallCopyRatioSegments`, `CollectAllelicCounts`, `CollectReadCounts`, `CreateReadCountPanelOfNormals`, `DenoiseReadCounts`, `DetermineGermlineContigPloidy`, `FilterIntervals`, `GermlineCNVCaller`, `ModelSegments`, `PostprocessGermlineCNVCalls`, `PreprocessIntervals`, `PlotDenoisedCopyRatios`, and `PlotModeledSegments`\r\n\r\n* New tools:\r\n    * Added ports of `FastaAlternateReferenceMaker` and `FastaReferenceMaker` from GATK3 (#5549)\r\n    * `RevertSamSpark`: a parallelized, Spark-based implementation of `RevertSam` from Picard (#5395)\r\n    * `CompareIntervalLists`: simple new tool to compare interval lists (#3702)\r\n    * `CountBasesInReference`: simple new tool to count bases in a reference file (#5549)\r\n    * `PrintBGZFBlockInformation`: a tool to dump information about blocks in a BGZF file (#4239)\r\n\r\n* `Mutect2`\r\n    * Mutect2 now works with multiple tumor and normal samples! (#5560)\r\n    * First iteration of a reference confidence GVCF-like output for Mutect2 to enable mitochondrial joint calling (#5312)\r\n    * Changed default blocking and NON-REF LOD params for Mutect2 GVCF mode (#5615)\r\n    * Changed defaults for mitochondria mode now that we have adaptive pruning (#5544)\r\n    * Fixed an edge case bug when Mutect2 sees a variant with population AF = 1 (#5535)\r\n    * Fixed an edge case of zero-depth in `FilterMutectCalls` germline filter (#5578)\r\n    * Fixed an edge case for the Mutect2 germline resource (#5563)\r\n    * Tweaked the Mutect2 germline filter (#5595)\r\n    * Put new orientation bias model in Mutect2 NIO wdl (#5580)\r\n    * Improve proposed tumor in normal docs to account for new Mutect2 options (#5555)\r\n\r\n* Added a copy of the mitochondria best practices pipeline (#5566) (#5612)\r\n\r\n* `HaplotypeCaller`\r\n    * New allele frequency / qual score model is now the default in HaplotypeCaller and GenotypeGVCFs (#5484)\r\n    * Simplified and sped `KBestHaplotypeFinder` by replacing recursion with Dijkstra's algorithm (#5462) (#5554)\r\n    * Forward input BAM @PG header lines to `-bamout` output BAM (#3065)\r\n    * Small performance improvement in GVCF mode (#5470)\r\n\r\n* `CNV Tools`\r\n    * Out of beta, as mentioned above! (#5596)\r\n    * Added per-sample denoised coverage output to gCNV (#5584)\r\n    * `ModelSegments`: Added separate allele-count thresholds for the normal and tumor (#5556)\r\n    * `ModelSegments`: Added `MinibatchSliceSampler` and replaced naive subsampling (#5575)\r\n    * Restored array output in gCNV WDLs for efficient postprocessing. (#5490)\r\n\r\n* Changed tagged argument syntax from `--argument tag:value` to `--argument:tag value` (#5526)\r\n    * For example, `--resource known,known=true,prior=10.0:myFile` becomes `--resource:known,known=true,prior=10.0 myFile`\r\n    * This change affects `VariantRecalibrator`, `VariantEval`, `VariantFiltration`, and `VariantAnnotator`\r\n\r\n* `Funcotator`\r\n    * Out of beta, as mentioned above! (#5621)\r\n    * New datasource release that fixes many issues and adds `gnomAD` support (#5614)\r\n    * VCF Data Sources now preserve the FILTER field (#5598)\r\n    * Funcotator now gets the NCBI build version from the datasource config file (#5522)\r\n    * Funcotator now ignores transcript version numbers when matching on transcript ID (#5557)\r\n    * Funcotator now uses the GATK-wide version number (#5520)\r\n    * Updated Funcotator tool documentation (#5620)\r\n\r\n* `MarkDuplicatesSpark`\r\n    * Out of beta, as mentioned above! (#5603)\r\n    * Added the ability for MarkDuplicatesSpark to accept multiple bam inputs (#5430)\r\n    * Fixed MarkDuplicateSpark mutex argument references (#5538)\r\n\r\n* Spark tools\r\n    * Support for distributed BAI index creation, and option for enabling or disabling writing BAI and SBI files on Spark (#5485)\r\n    * Get `HaplotypeCallerSpark` \"strict mode\" running on an exome (#5475)\r\n    * Added an option for enabling or disabling writing tabix indexes for bgzipped VCF files from Spark (#5574)\r\n    * Fixed overflow bug in `GatkSparkTool.getRecommendedNumReducers()` (#5586)\r\n\r\n* `GenomicsDB`\r\n    * Migrated from `com.intel.genomicsdb` to `org.genomicsdb` (#5587) (#5608)\r\n    * GenomicsDB now matches CombineGVCFs with input spanning deletions (#5397)\r\n    * Define GenomicsDB \"partitions\" over the span of the input intervals in order to dramatically improve exome performance (#5540)\r\n\r\n* Miscellaneous Changes\r\n    * Added liftover wdls and jsons for gnomAD 2.1 (#5604)\r\n    * Added script to create Hg38 to B37 liftover chain (#5579)\r\n    * Allow variant walkers to configure their caching behavior (#3480)\r\n    * Bug fix for using a `ReservoirDownsampler` with a `ReadsDownsamplingIterator` (#5594)\r\n    * Started migration to a new URI abstraction (#5526)\r\n    * Fixed inclusion of default read filters in GATK documentation (#5576)\r\n    * Put the actual date/time in the generated GATK documentation (#5567)\r\n    * Pair-HMM alignment algorithm description fix (#5528)\r\n    * Make ReadFilter and Annotation packages configurable (#5573)\r\n    * Fix to make `gatk --version` print the version instead of throwing an exception (#5537)\r\n    * Added warning message reminding user to add the allele specific annotation group when needed (#3042)\r\n    * Fix for intermittent `LeftAlignAndTrimVariants` test failures (#5519)\r\n    * Restored link in `VariantFiltration` docs to point to update online JEXL doc. (#5525)\r\n    * Moved `BucketUtils.deleteOnExit()` and `deleteRecursively()` to `IOUtils` (#5332)\r\n    * Source the tab completion script in the GATK docker image (#5552)\r\n    * Added GATK jar to CLASSPATH in docker image (#3866)\r\n    * Updated travis github badge link (#5617)\r\n    * Removed offline CRAN repository from build (#5593)\r\n\r\n* Dependencies\r\n    * Updated htsjdk to version 2.18.2 (#5585)\r\n    * Updated picard to version 2.18.25 (#5597)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.1.0.0",
        "name": "4.1.0.0",
        "release_id": 15255621,
        "tag": "4.1.0.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.1.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/15255621",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/15255621",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.1.0.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2018-12-17T18:40:43Z",
        "date_published": "2018-12-17T18:47:57Z",
        "description": "Highlights of this release include support for outputting phased variants in `HaplotypeCaller`/`Mutect2`, restoring the `--include-non-variant-sites` argument to `GenotypeGVCFs`, a port of the GATK3 tool `VariantEval`, a new library (Disq, https://github.com/disq-bio/disq) for working with BAM/CRAM/VCF/etc. formats on Spark, and GCS (Google Cloud Storage) support in `Funcotator`.\r\n\r\nAs usual, a docker image for this release can be downloaded from https://hub.docker.com/r/broadinstitute/gatk/\r\n\r\nFull list of changes in this release:\r\n\r\n* `HaplotypeCaller`/`Mutect2`\r\n    * Output VCF spec-compliant phased variants in HaplotypeCaller and Mutect2\r\n    * Added an experimental adaptive pruning option for local assembly (#5473)\r\n    * Improved implementation of allele-specific new qual (#5460)\r\n    * Use cigar complexity to break ties in uninformative reads' best haplotypes (#5359)\r\n    * Improved handling of regions that are too short after trimming in HaplotypeCaller and in Mutect2 (Closes issue #5079)\r\n    * Optimization in `CigarUtils` to shortcut to M-only CIGAR when provably optimal (#5466)\r\n    * Changed SUPPORTED_ALLELES_TAG from SA to XA (#5418)\r\n\r\n* `HaplotypeCaller`\r\n    * Fixed bug in GGA mode caused by split multallic sites with genotypes (#5365)\r\n    * The debug command line argument is now passed correctly in HaplotypeCaller (fixed issue #4943) (#5455)\r\n\r\n* `Mutect2`\r\n    * Big improvements to CalculateContamination's model for determining hom alt sites (#5413)\r\n    * Reduce false negatives from mapping quality filter on long indels in Mutect2 (#5497)\r\n    * Added a mismatch ratio option in realignment filter (#5501)\r\n    * Made Mutect2 read position filter default much less stringent (#5487)\r\n    * Fixed M2 bug for germline resources with AF=. (#5442)\r\n    * Fix read position annotation bug in M2 filter (#5495)\r\n    * Cleaner Mutect2 VCF fields (#5510)\r\n    * Moved PerAlleleAnnotations to the INFO field (#5518)\r\n    * Removed unnecessary inheritance of M2 filtering arguments collection (#5498)\r\n\r\n* `GenotypeGVCFs`\r\n    * Restored the --include-non-variant-sites argument from GATK3 to GenotypeGVCFs (#5219)\r\n\r\n* Ported the GATK3 tool `VariantEval` to GATK4 (#5043)\r\n\r\n* Replaced the Hadoop-BAM library with the newly-developed Disq library (https://github.com/disq-bio/disq) for efficiently working with BAM/CRAM/VCF/etc. formats on Spark (#5138)\r\n    * Improves Spark performance across-the-board, and fixes many edge-case bugs in Hadoop-BAM\r\n\r\n* `Funcotator`\r\n    * Added GCS support to Funcotator data sources, so that data sources can now be accessed directly from GCS buckets (#5425)\r\n    * Added support for annotating 5'/3' flanks (#5403)\r\n    * Funcotator now creates default annotations for difficult variants. (#5374)\r\n    * Funcotator now can create annotations for symbollic alleles and masked alleles (#5406)\r\n    * Funcotator now can match between hg19 and b37 data sources. (#5491)\r\n    * Added in regression tests and fixes for correctness of many annotations (#5302)\r\n    * Now DE_NOVO_START_IN_FRAME and DE_NOVO_START_OUT_FRAME are correct. (#5357)\r\n    * Added cDNA Strings for Intronic Variants (#5321)\r\n    * VCF data sources create an ID field for the ID of the variant\r\nused for the annotation (#5327)\r\n    * Funcotator now computes MT protein changes. (#5361)\r\n    * Funcotator now correctly populates transcript position. (#5380)\r\n    * Added a script that can create data sources from BED files. (#5438)\r\n    * Updated testing Gencode data sources to fully exercise test data set (#5423)\r\n    * Moved validation test data out of large files area. (#5381)\r\n    * Updated top-level class documentation for Funcotator. (#4655)\r\n    * Added scripts to liftover gnomAD. Also bugfixes for Funcotator NIO. (#5514)\r\n\r\n* `HaplotypeCallerSpark`\r\n    * Added a \"strict mode\" that allows `HaplotypeCallerSpark` to closely match the output of the regular `HaplotypeCaller` (#5416)\r\n    * Now extends AssemblyRegionWalkerSpark (#5386)\r\n\r\n* `MarkDuplicatesSpark`: Added a few of the remaining unimplemented useful features from Picard (#5377)\r\n\r\n* `CNV workflows`\r\n    * Changed `FilterIntervals` to operate on the intersection of intervals in all inputs. (#5408)\r\n    * Fixed RAM usage parameter error in combine_tracks.wdl (#5358)\r\n    * Various other improvements to combine_tracks.wdl (#5384)\r\n    * Fixed gCNV WDL broken by Cromwell update on FireCloud. (#5407)\r\n    * Replaced bash script in gCNV ScatterIntervals task with updated version of IntervalListTools. (#5414)\r\n\r\n* `CNNScoreVariants`\r\n    * Check for and require hardware AVX support (#5291)\r\n\r\n* Changed `SelectVariants` so that it can handle multiple rsIDs separated by ';' in a VCF file (#5464)\r\n\r\n* Miscellaneous Changes\r\n    * Added `setIsUnplaced()` to the `GATKRead` API to distinguish reads with no mapping information (#5320)\r\n    * Fixed an integer overflow bug in the `RMSMappingQuality` annotation (#5435)\r\n    * Fixed floating-point bug in MannWhitneyU on some JVMs. (#5371)\r\n    * Standardized the output argument for `LeftAlignIndels` (#5474)\r\n    * `SplitIntervals` now produces an `.interval_list` file (#5392)\r\n    * Fixed a bug with GATK_GCS_STAGING in the GATK launcher script #1338 (#5452)\r\n    * Added ExampleReadWalkerWithVariantsSpark.java and tests (#5289)\r\n    * Add description getter and javadoc in GATKReportTable (#5443)\r\n    * Fixed message in GATKAnnotationPluginDescription (#5444)\r\n    * Replaced some uses of PrintWriter (#5461)\r\n    * Refactor GVCFWriter to allow push/pull iteration. (#5311)\r\n    * Add scripts/dataproc-cluster-ui to release bundle. (#5401)\r\n    * Marked `VariantAnnotator` as a `@DocumentedFeature` (#5480)\r\n    * Removed obsolete intel conda environment references. (#5482)\r\n    * Deleted the CountSet class (#5467)\r\n    * Test framework: disabled gcloud login on travis for non-cloud non-wdl tests (#5335)\r\n    * Updated Spark scripts to reflect changes from #5386 and #5127. (#5415)\r\n    * Fixed jexl logging and updated VariantFiltration doc. (#5422)\r\n    * Fixed some dead links in the README (#5405)\r\n\r\n* Dependencies\r\n    * Updated htsjdk to 2.18.1 (#5486)\r\n    * Updated Picard to 2.18.16. (#5412)\r\n    * Updated Intel-GKL dependency to 8.6 (#5463)",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.0.12.0",
        "name": "4.0.12.0",
        "release_id": 14577093,
        "tag": "4.0.12.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.0.12.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/14577093",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/14577093",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.0.12.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "lbergelson",
          "type": "User"
        },
        "date_created": "2018-10-23T15:15:54Z",
        "date_published": "2018-10-23T20:22:02Z",
        "description": "A release which includes major improvements to Mitochondrial calling in Mutect2 as well as bug fixes and improvements:\r\n\r\nAs always a docker is available here: https://hub.docker.com/r/broadinstitute/gatk/\r\n\r\nMutect2 and HaplotypeCaller changes:\r\n*  Added `--mitochondria-mode` to `Mutect2` and `FilterMutectCalls`. This increases sensitivity and only applies filters that are optimized for mitochondria. A best practices WDL for calling mitochondrial variants on WGS data will be available in the future.  (#5193)\r\n\r\n* Strand based annotations will use both reads in an overlapping read pair  (#5286)\r\n* Realignment filter annotates the VCF with passing and failing read counts (#5328)\r\n* New filters and annotation to support blood biopsy that count and filter based on N's at variant sites (#5317)\r\n* Fixed bug for M2 GGA alleles with zero coverage (#5303)\r\n* Fixed error in genotype given alleles mode when input alleles have genotypes (#5341)  #5336 \r\n* Add new annotations to bamout to make understanding calls easier (#5215)\r\n* Fixed a typo.\r\n\r\nCNV Pipeline:\r\n* Added FilterIntervals to perform annotation-based and count-based filtering in the gCNV pipeline. (#5307) closes #2992 #4558\r\n\r\nSpark:\r\n* Removed WellformedReadFilter from CountReadsSpark (#5329)\r\n* Support fasta.gz in GATKSparkTool (#5290) closes #5258\r\n\r\nOther:\r\n* CNN variant update models validate scores cleanup training (#5175)\r\n* combine_tracks.wdl supports GISTIC2 conversion (and bugfix) (#5287) closes #5284 #5283\r\n* handle normal reads in validation sample in BasicSomaticValidator (#5322)\r\n\r\nGenomicsDB:\r\n* Allow for hdfs and gcs URI's to be passed to GenomicsDB (#5197)\r\n\r\nSelectVariants:\r\n* Enable SelectVariants to drop specific annotation fields from output vcf. (#5254) closes #5235\r\n\r\nSplitNCigarReads:\r\n* Added defensive check to OverhangFixingManager splices for non-reference spanning reads (#5298) closes #5293\r\n* Fixed SplitNCigarReads ArrayIndexOutOfBounds error for reads with long deletions (#5285) closes #5230\r\n\r\nTesting:\r\n* Added a toggle to update the expected outputs in HaplotypeCallerIntegrationTest (#5324) \r\n* Added a new servicekey.json for travis (#5308) closes #5305  \r\n* Added full-sized B37 and HG38 references to our large test data (#5309) closes #5111  \r\n* Added in new data sources for funcotator testing. (#5296)\r\n\r\n\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.0.11.0",
        "release_id": 13612877,
        "tag": "4.0.11.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.0.11.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/13612877",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/13612877",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.0.11.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2018-10-09T19:01:46Z",
        "date_published": "2018-10-09T19:29:12Z",
        "description": "This is a small release that improves the calculation of the `MQ` (mapping quality) annotation, which provides an estimate of the overall mapping quality of reads supporting a variant call. It also introduces a number of experimental improvements to the CNV workflows, as well as a bug fix to `LocusWalkerSpark`.\r\n\r\nAs usual, a docker image for this release can be downloaded from https://hub.docker.com/r/broadinstitute/gatk/\r\n\r\nFull list of changes in this release:\r\n\r\n*  Improve MQ calculation accuracy (#4969)\r\n    * Change raw MQ to a tuple of (sumSquaredMQs, totalDepth) for better accuracy where there are lots of uninformative reads or called single-sample variants with homRef genotypes.  \r\n    * Note that incorporating this change into a pipeline will require a concomitant update to this version for GenomicsDBImport and GenotypeGVCFs.\r\n\r\n*  Updated `SimpleGermlineTagger` and somatic CNV experimental post-processing workflow with several experimental changes that improve precision results, and expand possible evaluations, of GATK CNV (#5252)\r\n    * New script `combine_tracks.wdl` for post-processing somatic CNV calls.  This wdl will perform two operations:\r\n        * Increases precision by removing:\r\n            * germline segments.  As a result, the WDL requires the matched normal segments.\r\n            * Areas of common germline activity or error from other cancer studies.\r\n        * Converts the tumor model seg file to the same format as AllelicCapSeg, which can be read by ABSOLUTE.  This is currently done inline in the WDL.  \r\n            * This is not a trivial conversion, since each segment must be called whether it is balanced or not (MAF =? 0.5).  The current algorithm relies on hard filtering and may need updating pending evaluation.\r\n            * For more information about AllelicCapSeg and ABSOLUTE, see: \r\n                * Carter et al. *Absolute quantification of somatic DNA alterations in human cancer*, Nat Biotechnol. 2012 May; 30(5): 413\u2013421 \r\n                * https://software.broadinstitute.org/cancer/cga/absolute \r\n                * Brastianos, P.K., Carter S.L., et al. *Genomic Characterization of Brain Metastases Reveals Branched Evolution and Potential Therapeutic Targets* (2015) Cancer Discovery PMID:26410082\r\n    * Changes to GATK tools to support the above:\r\n        * `SimpleGermlineTagger` now uses reciprocal overlap to in addition to breakpoint matching when determining a possible germline event.  This greatly improved results in areas near centromeres.\r\n        * Added tool `MergeAnnotatedRegionsByAnnotation`.  This simple tool will merge genomic regions (specified in a tsv) when given annotations (columns) contain exact values in neighboring segments and the segments are within a specified maximum genomic distance.  \r\n    * New scripts `multi_combine_tracks.wdl` and `aggregate_combine_tracks.wdl` which run `combine_tracks.wdl` on multiple pairs and combine the results into one seg file for easy consumption by IGV.\r\n\r\n*  `LocusWalkerSpark`: fix issue where intervals with no reads were being dropped (#5222)\r\n    * This fixes the bug reported in https://github.com/broadinstitute/gatk/issues/3823\r\n\r\n*  Added `SparkTestUtils.roundTripThroughJavaSerialization()` method for better serialization testing on Spark (#5257)\r\n\r\n*  Build system: set the same compiler flags for all gradle JavaCompile tasks (#5256)",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.0.10.1",
        "name": "4.0.10.1",
        "release_id": 13336631,
        "tag": "4.0.10.1",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.0.10.1",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/13336631",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/13336631",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.0.10.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "droazen",
          "type": "User"
        },
        "date_created": "2018-10-03T22:31:22Z",
        "date_published": "2018-10-03T22:36:12Z",
        "description": "Highlights of this release include a new tool `ReblockGVCF`, a bug fix for a crash in `Mutect2`, and a more efficient distribution mechanism for the reference and VCFs in Spark tools.\r\n\r\nAs usual, a docker image for this release can be downloaded from https://hub.docker.com/r/broadinstitute/gatk/\r\n\r\nFull list of changes in this release:\r\n\r\n* Added a new experimental tool `ReblockGVCF` (#4940)\r\n    * A tool to merge reference blocks in single-sample GVCFs for smaller filesizes\r\n\r\n* `Mutect2`: \r\n    * Fixed a bug in the `PalindromeArtifactClipReadTransformer` (#5241)\r\n        * This filter would crash with an out-of-bounds error for fragment lengths and/or mate start positions that went off the end of a contig.\r\n    * Changed the way the log10AlleleFractions are calculated in `SomaticLikelihoodsEngine`: now we use the mean of the posterior of the allele fractions. (#5231)\r\n    * Reword comments in Mutect2 WDL to not refer to the old orientation bias filter as deprecated. (#5196)\r\n    * Cited CGA in Mutect docs (#5228)\r\n\r\n* `HaplotypeCaller`: Allow MNP calling in GVCF mode with stern warnings about not trying joint-genotyping from the resulting GVCFs. (#5182)\r\n    * `HaplotypeCaller` will now allow you to output MNPs in GVCF mode with a warning, however since joint genotyping of MNPs is unsupported, `CombineGVCFs` and `GenomicsDBImport` will now refuse to process GVCFs containing MNPs.\r\n\r\n* `GATK Spark tools`:\r\n    * Migrated most Spark tools that take a reference and/or VCF to use Spark's intrinsic file copying mechanism instead of broadcast to distribute the reference and VCFs to worker nodes (#5127) (#5221)\r\n        * This improves the performance of Spark tools that take a reference and/or VCF as side inputs, as the new distribution mechanism doesn't load the entire contents of the files into memory like broadcast did.\r\n        * As a side effect of this change, support for 2bit references has been removed from tools that were migrated to the new distribution mechanism (in particular, `BaseRecalibratorSpark` and `HaplotypeCallerSpark`).\r\n        * The CNV Spark tools have not yet been migrated, and still support 2bit references for now.\r\n    * Bug fix: ensure that intervals with no reads are not dropped by the `SparkSharder` (#5248)\r\n\r\n* `Funcotator`:\r\n    * Added command line exclusion lists, so that users can prune fields from the output. (#5226)\r\n    * Added Funcotator excluded fields option explicitly to the M2 WDLs. (#5242)\r\n\r\n* Fix a multithreaded race condition in `GenotypeLikelihoodCalculators` by synchronizing updates of shared genotype likelihood tables. (#5071)\r\n    * This bug affected `HaplotypeCallerSpark`, but not the regular `HaplotypeCaller`\r\n\r\n* `GenomicsDB`: added in machinery to allow per-annotation combine operations to be specified (#4993)\r\n\r\n* `GATK Engine`: Hooked up `CountingVariantFilter` to `VariantWalkers` (#4954)\r\n\r\n* `StreamingPythonScriptExecutor`: added a new message to the `StreamingProcessController` ack FIFO protocol to allow additional message detail to be passed as part of a negative ack. (#5170)\r\n    * This improves exception message propagation for fatal errors when running Python tools.\r\n\r\n* `gCNV WDLs`: \r\n    * Tar calls from all samples. (#5225)\r\n        * This fixes an issue where the gCNV WGS cohort germline WDL was outputting vcf files with names that do not correspond to the actual samples inside the files.\r\n    * Added multi-sample functionality to gCNV case mode WDL, and added a wrapper for gCNV case mode WDL to help optimize cloud computation cost. Also optimized how data is sent to postprocessing task in gCNV WDLs. (#5176)\r\n\r\n* `gCNV kernel`: Enforced ViterbiSegmentationEngine to analyze single samples only (#5176)\r\n\r\n* Added a `dataproc-cluster-ui` script to easily open the Spark UI on dataproc clusters (#5188)\r\n\r\n* Fixed pom issues that prevented publishing to maven central (#5224)\r\n\r\n* Added `tabix` to the docker base image (#5247)\r\n",
        "html_url": "https://github.com/broadinstitute/gatk/releases/tag/4.0.10.0",
        "name": "4.0.10.0",
        "release_id": 13233696,
        "tag": "4.0.10.0",
        "tarball_url": "https://api.github.com/repos/broadinstitute/gatk/tarball/4.0.10.0",
        "type": "Release",
        "url": "https://api.github.com/repos/broadinstitute/gatk/releases/13233696",
        "value": "https://api.github.com/repos/broadinstitute/gatk/releases/13233696",
        "zipball_url": "https://api.github.com/repos/broadinstitute/gatk/zipball/4.0.10.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "<a name=\"intellij_gradle_refresh\">Updating the Intellij project when dependencies change</a>",
        "parent_header": [
          "<a name=\"developers\">For GATK Developers</a>"
        ],
        "type": "Text_excerpt",
        "value": "If there are dependency changes in `build.gradle` it is necessary to refresh the gradle project. This is easily done with the following steps.\n\n* Open the gradle tool window  ( \"View\" -> \"Tool Windows\" -> \"Gradle\" )\n* Click the refresh button in the Gradle tool window.  It is in the top left of the gradle view and is represented by two blue arrows.\n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "contact",
    "contributors",
    "faq",
    "support",
    "identifier"
  ],
  "somef_provenance": {
    "date": "2024-11-04 06:01:06",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1701
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "<a name=\"quickstart\">Quick Start Guide</a>",
        "type": "Text_excerpt",
        "value": "* Build the GATK: `./gradlew bundle` (creates `gatk-VERSION.zip` in `build/`)\n* Get help on running the GATK: `./gatk --help`\n* Get a list of available tools: `./gatk --list`\n* Run a tool: `./gatk PrintReads -I src/test/resources/NA12878.chr17_69k_70k.dictFix.bam -O output.bam`\n* Get help on a particular tool: `./gatk PrintReads --help`\n"
      },
      "source": "https://raw.githubusercontent.com/broadinstitute/gatk/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "workflows": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_wdl/ReadsPipelineSpark.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/spark_wdl/ReadsPipelineSparkMulticore.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator_wdl/funcotator.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/cnv_wdl/somatic/cnv_somatic_funcotate_seg_workflow.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/cnv_wdl/somatic/cnv_somatic_pair_workflow.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/cnv_wdl/somatic/cnv_somatic_panel_workflow.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/cnv_wdl/somatic/cnv_somatic_oncotator_workflow.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/cnv_wdl/germline/joint_call_exome_cnvs.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/cnv_wdl/germline/cnv_germline_case_workflow.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/cnv_wdl/germline/cnv_germline_case_scattered_workflow.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/mitochondria_m2_wdl/MitochondriaPipeline.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/mitochondria_m2_wdl/AlignmentPipeline.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/mitochondria_m2_wdl/AlignAndCall.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/pathseq/wdl/pathseq_pipeline.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/gnomAD/liftoverVcf.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/gnomAD/subsetGnomadInfoFieldToAlleleFrequencies.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/gnomAD/indexFeatureFile.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/gnomAD/createGnomadAlleleFreqTsv.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/gnomAD/removeInfoFieldAnnotationsFromVcf.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/gnomAD/mergeVcfs.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/gnomAD/indexFastaFile.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/funcotator/data_sources/gnomAD/gatherVcfsCloud.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/vcf_site_level_filtering_wdl/JointVcfFiltering.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/mutect2_wdl/mutect_resources.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/mutect2_wdl/mutect2_pon.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/mutect2_wdl/mutect2.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/unsupported/combine_tracks_postprocessing_cnv/aggregate_combined_tracks.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/unsupported/combine_tracks_postprocessing_cnv/multi_combine_tracks.wdl"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/broadinstitute/gatk/master/scripts/unsupported/combine_tracks_postprocessing_cnv/combine_tracks.wdl"
      },
      "technique": "file_exploration"
    }
  ]
}