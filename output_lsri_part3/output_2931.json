{
  "acknowledgement": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Acknowledgements",
        "parent_header": [
          "NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes"
        ],
        "type": "Text_excerpt",
        "value": "-\nWe have used/modified codes from the following projects:\n  + [CGIntrinsics](https://github.com/zhengqili/CGIntrinsics):\n    + codes for loading data from the CGI, SAW and IIW datasets in ```./data/intrinsics/```.\n    + codes for evaluating shading and reflectance estimation in ```./test/```.\n    (These codes are originally provided by [IIW](http://opensurfaces.cs.cornell.edu/intrinsic/#) \n    and [SAW](http://opensurfaces.cs.cornell.edu/saw/)) \n  + [Revisiting_Single_Depth_Estimation](https://github.com/JunjH/Revisiting_Single_Depth_Estimation):\n    + the network structure of normal estimation module in ```./models/Hu_nets/```\n    \n"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "application_domain": [
    {
      "confidence": 48.44,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citation",
        "parent_header": [
          "NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes"
        ],
        "type": "Text_excerpt",
        "value": "-\nIf you find this code useful for your research, please cite:\n  ```\n  @article{luo2020niid,\n    title={NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes},\n    author={Luo, Jundan and Huang, Zhaoyang and Li, Yijin and Zhou, Xiaowei and Zhang, Guofeng and Bao, Hujun},\n    journal={IEEE Transactions on Visualization and Computer Graphics},\n    year={2020},\n    publisher={IEEE}\n  }\n  ```\n"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Luo, Jundan and Huang, Zhaoyang and Li, Yijin and Zhou, Xiaowei and Zhang, Guofeng and Bao, Hujun",
        "format": "bibtex",
        "title": "NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes",
        "type": "Text_excerpt",
        "value": "@article{luo2020niid,\n    publisher = {IEEE},\n    year = {2020},\n    journal = {IEEE Transactions on Visualization and Computer Graphics},\n    author = {Luo, Jundan and Huang, Zhaoyang and Li, Yijin and Zhou, Xiaowei and Zhang, Guofeng and Bao, Hujun},\n    title = {NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes},\n}"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/zju3dv/NIID-Net"
      },
      "technique": "GitHub_API"
    }
  ],
  "contact": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Contact",
        "parent_header": [
          "NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes"
        ],
        "type": "Text_excerpt",
        "value": "-\nPlease open an issue or contact Jundan Luo (<jundanluo22@gmail.com>) if you have any questions or any feedback."
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-08-28T02:43:55Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-05-12T02:11:39Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Code for \"NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes\" TVCG"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.8924150715060025,
      "result": {
        "original_header": "Updates",
        "type": "Text_excerpt",
        "value": "-\n+ 16/April/2023: Migrated to Pytorch 1.7.1\n \n"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9611402173753295,
      "result": {
        "original_header": "Precomputed results",
        "type": "Text_excerpt",
        "value": "- We provide visualized output on the SAW test set (note that SAW test data includes IIW test data and NYUv2 test data):\n[SAW_pred_imgs.zip](https://drive.google.com/file/d/18LI7CgTW0tVglF0u3Nirp1kZxca25iDJ/view?usp=sharing).\nAnd precision-recall measurements \n([precision-recall_curves.zip](https://drive.google.com/file/d/1WhxxN5sSVLLoet1ruk9VHzh_r-WhfRfs/view?usp=sharing))\nwhich can be used to draw the precision-recall curves. \n- If you want to compare on some applications (e.g., image editing), we strongly recommend using the original float32 output of the network\ninstead of the 8-bit low-precision visualized images. \n"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9966228231394962,
      "result": {
        "original_header": "Copyright",
        "type": "Text_excerpt",
        "value": "-\n```\n  Copyright (c) ZJU-SenseTime Joint Lab of 3D Vision. All Rights Reserved.\n\n  Permission to use, copy, modify and distribute this software and its\n  documentation for educational, research and non-profit purposes only.\n\n  The above copyright notice and this permission notice shall be included in all\n  copies or substantial portions of the Software.\n\n  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n  SOFTWARE.\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/zju3dv/NIID-Net/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 7
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/zju3dv/NIID-Net/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "zju3dv/NIID-Net"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/tools/install.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/./assets/comparison.jpg"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/./assets/demo1.jpg"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 0.9315158741687692,
      "result": {
        "original_header": "Updates",
        "type": "Text_excerpt",
        "value": "-\n+ 16/April/2023: Migrated to Pytorch 1.7.1\n \n"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/zju3dv/NIID-Net/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "intrinsic-image-decomposition, intrinsic-images, pytorch"
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "Copyright (c) ZJU-SenseTime Joint Lab of 3D Vision. All Rights Reserved.\n\nPermission to use, copy, modify and distribute this software and its\ndocumentation for educational, research and non-profit purposes only.\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/LICENSE",
      "technique": "file_exploration"
    }
  ],
  "logo": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/./assets/NIID-Net.png"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "NIID-Net"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "zju3dv"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 282891,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 190,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Dependencies",
        "parent_header": [
          "NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes"
        ],
        "type": "Text_excerpt",
        "value": "-\n+ Python 3.6\n+ PyTorch 1.7.1 (original [version](https://github.com/zju3dv/NIID-Net/tree/pytorch_0.3.1) using PyTorch 0.3.1)\n+ torchvision 0.8.2\n+ [Visdom](https://github.com/facebookresearch/visdom) 0.1.8.9 \n+ We provide the ```requirements.txt``` file for other dependencies.\n"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Running",
        "parent_header": [
          "NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes"
        ],
        "type": "Text_excerpt",
        "value": "-\n+ ##### Configuration\n  + ```options/config.py``` is the configuration file:\n    + ```TestOptions``` for test\n    + ```TrainIIDOptions``` for training the IID-Net\n  + Some variables may need to be modified:\n    ```\n      dataset_root #\n      checkpoints_dir # visualized results will be saved here\n      offline # if you do not need Visdom, set it True\n      pretrained_file #\n      gpu_devices # the indexes of GPU devices, or set None to run CPU version \n      batch_size_intrinsics # batch size for training on the CGIntrinsics dataset\n    ```\n  + Note that only test mode supports CPU version (with ```gpu_devices=None```). \n  We recommend you to use the **GPU** version.\n+ ##### Pre-trained model\n    [Google Drive](https://drive.google.com/file/d/160NzDEmC8okb6vgTNTyzmhaYa-Lqo-Ft/view?usp=sharing)\n    (or [Baidu Net Disk](https://pan.baidu.com/s/1n45ZwuYZpUA8vp-9V-ca9Q) with code ```uj3n```)\n    \n+ ##### Demo\n  + ```python decompose.py```\n  + The default input and output directory is ```./examples/```\n+ ##### Test\n  + ```python evaluate.py```\n  + The default output directory is ```./checkpoints/```\n+ ##### Train\n  + ```python train_IID.py```\n\n\nResults\n- \n#### Precomputed results\n- We provide visualized output on the SAW test set (note that SAW test data includes IIW test data and NYUv2 test data):\n[SAW_pred_imgs.zip](https://drive.google.com/file/d/18LI7CgTW0tVglF0u3Nirp1kZxca25iDJ/view?usp=sharing).\nAnd precision-recall measurements \n([precision-recall_curves.zip](https://drive.google.com/file/d/1WhxxN5sSVLLoet1ruk9VHzh_r-WhfRfs/view?usp=sharing))\nwhich can be used to draw the precision-recall curves. \n- If you want to compare on some applications (e.g., image editing), we strongly recommend using the original float32 output of the network\ninstead of the 8-bit low-precision visualized images.\n\n\n#### Comparison\n![comparison](./assets/comparison.jpg)\n\n#### Image sequence editing\n![editing](./assets/demo1.jpg)\n\nAcknowledgements\n-\nWe have used/modified codes from the following projects:\n  + [CGIntrinsics](https://github.com/zhengqili/CGIntrinsics):\n    + codes for loading data from the CGI, SAW and IIW datasets in ```./data/intrinsics/```.\n    + codes for evaluating shading and reflectance estimation in ```./test/```.\n    (These codes are originally provided by [IIW](http://opensurfaces.cs.cornell.edu/intrinsic/#) \n    and [SAW](http://opensurfaces.cs.cornell.edu/saw/)) \n  + [Revisiting_Single_Depth_Estimation](https://github.com/JunjH/Revisiting_Single_Depth_Estimation):\n    + the network structure of normal estimation module in ```./models/Hu_nets/```\n    \n\nCitation\n-\nIf you find this code useful for your research, please cite:\n  ```\n  @article{luo2020niid,\n    title={NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes},\n    author={Luo, Jundan and Huang, Zhaoyang and Li, Yijin and Zhou, Xiaowei and Zhang, Guofeng and Bao, Hujun},\n    journal={IEEE Transactions on Visualization and Computer Graphics},\n    year={2020},\n    publisher={IEEE}\n  }\n  ```\n\nCopyright\n-\n```\n  Copyright (c) ZJU-SenseTime Joint Lab of 3D Vision. All Rights Reserved.\n\n  Permission to use, copy, modify and distribute this software and its\n  documentation for educational, research and non-profit purposes only.\n\n  The above copyright notice and this permission notice shall be included in all\n  copies or substantial portions of the Software.\n\n  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n  SOFTWARE.\n```\n\nContact\n-\nPlease open an issue or contact Jundan Luo (<jundanluo22@gmail.com>) if you have any questions or any feedback."
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Configuration",
        "parent_header": [
          "NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes",
          "Running"
        ],
        "type": "Text_excerpt",
        "value": "# NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes\n[[paper]](https://ieeexplore.ieee.org/document/9199573) \n[[supplement]](http://www.cad.zju.edu.cn/home/gfzhang/papers/NIID-Net/NIID-Net-supple.pdf)\n[[presentation]](https://youtu.be/BvoYwCdzoZU)\n[[demo]](https://youtu.be/0MadIlfqles) \n\n![architecture](./assets/NIID-Net.png)\n\n\nUpdates\n-\n+ 16/April/2023: Migrated to Pytorch 1.7.1\n\nDependencies\n-\n+ Python 3.6\n+ PyTorch 1.7.1 (original [version](https://github.com/zju3dv/NIID-Net/tree/pytorch_0.3.1) using PyTorch 0.3.1)\n+ torchvision 0.8.2\n+ [Visdom](https://github.com/facebookresearch/visdom) 0.1.8.9 \n+ We provide the ```requirements.txt``` file for other dependencies.\n\nDatasets\n-\n#### Intrinsic image datasets\n+ Follow [CGIntrinsics](https://github.com/zhengqili/CGIntrinsics) to download CGI, IIW and SAW datasets. \nNote that Z. Li and N. Snavely augment the original [IIW](http://opensurfaces.cs.cornell.edu/intrinsic/#) and [SAW](http://opensurfaces.cs.cornell.edu/saw/) datasets.\n+ You do not need to download the CGI dataset if you are not going to train the model.\n+ Put the datasets in the ```./dataset/``` folder. The final directory structure:\n    ```\n    NIID-Net project\n    |---README.md\n    |---...\n    |---dataset\n        |---CGIntrinsics\n            |---intrinsics_final\n            |   |---images   \n            |   |---rendered\n            |   |---...\n            |---IIW\n            |   |---data\n            |   |---test_list\n            |   |---...\n            |---SAW\n                |---saw_images_512\n                |---saw_pixel_labels\n                |---saw_splits\n                |---train_list\n    ```\n\n#### Depth/surface normal dataset\n+ Follow [Revisiting_Single_Depth_Estimation](https://github.com/JunjH/Revisiting_Single_Depth_Estimation) to \ndownload their extracted NYU-v2 subset.\n+ Unzip ```data.zip``` and rename the directory as ```NYU_v2```\n+ To compute surface normal maps,\n  + install ```open3d==0.8.0``` \n  + ```python ./tools/data_preprocess_normal.py --dataset_dir {NYU_v2 dataset path}  --num_workers {number of processes}```\n\n\nRunning\n-\n+ ##### Configuration\n  + ```options/config.py``` is the configuration file:\n    + ```TestOptions``` for test\n    + ```TrainIIDOptions``` for training the IID-Net\n  + Some variables may need to be modified:\n    ```\n      dataset_root #\n      checkpoints_dir # visualized results will be saved here\n      offline # if you do not need Visdom, set it True\n      pretrained_file #\n      gpu_devices # the indexes of GPU devices, or set None to run CPU version \n      batch_size_intrinsics # batch size for training on the CGIntrinsics dataset\n    ```\n  + Note that only test mode supports CPU version (with ```gpu_devices=None```). \n  We recommend you to use the **GPU** version.\n+ ##### Pre-trained model\n    [Google Drive](https://drive.google.com/file/d/160NzDEmC8okb6vgTNTyzmhaYa-Lqo-Ft/view?usp=sharing)\n    (or [Baidu Net Disk](https://pan.baidu.com/s/1n45ZwuYZpUA8vp-9V-ca9Q) with code ```uj3n```)\n    \n+ ##### Demo\n  + ```python decompose.py```\n  + The default input and output directory is ```./examples/```\n+ ##### Test\n  + ```python evaluate.py```\n  + The default output directory is ```./checkpoints/```\n+ ##### Train\n  + ```python train_IID.py```\n\n\nResults\n- \n#### Precomputed results\n- We provide visualized output on the SAW test set (note that SAW test data includes IIW test data and NYUv2 test data):\n[SAW_pred_imgs.zip](https://drive.google.com/file/d/18LI7CgTW0tVglF0u3Nirp1kZxca25iDJ/view?usp=sharing).\nAnd precision-recall measurements \n([precision-recall_curves.zip](https://drive.google.com/file/d/1WhxxN5sSVLLoet1ruk9VHzh_r-WhfRfs/view?usp=sharing))\nwhich can be used to draw the precision-recall curves. \n- If you want to compare on some applications (e.g., image editing), we strongly recommend using the original float32 output of the network\ninstead of the 8-bit low-precision visualized images.\n\n\n#### Comparison\n![comparison](./assets/comparison.jpg)\n\n#### Image sequence editing\n![editing](./assets/demo1.jpg)\n\nAcknowledgements\n-\nWe have used/modified codes from the following projects:\n  + [CGIntrinsics](https://github.com/zhengqili/CGIntrinsics):\n    + codes for loading data from the CGI, SAW and IIW datasets in ```./data/intrinsics/```.\n    + codes for evaluating shading and reflectance estimation in ```./test/```.\n    (These codes are originally provided by [IIW](http://opensurfaces.cs.cornell.edu/intrinsic/#) \n    and [SAW](http://opensurfaces.cs.cornell.edu/saw/)) \n  + [Revisiting_Single_Depth_Estimation](https://github.com/JunjH/Revisiting_Single_Depth_Estimation):\n    + the network structure of normal estimation module in ```./models/Hu_nets/```\n    \n\nCitation\n-\nIf you find this code useful for your research, please cite:\n  ```\n  @article{luo2020niid,\n    title={NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes},\n    author={Luo, Jundan and Huang, Zhaoyang and Li, Yijin and Zhou, Xiaowei and Zhang, Guofeng and Bao, Hujun},\n    journal={IEEE Transactions on Visualization and Computer Graphics},\n    year={2020},\n    publisher={IEEE}\n  }\n  ```\n\nCopyright\n-\n```\n  Copyright (c) ZJU-SenseTime Joint Lab of 3D Vision. All Rights Reserved.\n\n  Permission to use, copy, modify and distribute this software and its\n  documentation for educational, research and non-profit purposes only.\n\n  The above copyright notice and this permission notice shall be included in all\n  copies or substantial portions of the Software.\n\n  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n  SOFTWARE.\n```\n\nContact\n-\nPlease open an issue or contact Jundan Luo (<jundanluo22@gmail.com>) if you have any questions or any feedback."
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Pre-trained model",
        "parent_header": [
          "NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes",
          "Running"
        ],
        "type": "Text_excerpt",
        "value": "# NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes\n[[paper]](https://ieeexplore.ieee.org/document/9199573) \n[[supplement]](http://www.cad.zju.edu.cn/home/gfzhang/papers/NIID-Net/NIID-Net-supple.pdf)\n[[presentation]](https://youtu.be/BvoYwCdzoZU)\n[[demo]](https://youtu.be/0MadIlfqles) \n\n![architecture](./assets/NIID-Net.png)\n\n\nUpdates\n-\n+ 16/April/2023: Migrated to Pytorch 1.7.1\n\nDependencies\n-\n+ Python 3.6\n+ PyTorch 1.7.1 (original [version](https://github.com/zju3dv/NIID-Net/tree/pytorch_0.3.1) using PyTorch 0.3.1)\n+ torchvision 0.8.2\n+ [Visdom](https://github.com/facebookresearch/visdom) 0.1.8.9 \n+ We provide the ```requirements.txt``` file for other dependencies.\n\nDatasets\n-\n#### Intrinsic image datasets\n+ Follow [CGIntrinsics](https://github.com/zhengqili/CGIntrinsics) to download CGI, IIW and SAW datasets. \nNote that Z. Li and N. Snavely augment the original [IIW](http://opensurfaces.cs.cornell.edu/intrinsic/#) and [SAW](http://opensurfaces.cs.cornell.edu/saw/) datasets.\n+ You do not need to download the CGI dataset if you are not going to train the model.\n+ Put the datasets in the ```./dataset/``` folder. The final directory structure:\n    ```\n    NIID-Net project\n    |---README.md\n    |---...\n    |---dataset\n        |---CGIntrinsics\n            |---intrinsics_final\n            |   |---images   \n            |   |---rendered\n            |   |---...\n            |---IIW\n            |   |---data\n            |   |---test_list\n            |   |---...\n            |---SAW\n                |---saw_images_512\n                |---saw_pixel_labels\n                |---saw_splits\n                |---train_list\n    ```\n\n#### Depth/surface normal dataset\n+ Follow [Revisiting_Single_Depth_Estimation](https://github.com/JunjH/Revisiting_Single_Depth_Estimation) to \ndownload their extracted NYU-v2 subset.\n+ Unzip ```data.zip``` and rename the directory as ```NYU_v2```\n+ To compute surface normal maps,\n  + install ```open3d==0.8.0``` \n  + ```python ./tools/data_preprocess_normal.py --dataset_dir {NYU_v2 dataset path}  --num_workers {number of processes}```\n\n\nRunning\n-\n+ ##### Configuration\n  + ```options/config.py``` is the configuration file:\n    + ```TestOptions``` for test\n    + ```TrainIIDOptions``` for training the IID-Net\n  + Some variables may need to be modified:\n    ```\n      dataset_root #\n      checkpoints_dir # visualized results will be saved here\n      offline # if you do not need Visdom, set it True\n      pretrained_file #\n      gpu_devices # the indexes of GPU devices, or set None to run CPU version \n      batch_size_intrinsics # batch size for training on the CGIntrinsics dataset\n    ```\n  + Note that only test mode supports CPU version (with ```gpu_devices=None```). \n  We recommend you to use the **GPU** version.\n+ ##### Pre-trained model\n    [Google Drive](https://drive.google.com/file/d/160NzDEmC8okb6vgTNTyzmhaYa-Lqo-Ft/view?usp=sharing)\n    (or [Baidu Net Disk](https://pan.baidu.com/s/1n45ZwuYZpUA8vp-9V-ca9Q) with code ```uj3n```)\n    \n+ ##### Demo\n  + ```python decompose.py```\n  + The default input and output directory is ```./examples/```\n+ ##### Test\n  + ```python evaluate.py```\n  + The default output directory is ```./checkpoints/```\n+ ##### Train\n  + ```python train_IID.py```\n\n\nResults\n- \n#### Precomputed results\n- We provide visualized output on the SAW test set (note that SAW test data includes IIW test data and NYUv2 test data):\n[SAW_pred_imgs.zip](https://drive.google.com/file/d/18LI7CgTW0tVglF0u3Nirp1kZxca25iDJ/view?usp=sharing).\nAnd precision-recall measurements \n([precision-recall_curves.zip](https://drive.google.com/file/d/1WhxxN5sSVLLoet1ruk9VHzh_r-WhfRfs/view?usp=sharing))\nwhich can be used to draw the precision-recall curves. \n- If you want to compare on some applications (e.g., image editing), we strongly recommend using the original float32 output of the network\ninstead of the 8-bit low-precision visualized images.\n\n\n#### Comparison\n![comparison](./assets/comparison.jpg)\n\n#### Image sequence editing\n![editing](./assets/demo1.jpg)\n\nAcknowledgements\n-\nWe have used/modified codes from the following projects:\n  + [CGIntrinsics](https://github.com/zhengqili/CGIntrinsics):\n    + codes for loading data from the CGI, SAW and IIW datasets in ```./data/intrinsics/```.\n    + codes for evaluating shading and reflectance estimation in ```./test/```.\n    (These codes are originally provided by [IIW](http://opensurfaces.cs.cornell.edu/intrinsic/#) \n    and [SAW](http://opensurfaces.cs.cornell.edu/saw/)) \n  + [Revisiting_Single_Depth_Estimation](https://github.com/JunjH/Revisiting_Single_Depth_Estimation):\n    + the network structure of normal estimation module in ```./models/Hu_nets/```\n    \n\nCitation\n-\nIf you find this code useful for your research, please cite:\n  ```\n  @article{luo2020niid,\n    title={NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes},\n    author={Luo, Jundan and Huang, Zhaoyang and Li, Yijin and Zhou, Xiaowei and Zhang, Guofeng and Bao, Hujun},\n    journal={IEEE Transactions on Visualization and Computer Graphics},\n    year={2020},\n    publisher={IEEE}\n  }\n  ```\n\nCopyright\n-\n```\n  Copyright (c) ZJU-SenseTime Joint Lab of 3D Vision. All Rights Reserved.\n\n  Permission to use, copy, modify and distribute this software and its\n  documentation for educational, research and non-profit purposes only.\n\n  The above copyright notice and this permission notice shall be included in all\n  copies or substantial portions of the Software.\n\n  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n  SOFTWARE.\n```\n\nContact\n-\nPlease open an issue or contact Jundan Luo (<jundanluo22@gmail.com>) if you have any questions or any feedback."
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Test",
        "parent_header": [
          "NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes",
          "Running"
        ],
        "type": "Text_excerpt",
        "value": "# NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes\n[[paper]](https://ieeexplore.ieee.org/document/9199573) \n[[supplement]](http://www.cad.zju.edu.cn/home/gfzhang/papers/NIID-Net/NIID-Net-supple.pdf)\n[[presentation]](https://youtu.be/BvoYwCdzoZU)\n[[demo]](https://youtu.be/0MadIlfqles) \n\n![architecture](./assets/NIID-Net.png)\n\n\nUpdates\n-\n+ 16/April/2023: Migrated to Pytorch 1.7.1\n\nDependencies\n-\n+ Python 3.6\n+ PyTorch 1.7.1 (original [version](https://github.com/zju3dv/NIID-Net/tree/pytorch_0.3.1) using PyTorch 0.3.1)\n+ torchvision 0.8.2\n+ [Visdom](https://github.com/facebookresearch/visdom) 0.1.8.9 \n+ We provide the ```requirements.txt``` file for other dependencies.\n\nDatasets\n-\n#### Intrinsic image datasets\n+ Follow [CGIntrinsics](https://github.com/zhengqili/CGIntrinsics) to download CGI, IIW and SAW datasets. \nNote that Z. Li and N. Snavely augment the original [IIW](http://opensurfaces.cs.cornell.edu/intrinsic/#) and [SAW](http://opensurfaces.cs.cornell.edu/saw/) datasets.\n+ You do not need to download the CGI dataset if you are not going to train the model.\n+ Put the datasets in the ```./dataset/``` folder. The final directory structure:\n    ```\n    NIID-Net project\n    |---README.md\n    |---...\n    |---dataset\n        |---CGIntrinsics\n            |---intrinsics_final\n            |   |---images   \n            |   |---rendered\n            |   |---...\n            |---IIW\n            |   |---data\n            |   |---test_list\n            |   |---...\n            |---SAW\n                |---saw_images_512\n                |---saw_pixel_labels\n                |---saw_splits\n                |---train_list\n    ```\n\n#### Depth/surface normal dataset\n+ Follow [Revisiting_Single_Depth_Estimation](https://github.com/JunjH/Revisiting_Single_Depth_Estimation) to \ndownload their extracted NYU-v2 subset.\n+ Unzip ```data.zip``` and rename the directory as ```NYU_v2```\n+ To compute surface normal maps,\n  + install ```open3d==0.8.0``` \n  + ```python ./tools/data_preprocess_normal.py --dataset_dir {NYU_v2 dataset path}  --num_workers {number of processes}```\n\n\nRunning\n-\n+ ##### Configuration\n  + ```options/config.py``` is the configuration file:\n    + ```TestOptions``` for test\n    + ```TrainIIDOptions``` for training the IID-Net\n  + Some variables may need to be modified:\n    ```\n      dataset_root #\n      checkpoints_dir # visualized results will be saved here\n      offline # if you do not need Visdom, set it True\n      pretrained_file #\n      gpu_devices # the indexes of GPU devices, or set None to run CPU version \n      batch_size_intrinsics # batch size for training on the CGIntrinsics dataset\n    ```\n  + Note that only test mode supports CPU version (with ```gpu_devices=None```). \n  We recommend you to use the **GPU** version.\n+ ##### Pre-trained model\n    [Google Drive](https://drive.google.com/file/d/160NzDEmC8okb6vgTNTyzmhaYa-Lqo-Ft/view?usp=sharing)\n    (or [Baidu Net Disk](https://pan.baidu.com/s/1n45ZwuYZpUA8vp-9V-ca9Q) with code ```uj3n```)\n    \n+ ##### Demo\n  + ```python decompose.py```\n  + The default input and output directory is ```./examples/```\n+ ##### Test\n  + ```python evaluate.py```\n  + The default output directory is ```./checkpoints/```\n+ ##### Train\n  + ```python train_IID.py```\n\n\nResults\n- \n#### Precomputed results\n- We provide visualized output on the SAW test set (note that SAW test data includes IIW test data and NYUv2 test data):\n[SAW_pred_imgs.zip](https://drive.google.com/file/d/18LI7CgTW0tVglF0u3Nirp1kZxca25iDJ/view?usp=sharing).\nAnd precision-recall measurements \n([precision-recall_curves.zip](https://drive.google.com/file/d/1WhxxN5sSVLLoet1ruk9VHzh_r-WhfRfs/view?usp=sharing))\nwhich can be used to draw the precision-recall curves. \n- If you want to compare on some applications (e.g., image editing), we strongly recommend using the original float32 output of the network\ninstead of the 8-bit low-precision visualized images.\n\n\n#### Comparison\n![comparison](./assets/comparison.jpg)\n\n#### Image sequence editing\n![editing](./assets/demo1.jpg)\n\nAcknowledgements\n-\nWe have used/modified codes from the following projects:\n  + [CGIntrinsics](https://github.com/zhengqili/CGIntrinsics):\n    + codes for loading data from the CGI, SAW and IIW datasets in ```./data/intrinsics/```.\n    + codes for evaluating shading and reflectance estimation in ```./test/```.\n    (These codes are originally provided by [IIW](http://opensurfaces.cs.cornell.edu/intrinsic/#) \n    and [SAW](http://opensurfaces.cs.cornell.edu/saw/)) \n  + [Revisiting_Single_Depth_Estimation](https://github.com/JunjH/Revisiting_Single_Depth_Estimation):\n    + the network structure of normal estimation module in ```./models/Hu_nets/```\n    \n\nCitation\n-\nIf you find this code useful for your research, please cite:\n  ```\n  @article{luo2020niid,\n    title={NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes},\n    author={Luo, Jundan and Huang, Zhaoyang and Li, Yijin and Zhou, Xiaowei and Zhang, Guofeng and Bao, Hujun},\n    journal={IEEE Transactions on Visualization and Computer Graphics},\n    year={2020},\n    publisher={IEEE}\n  }\n  ```\n\nCopyright\n-\n```\n  Copyright (c) ZJU-SenseTime Joint Lab of 3D Vision. All Rights Reserved.\n\n  Permission to use, copy, modify and distribute this software and its\n  documentation for educational, research and non-profit purposes only.\n\n  The above copyright notice and this permission notice shall be included in all\n  copies or substantial portions of the Software.\n\n  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n  SOFTWARE.\n```\n\nContact\n-\nPlease open an issue or contact Jundan Luo (<jundanluo22@gmail.com>) if you have any questions or any feedback."
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Train",
        "parent_header": [
          "NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes",
          "Running"
        ],
        "type": "Text_excerpt",
        "value": "# NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes\n[[paper]](https://ieeexplore.ieee.org/document/9199573) \n[[supplement]](http://www.cad.zju.edu.cn/home/gfzhang/papers/NIID-Net/NIID-Net-supple.pdf)\n[[presentation]](https://youtu.be/BvoYwCdzoZU)\n[[demo]](https://youtu.be/0MadIlfqles) \n\n![architecture](./assets/NIID-Net.png)\n\n\nUpdates\n-\n+ 16/April/2023: Migrated to Pytorch 1.7.1\n\nDependencies\n-\n+ Python 3.6\n+ PyTorch 1.7.1 (original [version](https://github.com/zju3dv/NIID-Net/tree/pytorch_0.3.1) using PyTorch 0.3.1)\n+ torchvision 0.8.2\n+ [Visdom](https://github.com/facebookresearch/visdom) 0.1.8.9 \n+ We provide the ```requirements.txt``` file for other dependencies.\n\nDatasets\n-\n#### Intrinsic image datasets\n+ Follow [CGIntrinsics](https://github.com/zhengqili/CGIntrinsics) to download CGI, IIW and SAW datasets. \nNote that Z. Li and N. Snavely augment the original [IIW](http://opensurfaces.cs.cornell.edu/intrinsic/#) and [SAW](http://opensurfaces.cs.cornell.edu/saw/) datasets.\n+ You do not need to download the CGI dataset if you are not going to train the model.\n+ Put the datasets in the ```./dataset/``` folder. The final directory structure:\n    ```\n    NIID-Net project\n    |---README.md\n    |---...\n    |---dataset\n        |---CGIntrinsics\n            |---intrinsics_final\n            |   |---images   \n            |   |---rendered\n            |   |---...\n            |---IIW\n            |   |---data\n            |   |---test_list\n            |   |---...\n            |---SAW\n                |---saw_images_512\n                |---saw_pixel_labels\n                |---saw_splits\n                |---train_list\n    ```\n\n#### Depth/surface normal dataset\n+ Follow [Revisiting_Single_Depth_Estimation](https://github.com/JunjH/Revisiting_Single_Depth_Estimation) to \ndownload their extracted NYU-v2 subset.\n+ Unzip ```data.zip``` and rename the directory as ```NYU_v2```\n+ To compute surface normal maps,\n  + install ```open3d==0.8.0``` \n  + ```python ./tools/data_preprocess_normal.py --dataset_dir {NYU_v2 dataset path}  --num_workers {number of processes}```\n\n\nRunning\n-\n+ ##### Configuration\n  + ```options/config.py``` is the configuration file:\n    + ```TestOptions``` for test\n    + ```TrainIIDOptions``` for training the IID-Net\n  + Some variables may need to be modified:\n    ```\n      dataset_root #\n      checkpoints_dir # visualized results will be saved here\n      offline # if you do not need Visdom, set it True\n      pretrained_file #\n      gpu_devices # the indexes of GPU devices, or set None to run CPU version \n      batch_size_intrinsics # batch size for training on the CGIntrinsics dataset\n    ```\n  + Note that only test mode supports CPU version (with ```gpu_devices=None```). \n  We recommend you to use the **GPU** version.\n+ ##### Pre-trained model\n    [Google Drive](https://drive.google.com/file/d/160NzDEmC8okb6vgTNTyzmhaYa-Lqo-Ft/view?usp=sharing)\n    (or [Baidu Net Disk](https://pan.baidu.com/s/1n45ZwuYZpUA8vp-9V-ca9Q) with code ```uj3n```)\n    \n+ ##### Demo\n  + ```python decompose.py```\n  + The default input and output directory is ```./examples/```\n+ ##### Test\n  + ```python evaluate.py```\n  + The default output directory is ```./checkpoints/```\n+ ##### Train\n  + ```python train_IID.py```\n\n"
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "download",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 10:55:28",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 45
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Demo",
        "parent_header": [
          "NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes",
          "Running"
        ],
        "type": "Text_excerpt",
        "value": "# NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes\n[[paper]](https://ieeexplore.ieee.org/document/9199573) \n[[supplement]](http://www.cad.zju.edu.cn/home/gfzhang/papers/NIID-Net/NIID-Net-supple.pdf)\n[[presentation]](https://youtu.be/BvoYwCdzoZU)\n[[demo]](https://youtu.be/0MadIlfqles) \n\n![architecture](./assets/NIID-Net.png)\n\n\nUpdates\n-\n+ 16/April/2023: Migrated to Pytorch 1.7.1\n\nDependencies\n-\n+ Python 3.6\n+ PyTorch 1.7.1 (original [version](https://github.com/zju3dv/NIID-Net/tree/pytorch_0.3.1) using PyTorch 0.3.1)\n+ torchvision 0.8.2\n+ [Visdom](https://github.com/facebookresearch/visdom) 0.1.8.9 \n+ We provide the ```requirements.txt``` file for other dependencies.\n\nDatasets\n-\n#### Intrinsic image datasets\n+ Follow [CGIntrinsics](https://github.com/zhengqili/CGIntrinsics) to download CGI, IIW and SAW datasets. \nNote that Z. Li and N. Snavely augment the original [IIW](http://opensurfaces.cs.cornell.edu/intrinsic/#) and [SAW](http://opensurfaces.cs.cornell.edu/saw/) datasets.\n+ You do not need to download the CGI dataset if you are not going to train the model.\n+ Put the datasets in the ```./dataset/``` folder. The final directory structure:\n    ```\n    NIID-Net project\n    |---README.md\n    |---...\n    |---dataset\n        |---CGIntrinsics\n            |---intrinsics_final\n            |   |---images   \n            |   |---rendered\n            |   |---...\n            |---IIW\n            |   |---data\n            |   |---test_list\n            |   |---...\n            |---SAW\n                |---saw_images_512\n                |---saw_pixel_labels\n                |---saw_splits\n                |---train_list\n    ```\n\n#### Depth/surface normal dataset\n+ Follow [Revisiting_Single_Depth_Estimation](https://github.com/JunjH/Revisiting_Single_Depth_Estimation) to \ndownload their extracted NYU-v2 subset.\n+ Unzip ```data.zip``` and rename the directory as ```NYU_v2```\n+ To compute surface normal maps,\n  + install ```open3d==0.8.0``` \n  + ```python ./tools/data_preprocess_normal.py --dataset_dir {NYU_v2 dataset path}  --num_workers {number of processes}```\n\n\nRunning\n-\n+ ##### Configuration\n  + ```options/config.py``` is the configuration file:\n    + ```TestOptions``` for test\n    + ```TrainIIDOptions``` for training the IID-Net\n  + Some variables may need to be modified:\n    ```\n      dataset_root #\n      checkpoints_dir # visualized results will be saved here\n      offline # if you do not need Visdom, set it True\n      pretrained_file #\n      gpu_devices # the indexes of GPU devices, or set None to run CPU version \n      batch_size_intrinsics # batch size for training on the CGIntrinsics dataset\n    ```\n  + Note that only test mode supports CPU version (with ```gpu_devices=None```). \n  We recommend you to use the **GPU** version.\n+ ##### Pre-trained model\n    [Google Drive](https://drive.google.com/file/d/160NzDEmC8okb6vgTNTyzmhaYa-Lqo-Ft/view?usp=sharing)\n    (or [Baidu Net Disk](https://pan.baidu.com/s/1n45ZwuYZpUA8vp-9V-ca9Q) with code ```uj3n```)\n    \n+ ##### Demo\n  + ```python decompose.py```\n  + The default input and output directory is ```./examples/```\n+ ##### Test\n  + ```python evaluate.py```\n  + The default output directory is ```./checkpoints/```\n+ ##### Train\n  + ```python train_IID.py```\n\n\nResults\n- \n#### Precomputed results\n- We provide visualized output on the SAW test set (note that SAW test data includes IIW test data and NYUv2 test data):\n[SAW_pred_imgs.zip](https://drive.google.com/file/d/18LI7CgTW0tVglF0u3Nirp1kZxca25iDJ/view?usp=sharing).\nAnd precision-recall measurements \n([precision-recall_curves.zip](https://drive.google.com/file/d/1WhxxN5sSVLLoet1ruk9VHzh_r-WhfRfs/view?usp=sharing))\nwhich can be used to draw the precision-recall curves. \n- If you want to compare on some applications (e.g., image editing), we strongly recommend using the original float32 output of the network\ninstead of the 8-bit low-precision visualized images.\n\n\n#### Comparison\n![comparison](./assets/comparison.jpg)\n\n#### Image sequence editing\n![editing](./assets/demo1.jpg)\n\nAcknowledgements\n-\nWe have used/modified codes from the following projects:\n  + [CGIntrinsics](https://github.com/zhengqili/CGIntrinsics):\n    + codes for loading data from the CGI, SAW and IIW datasets in ```./data/intrinsics/```.\n    + codes for evaluating shading and reflectance estimation in ```./test/```.\n    (These codes are originally provided by [IIW](http://opensurfaces.cs.cornell.edu/intrinsic/#) \n    and [SAW](http://opensurfaces.cs.cornell.edu/saw/)) \n  + [Revisiting_Single_Depth_Estimation](https://github.com/JunjH/Revisiting_Single_Depth_Estimation):\n    + the network structure of normal estimation module in ```./models/Hu_nets/```\n    \n\nCitation\n-\nIf you find this code useful for your research, please cite:\n  ```\n  @article{luo2020niid,\n    title={NIID-Net: Adapting Surface Normal Knowledge for Intrinsic Image Decomposition in Indoor Scenes},\n    author={Luo, Jundan and Huang, Zhaoyang and Li, Yijin and Zhou, Xiaowei and Zhang, Guofeng and Bao, Hujun},\n    journal={IEEE Transactions on Visualization and Computer Graphics},\n    year={2020},\n    publisher={IEEE}\n  }\n  ```\n\nCopyright\n-\n```\n  Copyright (c) ZJU-SenseTime Joint Lab of 3D Vision. All Rights Reserved.\n\n  Permission to use, copy, modify and distribute this software and its\n  documentation for educational, research and non-profit purposes only.\n\n  The above copyright notice and this permission notice shall be included in all\n  copies or substantial portions of the Software.\n\n  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n  SOFTWARE.\n```\n\nContact\n-\nPlease open an issue or contact Jundan Luo (<jundanluo22@gmail.com>) if you have any questions or any feedback."
      },
      "source": "https://raw.githubusercontent.com/zju3dv/NIID-Net/master/README.md",
      "technique": "header_analysis"
    }
  ]
}