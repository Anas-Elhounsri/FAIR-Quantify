{
  "application_domain": [
    {
      "confidence": 18.01,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/yoterel/STORM-Net"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-11-06T14:10:24Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-02-22T15:17:47Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "STORM: Simple and Timely Optode Registration Method for Functional Near-Infrared Spectroscopy (fNIRS)"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Introduction",
        "type": "Text_excerpt",
        "value": "This application is designed to provide an accurate estimation of the position of points of interest (usually, sensors) on a fNIRS probing cap mounted on a subject\u2019s head, based on a short video. This is needed because the cap is almost never placed in perfect alignment with the actual position it was intended for (mostly due to human error, but also because of differences in skull structures and cap deformations).\n\nIn other words, given a short video of the subject wearing the fNIRS cap, the application outputs the coordinates of every point of interest on the cap in (a statistical) MNI coordinate system (or, if required, in the original coordinates system that can be later transformed to MNI using external tools such as [SPM fNIRS](https://www.nitrc.org/projects/spm_fnirs/)).\n\nThere are 3 modes of operation:\n1. A GUI that allows manual annotation of the data / supervising the automatic method - this is recommended for first time users.\n2. A command-line interface which is a ready-to-use end-to-end script that performs calibration given a video file - use this when you are comfortable with the GUI and its results.\n3. An experimental mode that allows reproducing all results from the original manuscript.\n\nThe repository also contains:\n- Python scripts for training the neural networks discussed in the paper. Description of how to do this is down below. \n- A synthetic data generator implemented in [Unity](https://unity.com/).\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9837127578092912,
      "result": {
        "original_header": "STORM-Net",
        "type": "Text_excerpt",
        "value": "Simple and Timely Optode Registration Method for Functional Near-Infrared Spectroscopy (FNIRS) \nAll files in the repository are an implementation of the original manuscript and research ([preprint](https://doi.org/10.1101/2020.12.29.424683)).\n \n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9683904567737383,
      "result": {
        "original_header": "Template Model File",
        "type": "Text_excerpt",
        "value": "The exact format of this file is now specified.\nThe file is a space delimited human readable file, where each row contains 4 values:\n1. A unique name of the optode (some names are reserved, see below).\n2. Three numerical values: X, Y, Z representing the location of this optode (note: values must be supplied in cm or inch).\nThe coordinate system these values are supplied in are not important, as they are transformed to STORM-Nets internal right-handed system (described below). However, the transformation to this new coordinate system relies on your measurements making physical sense, e.g. the nasion is located between the eyes, rpa is on the same side of the brain as the right eye, etc.\n \n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9669727994283991,
      "result": {
        "original_header": "List of mandatory locations in template file",
        "type": "Text_excerpt",
        "value": "Notes:\n- Use these names (exactly, without double quoutes) as the first field for the file to be parsed correctly.\n- The stickers can be placed anywhere **on the cap** as long as the three frontal ones are not colinear, but we recommend using fp1, fpz + 1cm, fp2, cz as their location.\n- middle_triangle: we actually used a location 1 cm above fpz in our experiments (can be seen marked by a green sticker in the example video) - this eliminated the risk of coliniearity (but this specific location is not mandatory).\n- Do not use any of these names for your sensors. They are reserved. \n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9972000294230389,
      "result": {
        "original_header": "STORM-Net coordinate system",
        "type": "Text_excerpt",
        "value": "This application uses the following coordiante system internally (notice it is right-handed):\n- X axis is from left to right ear\n- Y axis is from back to front of head\n- Z axis is from bottom to top of head\n- The origin is defined by (x,y,z) = ((lefteye.x+righteye.x) / 2, cz.y, (lefteye.z+righteye.z) / 2)\n- Scale is cm. If \"CZ\" is too close to origin in terms of cm, the code scales it to cm (by assuming it is measured in inches).\n \n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9001334902967539,
      "result": {
        "original_header": "Troubleshooting issues",
        "type": "Text_excerpt",
        "value": "1. Please skim through the closed github issues to see if someone already posted a similar question.\n2. For any new issue, please first open a github issue and explain your problem (we kindly request not to email any of the authors at this stage, github issues will be answered in a timely fashion).\n3. If this doesn't help, the corrosponding author is available for online meetings to resolve any edge case.\n \n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 3: Download all pre-trained neural network models.",
        "parent_header": [
          "Quick installation guide"
        ],
        "type": "Text_excerpt",
        "value": "Download from [here](https://osf.io/3j6u2/download), and place them under the [models](CapCalibrator/models) folder (after extracting).\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 4: Download all precompiled binaries for the renderer.",
        "parent_header": [
          "Quick installation guide"
        ],
        "type": "Text_excerpt",
        "value": "Download from here:\\\n[Windows](https://osf.io/n382w/download)\\\n[Linux](https://osf.io/56a28/download)\\\n[Mac](https://osf.io/gfpcw/download)\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/yoterel/STORM-Net/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 1
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/yoterel/STORM-Net/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "yoterel/STORM-Net"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "STORM-Net"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/resource/img1.png"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/resource/img2.png"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Quick installation guide",
        "type": "Text_excerpt",
        "value": "Note: tested on Windows 10 and Ubuntu 18.04. Should work for Mac as well."
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 2: Navigate to the STORM-Net directory, then create a virtual environment using conda.",
        "parent_header": [
          "Quick installation guide"
        ],
        "type": "Text_excerpt",
        "value": "We recommend installing [Miniconda](https://docs.conda.io/en/latest/miniconda.html) for this, but you can also [Install Anaconda](https://www.anaconda.com/products/individual/get-started) if needed, then create an environment using the environment.yml file in this repository:\n\n`conda env create -n env -f environment.yml`\n\nNote: For dlib (part of the requirements in the environment file), you must have some modern C++ able compiler like [visual studio](https://visualstudio.microsoft.com/) or gcc installed on your computer.\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Modes of operation",
        "parent_header": [
          "Quick installation guide"
        ],
        "type": "Text_excerpt",
        "value": "The file [main.py](CapCalibrator/main.py) is the entry point of the application. Some common use cases are shown below:\n\n`python main.py --mode gui`\n\n`python main.py --mode auto --video path_to_video_file --template path_to_template_file`\n\n`python main.py --mode experimental --video path_to_video_folder --template path_to_template_file --gt path_to_ground_truth_measurements_folder`\n\nThe mode \"gui\" indicates to the application that the user wants to use the GUI and supervise the process of annotation and registration and to correct it if needed. This is recommended when possible. Note the GUI contains other useful functions such as viewing a template model and finetunning the neural networks.\n\nThe mode \"auto\" indicates to the application that the user wants it to automatically annotate the video without any supervision. This is recommended for live sessions and when the system was oberved to perform well with a certain template model. Note that using this mode the application requires two additional paramters which are the path to the raw video file and to a template model file.\n\nThe mode \"experimental\" indicates to the application that the user wants to reproduce all results in the original paper. Note this requires the original dataset used and is available upon request from the corrosponding author.\n\nFor all command line options, see:\n\n`python main.py --help`\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Usage guide",
        "type": "Text_excerpt",
        "value": "In a nutshell, you will need to perform the \"offline step\" (slow) prior to performing registration (once per phsyical cap), and an \"online step\" (fast) every time you want to coregister, e.g. a subject was mounted with the cap and you want to coregister.\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Offline step",
        "parent_header": [
          "Usage guide"
        ],
        "type": "Text_excerpt",
        "value": "The offline step (and infact, the online step too) rely on a pre-measured template model file as an input. An example of such file is located under the [example_models](example_models) directory. This template model was obtained using a 3D digitizer, but can similarly be obtained in any way as long as the measurements are accurate to a satisfactory degree (e.g. by 3D scanning, or taking a video and running [COLMAP](https://colmap.github.io/)).\nIt is strongly recommended to create your own template model file **per physical cap model you are using in your lab**, this will lead to best accuracies.\n\nfor the exact format, and list of the minimum neccessary points that are required in this file, see below in the section \"Template Model File\".\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Rendering",
        "parent_header": [
          "Usage guide",
          "Offline step"
        ],
        "type": "Text_excerpt",
        "value": "After creating a template model for a new cap, the offline step first stage can be performed (rendering synthetic data).\nTo create synthetic data, you can either use the GUI -> \"Offline Step\" -> \"Render\" (set the appropriate fields, we recommend usings a minimum of 100000 iterations) or use the [render script](DataSynth/render.py) directly. Notice this script requires the template model file path and the renderer executable path as input:\\\n   `python render.py path_to_template_model_file path_to_output_folder --exe path_to_renderer_executable --log path_to_output_log --iterations 100000`\n   \n   For all command line options when rendering see:\n   \n   `python render.py --help`\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Training",
        "parent_header": [
          "Usage guide",
          "Offline step"
        ],
        "type": "Text_excerpt",
        "value": "Following rendering, the second stage of the offline step can be performed (training STORM-Net).\nTo train the network on the renderings you produced from the previous step, use the GUI -> \"Offline Step\" -> \"Train\" (set the appropriate fields) or use the [train script](CapCalibrator/torch_train.py) directly. We recommend using a gpu to speed up the training process:\\\n   `python torch_train.py my_new_model_name path_to_synthetic_data_folder --gpu_id 2`\n   \n   For all command line options see:\n   \n   `python torch_train.py --help`\n   \n When training is done, a model file will be availble in the [models](CapCalibrator/models) directory.\n   \n Note: we strongly suggest to train until validation loss reaches atleast 0.2 - do not stop before this.\n   "
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Online step",
        "parent_header": [
          "Usage guide"
        ],
        "type": "Text_excerpt",
        "value": "Now that we have a trained model, we can take a video of the subject wearing the cap.\nThe video must be taken using the camera path described in the paper, see [example video](https://github.com/yoterel/STORM-Net/blob/master/example_videos/example_video.mp4) for how a proper video should be taken). Notice the video must have a 16\u22369 aspect ratio (for example, 1920x1080 resolution) and must be taken horizontally.\n\nin the GUI -> \"Online Step\", select File from the menu, and load the required inputs one by one (Storm-Net model you trained, the video itself, and the template model recorded for the offline step). STORM-Net will automatically select 10 frames for you, and you can either automatically annotate the facial landmarks and stickers, or manually do so, for each frame (use the arrow keys to scroll between the frames, and mouse to annotate). Note that marking the stickers does not need to have any order, i.e. CAP1, CAP2, CAP3, CAP4 are some sticker locations that can be seen in a frame, and their order doesn't matter (in the frame itself, or between frames).\n\nAfter annotation is done, click Video in the top menu, and then \"co-register\". You can save the result to any file.\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9999951096234193,
      "result": {
        "original_header": "STORM-Net",
        "type": "Text_excerpt",
        "value": "\n<table>\n  <tr>\n    <td> <img src=\"https://github.com/yoterel/STORM-Net/blob/master/resource/img1.png\"  alt=\"1\" width = 400px height = 400px ></td>\n    <td><img src=\"https://github.com/yoterel/STORM-Net/blob/master/resource/img2.png\" alt=\"2\" width = 400px height = 400px></td>\n   </tr>\n</table> \n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/yoterel/STORM-Net/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License\n\nBy exercising the Licensed Rights (defined below), You accept and agree to be\nbound by the terms and conditions of this Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International Public License\n(\"Public License\"). To the extent this Public License may be interpreted as a\ncontract, You are granted the Licensed Rights in consideration of Your\nacceptance of these terms and conditions, and the Licensor grants You such\nrights in consideration of benefits the Licensor receives from making the\nLicensed Material available under these terms and conditions.\n\nSection 1 \u2013 Definitions.\n\n    a. Adapted Material means material subject to Copyright and Similar Rights\n       that is derived from or based upon the Licensed Material and in which\n       the Licensed Material is translated, altered, arranged, transformed, or\n       otherwise modified in a manner requiring permission under the Copyright\n       and Similar Rights held by the Licensor. For purposes of this Public\n       License, where the Licensed Material is a musical work, performance, or\n       sound recording, Adapted Material is always produced where the Licensed\n       Material is synched in timed relation with a moving image.\n    b. Copyright and Similar Rights means copyright and/or similar rights\n       closely related to copyright including, without limitation,\n       performance, broadcast, sound recording, and Sui Generis Database\n       Rights, without regard to how the rights are labeled or categorized.\n       For purposes of this Public License, the rights specified in Section\n       2(b)(1)-(2) are not Copyright and Similar Rights.\n    c. Effective Technological Measures means those measures that, in the\n       absence of proper authority, may not be circumvented under laws\n       fulfilling obligations under Article 11 of the WIPO Copyright Treaty\n       adopted on December 20, 1996, and/or similar international agreements.\n    d. Exceptions and Limitations means fair use, fair dealing, and/or any\n       other exception or limitation to Copyright and Similar Rights that\n       applies to Your use of the Licensed Material.\n    e. Licensed Material means the artistic or literary work, database, or\n       other material to which the Licensor applied this Public License.\n    f. Licensed Rights means the rights granted to You subject to the terms\n       and conditions of this Public License, which are limited to all\n       Copyright and Similar Rights that apply to Your use of the Licensed\n       Material and that the Licensor has authority to license.\n    g. Licensor means the individual(s) or entity(ies) granting rights under\n       this Public License.\n    h. NonCommercial means not primarily intended for or directed towards\n       commercial advantage or monetary compensation. For purposes of this\n       Public License, the exchange of the Licensed Material for other\n       material subject to Copyright and Similar Rights by digital\n       file-sharing or similar means is NonCommercial provided there is no\n       payment of monetary compensation in connection with the exchange.\n    i. Share means to provide material to the public by any means or process\n       that requires permission under the Licensed Rights, such as\n       reproduction, public display, public performance, distribution,\n       dissemination, communication, or importation, and to make material\n       available to the public including in ways that members of the public\n       may access the material from a place and at a time individually chosen\n       by them.\n    j. Sui Generis Database Rights means rights other than copyright resulting\n       from Directive 96/9/EC of the European Parliament and of the Council of\n       11 March 1996 on the legal protection of databases, as amended and/or\n       succeeded, as well as other essentially equivalent rights anywhere in\n       the world.\n    k. You means the individual or entity exercising the Licensed Rights under\n       this Public License. Your has a corresponding meaning.\n\nSection 2 \u2013 Scope.\n\n    a. License grant.\n        1. Subject to the terms and conditions of this Public License, the\n           Licensor hereby grants You a worldwide, royalty-free,\n           non-sublicensable, non-exclusive, irrevocable license to exercise\n           the Licensed Rights in the Licensed Material to:\n            A. reproduce and Share the Licensed Material, in whole or in part,\n               for NonCommercial purposes only; and\n            B. produce and reproduce, but not Share, Adapted Material for\n               NonCommercial purposes only.\n        2. Exceptions and Limitations. For the avoidance of doubt, where\n           Exceptions and Limitations apply to Your use, this Public License\n           does not apply, and You do not need to comply with its terms and\n           conditions.\n        3. Term. The term of this Public License is specified in Section 6(a).\n        4. Media and formats; technical modifications allowed. The Licensor\n           authorizes You to exercise the Licensed Rights in all media and\n           formats whether now known or hereafter created, and to make\n           technical modifications necessary to do so. The Licensor waives\n           and/or agrees not to assert any right or authority to forbid You\n           from making technical modifications necessary to exercise the\n           Licensed Rights, including technical modifications necessary to\n           circumvent Effective Technological Measures. For purposes of this\n           Public License, simply making modifications authorized by this\n           Section 2(a)(4) never produces Adapted Material.\n        5. Downstream recipients.\n            A. Offer from the Licensor \u2013 Licensed Material. Every recipient of\n               the Licensed Material automatically receives an offer from the\n               Licensor to exercise the Licensed Rights under the terms and\n               conditions of this Public License.\n            B. No downstream restrictions. You may not offer or impose any\n               additional or different terms or conditions on, or apply any\n               Effective Technological Measures to, the Licensed Material if\n               doing so restricts exercise of the Licensed Rights by any\n               recipient of the Licensed Material.\n        6. No endorsement. Nothing in this Public License constitutes or may\n           be construed as permission to assert or imply that You are, or that\n           Your use of the Licensed Material is, connected with, or sponsored,\n           endorsed, or granted official status by, the Licensor or others\n           designated to receive attribution as provided in Section\n           3(a)(1)(A)(i).\n\n    b. Other rights.\n        1. Moral rights, such as the right of integrity, are not licensed\n           under this Public License, nor are publicity, privacy, and/or other\n           similar personality rights; however, to the extent possible, the\n           Licensor waives and/or agrees not to assert any such rights held by\n           the Licensor to the limited extent necessary to allow You to\n           exercise the Licensed Rights, but not otherwise.\n        2. Patent and trademark rights are not licensed under this Public\n           License.\n        3. To the extent possible, the Licensor waives any right to collect\n           royalties from You for the exercise of the Licensed Rights, whether\n           directly or through a collecting society under any voluntary or\n           waivable statutory or compulsory licensing scheme. In all other\n           cases the Licensor expressly reserves any right to collect such\n           royalties, including when the Licensed Material is used other than\n           for NonCommercial purposes.\n\nSection 3 \u2013 License Conditions.\n\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\n    a. Attribution.\n\n        1. If You Share the Licensed Material, You must:\n            A. retain the following if it is supplied by the Licensor with the\n               Licensed Material:\n                i. identification of the creator(s) of the Licensed Material\n                   and any others designated to receive attribution, in any\n                   reasonable manner requested by the Licensor (including by\n                   pseudonym if designated);\n                ii. a copyright notice;\n                iii. a notice that refers to this Public License;\n                iv. a notice that refers to the disclaimer of warranties;\n                v. a URI or hyperlink to the Licensed Material to the extent\n                   reasonably practicable;\n            B. indicate if You modified the Licensed Material and retain an\n               indication of any previous modifications; and\n            C. indicate the Licensed Material is licensed under this Public\n               License, and include the text of, or the URI or hyperlink to,\n               this Public License.\n\n            For the avoidance of doubt, You do not have permission under this\n            Public License to Share Adapted Material.\n\n        2. You may satisfy the conditions in Section 3(a)(1) in any reasonable\n           manner based on the medium, means, and context in which You Share\n           the Licensed Material. For example, it may be reasonable to satisfy\n           the conditions by providing a URI or hyperlink to a resource that\n           includes the required information.\n        3. If requested by the Licensor, You must remove any of the\n           information required by Section 3(a)(1)(A) to the extent reasonably\n           practicable.\n\nSection 4 \u2013 Sui Generis Database Rights.\n\nWhere the Licensed Rights include Sui Generis Database Rights that apply to\nYour use of the Licensed Material:\n\n    a. for the avoidance of doubt, Section 2(a)(1) grants You the right to\n       extract, reuse, reproduce, and Share all or a substantial portion of\n       the contents of the database for NonCommercial purposes only and\n       provided You do not Share Adapted Material;\n    b. if You include all or a substantial portion of the database contents in\n       a database in which You have Sui Generis Database Rights, then the\n       database in which You have Sui Generis Database Rights (but not its\n       individual contents) is Adapted Material; and\n    c. You must comply with the conditions in Section 3(a) if You Share all or\n       a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace\nYour obligations under this Public License where the Licensed Rights include\nother Copyright and Similar Rights.\n\nSection 5 \u2013 Disclaimer of Warranties and Limitation of Liability.\n\n    a. Unless otherwise separately undertaken by the Licensor, to the extent\n       possible, the Licensor offers the Licensed Material as-is and\n       as-available, and makes no representations or warranties of any kind\n       concerning the Licensed Material, whether express, implied, statutory,\n       or other. This includes, without limitation, warranties of title,\n       merchantability, fitness for a particular purpose, non-infringement,\n       absence of latent or other defects, accuracy, or the presence or\n       absence of errors, whether or not known or discoverable. Where\n       disclaimers of warranties are not allowed in full or in part, this\n       disclaimer may not apply to You.\n    b. To the extent possible, in no event will the Licensor be liable to You\n       on any legal theory (including, without limitation, negligence) or\n       otherwise for any direct, special, indirect, incidental, consequential,\n       punitive, exemplary, or other losses, costs, expenses, or damages\n       arising out of this Public License or use of the Licensed Material,\n       even if the Licensor has been advised of the possibility of such\n       losses, costs, expenses, or damages. Where a limitation of liability is\n       not allowed in full or in part, this limitation may not apply to You.\n    c. The disclaimer of warranties and limitation of liability provided above\n       shall be interpreted in a manner that, to the extent possible, most\n       closely approximates an absolute disclaimer and waiver of all\n       liability.\n\nSection 6 \u2013 Term and Termination.\n\n    a. This Public License applies for the term of the Copyright and Similar\n       Rights licensed here. However, if You fail to comply with this Public\n       License, then Your rights under this Public License terminate\n       automatically.\n    b. Where Your right to use the Licensed Material has terminated under\n       Section 6(a), it reinstates:\n        1. automatically as of the date the violation is cured, provided it is\n           cured within 30 days of Your discovery of the violation; or\n        2. upon express reinstatement by the Licensor.\n\n      For the avoidance of doubt, this Section 6(b) does not affect any right\n      the Licensor may have to seek remedies for Your violations of this\n      Public License.\n\n    c. For the avoidance of doubt, the Licensor may also offer the Licensed\n       Material under separate terms or conditions or stop distributing the\n       Licensed Material at any time; however, doing so will not terminate\n       this Public License.\n    d. Sections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 \u2013 Other Terms and Conditions.\n\n    a. The Licensor shall not be bound by any additional or different terms or\n       conditions communicated by You unless expressly agreed.\n    b. Any arrangements, understandings, or agreements regarding the Licensed\n       Material not stated herein are separate from and independent of the\n       terms and conditions of this Public License.\n\nSection 8 \u2013 Interpretation.\n\n    a. For the avoidance of doubt, this Public License does not, and shall not\n       be interpreted to, reduce, limit, restrict, or impose conditions on any\n       use of the Licensed Material that could lawfully be made without\n       permission under this Public License.\n    b. To the extent possible, if any provision of this Public License is\n       deemed unenforceable, it shall be automatically reformed to the minimum\n       extent necessary to make it enforceable. If the provision cannot be\n       reformed, it shall be severed from this Public License without\n       affecting the enforceability of the remaining terms and conditions.\n    c. No term or condition of this Public License will be waived and no\n       failure to comply consented to unless expressly agreed to by the Licensor.\n    d. Nothing in this Public License constitutes or may be interpreted as a\n       limitation upon, or waiver of, any privileges and immunities that apply\n       to the Licensor or You, including from the legal processes of any\n       jurisdiction or authority.\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/LICENSE.md",
      "technique": "file_exploration"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "STORM-Net"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "yoterel"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 345785,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "C#",
        "size": 250581,
        "type": "Programming_language",
        "value": "C#"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "C++",
        "size": 107180,
        "type": "Programming_language",
        "value": "C++"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "ShaderLab",
        "size": 64636,
        "type": "Programming_language",
        "value": "ShaderLab"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "HLSL",
        "size": 9089,
        "type": "Programming_language",
        "value": "HLSL"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "CMake",
        "size": 4014,
        "type": "Programming_language",
        "value": "CMake"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 1: Clone this repository to get a copy of the code to run locally.",
        "parent_header": [
          "Quick installation guide"
        ],
        "type": "Text_excerpt",
        "value": "Clone the repository by downloading it [directly](https://github.com/yoterel/STORM-Net/archive/master.zip) or by using the git command line tool:\\\n`git clone https://github.com/yoterel/STORM-Net.git`\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 5: Run STORM-Net in gui mode.",
        "parent_header": [
          "Quick installation guide"
        ],
        "type": "Text_excerpt",
        "value": "First remember to activate the environment you created\n`conda activate env`\nThen navigate to [main.py](CapCalibrator/main.py), and run:\\\n`python main.py --mode gui`\n\nNote: in Linux you might need to unset pythonpath before the application can be run successfully:\\\n`unset PYTHONPATH`\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "requirements",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 12:28:19",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 8
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 1: Clone this repository to get a copy of the code to run locally.",
        "parent_header": [
          "Quick installation guide"
        ],
        "type": "Text_excerpt",
        "value": "Clone the repository by downloading it [directly](https://github.com/yoterel/STORM-Net/archive/master.zip) or by using the git command line tool:\\\n`git clone https://github.com/yoterel/STORM-Net.git`\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Usage guide",
        "type": "Text_excerpt",
        "value": "In a nutshell, you will need to perform the \"offline step\" (slow) prior to performing registration (once per phsyical cap), and an \"online step\" (fast) every time you want to coregister, e.g. a subject was mounted with the cap and you want to coregister.\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Offline step",
        "parent_header": [
          "Usage guide"
        ],
        "type": "Text_excerpt",
        "value": "The offline step (and infact, the online step too) rely on a pre-measured template model file as an input. An example of such file is located under the [example_models](example_models) directory. This template model was obtained using a 3D digitizer, but can similarly be obtained in any way as long as the measurements are accurate to a satisfactory degree (e.g. by 3D scanning, or taking a video and running [COLMAP](https://colmap.github.io/)).\nIt is strongly recommended to create your own template model file **per physical cap model you are using in your lab**, this will lead to best accuracies.\n\nfor the exact format, and list of the minimum neccessary points that are required in this file, see below in the section \"Template Model File\".\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Rendering",
        "parent_header": [
          "Usage guide",
          "Offline step"
        ],
        "type": "Text_excerpt",
        "value": "After creating a template model for a new cap, the offline step first stage can be performed (rendering synthetic data).\nTo create synthetic data, you can either use the GUI -> \"Offline Step\" -> \"Render\" (set the appropriate fields, we recommend usings a minimum of 100000 iterations) or use the [render script](DataSynth/render.py) directly. Notice this script requires the template model file path and the renderer executable path as input:\\\n   `python render.py path_to_template_model_file path_to_output_folder --exe path_to_renderer_executable --log path_to_output_log --iterations 100000`\n   \n   For all command line options when rendering see:\n   \n   `python render.py --help`\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Training",
        "parent_header": [
          "Usage guide",
          "Offline step"
        ],
        "type": "Text_excerpt",
        "value": "Following rendering, the second stage of the offline step can be performed (training STORM-Net).\nTo train the network on the renderings you produced from the previous step, use the GUI -> \"Offline Step\" -> \"Train\" (set the appropriate fields) or use the [train script](CapCalibrator/torch_train.py) directly. We recommend using a gpu to speed up the training process:\\\n   `python torch_train.py my_new_model_name path_to_synthetic_data_folder --gpu_id 2`\n   \n   For all command line options see:\n   \n   `python torch_train.py --help`\n   \n When training is done, a model file will be availble in the [models](CapCalibrator/models) directory.\n   \n Note: we strongly suggest to train until validation loss reaches atleast 0.2 - do not stop before this.\n   "
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Online step",
        "parent_header": [
          "Usage guide"
        ],
        "type": "Text_excerpt",
        "value": "Now that we have a trained model, we can take a video of the subject wearing the cap.\nThe video must be taken using the camera path described in the paper, see [example video](https://github.com/yoterel/STORM-Net/blob/master/example_videos/example_video.mp4) for how a proper video should be taken). Notice the video must have a 16\u22369 aspect ratio (for example, 1920x1080 resolution) and must be taken horizontally.\n\nin the GUI -> \"Online Step\", select File from the menu, and load the required inputs one by one (Storm-Net model you trained, the video itself, and the template model recorded for the offline step). STORM-Net will automatically select 10 frames for you, and you can either automatically annotate the facial landmarks and stickers, or manually do so, for each frame (use the arrow keys to scroll between the frames, and mouse to annotate). Note that marking the stickers does not need to have any order, i.e. CAP1, CAP2, CAP3, CAP4 are some sticker locations that can be seen in a frame, and their order doesn't matter (in the frame itself, or between frames).\n\nAfter annotation is done, click Video in the top menu, and then \"co-register\". You can save the result to any file.\n"
      },
      "source": "https://raw.githubusercontent.com/yoterel/STORM-Net/master/README.md",
      "technique": "header_analysis"
    }
  ]
}