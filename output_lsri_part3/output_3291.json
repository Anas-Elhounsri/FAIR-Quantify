{
  "application_domain": [
    {
      "confidence": 50.64,
      "result": {
        "type": "String",
        "value": "Natural Language Processing"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citing Supporting Repositories",
        "type": "Text_excerpt",
        "value": "For the ESM module used during encoding and masking filling ([GitHub](https://github.com/facebookresearch/esm)):\n\n```bibtex\n@article{Rives2021,\nauthor = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and Fergus, Rob},\ndoi = {10.1073/pnas.2016239118},\njournal = {Proceedings of the National Academy of Sciences},\ntitle = {{Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences}},\nurl = {http://www.pnas.org/lookup/doi/10.1073/pnas.2016239118},\nvolume = {118},\nyear = {2021}\n}\n\n}\n\n```\n\nFor the MSA Transformer used during encoding and mask filling ([GitHub](https://github.com/facebookresearch/esm)):\n\n```bibtex\n@article{rao2021msa,\n  author = {Rao, Roshan and Liu, Jason and Verkuil, Robert and Meier, Joshua and Canny, John F. and Abbeel, Pieter and Sercu, Tom and Rives, Alexander},\n  title={MSA Transformer},\n  year={2021},\n  doi={10.1101/2021.02.12.430858},\n  url={https://www.biorxiv.org/content/10.1101/2021.02.12.430858},\n  journal={bioRxiv}\n}\n```\n\nFor ProtBert and ProtBert-BFD used during encoding and mask filling ([GitHub](https://github.com/agemagician/ProtTrans)):\n\n```bibtex\n@article {Elnaggar2020.07.12.199554,\n\tauthor = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and BHOWMIK, DEBSINDHU and Rost, Burkhard},\n\ttitle = {ProtTrans: Towards Cracking the Language of Life{\\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing},\n\telocation-id = {2020.07.12.199554},\n\tyear = {2020},\n\tdoi = {10.1101/2020.07.12.199554},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tURL = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554},\n\teprint = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554.full.pdf},\n\tjournal = {bioRxiv}\n}\n```\n\nFor DeepSequence, used for zero-shot prediction ([GitHub](https://github.com/debbiemarkslab/DeepSequence)):\n\n```bibtex\n@article{Riesselman2018,\nauthor = {Riesselman, Adam J and Ingraham, John B and Marks, Debora S},\ndoi = {10.1038/s41592-018-0138-4},\njournal = {Nature Methods},\npages = {816--822},\ntitle = {Deep generative models of genetic variation capture the effects of mutations},\nurl = {http://dx.doi.org/10.1038/s41592-018-0138-4 http://www.nature.com/articles/s41592-018-0138-4},\nvolume = {15},\nyear = {2018}\n}\n```\n\nFor the EVcouplings webapp, used for zero-shot prediction ([GitHub](https://github.com/debbiemarkslab/EVcouplings)):\n\n```bibtex\n@article{Hopf2019,\nauthor = {Hopf, Thomas A. and Green, Anna G. and Schubert, Benjamin and Mersmann, Sophia and Sch{\\\"{a}}rfe, Charlotta P I and Ingraham, John B. and Toth-Petroczy, Agnes and Brock, Kelly and Riesselman, Adam J. and Palmedo, Perry and Kang, Chan and Sheridan, Robert and Draizen, Eli J. and Dallago, Christian and Sander, Chris and Marks, Debora S.},\ndoi = {10.1093/bioinformatics/bty862},\njournal = {Bioinformatics},\npages = {1582--1584},\ntitle = {The EVcouplings Python framework for coevolutionary sequence analysis},\nurl = {https://academic.oup.com/bioinformatics/article/35/9/1582/5124274},\nvolume = {35},\nyear = {2019}\n}\n```\n\nFor EVmutation, used for zero-shot prediction ([GitHub](https://github.com/debbiemarkslab/EVmutation)):\n\n```bibtex\n@article{Hopf2017,\nauthor = {Hopf, Thomas A. and Ingraham, John B. and Poelwijk, Frank J. and Sch{\\\"{a}}rfe, Charlotta P.I. and Springer, Michael and Sander, Chris and Marks, Debora S.},\ndoi = {10.1038/nbt.3769},\njournal = {Nature Biotechnology},\npages = {128--135},\ntitle = {Mutation effects predicted from sequence co-variation},\nurl = {http://dx.doi.org/10.1038/nbt.3769},\nvolume = {35},\nyear = {2017}\n}\n```\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and Fergus, Rob",
        "doi": "10.1073/pnas.2016239118",
        "format": "bibtex",
        "title": "{Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences}",
        "type": "Text_excerpt",
        "url": "http://www.pnas.org/lookup/doi/10.1073/pnas.2016239118",
        "value": "@article{Rives2021,\n    year = {2021},\n    volume = {118},\n    url = {http://www.pnas.org/lookup/doi/10.1073/pnas.2016239118},\n    title = {{Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences}},\n    journal = {Proceedings of the National Academy of Sciences},\n    doi = {10.1073/pnas.2016239118},\n    author = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and Fergus, Rob},\n}"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Rao, Roshan and Liu, Jason and Verkuil, Robert and Meier, Joshua and Canny, John F. and Abbeel, Pieter and Sercu, Tom and Rives, Alexander",
        "doi": "10.1101/2021.02.12.430858",
        "format": "bibtex",
        "title": "MSA Transformer",
        "type": "Text_excerpt",
        "url": "https://www.biorxiv.org/content/10.1101/2021.02.12.430858",
        "value": "@article{rao2021msa,\n    journal = {bioRxiv},\n    url = {https://www.biorxiv.org/content/10.1101/2021.02.12.430858},\n    doi = {10.1101/2021.02.12.430858},\n    year = {2021},\n    title = {MSA Transformer},\n    author = {Rao, Roshan and Liu, Jason and Verkuil, Robert and Meier, Joshua and Canny, John F. and Abbeel, Pieter and Sercu, Tom and Rives, Alexander},\n}"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and BHOWMIK, DEBSINDHU and Rost, Burkhard",
        "doi": "10.1101/2020.07.12.199554",
        "format": "bibtex",
        "title": "ProtTrans: Towards Cracking the Language of Life{\\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing",
        "type": "Text_excerpt",
        "url": "https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554",
        "value": "@article{Elnaggar2020.07.12.199554,\n    journal = {bioRxiv},\n    eprint = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554.full.pdf},\n    url = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554},\n    publisher = {Cold Spring Harbor Laboratory},\n    doi = {10.1101/2020.07.12.199554},\n    year = {2020},\n    elocation-id = {2020.07.12.199554},\n    title = {ProtTrans: Towards Cracking the Language of Life{\\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing},\n    author = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and BHOWMIK, DEBSINDHU and Rost, Burkhard},\n}"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Riesselman, Adam J and Ingraham, John B and Marks, Debora S",
        "doi": "10.1038/s41592-018-0138-4",
        "format": "bibtex",
        "title": "Deep generative models of genetic variation capture the effects of mutations",
        "type": "Text_excerpt",
        "url": "http://dx.doi.org/10.1038/s41592-018-0138-4 http://www.nature.com/articles/s41592-018-0138-4",
        "value": "@article{Riesselman2018,\n    year = {2018},\n    volume = {15},\n    url = {http://dx.doi.org/10.1038/s41592-018-0138-4 http://www.nature.com/articles/s41592-018-0138-4},\n    title = {Deep generative models of genetic variation capture the effects of mutations},\n    pages = {816--822},\n    journal = {Nature Methods},\n    doi = {10.1038/s41592-018-0138-4},\n    author = {Riesselman, Adam J and Ingraham, John B and Marks, Debora S},\n}"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Hopf, Thomas A. and Green, Anna G. and Schubert, Benjamin and Mersmann, Sophia and Sch{\\\"{a}}rfe, Charlotta P I and Ingraham, John B. and Toth-Petroczy, Agnes and Brock, Kelly and Riesselman, Adam J. and Palmedo, Perry and Kang, Chan and Sheridan, Robert and Draizen, Eli J. and Dallago, Christian and Sander, Chris and Marks, Debora S.",
        "doi": "10.1093/bioinformatics/bty862",
        "format": "bibtex",
        "title": "The EVcouplings Python framework for coevolutionary sequence analysis",
        "type": "Text_excerpt",
        "url": "https://academic.oup.com/bioinformatics/article/35/9/1582/5124274",
        "value": "@article{Hopf2019,\n    year = {2019},\n    volume = {35},\n    url = {https://academic.oup.com/bioinformatics/article/35/9/1582/5124274},\n    title = {The EVcouplings Python framework for coevolutionary sequence analysis},\n    pages = {1582--1584},\n    journal = {Bioinformatics},\n    doi = {10.1093/bioinformatics/bty862},\n    author = {Hopf, Thomas A. and Green, Anna G. and Schubert, Benjamin and Mersmann, Sophia and Sch{\\\"{a}}rfe, Charlotta P I and Ingraham, John B. and Toth-Petroczy, Agnes and Brock, Kelly and Riesselman, Adam J. and Palmedo, Perry and Kang, Chan and Sheridan, Robert and Draizen, Eli J. and Dallago, Christian and Sander, Chris and Marks, Debora S.},\n}"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "author": "Hopf, Thomas A. and Ingraham, John B. and Poelwijk, Frank J. and Sch{\\\"{a}}rfe, Charlotta P.I. and Springer, Michael and Sander, Chris and Marks, Debora S.",
        "doi": "10.1038/nbt.3769",
        "format": "bibtex",
        "title": "Mutation effects predicted from sequence co-variation",
        "type": "Text_excerpt",
        "url": "http://dx.doi.org/10.1038/nbt.3769",
        "value": "@article{Hopf2017,\n    year = {2017},\n    volume = {35},\n    url = {http://dx.doi.org/10.1038/nbt.3769},\n    title = {Mutation effects predicted from sequence co-variation},\n    pages = {128--135},\n    journal = {Nature Biotechnology},\n    doi = {10.1038/nbt.3769},\n    author = {Hopf, Thomas A. and Ingraham, John B. and Poelwijk, Frank J. and Sch{\\\"{a}}rfe, Charlotta P.I. and Springer, Michael and Sander, Chris and Marks, Debora S.},\n}"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/fhalab/MLDE"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-12-04T18:48:18Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-09-12T03:11:52Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "A machine-learning package for navigating combinatorial protein fitness landscapes."
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9656885317577162,
      "result": {
        "original_header": "MLDE",
        "type": "Text_excerpt",
        "value": "A machine-learning package for navigating combinatorial protein fitness landscapes. This repository accompanies our work \"[Informed training set design enables efficient machine learning-assisted directed protein evolution](https://doi.org/10.1016/j.cels.2021.07.008)\".\n \n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9955111669042689,
      "result": {
        "original_header": "Major Changes",
        "type": "Text_excerpt",
        "value": "2. Zero-shot prediction: We have added script support for zero-shot prediction using various sequence-based strategies. We have found that eliminating holes from training data greatly improves MLDE outcome. This focused training MLDE (ftMLDE) can be accomplished by using zero-shot prediction strategies to focus laboratory screening efforts on variants with higher probability of retaining function. From simple command line inputs, MLDE V1.0.0 enables zero-shot prediction of the fitness of all members of combinatorial design spaces using [EVmutation](https://doi.org/10.1038/nbt.3769), [DeepSequence](https://doi.org/10.1038/s41592-018-0138-4), and masked token filling using models from [ESM](https://github.com/facebookresearch/esm/tree/v0.3.0#evolutionary-scale-modeling) and [ProtTrans](https://github.com/agemagician/ProtTrans#prottrans). Details on EVmutation and DeepSequence can be found in their original papers. Details on mask filling can be found in our accompanying [paper](). We welcome additional suggestions on zero-shot strategies to wrap in the MLDE pipeline.\n3. To accomodate the new packages and code, two new conda environment files have been provided (mlde2.yml and deep_sequence.yml).\n4. Addition of a script for replicating simulations performed in our accompanying paper.\n \n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8996132565734395,
      "result": {
        "original_header": "Program Details",
        "type": "Text_excerpt",
        "value": "For making predictions, the top N model architectures (those with the lowest cross-validation error) are first identified. For each of the top N model architectures, predictions are made on the unsampled combinations by averaging the predictions of the k\uf0b4N model instances stored during cross validation. For instance, if testing the top 3 model architectures identified from 5-fold cross-validation, this means that the predictions of 3 x 5 = 15 total models (3 architectures x 5 model instances/architecture saved during cross validation) are used for prediction.\n \n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/fhalab/MLDE/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 26
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/fhalab/MLDE/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "fhalab/MLDE"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "MLDE"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/fhalab/MLDE/main/build_embeddings_run_zero_shot.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/fhalab/MLDE/main/run_pytests.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "type": "Text_excerpt",
        "value": "```bash\ngit clone --recurse-submodules https://github.com/fhalab/MLDE.git\n```\n\n2. The repository will be downloaded with the `mlde.yml`, `mlde2.yml`, and `deep_sequence.yml` anaconda environment templates in the top-level directory. Due to incompatibilities between the CUDA Toolkit version required by tensorflow 1.13 (used by tape-neurips2019 and the version of Keras used to perform MLDE predictions) and Pytorch >v1.5 (required by models from ESM), we could not find a single stable environment in which new code was compatible with old. Thus, **the `mlde.yml` environment should be used for all functionality from V0.0.0 (encoding with tape-neurips 2019 models as well as making predicitons with MLDE)** and **the `mlde2.yml` environment should be used for all functionality new in V1.0.0 (encoding with ESM and ProtBert models and for making zero-shot predictions)**. Note that all environments assume you have pre-installed CUDA and have a CUDA-capable GPU on your system. If you would rather work in a separate environment, dependencies for the full MLDE codebase can be found [here](#Dependencies). To build the conda environments, run\n\n```bash\ncd ./MLDE\nconda env create -f mlde.yml\nconda env create -f mlde2.yml\nconda env create -f deep_sequence.yml\n```\n\nThe environments must be created from within the MLDE repository, otherwise [tape-neurips2019](https://github.com/songlab-cal/tape-neurips2019) will not be correctly installed.\n\n3. Finally, we need to download the model weights needed for generating learned embeddings using [tape-neurips2019](https://github.com/songlab-cal/tape-neurips2019). Navigate to the tape-neurips2019 submodule and download the model weights as below:\n\n```bash\ncd ./code/tape-neurips2019\n./download_pretrained_models.sh\n```\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Basic Tests",
        "parent_header": [
          "Installation",
          "Installation Validation"
        ],
        "type": "Text_excerpt",
        "value": "```bash\nconda activate mlde\npython generate_encoding.py transformer GB1_T2Q --fasta\n  ./code/validation/basic_test_data/2GI9.fasta --positions V39 D40 --batches 1\n```\n\nTo run predict_zero_shot.py:\n\n```bash\nconda activate mlde2\npython predict_zero_shot.py --positions V39 D40 --models esm1_t6_43M_UR50S\n--fasta ./code/validation/basic_test_data/2GI9.fasta --include_conditional\n```\n\nTo run execute_mlde.py:\n\n```bash\nconda activate mlde\npython execute_mlde.py ./code/validation/basic_test_data/InputValidationData.csv\n  ./code/validation/basic_test_data/GB1_T2Q_georgiev_Normalized.npy\n  ./code/validation/basic_test_data/GB1_T2Q_ComboToIndex.pkl\n  --model_params ./code/validation/basic_test_data/TestMldeParams.csv\n  --hyperopt\n```\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Pytest Validation",
        "parent_header": [
          "Installation",
          "Installation Validation"
        ],
        "type": "Text_excerpt",
        "value": "```bash\nPYTHONPATH=$PathToMldeRepo ./run_pytests.sh\n```\n\nWhere $PathToMldeRepo should be the path to the folder in which `run_pytests.sh` is stored. Note that these tests can take a while, so it is best to run overnight.\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/fhalab/MLDE/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Other",
        "spdx_id": "NOASSERTION",
        "type": "License",
        "url": null,
        "value": null
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "Copyright \u00a9 2020, California Institute of Technology based on research performed under the NSF Division of Chemical, Bioengineering, Environmental and Transport Systems Grant CBET 1937902. All rights reserved.\n\n\nRedistribution and use in source and binary forms for academic and other non-commercial purposes with or without modification, are permitted provided that the following conditions are met:\n \n\n- Redistributions of source code, including modified source code, must retain the above copyright notice, this list of conditions and the following disclaimer.\n \n\n- Redistributions in binary form or a modified form of the source code must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n \n\n- Neither the name of Caltech, any of its trademarks, the names of its employees, nor contributors to the source code may be used to endorse or promote products derived from this software without specific prior written permission.\n \n\n- Where a modified version of the source code is redistributed publicly in source or binary forms, the modified source code must be published in a freely accessible manner, or otherwise redistributed at no charge to anyone requesting a copy of the modified source code, subject to the same terms as this agreement.\n \n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/LICENSE.md",
      "technique": "file_exploration"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "MLDE"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "fhalab"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 476355,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 5424,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 3608,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "brucejwittmann",
          "type": "User"
        },
        "date_created": "2021-08-20T00:42:25Z",
        "date_published": "2021-08-20T00:48:29Z",
        "description": "Major changes include:\r\n\r\n1. Major overhaul to the underlying package structure.\r\n2. Support for deriving encodings from ESM and ProtTrans.\r\n3. Support for zero-shot prediction of combinatorial library fitness. Currently supported strategies are EVmutation, DeepSequence, and masked-token prediction with models from ESM and ProtTrans.\r\n4. Script support for replicating simulated MLDE experiments.\r\n",
        "html_url": "https://github.com/fhalab/MLDE/releases/tag/v1.0.0",
        "name": "Cell Systems Publication Accompaniment ",
        "release_id": 48151284,
        "tag": "v1.0.0",
        "tarball_url": "https://api.github.com/repos/fhalab/MLDE/tarball/v1.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/fhalab/MLDE/releases/48151284",
        "value": "https://api.github.com/repos/fhalab/MLDE/releases/48151284",
        "zipball_url": "https://api.github.com/repos/fhalab/MLDE/zipball/v1.0.0"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "brucejwittmann",
          "type": "User"
        },
        "date_created": "2020-12-05T02:28:41Z",
        "date_published": "2021-07-02T00:25:25Z",
        "description": "This is the same as v0.0.0. This release is being made to link the repository to a permanent archive. ",
        "html_url": "https://github.com/fhalab/MLDE/releases/tag/v0.0.1",
        "name": "Release to Make DOI for Record Keeping",
        "release_id": 45594111,
        "tag": "v0.0.1",
        "tarball_url": "https://api.github.com/repos/fhalab/MLDE/tarball/v0.0.1",
        "type": "Release",
        "url": "https://api.github.com/repos/fhalab/MLDE/releases/45594111",
        "value": "https://api.github.com/repos/fhalab/MLDE/releases/45594111",
        "zipball_url": "https://api.github.com/repos/fhalab/MLDE/zipball/v0.0.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "brucejwittmann",
          "type": "User"
        },
        "date_created": "2020-12-05T02:28:41Z",
        "date_published": "2020-12-05T02:31:38Z",
        "html_url": "https://github.com/fhalab/MLDE/releases/tag/v0.0.0",
        "name": "bioRxiv_Publication_Accompaniment ",
        "release_id": 34864427,
        "tag": "v0.0.0",
        "tarball_url": "https://api.github.com/repos/fhalab/MLDE/tarball/v0.0.0",
        "type": "Release",
        "url": "https://api.github.com/repos/fhalab/MLDE/releases/34864427",
        "value": "https://api.github.com/repos/fhalab/MLDE/releases/34864427",
        "zipball_url": "https://api.github.com/repos/fhalab/MLDE/zipball/v0.0.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "MLDE Software",
        "parent_header": [
          "Dependencies",
          "Software"
        ],
        "type": "Text_excerpt",
        "value": "  - python=3.7.3\n  - numpy=1.16.4\n  - pandas=0.25.3\n  - tqdm=4.32.1\n  - biopython=1.74\n  - hyperopt=0.2.2\n  - scipy=1.3.0\n  - scikit-learn=0.21.2\n  - tensorflow-gpu=1.13.1\n  - keras=2.2.5\n  - xgboost=0.90\n  - nltk=3.4.4\n  - psutil\n  - pip\n  - pytorch>=1.5\n  - torchvision\n  - torchaudio\n  - cudatoolkit\n  - transformers\n\n  - evcouplings\n\nAny specific versions listed were those used during the development of MLDE. There should be some leeway if users use different versions, though if running in a new environment, it is strongly recommended to perform the [pytest validation](#Installation-Validation) first.\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "DeepSequence",
        "parent_header": [
          "Dependencies",
          "Software"
        ],
        "type": "Text_excerpt",
        "value": "  - python=2.7\n  - theano=1.0.1\n  - cudatoolkit=10.1\n  - cudnn\n  - biopython\n  - pandas\n  - backports.functools_lru_cache\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "download",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 12:15:59",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 118
      },
      "technique": "GitHub_API"
    }
  ],
  "support": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citing Supporting Repositories",
        "type": "Text_excerpt",
        "value": "For the ESM module used during encoding and masking filling ([GitHub](https://github.com/facebookresearch/esm)):\n\n```bibtex\n@article{Rives2021,\nauthor = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and Fergus, Rob},\ndoi = {10.1073/pnas.2016239118},\njournal = {Proceedings of the National Academy of Sciences},\ntitle = {{Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences}},\nurl = {http://www.pnas.org/lookup/doi/10.1073/pnas.2016239118},\nvolume = {118},\nyear = {2021}\n}\n\n}\n\n```\n\nFor the MSA Transformer used during encoding and mask filling ([GitHub](https://github.com/facebookresearch/esm)):\n\n```bibtex\n@article{rao2021msa,\n  author = {Rao, Roshan and Liu, Jason and Verkuil, Robert and Meier, Joshua and Canny, John F. and Abbeel, Pieter and Sercu, Tom and Rives, Alexander},\n  title={MSA Transformer},\n  year={2021},\n  doi={10.1101/2021.02.12.430858},\n  url={https://www.biorxiv.org/content/10.1101/2021.02.12.430858},\n  journal={bioRxiv}\n}\n```\n\nFor ProtBert and ProtBert-BFD used during encoding and mask filling ([GitHub](https://github.com/agemagician/ProtTrans)):\n\n```bibtex\n@article {Elnaggar2020.07.12.199554,\n\tauthor = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and BHOWMIK, DEBSINDHU and Rost, Burkhard},\n\ttitle = {ProtTrans: Towards Cracking the Language of Life{\\textquoteright}s Code Through Self-Supervised Deep Learning and High Performance Computing},\n\telocation-id = {2020.07.12.199554},\n\tyear = {2020},\n\tdoi = {10.1101/2020.07.12.199554},\n\tpublisher = {Cold Spring Harbor Laboratory},\n\tURL = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554},\n\teprint = {https://www.biorxiv.org/content/early/2020/07/21/2020.07.12.199554.full.pdf},\n\tjournal = {bioRxiv}\n}\n```\n\nFor DeepSequence, used for zero-shot prediction ([GitHub](https://github.com/debbiemarkslab/DeepSequence)):\n\n```bibtex\n@article{Riesselman2018,\nauthor = {Riesselman, Adam J and Ingraham, John B and Marks, Debora S},\ndoi = {10.1038/s41592-018-0138-4},\njournal = {Nature Methods},\npages = {816--822},\ntitle = {Deep generative models of genetic variation capture the effects of mutations},\nurl = {http://dx.doi.org/10.1038/s41592-018-0138-4 http://www.nature.com/articles/s41592-018-0138-4},\nvolume = {15},\nyear = {2018}\n}\n```\n\nFor the EVcouplings webapp, used for zero-shot prediction ([GitHub](https://github.com/debbiemarkslab/EVcouplings)):\n\n```bibtex\n@article{Hopf2019,\nauthor = {Hopf, Thomas A. and Green, Anna G. and Schubert, Benjamin and Mersmann, Sophia and Sch{\\\"{a}}rfe, Charlotta P I and Ingraham, John B. and Toth-Petroczy, Agnes and Brock, Kelly and Riesselman, Adam J. and Palmedo, Perry and Kang, Chan and Sheridan, Robert and Draizen, Eli J. and Dallago, Christian and Sander, Chris and Marks, Debora S.},\ndoi = {10.1093/bioinformatics/bty862},\njournal = {Bioinformatics},\npages = {1582--1584},\ntitle = {The EVcouplings Python framework for coevolutionary sequence analysis},\nurl = {https://academic.oup.com/bioinformatics/article/35/9/1582/5124274},\nvolume = {35},\nyear = {2019}\n}\n```\n\nFor EVmutation, used for zero-shot prediction ([GitHub](https://github.com/debbiemarkslab/EVmutation)):\n\n```bibtex\n@article{Hopf2017,\nauthor = {Hopf, Thomas A. and Ingraham, John B. and Poelwijk, Frank J. and Sch{\\\"{a}}rfe, Charlotta P.I. and Springer, Michael and Sander, Chris and Marks, Debora S.},\ndoi = {10.1038/nbt.3769},\njournal = {Nature Biotechnology},\npages = {128--135},\ntitle = {Mutation effects predicted from sequence co-variation},\nurl = {http://dx.doi.org/10.1038/nbt.3769},\nvolume = {35},\nyear = {2017}\n}\n```\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Inputs for generate_encoding.py",
        "parent_header": [
          "General Use",
          "Generating Encodings with generate_encoding.py"
        ],
        "type": "Text_excerpt",
        "value": "| Argument | Type | Description |\n|:---------|---------------|-------------|\n| encoding | Required Positional Argument | This argument sets the encoding type used. Choices include \"onehot\", \"georgiev\", \"resnet\", \"bepler\", \"unirep\", \"transformer\", \"lstm\", \"esm_msa1_t12_100M_UR50S\", \"esm1b_t33_650M_UR50S\", \"esm1_t34_670M_UR50S\", \"esm1_t34_670M_UR50D\", \"esm1_t34_670M_UR100\", \"esm1_t12_85M_UR50S\", \"esm1_t6_43M_UR50S\", \"prot_bert_bfd\", or \"prot_bert\". Models should be typed exactly as given in this table (including casing) when passed into generate_encoding.py.|\n| protein_name | Required Positional Argument | Nickname for the protein. Will be used to prefix output files.|\n| --fasta | Required Keyword Argument for Learned Embeddings | The parent protein amino acid sequence in fasta file format. If using the MSA Transformer from ESM (esm_msa1_t12_100M_UR50S), then this should be a .a2m or .a3m file where the first sequence in the alignment is the reference sequence. See the section on [Building an Alignment for MSA Transformer](#building-an-alignment-for-msa-transformer) for instructions on preparing an msa for an input.|\n| --positions | Required Keyword Argument for Learned Embeddings | The positons and parent amino acids to include in the combinatorial library. Input format must be \"AA# AA#\" and should be in numerical order of the positions. For instance, to mutate positions Q3 and F97 in combination, the flag would be written as `--positions Q3 F97`.|\n| --n_combined | Required Keyword Argument for Georgiev or Onehot Encodings | The number of positions in the combinatorial space. |\n| --output | Optional Keyword Argument | Output location for saving data. Default is the current working directory if not specified. |\n| --batches | Optional Keyword Argument | Generating the largest embedding spaces can require high levels of system RAM. This parameter dictates how many batches to split a job into. If not specified, the program will attempt to automatically determine the appropriate number of batches given the available RAM on your system. |\n| --batch_size | Optional Keyword Argument | Sets the batch size of calculation for the ESM and ProtTrans models. If processing on a GPU, increasing batch size can result in shorter processing time. The default is \"4\" when not explicitly set. This default is too small for most models, and is set to accommodate the largest models on standard commercial GPUs. |\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Examples for generate_encoding.py",
        "parent_header": [
          "General Use",
          "Generating Encodings with generate_encoding.py"
        ],
        "type": "Text_excerpt",
        "value": "```bash\nconda activate mlde\npython generate_encoding.py georgiev example_protein --n_combined 4\n```\n\nThe below example is a command line call for generating transformer embeddings for the 4-site GB1 combinatorial library discussed in our work. Note that this command could be used for generating any of the learned embeddings (with appropriate substitution of the `encoding` argument). Because embeddings are context-aware, these encodings should not be used for another protein.\n\n```bash\nconda activate mlde\npython generate_encoding.py transformer GB1_T2Q\n  --fasta ./code/validation/basic_test_data//2GI9.fasta\n  --positions V39 D40 G41 V54 --batches 4\n```\n\nNote that the `mlde2` environment would be used for ESM and ProtTrans-based models in the above example. The input fasta file looks as below:\n\n```\n>GB1_T2Q\nMQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE\n```\n\nNote that if using the MSA transformer, then the fasta file should be replaced with a .a2m file. The first few rows of this file will look something like the below (using GB1 as an example):\n\n```\n>TARGET/1-56\nmqykLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKtftvte\n>UniRef100_Q53975/224-278\n.tykLVVKGNTFSGETTTKAIDTATAEKEFKQYATANNVDGEWSYDDATKtftvte\n>UniRef100_Q53975/294-348\n.tykLIVKGNTFSGETTTKAVDAETAEKAFKQYATANNVDGEWSYDDATKtftvte\n>UniRef100_Q53975/364-418\n.tykLIVKGNTFSGETTTKAIDAATAEKEFKQYATANGVDGEWSYDDATKtftvte\n>UniRef100_Q53975/434-488\n.tykLIVKGNTFSGETTTKAVDAETAEKAFKQYANENGVYGEWSYDDATKtftvte\n>UniRef100_Q53975/504-558\n.tykLVINGKTLKGETTTKAVDAETAEKAFKQYANENGVDGVWTYDDATKtftvte\n```\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Outputs for generate_encoding.py",
        "parent_header": [
          "General Use",
          "Generating Encodings with generate_encoding.py"
        ],
        "type": "Text_excerpt",
        "value": "The \"Encodings\" folder will contain the below files (\"\\$NAME\" is from the `name` argument of generate_encoding.py; \"\\$ENCODING\" is from the `encoding` argument):\n\n| Filename | Description |\n|:---------|-------------|\n|\\$NAME_\\$ENCODING_Normalized.npy| Numpy array containing the mean-centered, unit-scaled amino acid embeddings. These are the embeddings that will typically be used for generating predictions, and take the shape $20^C x C x L$, where $C$ is the number of amino acid positions combined and $L$ is the number of latent dimensions per amino acid for the encoding.|\n|\\$NAME_\\$ENCODING_UnNormalized.npy| Numpy array containing the unnormalized amino acid embeddings. This tensor will take the same shape as \\$NAME_\\$ENCODING_Normalized.npy.|\n|\\$NAME_\\$ENCODING_ComboToIndex.pkl| A pickle file containing a dictionary linking amino acid combination to the index of that combination in the output encoding tensors. Note that combinations are reported in order of amino acid index (e.g. a combination of A14, C23, Y60, and W91 would be written as \"ACYW\").|\n|\\$NAME_\\$ENCODING_IndexToCombo.pkl| A pickle file containing a dictionary that relates index in the encoding tensor to the combination.|\n\nNote that when encoding is \"onehot\", only unnormalized embeddings will be returned.\n\nThe \"Fastas\" directory is only populated when models from TAPE are run. It contains fasta files with all sequences used to generate embeddings, split into batches as appropriate.\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Inputs for predict_zero_shot.py",
        "parent_header": [
          "General Use",
          "Zero Shot Prediction with predict_zero_shot.py and run_deepsequence.py"
        ],
        "type": "Text_excerpt",
        "value": "\n| Argument | Type | Description |\n|:---------|---------------|-------------|\n| --positions | Required Keyword Argument | The positons and parent amino acids to include in the combinatorial library. Input format must be \"AA# AA#\" and should be in numerical order of the positions. For instance, to mutate positions Q3 and F97 in combination, the flag would be written as `--positions Q3 F97`.|\n| --models | Required Keyword Argument | The models to use for zero-shot prediction. Options include \"EVmutation\", \"esm_msa1_t12_100M_UR50S\", \"esm1b_t33_650M_UR50S\", \"esm1_t34_670M_UR50S\", \"esm1_t34_670M_UR50D\", \"esm1_t34_670M_UR100\", \"esm1_t12_85M_UR50S\", \"esm1_t6_43M_UR50S\", \"prot_bert_bfd\", or \"prot_bert\". Models should be typed exactly as given in this table (including casing) when passed into predict_zero_shot.py. Note that multiple models can be passed in in a single run by separating each model name with a space (e.g. `--models esm1_t34_670M_UR50S prot_bert` would provide zero-shot predictions using mask filling with both esm1_t34_670M_UR50S and protbert). |\n| --fasta | Required Keyword Argument for all but the MSA transformer | The parent protein amino acid sequence in fasta file format. |\n| --alignment | Required Keyword Argument for the MSA Transformer | A .a2m or .a3m file containing the parent sequence (which should be first in the alignment) and all aligned sequences. See the below section on [Building an Alignment for MSA Transformer](#building-an-alignment-for-msa-transformer) for further details|\n| --evmutation_model | Required Keyword Argument for EVmutation | A model parameters file describing an EVmutation model. See the below section on [Building an Alignment for DeepSequence and Obtaining an EVmutation Model](#building-an-alignment-for-deepSequence-and-obtaining-an-evmutation-model) for further details.\n| --include_conditional | Flag | By default, any mask-filling model will use naive probability for calculating zero-shot predictions. If this flag is included, then predictions using conditional probability will also be returned. |\n| --mask_col | Flag | This only applies to the MSA transformer. By default, only positions in the reference sequence are masked during mask-filling. If this flag is included, then the entire alignment column is masked instead. |\n| --batch_size | Optional Keyword Argument | Sets the batch size of calculation for the ESM and ProtTrans models. If processing on a GPU, increasing batch size can result in shorter processing time. The default is \"4\" when not explicitly set. This default is too small for most models, and is set to accommodate the largest models on standard commercial GPUs. |\n| --output | Optional Keyword Argument | Output location for saving data. Default is the current working directory if not specified. |\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Inputs for run_deepsequence.py",
        "parent_header": [
          "General Use",
          "Zero Shot Prediction with predict_zero_shot.py and run_deepsequence.py"
        ],
        "type": "Text_excerpt",
        "value": "Inputs to `run_deepsequence.py` are given below:\n\n\n| Argument | Type | Description |\n|:---------|---------------|-------------|\n| alignment | Required Positional Argument | A .a2m file containing the parent sequence (which should be first in the alignment) and all aligned sequences. .a3m files will not work here. See the below section on [Building an Alignment for DeepSequence and Obtaining an EVmutation Model](#building-an-alignment-for-deepSequence-and-obtaining-an-evmutation-model) for further details.|\n| --positions | Required Keyword Argument | The positons and parent amino acids to include in the combinatorial library. Input format must be \"AA# AA#\" and should be in numerical order of the positions. For instance, to mutate positions Q3 and F97 in combination, the flag would be written as `--positions Q3 F97`.|\n| --output | Optional Keyword Argument | Output location for saving data. Default is the current working directory if not specified. |\n| --save_model | Flag | Whether or not to save the model parameters of the trained VAE model. If set, then parameters will be saved to the DeepSequence submodule at /examples/params/ |\n| --no_cudnn | Flag | DeepSequence runs on Theano, which is no longer a supported package. As a result, newer cuDNN libraries are no longer compatbile with its code and can cause this script to fail. If you are running into compatibility issues when running this code, set this flag to turn off use of cuDNN; this will slow down computation, but should not change the output. |\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Building an Alignment for DeepSequence and Obtaining an EVmutation Model",
        "parent_header": [
          "General Use",
          "Zero Shot Prediction with predict_zero_shot.py and run_deepsequence.py"
        ],
        "type": "Text_excerpt",
        "value": "The optimal parameters for the homology search will vary depending on the protein, and we refer users to the original [EVmutation](https://doi.org/10.1038/nbt.3769) and [DeepSequence](https://doi.org/10.1038/s41592-018-0138-4) papers for information on how to best tune them. Once the homology search is complete, alignments can be downloaded in a2m format by navigating to the \"Downloads\" tab of the results interface and clicking \"Sequence alignment\" in the \"Sequence alignment\" section. The model parameters for an EVmutation model trained using this alignment can also be obtained by clicking \"EVcouplings model parameters\"; the downloaded file should be input as the `--evmutation_model` argument in predict_zero_shot.py.\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Examples for predict_zero_shot.py",
        "parent_header": [
          "General Use",
          "Zero Shot Prediction with predict_zero_shot.py and run_deepsequence.py"
        ],
        "type": "Text_excerpt",
        "value": "predict_zero_shot.py should be run from within the mlde2 environment. The below code would make zero-shot predictions for the 4-site GB1 combinatorial library using EVmutation, the MSA Transformer, and ESM1b, considering both conditional and naive probability for the mask-filling models.\n\n```bash\nconda activate mlde2\npython predict_zero_shot.py --positions V39 D40 G41 V54 --models esm1b_t33_650M_UR50S\n    EVmutation esm_msa1_t12_100M_UR50S --fasta ./code/validation/basic_test_data/2GI9.fasta\n    --alignment ./code/validation/basic_test_data/GB1_Alignment.a2m\n    --evmutation_model ./code/validation/basic_test_data/GB1_EVcouplingsModel.model\n    --include_conditional --batch_size 32 --output ~/Downloads\n```\n\nNote that not all of the flags included above need to be used; they are present just for demonstration purposes. Inclusion of `--mask_col` would result in masking the full columns when performing mask filling with the MSA transformer.\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Examples for run_deep_sequence.py",
        "parent_header": [
          "General Use",
          "Zero Shot Prediction with predict_zero_shot.py and run_deepsequence.py"
        ],
        "type": "Text_excerpt",
        "value": "Within the deep_sequence environment, DeepSequence can be run with the below command:\n\n```bash\nconda activate deep_sequence\npython run_deep_sequence.py ./code/validation/basic_test_data/GB1_Alignment.a2m\n  --positions V39 D40 --output ~/Downloads --save_model --no_cudnn\n```\n\nNote that all flags/arguments are shown for sake of example. In practice, the `--output`, `--save_model`, and `--no_cudnn` flags may be ommitted and the code will still run.\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Outputs for predict_zero_shot.py and run_deep_sequence.py",
        "parent_header": [
          "General Use",
          "Zero Shot Prediction with predict_zero_shot.py and run_deepsequence.py"
        ],
        "type": "Text_excerpt",
        "value": "Both `predict_zero_shot.py` and `run_deep_sequence.py` output a single csv file to the directory specified by `--output`. The first column in this csv file is \"Combo\", which contains the 4-letter shorthand notation for all combinations possible in the defined combinatorial library. For instance, the wild type combo V39, D40, G41, V54 for GB1 would be written as \"VDGV\" in this output. For the `run_deep_sequence.py` output, the only other column is \"DeepSequence\", which contains all predictions made by the trained model. For the `predict_zero_shot.py` output, all remaining columns are the outputs from the different zero-shot predictors requested with the `--models` argument along with any other relevant information about the prediction separated by a \"-\" delimiter. For instance, the column name \"esm1_t34_670M_UR50S-Naive\" would indicate that the output corresponds to predictions using the esm1_t34_670M_UR50S model from ESM using naive probability for mask filling. Outputs from EVmutation and DeepSequence are \u0394ELBo, while outputs from all other models are log probabilities. **For all models, a higher (less negative) zero-shot score means greater confidence that that variant will be functional.**\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Inputs for execute_mlde.py",
        "parent_header": [
          "General Use",
          "Making Predictions with execute_mlde.py"
        ],
        "type": "Text_excerpt",
        "value": "|:---------|-----------|-------------|\n| training_data | Required Argument | A csv file containing the sequence-function information for sampled combinations. More information on this file can be found [below](#trainingdata.csv). |\n| encoding_data | Required Argument | A numpy array containing the embedding information for the full combinatorial space. Encoding arrays generated by generate_encoding.py can be passed directly in here. Custom encodings can be passed in here too, the details of which are discussed [below](#custom-encodings). |\n| combo_to_ind_dict | Required Argument | A pickle file containing a dictionary that links a combination to its index. The ComboToIndex.pkl file output by generate_encoding.py can be passed in directly here. |\n| model_params | Optional Argument | A csv file dictating which inbuilt MLDE models to use as well as how many rounds of hyperparameter optimization to perform. The makeup of this file is discussed [below](#mldeparameters.csv). |\n| output | Optional Argument | The location to save the results. Default is the current working directory. |\n| n_averaged | Optional Argument | The number of top-performing models to average to get final prediction results. Default is 3. |\n| n_cv | Optional Argument | The number of rounds of cross validation to perform during training. Default is 5. |\n| no_shuffle | Flag | When set, the indices of the training data will **not** be shuffled for cross-validation. Default is to shuffle indices.|\n| hyperopt | Flag | When set, hyperparameter optimization will also be performed. Note that this can greatly increase the run time of MLDE depending on the models included in the run. The default is to not perform hyperparameter optimization. |\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "TrainingData.csv",
        "parent_header": [
          "General Use",
          "Making Predictions with execute_mlde.py",
          "Inputs for execute_mlde.py"
        ],
        "type": "Text_excerpt",
        "value": "| AACombo | Fitness |\n|:--------|---------|\n| CCCC | 0.5451 |\n| WCPC | 0.0111 |\n| WPGC | 0.0097 |\n| WGPP | 0.0022 |\n\nThe two column headers must always be present and always have the same name. Sequence is input as the combination identity, which is the amino acid present at each position in order. For instance, a combination of A14, C23, Y60, and W91 would be written as \"ACYW\".\n\nWhile not strictly required, it is recommended to normalize fitness in some manner. Common normalization factors would be the fitness of the parent protein or the maximum fitness in the training data.\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Custom Encodings",
        "parent_header": [
          "General Use",
          "Making Predictions with execute_mlde.py",
          "Inputs for execute_mlde.py"
        ],
        "type": "Text_excerpt",
        "value": "Note that for all but the convolutional neural networks, the last 2 dimensions of the input space will be flattened before processing. In other words, convolutional networks are trained on 2D encodings and all other models on 1D encodings.\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "mlde_parameters.csv",
        "parent_header": [
          "General Use",
          "Making Predictions with execute_mlde.py",
          "Inputs for execute_mlde.py"
        ],
        "type": "Text_excerpt",
        "value": "| ModelClass | SpecificModel | Include | NHyperopt |\n|:-----------|---------------|---------|-----------|\n|Keras | NoHidden | TRUE | 10\n|Keras | OneHidden | TRUE | 10\n|Keras | TwoHidden | TRUE | 10\n|Keras | OneConv | TRUE | 10\n|Keras | TwoConv | TRUE | 10\n|XGB | Tree | TRUE | 100\n|XGB | Linear | TRUE | 100\n|XGB | Tree-Tweedie | TRUE | 100\n|XGB | Linear-Tweedie | TRUE | 100\n|sklearn-regressor | Linear | TRUE | 100\n|sklearn-regressor | GradientBoostingRegressor | TRUE | 100\n|sklearn-regressor | RandomForestRegressor | TRUE | 100\n|sklearn-regressor | BayesianRidge | TRUE | 100\n|sklearn-regressor | LinearSVR | TRUE | 100\n|sklearn-regressor | ARDRegression | TRUE | 100\n|sklearn-regressor | KernelRidge | TRUE | 100\n|sklearn-regressor | BaggingRegressor | TRUE | 100\n|sklearn-regressor | LassoLarsCV | TRUE | 100\n|sklearn-regressor | DecisionTreeRegressor | TRUE | 100\n|sklearn-regressor | SGDRegressor | TRUE | 100\n|sklearn-regressor | KNeighborsRegressor | TRUE | 100\n|sklearn-regressor | ElasticNet | TRUE | 100\n\nThe column names should not be changed. Changing the \"Include\" column contents to 'FALSE' will stop a model from being included in the ensemble trained for MLDE; the same can be accomplished by simply deleting the row. The \"NHyperopt\" column contents can be changed to alter how many hyperparameter optimization rounds are performed when the `hyperopt` flag is thrown. Note that Keras-based models can take a long time for hyperparameter optimization, hence why only 10 rounds are performed by default.\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Examples for execute_mlde.py",
        "parent_header": [
          "General Use",
          "Making Predictions with execute_mlde.py"
        ],
        "type": "Text_excerpt",
        "value": "```bash\nconda activate mlde\npython execute_mlde.py .code/validation/basic_test_Data/InputValidationData.csv\n    ./code/validation/basic_test_data/GB1_T2Q_georgiev_Normalized.npy\n    ./code/validation/basic_test_data/GB1_T2Q_ComboToIndex.pkl\n    --model_params ../code/validation/basic_test_data/TestMldeParams.csv\n    --output ~/Downloads --n_averaged 5 --n_cv 10 --hyperopt\n```\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Outputs for execute_mlde.py",
        "parent_header": [
          "General Use",
          "Making Predictions with execute_mlde.py"
        ],
        "type": "Text_excerpt",
        "value": "| Filename | Description |\n|:---------|-------------|\n| PredictedFitness.csv | This csv file reports the average predicted fitness of the top models given by `n_averaged` for all possible combinations (including combinations in the training data). Whether a combination was present in the training data or not is clearly marked. |\n| LossSummaries.csv | This csv file reports the cross-validation training and testing error of the best models from each class. |\n| CompoundPreds.npy | This numpy file contains an array with shape `M x 20^C`, where `M` is the number of models and `C` is the number of amino acids in the combinatorial space. This array gives the average predictions of the top-M models for all possible combinations. For instance, index 0 gives the predictions of the best model; index 1 gives the average predictions of the top 2 models, and so on. |\n| IndividualPreds.npy| This numpy array gives the predictions of all models, ordered by the model's cross-validation testing error. This array is the same shape as CompoundPreds.npy. |\n| PredictionStandardDeviation.npy| This numpy array gives the standard deviation of predictions across the models generated from different cross-validation steps. It has the same shape and ordering as IndividualPreds.npy. |\n|HyperoptInfo.csv | This csv file gives details on the hyperparameter optimization procedure, including parameter values tested and associated cross-validation errors in each iteration. |\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Replicating Published Results with simulate_mlde.py",
        "parent_header": [
          "General Use"
        ],
        "type": "Text_excerpt",
        "value": "The CaltechData folder contains 4 objects:\n1. SimulationTrainingData\n2. AllSummaries_ByModel.csv\n3. AllSummaries_Ensemble.csv\n4. README.txt\n\nThe csv files contain summary information for all simulations performed in our work either for predictions made by independent models (ByModel) or an ensemble of the best models (Ensemble). Relevant information about the contents of these files can be found in the README.txt file. To perform simulations using `simulate_mlde.py`, you must first move the folder \"SimulationTrainingData\" to the top-level MLDE directory (i.e., in the same directory as `simulate_mlde.py`).\n\n`simulate_mlde.py` should be run in the `mlde` conda environment. Each call to `simulate_mlde.py` will perform simulations using one design condition tested in our work. For instance, a single run of `simulate_mlde.py` could be used to run the simulations using onehot encoding with 384 training points drawn from the top 6400 samples predicted by Triad. Arguments to the script are given in detail below:\n\n| Argument | Type | Description |\n|:---------|-----------|-------------|\n| encoding | Required Argument | The encoding to use for running simulations. Options include \"bepler\", \"esm1b_t33_650M_UR50S.npy\", \"georgiev\", \"lstm\", \"msa_transformer\", \"onehot\", \"ProtBert_BFD\", \"resnet\", \"transformer\", and \"unirep\". |\n| training_type | Required Argument | The type of training indices to use. Options include \"random\", \"triad\", \"evmutation\", \"msatransformer\", or \"sim\". \"random\" means simulations using random training data will be performed; \"triad\", \"evmutation\", and \"msatransformer\" all mean simulations using training data derived from zero-shot predictors will be performed; \"sim\" means simulations using artificially fitness-inflated training data will be performed. All options but \"random\" require additional information provided by the `training_specifics` keyword argument. |\n| training_samples | Required Argument | The amount of training data to use. Options include \"384\", \"48\", or \"24\". |\n| models_used | Required Argument | Options include \"CPU\", \"GPU\", \"LimitedSmall\", \"LimitedLarge\", and \"XGB\". Each option means simulations will be performed using models defined in a different MLDE parameter file. \"CPU\" will launch simulations run using all CPU-based models. \"GPU\" will launch simulations run using all GPU-based models. \"XGB\" will launch simulations run using all XGBoost-based models. \"LimitedSmall\" will launch all CPU-based models except for the sklearn ARDRegression, sklearn BaggingRegressor, and sklearn KNeighborsRegressor models -- these were the models omitted for simulations run using high-dimensional encodings with a small training set size. \"LimitedLarge\" will launch all CPU-based models except for the sklearn RandomForestRegressor, sklearn BaggingRegressor, and sklearn KNeighborsRegressor models -- these were the models omitted for simulations run using high-dimensional encodings with a large training set size. Simulations run using all options except \"GPU\" will spread across all available processors by default. Simulations run using \"GPU\" can only be run on a single thread. We provide \"XGB\" as a separate implementation as XGBoost models can require high levels of system RAM for larger encodings. For completing simulations efficiently, it can often be best to run XGBoost models independently on less cores (to save on RAM) and then perform all other CPU-based simulations across all available cores -- results can later be concatenated for processing. |\n| saveloc | Required Argument | Where to save results |\n| --training_specifics | Required Keyword Argument for All but `training_type = random` | This argument determines the sampling threshold to use when making simulations with zero-shot or artificially-fitness-inflated derived data. When `training_type` is \"triad\", \"evmutation\", or \"msatransformer\", options are \"1600\", \"3200\", \"6400\", \"9600\", \"12800\", \"16000\", or \"32000\". When `training_type` is \"sim\", options are \"0\", \"0.1\", \"0.3\", \"0.5\", or \"0.7\", which are the unnormalized fitness values of the thresholds used for the design of artifically fitness-inflated training data (i.e., 0.1 = 0.011 in our paper, 0.3 = 0.034, etc.) |\n| --sim_low | Optional Keyword Argument | The simulation index (0-indexed) to start at, inclusive. By default, this is \"0\". |\n| --sim_high | Optional Keyword Argument | The simulation index (0-indexed) to end at, exclusive. By default, this is \"2000\". |\n| --n_jobs | Optional Keyword Argument | The number of CPUs to split over. This is ignored when `models_used` is \"GPU\". By default, all available cores on the machine are used. |\n| --device | Optional Keyword Argument | The GPU index to run simulations on. This is only used when `models_used` is \"GPU\". |\n\nA single run of `simulate_mlde.py` will generate a single time-stamped folder. The time-stamp format is \"YYYYMMDD-HHMMSS\" (Y = year, M = month, D = day, H = 24-hour, M = minute, S = second). The output folder contains different levels of folders describing the different arguments passed to the script. The first level pertains to `training_type`, the second to `--training_specifics`, the third to `encoding`, the fourth to `models_used`, and the fifth to `training_samples`. When `training_type` is \"random\", the `--training_specifics` layer is labeled as \"random\" as well -- all other options for `--training_specifics` will give the number pertaining to the sampling threshold. The fifth (`training_samples`) layer contains folders with the outputs of every simulation run (with the folder name corresponding to the simulation index) as well as a log file. The log file gives details on the inputs to the program. Each simulation folder contains the files \"PredictionResults.csv\", \"SortedIndividualPreds.npy\", and \"SummaryResults.csv\". The \"PredictionResults.csv\" file is equivalent to \"PredictedFitness.csv\" from [Outputs for execute_mlde.py](#outputs-for-execute_mldepy) with only the best model used for predictions; the \"SortedIndividualPreds.npy\" file is equivalent to \"IndividualPreds.npy\" from [Outputs for execute_mlde.py](#outputs-for-execute_mldepy); the \"SummaryResults.csv\" file is equivalent to LossSummaries.csv from [Outputs for execute_mlde.py](#outputs-for-execute_mldepy). The folder architecture may seem strange at first, but is designed such that results from many different simulation conditions can be rapidly combined into a single folder structure (e.g., by using the `rsync` command to synchronize all levels). The output files of all simulations can be further processed to calculate the summary metrics given in the two csv files in the CaltechData download folder.\n\nA final note: some of the sklearn models used in MLDE are not always the most stable when trained with one-hot encodings. You may see an occasional warning that either \"sklearn-regressor-LassoLarsCV\" failed due misaligned dimensions or else that \"sklearn-regressor-ARDRegression\" failed due to an \"unrecoverable internal error\". These warnings should be sporadic (i.e., if every simulation throws this warning, you have a problem) but are not cause for concern -- MLDE internally handles failed models and drops them from analysis (to be specific, it assigns the failed model class an infinite cross-validation error, so unless you are averaging all model architectures in the ensemble you will not have a problem).\n"
      },
      "source": "https://raw.githubusercontent.com/fhalab/MLDE/main/README.md",
      "technique": "header_analysis"
    }
  ]
}