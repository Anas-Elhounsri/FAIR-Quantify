{
  "application_domain": [
    {
      "confidence": 37.68,
      "result": {
        "type": "String",
        "value": "Natural Language Processing"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/FuxuWang/MHCRoBERTa"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2021-09-07T04:11:28Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2023-09-11T14:43:23Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 0.926617959305299,
      "result": {
        "original_header": "process_MHC_pep.py:",
        "type": "Text_excerpt",
        "value": "tokenize the pretraining data and fine-tuning data\n```bash\npython3 process_MHC_pep.py\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8384934164207726,
      "result": {
        "original_header": "shuffle_and_split_pretrain.sh:",
        "type": "Text_excerpt",
        "value": "Shuffle and split pretraining data file into training, validation, and test data files.\n```bash\nbash shuffle_and_split_pretrain.sh preprocessed_data/uniprot_pretraining_data.csv \\\n\tpretraining_data/split_tokenized/ \\\n\tpretraining\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9801375624931861,
      "result": {
        "original_header": "pretrain.sh",
        "type": "Text_excerpt",
        "value": "| Name | Description | Example |\n| ----- | ------------------------------------------ | ------ |\n| PREFIX | Prefix for the model output files | pretrain |\n| NUM_GPUS | Number of GPUs to be used during pretraining | 2 |\n| OUTPUT_DIR | Output directory | Model output directory |\n| DATA_DIR | Binarized input data directory | Binarized input data directory |\n| ENCODER_EMBED_DIM | Dimension of embedding generated by the encoders | 768 |\n| ENCODER_LAYERS | Number of encoder layers in the model | 5 |\n| TOTAL_UPDATES | Total (maximum) number of updates during training | 125000 |\n| WARMUP_UPDATES | Total number of LR warm-up updates during training | 3125 |\n| PEAK_LEARNING_RATE | Peak learning rate for training | 0.0025 |\n| MAX_SENTENCES | Maximum number of sequences in each batch | 32 |\n| UPDATE_FREQ | Updates the model every UPDATE_FREQ batches | 64 | \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8871053980145444,
      "result": {
        "original_header": "shuffle_and_split.sh:",
        "type": "Text_excerpt",
        "value": "Shuffle and split fine-tuning data file into training, validation, and test data files.\n```bash\nbash shuffle_and_split.sh  preprocessed_data/tokenized_fine_tuning_data.csv \\\n\tpreprocessed_data/split_tokenized/full/ \\\n\ttokenized_fine_tuning_data\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8032850896567248,
      "result": {
        "original_header": "pep_MHC_interaction.sh:",
        "type": "Text_excerpt",
        "value": "Fine-tuned model for pep_MHC Interaction Prediction Task\n \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9109427309627277,
      "result": {
        "original_header": "evaluate_MHC_pep_model.py:",
        "type": "Text_excerpt",
        "value": "| Name | Description | Example |\n| ----- | ------------------------------------------ | ------ |\n| DATA | Path to input examples to predict. This should be formatted as a CSV with the columns, in order: tokenized to pep, tokenized to MHC, true label |\n| BINARIZED_DATA | Path to binarized pep_MHC_interaction data | \n| OUTPUT | Path to output file with model predictions |\n| MODEL_FOLDER | Model checkpoints folder. Will use checkpoint_best.pt file in the folder |\n| CLASSIFICATION_HEAD_NAME | Name of the trained classification head | pep_MHC_interaction_prediction |\n| BATCH_SIZE | Batch size for prediction | 256 |\n \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/FuxuWang/MHCRoBERTa/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 0
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/FuxuWang/MHCRoBERTa/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "FuxuWang/MHCRoBERTa"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/pep_MHC_interaction.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/pretrain.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/shuffle_and_split.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/preprocess_MHC-pep.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/shuffle_and_split_pretrain.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Requirements and Installation, install sentencepiece package",
        "type": "Text_excerpt",
        "value": "```bash\npip3 install sentencepiece\n```\n\n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "install fairseq package",
        "type": "Text_excerpt",
        "value": "```bash\ngit clone https://github.com/imonlius/fairseq.git\ncd fairseq\npip3 install --editable .\n```\n\n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.8608727117745801,
      "result": {
        "original_header": "shuffle_and_split_pretrain.sh:",
        "type": "Text_excerpt",
        "value": "| Name | Description | Example |\n| ----- | ------------------------------------------ | ------ |\n| INPUT | Input file | tokenized input directory |\n| OUTPUT | Output directory | output directory |\n| PREFIX | Prefix for output files | pretraining | \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8889129925830955,
      "result": {
        "original_header": "shuffle_and_split.sh:",
        "type": "Text_excerpt",
        "value": "| Name | Description | Example |\n| ----- | ------------------------------------------ | ------ |\n| INPUT | Input file | tokenized input directory |\n| OUTPUT | Output directory | output directory |\n| PREFIX | Prefix for output files | tokenized_data | \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9886123895195155,
      "result": {
        "original_header": "Preprocess/binarize MHC_pep data:",
        "type": "Text_excerpt",
        "value": "```bash\nbash preprocess_MHC-pep.sh\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8248872765820142,
      "result": {
        "original_header": "process_MHC_pep.py:",
        "type": "Text_excerpt",
        "value": "tokenize the pretraining data and fine-tuning data\n```bash\npython3 process_MHC_pep.py\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9014757258892381,
      "result": {
        "original_header": "shuffle_and_split_pretrain.sh:",
        "type": "Text_excerpt",
        "value": "Shuffle and split pretraining data file into training, validation, and test data files.\n```bash\nbash shuffle_and_split_pretrain.sh preprocessed_data/uniprot_pretraining_data.csv \\\n\tpretraining_data/split_tokenized/ \\\n\tpretraining\n```\n \n| Name | Description | Example |\n| ----- | ------------------------------------------ | ------ |\n| INPUT | Input file | tokenized input directory |\n| OUTPUT | Output directory | output directory |\n| PREFIX | Prefix for output files | pretraining | \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8843243528980484,
      "result": {
        "original_header": "pretrain.sh",
        "type": "Text_excerpt",
        "value": " Pre-train RoBERTa model\n```bash\nbash pretrain.sh pretrain 2 pretrained_model \\\n        pretraining/split_binarized/ \\\n        768 5 125000 3125 0.0025 32 64 3\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9023270781891758,
      "result": {
        "original_header": "shuffle_and_split.sh:",
        "type": "Text_excerpt",
        "value": "Shuffle and split fine-tuning data file into training, validation, and test data files.\n```bash\nbash shuffle_and_split.sh  preprocessed_data/tokenized_fine_tuning_data.csv \\\n\tpreprocessed_data/split_tokenized/full/ \\\n\ttokenized_fine_tuning_data\n```\n \n| Name | Description | Example |\n| ----- | ------------------------------------------ | ------ |\n| INPUT | Input file | tokenized input directory |\n| OUTPUT | Output directory | output directory |\n| PREFIX | Prefix for output files | tokenized_data | \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8202792751356421,
      "result": {
        "original_header": "Preprocess/binarize MHC_pep data:",
        "type": "Text_excerpt",
        "value": "```bash\nbash preprocess_MHC-pep.sh\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9191043645682753,
      "result": {
        "original_header": "evaluate_MHC_pep_model.py:",
        "type": "Text_excerpt",
        "value": "Predict pep_MHC_interaction using fine-tuned RoBERTa model\n```bash\npython3 evaluate_MHC_pep_model.py preprocessed_data/split_tokenized/full/tokenized_data.split.test.10 \\\n\tpreprocessed_data/split_binarized/ \\\n\tpredictions.tsv \\\n\tpep_MHC_interaction_prediction/pep_MHC_interaction.DIM_768.LAYERS_5.UPDATES_12500.WARMUP_3125.LR_0.0025.BATCH_4096.PATIENCE_3/checkpoints/ \\\n\tpep_MHC_interaction_prediction 256\n```\n \n| Name | Description | Example |\n| ----- | ------------------------------------------ | ------ |\n| DATA | Path to input examples to predict. This should be formatted as a CSV with the columns, in order: tokenized to pep, tokenized to MHC, true label |\n| BINARIZED_DATA | Path to binarized pep_MHC_interaction data | \n| OUTPUT | Path to output file with model predictions |\n| MODEL_FOLDER | Model checkpoints folder. Will use checkpoint_best.pt file in the folder |\n| CLASSIFICATION_HEAD_NAME | Name of the trained classification head | pep_MHC_interaction_prediction |\n| BATCH_SIZE | Batch size for prediction | 256 |\n \n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/FuxuWang/MHCRoBERTa/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "MHCRoBERTa"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "FuxuWang"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 10273,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 7403,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Requirements and Installation, install sentencepiece package",
        "type": "Text_excerpt",
        "value": "```bash\npip3 install sentencepiece\n```\n\n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "download",
    "contact",
    "contributors",
    "documentation",
    "license",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-05 23:53:34",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 3
      },
      "technique": "GitHub_API"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Example Usage:",
        "parent_header": [
          "pep_MHC_interaction.sh:"
        ],
        "type": "Text_excerpt",
        "value": "```bash\nbash pep_MHC_interaction.sh pep_MHC_interaction 2 pep_MHC_interaction_prediction \\\n        preprocessed_data/split_binarized \\\n        768 5 12500 3125 0.0025 32 64 1 3 \\\n        pretrained_model/pretrain.DIM_768.LAYERS_5.UPDATES_125000.WARMUP_3125.LR_0.0025.BATCH_4096.PATIENCE_3/checkpoints/checkpoint_best.pt \\\n      no 1\n```\n\n- Arguments\n\n| Name | Description | Example |\n| ----- | ------------------------------------------ | ------ |\n| PREFIX | Prefix for the model output files | pep_MHC_interaction |\n| NUM_GPUS | Number of GPUs to use for finetuning | 2 |\n| OUTPUT_DIR | Model output directory |\n| DATA_DIR | Binarized input data directory |\n| ENCODER_EMBED_DIM | Dimension of embedding generated by the encoders | 768 |\n| ENCODER_LAYERS | Number of encoder layers in the model | 5 |\n| TOTAL_UPDATES | Total (maximum) number of updates during training | 12500 |\n| WARMUP_UPDATES | Total number of LR warm-up updates during training | 3125 |\n| PEAK_LEARNING_RATE | Peak learning rate for training | 0.0025 |\n| MAX_SENTENCES | Maximum number of sequences in each batch | 32 |\n| UPDATE_FREQ | Updates the model every UPDATE_FREQ batches | 64 |\n| NUM_CLASSES | number of classes | 1 |\n| PATIENCE | Early stop training if valid performance doesn\u2019t improve for PATIENCE consecutive validation runs | 3 |\n| PRETRAIN_CHECKPOINT | Path to pretrained model checkpoint |\n| RESUME_TRAINING | Whether to resume training from previous finetuned model checkpoints | no |\n| USE-CLS-TOKEN | Use [cls] token instead of feature vector sum for RobertaClassificationHead | 1 |\n\n"
      },
      "source": "https://raw.githubusercontent.com/FuxuWang/MHCRoBERTa/master/README.md",
      "technique": "header_analysis"
    }
  ]
}