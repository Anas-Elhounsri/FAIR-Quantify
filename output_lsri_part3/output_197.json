{
  "acknowledgement": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Named Entity Recognition (NER)",
        "parent_header": [
          "BioALBERT",
          "Datasets"
        ],
        "type": "Text_excerpt",
        "value": "* [BC2GM](https://drive.google.com/drive/folders/130ei5___99HkOoaHg9KJhveC7sJM1Zw4?usp=sharing)\n* [BC4CHEMD](https://drive.google.com/drive/folders/1gASQyQoDtt7Ss2vXyTvR6EwdGzfFBXuJ?usp=sharing)\n* [BC5CDR (Disease)](https://drive.google.com/drive/folders/1BtEDXwj1bwSZfes8w-4S9mIZLdjTC4nj?usp=sharing)\n* [BC5CDR (Chemical)](https://drive.google.com/drive/folders/1b_C-vuOZ7ae1qUeuXZJhdyRSrWgygVMS?usp=sharing)\n* [JNLPBA](https://drive.google.com/drive/folders/1SMm-cY2XxKsyHvcIR2teNt97N-3zYaXc?usp=sharing)\n* [LINNAEUS](https://drive.google.com/drive/folders/1jQEgdQAdRweoh6vHSYeEOwya7w1VDg2p?usp=sharing)\n* [NCBI (Disease)](https://drive.google.com/drive/folders/1ESm_CF3cU0ZbKP2N8uXiMHz8wKARm-KW?usp=sharing)\n* [Species-800 (S800)](https://drive.google.com/drive/folders/1s2k1e7hnW1f9kHOv2AIEfZyL0c0SIBfT?usp=sharing)\n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "application_domain": [
    {
      "confidence": 90.23,
      "result": {
        "type": "String",
        "value": "Natural Language Processing"
      },
      "technique": "supervised_classification"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/usmaann/BioALBERT"
      },
      "technique": "GitHub_API"
    }
  ],
  "contact": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Contact Information",
        "parent_header": [
          "BioALBERT",
          "Fine-tuning BioBERT"
        ],
        "type": "Text_excerpt",
        "value": "If you have any questions, please submit a Github issue or contact Usman Naseem (usman.naseem@sydney.edu.au)\n\n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2020-09-07T05:29:47Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-02-20T05:21:35Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Benchmarking for Biomedical Natural Language Processing Tasks with a Domain Specific ALBERT"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9254412332400099,
      "result": {
        "original_header": "BioALBERT",
        "type": "Text_excerpt",
        "value": "Benchmarking for Biomedical Natural Language Processing Tasks with a Domain Specific ALBERT \n\nThis repository provides the pre-trained BioALBERT models, a biomedical language representation model trained on large domain specific (biomedical) corpora for designed for biomedical text mining tasks. Please refer to our paper [https://arxiv.org/abs/2107.04374] for more details. \n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9880663984767075,
      "result": {
        "original_header": "Datasets",
        "type": "Text_excerpt",
        "value": "We provide a pre-processed version of benchmark datasets for each task as follows: \n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8086178574987296,
      "result": {
        "original_header": "Question Answering (BioASQ)",
        "type": "Text_excerpt",
        "value": "Open each links and download the datasets you need. For BioASQ datasets, please refer to the [biobert repository](https://github.com/dmis-lab/biobert#datasets) \n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8820370055665866,
      "result": {
        "original_header": "NER",
        "type": "Text_excerpt",
        "value": "Each datasets contains four files, which are ```dev.tsv```, BASH2*, BASH3*, and BASH4*. Simply download a dataset from NER and put these files into the directory called BASH5*. Also, set BASH6* as a directory for NER outputs. For example, when fine-tuning on the BC2GM dataset,\n```\n$ export NER_DIR=./datasets/NER/BC2GM\n$ export OUTPUT_DIR=./NER_outputs\n```\nFollowing command runs fine-tuning code on NER with default arguments.\n```\n$ mkdir -p $OUTPUT_DIR\n$ python run_ner.py --do_train=true --do_eval=true --vocab_file=$BIOALBERT_DIR/vocab.txt --bert_config_file=$BIOALBERT_DIR/bert_config.json --init_checkpoint=$BIOALBERT_DIR/model.ckpt-1000000 --num_train_epochs=10.0 --data_dir=$NER_DIR --output_dir=$OUTPUT_DIR\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9026669886924421,
      "result": {
        "original_header": "RE",
        "type": "Text_excerpt",
        "value": "Each datasets contains there files, which are ```dev.tsv```, BASH2*, and BASH3*. Let BASH4* denote the folder of a single RE data set, BASH5* denote the task name (two options: gad, euadr), and BASH6* denote the RE output directory, take GAD as an example:\n```\n$ export RE_DIR=./datasets/RE/GAD/1\n$ export TASK_NAME=gad\n$ export OUTPUT_DIR=./re_outputs_1\n```\nFollowing command runs fine-tuning code on RE with default arguments.\n```\n$ python run_re.py --task_name=$TASK_NAME --do_train=true --do_eval=true --do_predict=true --vocab_file=$BIOBERT_DIR/vocab.txt --bert_config_file=$BIOBERT_DIR/bert_config.json --init_checkpoint=$BIOBERT_DIR/model.ckpt-1000000 --max_seq_length=128 --train_batch_size=32 --learning_rate=2e-5 --num_train_epochs=3.0 --do_lower_case=false --data_dir=$RE_DIR --output_dir=$OUTPUT_DIR\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Download",
        "parent_header": [
          "BioALBERT"
        ],
        "type": "Text_excerpt",
        "value": "We provide eight versions of pre-trained weights. Pre-training was based on the original ALBERT code, and training details are described in our paper (To be Published). Currently available versions of pre-trained weights are as follows:\n\n1) *[BioALBERT-Base v1.0 (PubMed)](https://drive.google.com/file/d/1sCU1vvSOWoWVAkOoWGUC3ZKraItLIoXD/view?usp=sharing) - based on ALBERT-base Model*\n\n2) *[BioALBERT-Base v1.0 (PubMed + PMC)](https://drive.google.com/file/d/1N2UekXKNqhbjQLbtipsm8rNPcaFEG-2I/view?usp=sharing) - based on ALBERT-base Model*\n \n3) *[BioALBERT-Base v1.0 (PubMed + MIMIC-III)](https://drive.google.com/file/d/1t9XUVMxEfRzVYU0M99NB9PIPSZWAFX4V/view?usp=sharing) - based on ALBERT-base Model*\n\n4) *[BioALBERT-Base v1.0 (PubMed + PMC + MIMIC-III)](https://drive.google.com/file/d/1SIBd_-GETHhMiZ7BgMdDPEUDjOjtN_bH/view?usp=sharing) - based on ALBERT-base Model*\n\n5) *[BioALBERT-Large v1.1 (PubMed)](https://drive.google.com/file/d/1uX5w8yaMyJta3Nit_3ayrL16tE-dO8Ew/view?usp=sharing) - based on ALBERT-Large Model*\n\n6) *[BioALBERT-Large v1.1 (PubMed + PMC)](https://drive.google.com/file/d/1WJp7KbWXPa-3QWpsXcN95smY6V2RRbcX/view?usp=sharing) - based on ALBERT-Large Model*\n\n7) *[BioALBERT-Large v1.1 (PubMed + MIMIC-III)](https://drive.google.com/file/d/1mZeW_0iQsCSIn86cW_XduaGnVtNGGXYp/view?usp=sharing) - based on ALBERT-Large Model*\n\n8) *[BioALBERT-Large v1.1 (PubMed + PMC + MIMIC-III)](https://drive.google.com/file/d/16KRtHf8Meze2Hcc4vK_GUNhG-9LY6_6P/view?usp=sharing) - based on ALBERT-Large Model*\n\n\nMake sure to specify the version of the pre-trained weights used in your work. \n\n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/usmaann/BioALBERT/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 5
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/usmaann/BioALBERT/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "usmaann/BioALBERT"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "BioALBERT"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "parent_header": [
          "BioALBERT"
        ],
        "type": "Text_excerpt",
        "value": "The following sections introduce the installation and fine-tuning process of BioALBERT based on PyTorch (python version <= 3.7).\n\nTo fine-tune BioALBERT, you need to download BioALBERT pre-training weights. After downloading the pre-trained weights, install BioALBERT using requirements.txt as follows:\n\n```\ngit clone https://github.com/usmaann/BioALBERT.git\ncd BioALBERT; pip install -r requirements.txt\n\n```\nNote that this repository is based on the [ALBERT](https://github.com/google-research/albert) repository by Google. See requirements.txt for other details.\n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9998419378709713,
      "result": {
        "original_header": "Fine-tuning BioBERT",
        "type": "Text_excerpt",
        "value": "After downloading one of the pre-trained weights, unzip it to any directory you want, we will denote it as ``` $BIOALBERT_DIR ```. For example, when using BioALBERT-Base v1.0 (PubMed), set the BIOALBERT_DIR environment variable to:\n```\n$ export BIOALBERT_DIR=./BioALBERT_PUBMED_BASE\n$ echo $BIOALBERT_DIR\n>>> ./BioALBERT_PUBMED_BASE\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9852948779998425,
      "result": {
        "original_header": "NER",
        "type": "Text_excerpt",
        "value": "Each datasets contains four files, which are ```dev.tsv```, BASH2*, BASH3*, and BASH4*. Simply download a dataset from NER and put these files into the directory called BASH5*. Also, set BASH6* as a directory for NER outputs. For example, when fine-tuning on the BC2GM dataset,\n```\n$ export NER_DIR=./datasets/NER/BC2GM\n$ export OUTPUT_DIR=./NER_outputs\n```\nFollowing command runs fine-tuning code on NER with default arguments.\n```\n$ mkdir -p $OUTPUT_DIR\n$ python run_ner.py --do_train=true --do_eval=true --vocab_file=$BIOALBERT_DIR/vocab.txt --bert_config_file=$BIOALBERT_DIR/bert_config.json --init_checkpoint=$BIOALBERT_DIR/model.ckpt-1000000 --num_train_epochs=10.0 --data_dir=$NER_DIR --output_dir=$OUTPUT_DIR\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8954756237778476,
      "result": {
        "original_header": "NER",
        "type": "Text_excerpt",
        "value": "Each datasets contains four files, which are ```dev.tsv```, BASH2*, BASH3*, and BASH4*. Simply download a dataset from NER and put these files into the directory called BASH5*. Also, set BASH6* as a directory for NER outputs. For example, when fine-tuning on the BC2GM dataset,\n```\n$ export NER_DIR=./datasets/NER/BC2GM\n$ export OUTPUT_DIR=./NER_outputs\n```\nFollowing command runs fine-tuning code on NER with default arguments.\n```\n$ mkdir -p $OUTPUT_DIR\n$ python run_ner.py --do_train=true --do_eval=true --vocab_file=$BIOALBERT_DIR/vocab.txt --bert_config_file=$BIOALBERT_DIR/bert_config.json --init_checkpoint=$BIOALBERT_DIR/model.ckpt-1000000 --num_train_epochs=10.0 --data_dir=$NER_DIR --output_dir=$OUTPUT_DIR\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.866349905574373,
      "result": {
        "original_header": "RE",
        "type": "Text_excerpt",
        "value": "Each datasets contains there files, which are ```dev.tsv```, BASH2*, and BASH3*. Let BASH4* denote the folder of a single RE data set, BASH5* denote the task name (two options: gad, euadr), and BASH6* denote the RE output directory, take GAD as an example:\n```\n$ export RE_DIR=./datasets/RE/GAD/1\n$ export TASK_NAME=gad\n$ export OUTPUT_DIR=./re_outputs_1\n```\nFollowing command runs fine-tuning code on RE with default arguments.\n```\n$ python run_re.py --task_name=$TASK_NAME --do_train=true --do_eval=true --do_predict=true --vocab_file=$BIOBERT_DIR/vocab.txt --bert_config_file=$BIOBERT_DIR/bert_config.json --init_checkpoint=$BIOBERT_DIR/model.ckpt-1000000 --max_seq_length=128 --train_batch_size=32 --learning_rate=2e-5 --num_train_epochs=3.0 --do_lower_case=false --data_dir=$RE_DIR --output_dir=$OUTPUT_DIR\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/usmaann/BioALBERT/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "BioALBERT"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "usmaann"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_papers": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2107.04374 with [BibTex]\n      (@misc{naseem2021benchmarking,\n      title={Benchmarking for Biomedical Natural Language Processing Tasks with a Domain Specific ALBERT}, \n      author={Usman Naseem and Adam G. Dunn and Matloob Khushi and Jinman Kim},\n      year={2021},\n      eprint={2107.04374},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://arxiv.org/abs/2107.04374] for more details.\n\n\n## Download\n\nWe provide eight versions of pre-trained weights. Pre-training was based on the original ALBERT code, and training details are described in our paper (To be Published"
      },
      "source": "https://raw.githubusercontent.com/usmaann/BioALBERT/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "run",
    "requirements",
    "contributors",
    "documentation",
    "license",
    "usage",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 00:07:02",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 27
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "non-software"
      },
      "technique": "software_type_heuristics"
    }
  ]
}