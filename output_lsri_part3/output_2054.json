{
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/hnolCol/ComplexFinder"
      },
      "technique": "GitHub_API"
    }
  ],
  "contact": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Contact",
        "type": "Text_excerpt",
        "value": "Of note, please use the Issue GitHub functionality of this repository to report bugs.\nNevertheless, you can contact us if you have any question or requests for a feature functions of the pipeline via [e-mail](mailto:h.nolte@age.mpg.de?subject=ComplexFinder%20Request). \n\n\n \n\n\n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2019-08-23T15:43:44Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-06-03T01:42:15Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Finds complexes from Blue-Native and SEC Fractionation Complexome Profiling Data. Each fraction is usually analysed by Liquid Chromatography coupled to Mass Spectrometry. (LC-MS/MS)"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9486941805432267,
      "result": {
        "original_header": "ComplexFinder",
        "type": "Text_excerpt",
        "value": " Finds complexes from Blue-Native and SEC Fractionation analyzed by Liquid Chromatogrpahy coupled to Mass Spectrometry. In \n principal it works with any separation technique resulting in co-elution signal profiles. To avoid licence issues and accumulation of old database files, please first download the database of choce (see below *Download Protein-Protein Interaction Data*). \n \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9785349412146696,
      "result": {
        "original_header": "Next Feature (Testing) Implementations (05.2021)",
        "type": "Text_excerpt",
        "value": "- [ ] Extend plotting capabibilties to extract profiles of features and complex\n```python\n\n#plotting selected feature's profile\nComplexFinder(analysisName=\"<Created folder in results folder>\").plotFeature()\n\n#plotting selected feature's distance metrics compared to the complete population (all features)\n#due to scaling of features prior training of the predictor, boxplot should display scaled and raw features.\nComplexFinder(analysisName=\"<Created folder in results folder>\").plotFeatureDistanceMetrics()\n\n#plotting features of complex found by ComplexFinder (clusterLabels == ID)\nComplexFinder(analysisName=\"<Created folder in results folder>\").plotComplexProfileByClusterLabel()\n\n#plotting features of known complex in database (correspondng to ComplexID column in the database - see below)\nComplexFinder(analysisName=\"<Created folder in results folder>\").plotComplexProfileInDatabaseByID() \n\n```\n- [ ] Test a DeepLearning Implementation\n- [ ] AlignedUMAP for stable complexes \n  \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9474527639153233,
      "result": {
        "original_header": "Workflow",
        "type": "Text_excerpt",
        "value": "For thousdands of features (peptides/protein) a signal was measured over different fractions. The applied technique separates protein clusters from each other. This package aims for different things: \n* signal processing including filtering, smoothing.\n* if more than one replicate is analysed, the profiles over fractions will be aligned. \n* identification of protein-protein interactions.\n* identification of protein cluster using diminesional reduction and density based clustering.  \nImportantly, ComplexFinder can also be utized to analyse the data without prior knowledge of protein connectivitiy (e.g. positive database). In this case, there are two options:  \nwhich are then subjected for dimensional reduction and HDBSCAN clusering. Importantly, when using the raw profile intensities, the derived UMAP representation is aligned using the top N correlated features between samples (e.g. same protein across all samples).  \nAs a next step, we want to identify clusters of proteins with predicted interaction. To this end, we are using the interaction probabiliy matrix obtained by the\nrandom forest classifier. We then apply the UMAP embedding calculton and apply HDBSCAN clustering. Again, we \nare using the CORUM database to quantify the the clustering result using the v-measure. Both techniques, UMAP and HDBSCAN are performed \nusing a paramter grid to cycle through different options and find the best clustering.  \n In cases of uisng the raw signal intensity or the distance metrics, those data are subjected to dimensional reduction (UMAP) and clustering (HDBSCAN). Noteworthy, other clusering algorithmns are available and can be utilized. HDBSCAN is however the default. \n \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9834026004399676,
      "result": {
        "original_header": "Depositing Data analyzed using ComplexFinder",
        "type": "Text_excerpt",
        "value": "If you analyzed your data using ComplexFinder, we highly recommend to upload the data along the raw fiiles deposition at mass spectrometry repisatories such as PRIDE / ProteomeXChange or similiar. Especially, the params.json file which is written to the results folder is of particular interest in order to reproduce the data analysis. Of note, if you upload the complete result folder, other users will be able to analyse these data using the plotting utilities of ComplexFinder.\n \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9504394903323756,
      "result": {
        "original_header": "Grouping of Replicates",
        "type": "Text_excerpt",
        "value": "The grouping parameter in ComplexFinder is used to group files, which is used to group replicates together. \nAssume, that we have 4 files, 2 KO and 2 WT files which we put together in the folder \"./data\". \nThe grouping will be used to calculate pariwise statistics between fitted peaks. Moreover, complex prediction and protein-protein prediction summary.\n```python\npathToFiles = os.path.join(\".\",\"data\")\nComplexFinder(\n    grouping = {\n        \"WT\" : [\"D3_WT_01.txt\",\"D3_WT_02.txt\"],\n        \"KO\" : [\"D3_KO_01.txt\",\"D3_KO_02.txt\"]\n    }\n            ).run(pathToFiles)\n```\n \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9683192735918189,
      "result": {
        "original_header": "Parameters",
        "type": "Text_excerpt",
        "value": "- [ ] requires updating for new parameters and additional documentation. \nSklearn library is used for predictions. Please check the comprehensive [documention](https://scikit-learn.org/stable/user_guide.html) for more details and for construction of a grid search dict. \n \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9817221607337691,
      "result": {
        "original_header": "Database Quality",
        "type": "Text_excerpt",
        "value": "For the prediction of protein-protein interactions the quality and size of the database is of importance.  \nAs a quick test, we performed predictions using 2000 randomly selected features of dataset D1 and siwtched the class labels (interactor vs non-interactor) of the database to train the classifier. We observed that the number of predicted protein-protein interaction was strongly reduced in after label switch of more than 5% of the features. We have used the CORUM human database for interactions. This highlights that the complexes in the database need to describe the complexome in the measured dataset accurately. The gold-standard is therefore the usage of a complex database that were experimentally validated, which is sadly often not possible due to the workload. \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8756125334752629,
      "result": {
        "original_header": "Usin SILAC - TMT peak centric quantifiaction",
        "type": "Text_excerpt",
        "value": "*in preparation*  \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9584770881598274,
      "result": {
        "original_header": "TMT",
        "type": "Text_excerpt",
        "value": "TMT allows for multiplexing in complexome experiments by labeling peptides with different tags that can be distinguished by different reporter ions using LC-MS/MS. Therefore the result (for example from a MaxQuant analysis) that is required are:\n* ProteinGroups.txt -> Feature IDs (protein IDs) versus iBAQ intensity in columns. This file is the base file to extract the signal profiles and on which the peak modelling will be performed. Alternatively, you can also sum all the TMT intensities. \n* ProteinGroups.txt -> Feature IDs (protein IDs) versus the TMT Intensities per channel. If you performed a 10-plex TMT analysis, this would result in Protein ID + (fraction x 10 (TMT channels)) columns. The TMT intensties should be next to each other for each fraction, please see the figure below. TMT01_fraction_01, TMT02_fraction_01 ... TMT10_fraction_01, TMT01_fraction_02. It is advisable to put a leading zero in the MaxQuant experiment name to get the correct order straight away (otherwise you may run into such an order: 1,10,11,12,...,2,21) \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.935214446253406,
      "result": {
        "original_header": "SILAC-TMT",
        "type": "Text_excerpt",
        "value": "A combination of SILAC and TMT allows either for extended mulitplexing (2 x SILAC Channel + 10plex TMT = 20 samples) or to follow an incoporation kinetic. To this end, cells are grown on SILAC media (for example heavy) for several passages leading to fully labelled cells. Then, the media is exchanged to light media and the cell start incoporating light amino acids into newly synthesized proteins. This enabled the determination of incorporation rates / turnover rates. When combining TMT and SILAC together, the light channel peptides + TMT represent the SILAC incoporation and heavy shows the break-down of proteins. In proliferating cells, the increase in biomass (cell growth) has to be considered. \nThe general strategy in complex finder for peak-centric quantification is shown in the figure above. For detected peaks, the FWHM is determined and the TMT intensities are summed over the respective fractions. Of note, for very small peaks this might a single fraction which is by default prevented. This can be allowed by setting the parameter (allowSingleFractionQuant to True). \n```python\nComplexFinder(allowSingleFractionQuant = True).run(...)\n```\n \nWe recommend to put the signal profiles in a folder (in the figure: myCoolAnalysis) and add the files. Create a new folder within myCoolAnalysis called 'q' in which you add the quantification data. If you put the quantification txt files in the same folder as the once for analysis, ComplexFinder will treat them as signal profiles and will try to fit model peaks to them etc.  \nTo calculated the fit parameter for a single order kinetic, we have to provide more information otherwise, the output will contain the TMT intensities for each peak indicated by *heavy* or *light*. ComplexFinder expects a raw TMT intensities (not log2) for  \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9288459536462783,
      "result": {
        "original_header": "Frequently asked questions",
        "type": "Text_excerpt",
        "value": " Please check the class argument databaseFilter of type dict. For example the default is \n ```python\n databaseFilter = {'Organism': [\"Human\"]}\n```\nThis means that the database is filtered on column 'Organism' using \"Human\" as the search string.   \nTherefore, you can provide any string that matches a model name in the lmfit package. Please note that, only peak parameters and constraints \nare implemented and tested for Gaussian, Lorentzian and Skewed Gaussian. So if your fit does not work, you may want to check the following\nfunction of the *Signal.py* class module.\n```python\ndef _addParams(self,modelParams,prefix,peakIdx,i):\n        \"\"\"\n        Adds parameter to the modelParam object\n\n        Parameters\n        ----------\n\n        mdeolParams : \n            modelParam object. Returned by model.make_params() (lmfit package) \n            Documentation: https://lmfit.github.io/lmfit-py/model.html\n\n        prefix : str\n            Prefix for the model (e.g. peak), defaults to f'm{i}_'.format(i)\n\n        peakIdx : int\n            Arary index at which the peak was detected in the Signal arary self.Y \n\n        i : int \n            index of detected models\n    \n        Returns\n        -------\n        None\n\n        \"\"\"\n        self._addParam(modelParams,\n                        name=prefix+'amplitude',\n                        max = self.Y[peakIdx[i]] * 9,\n                        value = self.Y[peakIdx[i]] * 2,\n                        min = self.Y[peakIdx[i]] * 1.2)\n\n        self._addParam(modelParams,\n                        name=prefix+'sigma', \n                        value = 0.255,\n                        min = 0.01, \n                        max = 2.5)\n\n        self._addParam(modelParams,\n                        name=prefix+'center', \n                        value = peakIdx[i],\n                        min = peakIdx[i] - 0.2, \n                        max = peakIdx[i] + 0.2)\n\n        ## enter other model params here, you may have to change the min and max\n        ## for the other parameters as well to get a nice fit. \n```\n \nPlease not that you also have to alter the functions *_getHeight* and *_getFWHM* for your peak models. \nYou can check the equations [here](http://openafox.com/science/peak-function-derivations.html). \nIn the future, we would like to implement the following features: \n* Web application with an easy uster interface to proide easy access to the pipeline\n* Implement more classifiers. \n* Test various peak models for better performance.  \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9218969810733086,
      "result": {
        "original_header": "Citation/Publication",
        "type": "Text_excerpt",
        "value": "If you found usage of this piepline helpful, please consider citation of the following paper. We highly appreciate any acknowledgement.  \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Download Protein-Protein Interaction Data",
        "parent_header": [
          "ComplexFinder"
        ],
        "type": "Text_excerpt",
        "value": "To run the ComplexFinder pipeline, you have to provide a positive set protein-protein interactions.\nBelow we provide examples and specific settings for frequently used databases of protein-protein interactions.\n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "CORUM",
        "parent_header": [
          "ComplexFinder",
          "Download Protein-Protein Interaction Data"
        ],
        "type": "Text_excerpt",
        "value": "Download the dataset from the [Website link](https://mips.helmholtz-muenchen.de/corum/) and save it to reference-data folder in ComplexFinder.\nIf not present, add a column with the header ComplexID providing a unique ID for each complex.\nThe CORUM database contains complexes for mammalian systems therefore we need to pass a filterDictionary as shown below (databaseFilter). \nYou can pass any column of the database as the key, and the target value for which we want to filter as a list. \nThe parameter databaseEntrySplitString gives the splitstring by which the Uniprot identifiers (or any other feature ID matching your input data) of complexes are separated.\n\n```python\nComplexFinder(\n    ...\n    databaseFilter = {'Organism': [\"Human\"]},\n    databaseIDColumn = \"subunits(UniProt IDs)\",\n    databaseEntrySplitString = \";\", \n    databaseFileName = \"CORUM.txt\" #depends on how you save the COURM database\n    ).run(...)\n```\n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Complex Portal",
        "parent_header": [
          "ComplexFinder",
          "Download Protein-Protein Interaction Data"
        ],
        "type": "Text_excerpt",
        "value": "Go the [Complex Portal Website](https://www.ebi.ac.uk/complexportal/home) and download the database (save it as HUMAN_COMPLEX_PORTAL.txt) for the utilized organismn. \n\n\n```python\nComplexFinder(\n    databaseFileName=\"HUMAN_COMPLEX_PORTAL.txt\", #depends on how you save the Complex Portal database\n    databaseIDColumn= \"Expanded participant list\",\n    databaseEntrySplitString = \"|\",              \n    databaseFilter = {}\n    ).run(...)\n\n```\n\n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "hu.Map 2.0",
        "parent_header": [
          "ComplexFinder",
          "Download Protein-Protein Interaction Data"
        ],
        "type": "Text_excerpt",
        "value": "The hu.MAP 2.0 has recently beend published and is available at this [link](http://humap2.proteincomplexes.org).\n\n```python\nComplexFinder(\n    databaseFileName=\"humap2.txt\", #depends on how you save the Complex Portal database\n    databaseIDColumn= \"subunits(UniProt IDs)\", #requires renaming\n    databaseEntrySplitString = \";\",              \n    databaseFilter = {\"Confidence\":[1,2,3,4]} #example to filter for a spcific complex confidence\n    ).run(...)\n\n```\n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/hnolCol/ComplexFinder/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 3
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/hnolCol/ComplexFinder/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "hnolCol/ComplexFinder"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "ComplexFinder"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master//img/workflow.png"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master//img/quantDetails.png"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master//img/ComplexFinder_folderStructure.png"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master//img/TMT_SILAC_STRAT.png"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master//img/TMT_SILAC_QUANT.png"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Installation",
        "parent_header": [
          "ComplexFinder"
        ],
        "type": "Text_excerpt",
        "value": "Download the zip file containing the source code from github.\nNavigate to the folder in terminal/command line tool.\nOn Mac / Linux:\n```\n#create virt env\npython3 -m venv env\n#activate\nsource env/bin/activate\n#install packages from req file\npip install -r requirements.txt\n```\nFor windows user:\n```\n#create virt env \npy -m venv env\n#actve\n.\\env\\Scripts\\activate\n#install packages from req file\npip3 install -r requirements.txt\n````\n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9894767201975858,
      "result": {
        "original_header": "Frequently asked questions",
        "type": "Text_excerpt",
        "value": "Therefore, you can provide any string that matches a model name in the lmfit package. Please note that, only peak parameters and constraints \nare implemented and tested for Gaussian, Lorentzian and Skewed Gaussian. So if your fit does not work, you may want to check the following\nfunction of the *Signal.py* class module.\n```python\ndef _addParams(self,modelParams,prefix,peakIdx,i):\n        \"\"\"\n        Adds parameter to the modelParam object\n\n        Parameters\n        ----------\n\n        mdeolParams : \n            modelParam object. Returned by model.make_params() (lmfit package) \n            Documentation: https://lmfit.github.io/lmfit-py/model.html\n\n        prefix : str\n            Prefix for the model (e.g. peak), defaults to f'm{i}_'.format(i)\n\n        peakIdx : int\n            Arary index at which the peak was detected in the Signal arary self.Y \n\n        i : int \n            index of detected models\n    \n        Returns\n        -------\n        None\n\n        \"\"\"\n        self._addParam(modelParams,\n                        name=prefix+'amplitude',\n                        max = self.Y[peakIdx[i]] * 9,\n                        value = self.Y[peakIdx[i]] * 2,\n                        min = self.Y[peakIdx[i]] * 1.2)\n\n        self._addParam(modelParams,\n                        name=prefix+'sigma', \n                        value = 0.255,\n                        min = 0.01, \n                        max = 2.5)\n\n        self._addParam(modelParams,\n                        name=prefix+'center', \n                        value = peakIdx[i],\n                        min = peakIdx[i] - 0.2, \n                        max = peakIdx[i] + 0.2)\n\n        ## enter other model params here, you may have to change the min and max\n        ## for the other parameters as well to get a nice fit. \n```\n \n # Requirements \nThe following python packages are required to run the scripts (from the requirements.txt file.) \n* asteval==0.9.19\n* certifi==2020.11.8\n* cycler==0.10.0\n* Cython==0.29.21\n* future==0.18.2\n* hdbscan==0.8.26\n* joblib==0.17.0\n* kiwisolver==1.3.1\n* llvmlite==0.34.0\n* lmfit==1.0.1\n* matplotlib==3.3.2\n* numba==0.51.2\n* numpy==1.19.4\n* pandas==1.1.4\n* Pillow==8.1.1\n* pyparsing==2.4.7\n* python-dateutil==2.8.1\n* pytz==2020.4\n* scikit-learn==0.23.2\n* scipy==1.5.4\n* seaborn==0.11.0\n* six==1.15.0\n* sklearn==0.0\n* threadpoolctl==2.1.0\n* umap-learn==0.4.6\n* uncertainties==3.1.4\n \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "invocation": [
    {
      "confidence": 0.8196570050609185,
      "result": {
        "original_header": "ComplexFinder",
        "type": "Text_excerpt",
        "value": "<img src=\"/img/complexFinderLogo.png\" height=\"150px\"> \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.8196570050609185,
      "result": {
        "original_header": "ComplexFinder Output",
        "type": "Text_excerpt",
        "value": "<img src=\"/img/ComplexFinder_folderStructure.png\" height=\"650px\"> \n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/hnolCol/ComplexFinder/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "bn-page, complexome, proteomics, python3"
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "MIT License\n\nCopyright (c) 2019 hnolCol\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/LICENSE",
      "technique": "file_exploration"
    }
  ],
  "logo": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master//img/complexFinderLogo.png"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "ComplexFinder"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "hnolCol"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 244708,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "somef_missing_categories": [
    "citation",
    "acknowledgement",
    "run",
    "requirements",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "has_build_file",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-06 07:31:23",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 7
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Usage Example",
        "parent_header": [
          "ComplexFinder"
        ],
        "type": "Text_excerpt",
        "value": "Upon downlaod and extraction of the package. You can find example data in the example-data folder. \nTo run the anaylsis, you can enter:\n```python\nfrom .src.main import ComplexFinder\nX = pd.read_table(\"./example-data/SILAC_01.txt\", sep = \"\\t\") #loading tab delimited txt file. \nComplexFinder(analysisName = \"ExampleRun_01\").run(X)\n```\nYou can also pass a folder path to run. This will yield in the anaylsis of each txt file in the folder.\n\n```python\nimport os\nfrom .src.main import ComplexFinder\nfolderPath = os.path(\".\",\"<my folder>\")\nComplexFinder().run(folderPath)\n```\nAdditionally, you can pass a list of datasets. However, we  recommend to copy required datasets in a separate folder.\nResults are saved by default in the results folder the ComplexFinder folder.\n"
      },
      "source": "https://raw.githubusercontent.com/hnolCol/ComplexFinder/master/README.md",
      "technique": "header_analysis"
    }
  ]
}