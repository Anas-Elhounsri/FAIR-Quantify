{
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citations",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Developer Notes"
        ],
        "type": "Text_excerpt",
        "value": "Kontou, E.E., Walter, A., Alka, O. et al. UmetaFlow: an untargeted metabolomics workflow for high-throughput data processing and analysis. J Cheminform 15, 52 (2023). https://doi.org/10.1186/s13321-023-00724-w\n\nPfeuffer J, Sachsenberg T, Alka O, et al. OpenMS \u2013 A platform for reproducible analysis of mass spectrometry data. J Biotechnol. 2017;261:142-148. doi:10.1016/j.jbiotec.2017.05.016\n\nD\u00fchrkop K, Fleischauer M, Ludwig M, et al. SIRIUS 4: a rapid tool for turning tandem mass spectra into metabolite structure information. Nat Methods. 2019;16(4):299-302. doi:10.1038/s41592-019-0344-8\n\nD\u00fchrkop K, Shen H, Meusel M, Rousu J, B\u00f6cker S. Searching molecular structure databases with tandem mass spectra using CSI:FingerID. Proc Natl Acad Sci. 2015;112(41):12580-12585. doi:10.1073/pnas.1509788112\n\nNothias LF, Petras D, Schmid R, et al. Feature-based molecular networking in the GNPS analysis environment. Nat Methods. 2020;17(9):905-908. doi:10.1038/s41592-020-0933-6\n\nSchmid R, Petras D, Nothias LF, et al. Ion identity molecular networking for mass spectrometry-based metabolomics in the GNPS environment. Nat Commun. 2021;12(1):3832. doi:10.1038/s41467-021-23953-9\n\nM\u00f6lder F, Jablonski KP, Letcher B, et al. Sustainable data analysis with Snakemake. Published online January 18, 2021. doi:10.12688/f1000research.29032.1\n\nde Jonge, N.F., Louwen, J.J.R., Chekmeneva, E. et al. MS2Query: reliable and scalable MS2 mass spectra-based analogue search. Nat Commun 14, 1752 (2023). doi:10.1038/s41467-023-37446-4\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/biosustain/snakemake_UmetaFlow"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2022-08-10T10:24:36Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-08-29T08:54:20Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "Untargeted metabolomics workflow for large-scale data processing and analysis implemented in Snakemake"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9950055686674716,
      "result": {
        "original_header": "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
        "type": "Text_excerpt",
        "value": "This is the Snakemake implementation of the [pyOpenMS workflow](https://github.com/biosustain/pyOpenMS_UmetaFlow.git) tailored by [Eftychia Eva Kontou](https://github.com/eeko-kon) and [Axel Walter](https://github.com/axelwalter).\n \n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9642643772320093,
      "result": {
        "original_header": "Workflow overview",
        "type": "Text_excerpt",
        "value": "1) File conversion: Simply add your Thermo raw files under the directory data/raw/ and they will be converted to centroid mzML files. If you have Agilent, Bruker, or other vendor files, skip that step (write \"FALSE\" for rule fileconversion in the config.yaml file - see more under \"Configure workflow\"), convert them independently using [proteowizard](https://proteowizard.sourceforge.io/) and add them under the data/mzML/ directory. \n2) Pre-processing: converting raw data to a feature table with a series of algorithms through feature detection, alignment and grouping. This step includes an optional removal of blank/QC samples if defined by the user. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values. \n3) Re-quantification (optional): Re-quantify all features with missing values across samples resulted from the pre-processing step for more reliable statistical analysis and data exploration. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values. \n4) Structural and formula predictions (SIRIUS and CSI:FingeID) and annotation of the feature matrix with those predictions (MSI level 3). \n6) Spectral matching with in-house or a publicly available library (MGF/MSP/mzML format) and annotation of the feature matrix with matches that have a score above 60 (MSI level 2). \n7) After FBMN or IIMN: Integrate Sirius and CSI predictions to the network (GraphML) and MSMS spectral library annotations to the feature matrix- MSI level 2 (optional). \n8) MS2Query: add another annotation step with a machine learning tool, MS2Query, that searches for exact spectral matches, as well as analogues, using Spec2Vec and MS2Deepscore. \n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9425964182107277,
      "result": {
        "original_header": "Config &amp; Schemas",
        "type": "Text_excerpt",
        "value": "* [Config & schemas](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html) define the input formatting and are important to generate `wildcards`. The idea of using `samples` and `units` came from [here](https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling).  \n \n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9567952362780306,
      "result": {
        "original_header": "Rules",
        "type": "Text_excerpt",
        "value": "* [Snakefile](workflow/Snakefile): the main entry of the pipeline which tells the final output to be generated and the rules being used\n* [common.smk](workflow/rules/common.smk): a rule that generates the variables used (sample names) & other helper scripts\n* [The main rules (*.smk)](workflow/rules/): the bash code that has been chopped into modular units, with defined input & output. Snakemake then chains this rules together to generate required jobs. This should be intuitive and makes things easier for adding / changing steps in the pipeline.\n \n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 0.9430611158749453,
      "result": {
        "original_header": "Environments",
        "type": "Text_excerpt",
        "value": "* Conda environments are defined as .yaml file in `workflow/envs`\n* Note that not all dependencies are compatible/available as conda libraries. Once installed, the virtual environment are stored in `.snakemake/conda` with unique hashes. The ALE and pilon are example where environment needs to be modified / dependencies need to be installed.\n* It might be better to utilise containers / dockers and cloud execution for \"hard to install\" dependencies\n* Custom dependencies and databases are stored in the `resources/` folder.\n* Snakemake dependencies with conda packages is one of the drawbacks and why [Nextflow](https://www.nextflow.io/) might be more preferable. Nevertheless, the pythonic language of snakemake enables newcomers to learn and develop their own pipeline faster.\n \n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/biosustain/snakemake_UmetaFlow/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "executable_example": [
    {
      "confidence": 1,
      "result": {
        "format": "jupyter_notebook",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/Create_dataset_tsv.ipynb"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/Create_dataset_tsv.ipynb",
      "technique": "file_exploration"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 7
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/biosustain/snakemake_UmetaFlow/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "biosustain/snakemake_UmetaFlow"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "images": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://pepkit.github.io/img/PEP-compatible-green.svg"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "regular_expression"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main//images/UmetaFlow_graph.svg"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "installation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 2: Install all dependencies",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "> **Mamba** and **Snakemake** dependencies:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Install [mambaforge](https://github.com/conda-forge/miniforge#mambaforge) for any system. This step is optional if the user already has conda installed, then replace mamba with conda for the following commands.\n>>\n>>Then install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html) through mamba with:\n>>\n>>      conda create -c conda-forge -c bioconda -n snakemake snakemake python=3.10.8\n>>      \n>**SIRIUS**, **ThermoRawFileParser** executables, and **MS2Query** models:\n>>Download the latest SIRIUS executable compatible with your operating system (linux or macOS), the ThermoRawFileParser (file converter executable for Thermo .RAW files) and MS2Query models. Use the following script to complete this step:\n>>\n>>      cd snakemake_UmetaFlow\n>>      SCRIPT_VERSION=\"0.1.5\"\n>>      wget -O setup_scripts.zip https://github.com/NBChub/umetaflow_tutorial/archive/refs/tags/$SCRIPT_VERSION.zip\n>>      unzip setup_scripts.zip && mv umetaflow_tutorial-$SCRIPT_VERSION/* setup_scripts/\n>>\n>>The important arguments here are the **ion mode** of your data (\"positive\" or \"negative\") which will fetch the respective modules for MS2Query and the **operating system** (\"osx64\" for macOS and \"linux64\" for linux) which will fetch the latest release of the sirius executable for your operating system (defaults: positive mode, osx64). Run the script with or without arguments.\n>>\n>>      bash setup_scripts/setup.sh -o \"osx64\" -m \"positive\"\n>>\n> Install **OpenMS 3.0.0**:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Grab OpenMS 3.0.0 [here](https://github.com/OpenMS/OpenMS/releases/tag/Release3.0.0).\n>>\n>>#### <span style=\"color: green\"> **For Linux(!) only** </span>\n>>Then add the binaries to your path (Linux):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n>>      source ~/.bashrc\n>>#### <span style=\"color: green\"> **For MacOS(!) only** </span>\n>>Then add the binaries to your path (MacOS) by opening one of these files in a text editor:\n>>\n>>      /etc/profile\n>>      ~/.bash_profile\n>>      ~/.bash_login (if .bash_profile does not exist)\n>>      ~/.profile (if .bash_login does not exist)\n>>and adding the path to the binaries at the very end (path-dependent):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n\n### Step 3: Configure workflow\nConfigure the workflow according to your metabolomics data and instrument method via editing the files in the `config/` folder. \n\n1. Adjust the `config.yaml` to: \n- Configure the workflow execution (write <span style=\"color: green\">TRUE</span>/<span style=\"color: red\">FALSE</span> if you want to run/skip a specific rules of the workflow)\n- Adjust the parameters in the configuration file for your dataset as explained in the commented section in the yaml file (e.g. positive/negative ionisation, etc.)\n\n2. Add all your files in the data/raw/ or data/mzML/ directory and generate the `dataset.tsv` table to specify the samples (filenames) that will be processed. \n\n    **Suggestion**: Use the Jupyter notebook [Create_dataset_tsv](./Create_dataset_tsv.ipynb) or simply run:\n    \n    python data_files.py\n\n- `config/dataset.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n#### If there are blanks/QC samples in the file list, then define them through the script.\n\n- `config/blanks.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n\n- `config/samples.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n\n### Step 4: Execute workflow\n\nActivate the conda environment:\n\n    mamba activate snakemake\n    \n\n#### Test the workflow with the example dataset\n    \nTest your configuration by performing a dry-run via\n\n    snakemake --use-conda -n\n\nExecute the workflow locally via\n\n    snakemake --use-conda --cores all\n\nSee the [Snakemake documentation](https://snakemake.readthedocs.io/en/stable/executable.html) for further details.\n\n### Step 5: Investigate results\n\nAll the results are in a .TSV format and can be opened simply with excel or using pandas dataframes. All the files under results/interim can be ignored and eventualy discarded.\n\n## Developer Notes\n\nAll the workflow outputs are silenced for performance enhancement through the flag `-no_progress` or  `>/dev/null` in each rule or `--quiet` for snakemake command (see Execute the workflow locally via) that one can remove. Nevertheless, the error outputs, if any, are written in the specified log files.\n\n### Config & Schemas\n\n* [Config & schemas](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html) define the input formatting and are important to generate `wildcards`. The idea of using `samples` and `units` came from [here](https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling).  \n\n### Rules\n\n* [Snakefile](workflow/Snakefile): the main entry of the pipeline which tells the final output to be generated and the rules being used\n* [common.smk](workflow/rules/common.smk): a rule that generates the variables used (sample names) & other helper scripts\n* [The main rules (*.smk)](workflow/rules/): the bash code that has been chopped into modular units, with defined input & output. Snakemake then chains this rules together to generate required jobs. This should be intuitive and makes things easier for adding / changing steps in the pipeline.\n\n### Environments\n\n* Conda environments are defined as .yaml file in `workflow/envs`\n* Note that not all dependencies are compatible/available as conda libraries. Once installed, the virtual environment are stored in `.snakemake/conda` with unique hashes. The ALE and pilon are example where environment needs to be modified / dependencies need to be installed.\n* It might be better to utilise containers / dockers and cloud execution for \"hard to install\" dependencies\n* Custom dependencies and databases are stored in the `resources/` folder.\n* Snakemake dependencies with conda packages is one of the drawbacks and why [Nextflow](https://www.nextflow.io/) might be more preferable. Nevertheless, the pythonic language of snakemake enables newcomers to learn and develop their own pipeline faster.\n\n### Test Data (only for testing the workflow with the example dataset)\n\n* Current test data are built from known metabolite producer strains or standard samples that have been analyzed with a Thermo Orbitrap IDX instrument. The presence of the metabolites and their fragmentation patterns has been manually confirmed using TOPPView.\n\n### Citations\n\nKontou, E.E., Walter, A., Alka, O. et al. UmetaFlow: an untargeted metabolomics workflow for high-throughput data processing and analysis. J Cheminform 15, 52 (2023). https://doi.org/10.1186/s13321-023-00724-w\n\nPfeuffer J, Sachsenberg T, Alka O, et al. OpenMS \u2013 A platform for reproducible analysis of mass spectrometry data. J Biotechnol. 2017;261:142-148. doi:10.1016/j.jbiotec.2017.05.016\n\nD\u00fchrkop K, Fleischauer M, Ludwig M, et al. SIRIUS 4: a rapid tool for turning tandem mass spectra into metabolite structure information. Nat Methods. 2019;16(4):299-302. doi:10.1038/s41592-019-0344-8\n\nD\u00fchrkop K, Shen H, Meusel M, Rousu J, B\u00f6cker S. Searching molecular structure databases with tandem mass spectra using CSI:FingerID. Proc Natl Acad Sci. 2015;112(41):12580-12585. doi:10.1073/pnas.1509788112\n\nNothias LF, Petras D, Schmid R, et al. Feature-based molecular networking in the GNPS analysis environment. Nat Methods. 2020;17(9):905-908. doi:10.1038/s41592-020-0933-6\n\nSchmid R, Petras D, Nothias LF, et al. Ion identity molecular networking for mass spectrometry-based metabolomics in the GNPS environment. Nat Commun. 2021;12(1):3832. doi:10.1038/s41467-021-23953-9\n\nM\u00f6lder F, Jablonski KP, Letcher B, et al. Sustainable data analysis with Snakemake. Published online January 18, 2021. doi:10.12688/f1000research.29032.1\n\nde Jonge, N.F., Louwen, J.J.R., Chekmeneva, E. et al. MS2Query: reliable and scalable MS2 mass spectra-based analogue search. Nat Commun 14, 1752 (2023). doi:10.1038/s41467-023-37446-4\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "<span style=\"color: green\"> **For both systems** </span>",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage",
          "Step 2: Install all dependencies"
        ],
        "type": "Text_excerpt",
        "value": "# UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems\n\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22657.14.0-brightgreen.svg)](https://snakemake.bitbucket.io)\n[![PEP compatible](https://pepkit.github.io/img/PEP-compatible-green.svg)](https://pep.databio.org)\n\nThis is the Snakemake implementation of the [pyOpenMS workflow](https://github.com/biosustain/pyOpenMS_UmetaFlow.git) tailored by [Eftychia Eva Kontou](https://github.com/eeko-kon) and [Axel Walter](https://github.com/axelwalter).\n\n## Workflow overview\n\nThe pipeline consists of seven interconnected steps:\n\n1) File conversion: Simply add your Thermo raw files under the directory data/raw/ and they will be converted to centroid mzML files. If you have Agilent, Bruker, or other vendor files, skip that step (write \"FALSE\" for rule fileconversion in the config.yaml file - see more under \"Configure workflow\"), convert them independently using [proteowizard](https://proteowizard.sourceforge.io/) and add them under the data/mzML/ directory.\n\n2) Pre-processing: converting raw data to a feature table with a series of algorithms through feature detection, alignment and grouping. This step includes an optional removal of blank/QC samples if defined by the user. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n3) Re-quantification (optional): Re-quantify all features with missing values across samples resulted from the pre-processing step for more reliable statistical analysis and data exploration. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n4) Structural and formula predictions (SIRIUS and CSI:FingeID) and annotation of the feature matrix with those predictions (MSI level 3).\n\n5) GNPSexport: generate all the files necessary to create a [FBMN](https://ccms-ucsd.github.io/GNPSDocumentation/featurebasedmolecularnetworking-with-openms/) or [IIMN](https://ccms-ucsd.github.io/GNPSDocumentation/fbmn-iin/#iimn-networks-with-collapsed-ion-identity-edges) job at GNPS. \n\n6) Spectral matching with in-house or a publicly available library (MGF/MSP/mzML format) and annotation of the feature matrix with matches that have a score above 60 (MSI level 2).\n\n7) After FBMN or IIMN: Integrate Sirius and CSI predictions to the network (GraphML) and MSMS spectral library annotations to the feature matrix- MSI level 2 (optional).\n\n8) MS2Query: add another annotation step with a machine learning tool, MS2Query, that searches for exact spectral matches, as well as analogues, using Spec2Vec and MS2Deepscore.\n\nSee [README](workflow/rules/README.md) file for details.\n### Overview\n![dag](/images/UmetaFlow_graph.svg)\n\n## Usage\n\n### Step 1: Clone the workflow\n\n[Clone](https://help.github.com/en/articles/cloning-a-repository) this repository to your local system, into the place where you want to perform the data analysis.\n   \n    git clone https://github.com/biosustain/snakemake_UmetaFlow.git\n\nMake sure to have the right access / SSH Key. If **not**, follow the steps:\n\nStep (i): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n\nStep (ii): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\n\n\n### Step 2: Install all dependencies\n\n> **Mamba** and **Snakemake** dependencies:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Install [mambaforge](https://github.com/conda-forge/miniforge#mambaforge) for any system. This step is optional if the user already has conda installed, then replace mamba with conda for the following commands.\n>>\n>>Then install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html) through mamba with:\n>>\n>>      conda create -c conda-forge -c bioconda -n snakemake snakemake python=3.10.8\n>>      \n>**SIRIUS**, **ThermoRawFileParser** executables, and **MS2Query** models:\n>>Download the latest SIRIUS executable compatible with your operating system (linux or macOS), the ThermoRawFileParser (file converter executable for Thermo .RAW files) and MS2Query models. Use the following script to complete this step:\n>>\n>>      cd snakemake_UmetaFlow\n>>      SCRIPT_VERSION=\"0.1.5\"\n>>      wget -O setup_scripts.zip https://github.com/NBChub/umetaflow_tutorial/archive/refs/tags/$SCRIPT_VERSION.zip\n>>      unzip setup_scripts.zip && mv umetaflow_tutorial-$SCRIPT_VERSION/* setup_scripts/\n>>\n>>The important arguments here are the **ion mode** of your data (\"positive\" or \"negative\") which will fetch the respective modules for MS2Query and the **operating system** (\"osx64\" for macOS and \"linux64\" for linux) which will fetch the latest release of the sirius executable for your operating system (defaults: positive mode, osx64). Run the script with or without arguments.\n>>\n>>      bash setup_scripts/setup.sh -o \"osx64\" -m \"positive\"\n>>\n> Install **OpenMS 3.0.0**:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Grab OpenMS 3.0.0 [here](https://github.com/OpenMS/OpenMS/releases/tag/Release3.0.0).\n>>\n>>#### <span style=\"color: green\"> **For Linux(!) only** </span>\n>>Then add the binaries to your path (Linux):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n>>      source ~/.bashrc\n>>#### <span style=\"color: green\"> **For MacOS(!) only** </span>\n>>Then add the binaries to your path (MacOS) by opening one of these files in a text editor:\n>>\n>>      /etc/profile\n>>      ~/.bash_profile\n>>      ~/.bash_login (if .bash_profile does not exist)\n>>      ~/.profile (if .bash_login does not exist)\n>>and adding the path to the binaries at the very end (path-dependent):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n\n### Step 3: Configure workflow\nConfigure the workflow according to your metabolomics data and instrument method via editing the files in the `config/` folder. \n\n1. Adjust the `config.yaml` to: \n- Configure the workflow execution (write <span style=\"color: green\">TRUE</span>/<span style=\"color: red\">FALSE</span> if you want to run/skip a specific rules of the workflow)\n- Adjust the parameters in the configuration file for your dataset as explained in the commented section in the yaml file (e.g. positive/negative ionisation, etc.)\n\n2. Add all your files in the data/raw/ or data/mzML/ directory and generate the `dataset.tsv` table to specify the samples (filenames) that will be processed. \n\n    **Suggestion**: Use the Jupyter notebook [Create_dataset_tsv](./Create_dataset_tsv.ipynb) or simply run:\n    \n    python data_files.py\n\n- `config/dataset.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n#### If there are blanks/QC samples in the file list, then define them through the script.\n\n- `config/blanks.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n\n- `config/samples.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n\n### Step 4: Execute workflow\n\nActivate the conda environment:\n\n    mamba activate snakemake\n    \n\n#### Test the workflow with the example dataset\n    \nTest your configuration by performing a dry-run via\n\n    snakemake --use-conda -n\n\nExecute the workflow locally via\n\n    snakemake --use-conda --cores all\n\nSee the [Snakemake documentation](https://snakemake.readthedocs.io/en/stable/executable.html) for further details.\n\n### Step 5: Investigate results\n\nAll the results are in a .TSV format and can be opened simply with excel or using pandas dataframes. All the files under results/interim can be ignored and eventualy discarded.\n\n## Developer Notes\n\nAll the workflow outputs are silenced for performance enhancement through the flag `-no_progress` or  `>/dev/null` in each rule or `--quiet` for snakemake command (see Execute the workflow locally via) that one can remove. Nevertheless, the error outputs, if any, are written in the specified log files.\n\n### Config & Schemas\n\n* [Config & schemas](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html) define the input formatting and are important to generate `wildcards`. The idea of using `samples` and `units` came from [here](https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling).  \n\n### Rules\n\n* [Snakefile](workflow/Snakefile): the main entry of the pipeline which tells the final output to be generated and the rules being used\n* [common.smk](workflow/rules/common.smk): a rule that generates the variables used (sample names) & other helper scripts\n* [The main rules (*.smk)](workflow/rules/): the bash code that has been chopped into modular units, with defined input & output. Snakemake then chains this rules together to generate required jobs. This should be intuitive and makes things easier for adding / changing steps in the pipeline.\n\n### Environments\n\n* Conda environments are defined as .yaml file in `workflow/envs`\n* Note that not all dependencies are compatible/available as conda libraries. Once installed, the virtual environment are stored in `.snakemake/conda` with unique hashes. The ALE and pilon are example where environment needs to be modified / dependencies need to be installed.\n* It might be better to utilise containers / dockers and cloud execution for \"hard to install\" dependencies\n* Custom dependencies and databases are stored in the `resources/` folder.\n* Snakemake dependencies with conda packages is one of the drawbacks and why [Nextflow](https://www.nextflow.io/) might be more preferable. Nevertheless, the pythonic language of snakemake enables newcomers to learn and develop their own pipeline faster.\n\n### Test Data (only for testing the workflow with the example dataset)\n\n* Current test data are built from known metabolite producer strains or standard samples that have been analyzed with a Thermo Orbitrap IDX instrument. The presence of the metabolites and their fragmentation patterns has been manually confirmed using TOPPView.\n\n### Citations\n\nKontou, E.E., Walter, A., Alka, O. et al. UmetaFlow: an untargeted metabolomics workflow for high-throughput data processing and analysis. J Cheminform 15, 52 (2023). https://doi.org/10.1186/s13321-023-00724-w\n\nPfeuffer J, Sachsenberg T, Alka O, et al. OpenMS \u2013 A platform for reproducible analysis of mass spectrometry data. J Biotechnol. 2017;261:142-148. doi:10.1016/j.jbiotec.2017.05.016\n\nD\u00fchrkop K, Fleischauer M, Ludwig M, et al. SIRIUS 4: a rapid tool for turning tandem mass spectra into metabolite structure information. Nat Methods. 2019;16(4):299-302. doi:10.1038/s41592-019-0344-8\n\nD\u00fchrkop K, Shen H, Meusel M, Rousu J, B\u00f6cker S. Searching molecular structure databases with tandem mass spectra using CSI:FingerID. Proc Natl Acad Sci. 2015;112(41):12580-12585. doi:10.1073/pnas.1509788112\n\nNothias LF, Petras D, Schmid R, et al. Feature-based molecular networking in the GNPS analysis environment. Nat Methods. 2020;17(9):905-908. doi:10.1038/s41592-020-0933-6\n\nSchmid R, Petras D, Nothias LF, et al. Ion identity molecular networking for mass spectrometry-based metabolomics in the GNPS environment. Nat Commun. 2021;12(1):3832. doi:10.1038/s41467-021-23953-9\n\nM\u00f6lder F, Jablonski KP, Letcher B, et al. Sustainable data analysis with Snakemake. Published online January 18, 2021. doi:10.12688/f1000research.29032.1\n\nde Jonge, N.F., Louwen, J.J.R., Chekmeneva, E. et al. MS2Query: reliable and scalable MS2 mass spectra-based analogue search. Nat Commun 14, 1752 (2023). doi:10.1038/s41467-023-37446-4\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "<span style=\"color: green\"> **For Linux(!) only** </span>",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage",
          "Step 2: Install all dependencies"
        ],
        "type": "Text_excerpt",
        "value": "# UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems\n\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22657.14.0-brightgreen.svg)](https://snakemake.bitbucket.io)\n[![PEP compatible](https://pepkit.github.io/img/PEP-compatible-green.svg)](https://pep.databio.org)\n\nThis is the Snakemake implementation of the [pyOpenMS workflow](https://github.com/biosustain/pyOpenMS_UmetaFlow.git) tailored by [Eftychia Eva Kontou](https://github.com/eeko-kon) and [Axel Walter](https://github.com/axelwalter).\n\n## Workflow overview\n\nThe pipeline consists of seven interconnected steps:\n\n1) File conversion: Simply add your Thermo raw files under the directory data/raw/ and they will be converted to centroid mzML files. If you have Agilent, Bruker, or other vendor files, skip that step (write \"FALSE\" for rule fileconversion in the config.yaml file - see more under \"Configure workflow\"), convert them independently using [proteowizard](https://proteowizard.sourceforge.io/) and add them under the data/mzML/ directory.\n\n2) Pre-processing: converting raw data to a feature table with a series of algorithms through feature detection, alignment and grouping. This step includes an optional removal of blank/QC samples if defined by the user. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n3) Re-quantification (optional): Re-quantify all features with missing values across samples resulted from the pre-processing step for more reliable statistical analysis and data exploration. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n4) Structural and formula predictions (SIRIUS and CSI:FingeID) and annotation of the feature matrix with those predictions (MSI level 3).\n\n5) GNPSexport: generate all the files necessary to create a [FBMN](https://ccms-ucsd.github.io/GNPSDocumentation/featurebasedmolecularnetworking-with-openms/) or [IIMN](https://ccms-ucsd.github.io/GNPSDocumentation/fbmn-iin/#iimn-networks-with-collapsed-ion-identity-edges) job at GNPS. \n\n6) Spectral matching with in-house or a publicly available library (MGF/MSP/mzML format) and annotation of the feature matrix with matches that have a score above 60 (MSI level 2).\n\n7) After FBMN or IIMN: Integrate Sirius and CSI predictions to the network (GraphML) and MSMS spectral library annotations to the feature matrix- MSI level 2 (optional).\n\n8) MS2Query: add another annotation step with a machine learning tool, MS2Query, that searches for exact spectral matches, as well as analogues, using Spec2Vec and MS2Deepscore.\n\nSee [README](workflow/rules/README.md) file for details.\n### Overview\n![dag](/images/UmetaFlow_graph.svg)\n\n## Usage\n\n### Step 1: Clone the workflow\n\n[Clone](https://help.github.com/en/articles/cloning-a-repository) this repository to your local system, into the place where you want to perform the data analysis.\n   \n    git clone https://github.com/biosustain/snakemake_UmetaFlow.git\n\nMake sure to have the right access / SSH Key. If **not**, follow the steps:\n\nStep (i): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n\nStep (ii): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\n\n\n### Step 2: Install all dependencies\n\n> **Mamba** and **Snakemake** dependencies:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Install [mambaforge](https://github.com/conda-forge/miniforge#mambaforge) for any system. This step is optional if the user already has conda installed, then replace mamba with conda for the following commands.\n>>\n>>Then install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html) through mamba with:\n>>\n>>      conda create -c conda-forge -c bioconda -n snakemake snakemake python=3.10.8\n>>      \n>**SIRIUS**, **ThermoRawFileParser** executables, and **MS2Query** models:\n>>Download the latest SIRIUS executable compatible with your operating system (linux or macOS), the ThermoRawFileParser (file converter executable for Thermo .RAW files) and MS2Query models. Use the following script to complete this step:\n>>\n>>      cd snakemake_UmetaFlow\n>>      SCRIPT_VERSION=\"0.1.5\"\n>>      wget -O setup_scripts.zip https://github.com/NBChub/umetaflow_tutorial/archive/refs/tags/$SCRIPT_VERSION.zip\n>>      unzip setup_scripts.zip && mv umetaflow_tutorial-$SCRIPT_VERSION/* setup_scripts/\n>>\n>>The important arguments here are the **ion mode** of your data (\"positive\" or \"negative\") which will fetch the respective modules for MS2Query and the **operating system** (\"osx64\" for macOS and \"linux64\" for linux) which will fetch the latest release of the sirius executable for your operating system (defaults: positive mode, osx64). Run the script with or without arguments.\n>>\n>>      bash setup_scripts/setup.sh -o \"osx64\" -m \"positive\"\n>>\n> Install **OpenMS 3.0.0**:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Grab OpenMS 3.0.0 [here](https://github.com/OpenMS/OpenMS/releases/tag/Release3.0.0).\n>>\n>>#### <span style=\"color: green\"> **For Linux(!) only** </span>\n>>Then add the binaries to your path (Linux):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n>>      source ~/.bashrc\n>>#### <span style=\"color: green\"> **For MacOS(!) only** </span>\n>>Then add the binaries to your path (MacOS) by opening one of these files in a text editor:\n>>\n>>      /etc/profile\n>>      ~/.bash_profile\n>>      ~/.bash_login (if .bash_profile does not exist)\n>>      ~/.profile (if .bash_login does not exist)\n>>and adding the path to the binaries at the very end (path-dependent):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n\n### Step 3: Configure workflow\nConfigure the workflow according to your metabolomics data and instrument method via editing the files in the `config/` folder. \n\n1. Adjust the `config.yaml` to: \n- Configure the workflow execution (write <span style=\"color: green\">TRUE</span>/<span style=\"color: red\">FALSE</span> if you want to run/skip a specific rules of the workflow)\n- Adjust the parameters in the configuration file for your dataset as explained in the commented section in the yaml file (e.g. positive/negative ionisation, etc.)\n\n2. Add all your files in the data/raw/ or data/mzML/ directory and generate the `dataset.tsv` table to specify the samples (filenames) that will be processed. \n\n    **Suggestion**: Use the Jupyter notebook [Create_dataset_tsv](./Create_dataset_tsv.ipynb) or simply run:\n    \n    python data_files.py\n\n- `config/dataset.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n#### If there are blanks/QC samples in the file list, then define them through the script.\n\n- `config/blanks.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n\n- `config/samples.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n\n### Step 4: Execute workflow\n\nActivate the conda environment:\n\n    mamba activate snakemake\n    \n\n#### Test the workflow with the example dataset\n    \nTest your configuration by performing a dry-run via\n\n    snakemake --use-conda -n\n\nExecute the workflow locally via\n\n    snakemake --use-conda --cores all\n\nSee the [Snakemake documentation](https://snakemake.readthedocs.io/en/stable/executable.html) for further details.\n\n### Step 5: Investigate results\n\nAll the results are in a .TSV format and can be opened simply with excel or using pandas dataframes. All the files under results/interim can be ignored and eventualy discarded.\n\n## Developer Notes\n\nAll the workflow outputs are silenced for performance enhancement through the flag `-no_progress` or  `>/dev/null` in each rule or `--quiet` for snakemake command (see Execute the workflow locally via) that one can remove. Nevertheless, the error outputs, if any, are written in the specified log files.\n\n### Config & Schemas\n\n* [Config & schemas](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html) define the input formatting and are important to generate `wildcards`. The idea of using `samples` and `units` came from [here](https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling).  \n\n### Rules\n\n* [Snakefile](workflow/Snakefile): the main entry of the pipeline which tells the final output to be generated and the rules being used\n* [common.smk](workflow/rules/common.smk): a rule that generates the variables used (sample names) & other helper scripts\n* [The main rules (*.smk)](workflow/rules/): the bash code that has been chopped into modular units, with defined input & output. Snakemake then chains this rules together to generate required jobs. This should be intuitive and makes things easier for adding / changing steps in the pipeline.\n\n### Environments\n\n* Conda environments are defined as .yaml file in `workflow/envs`\n* Note that not all dependencies are compatible/available as conda libraries. Once installed, the virtual environment are stored in `.snakemake/conda` with unique hashes. The ALE and pilon are example where environment needs to be modified / dependencies need to be installed.\n* It might be better to utilise containers / dockers and cloud execution for \"hard to install\" dependencies\n* Custom dependencies and databases are stored in the `resources/` folder.\n* Snakemake dependencies with conda packages is one of the drawbacks and why [Nextflow](https://www.nextflow.io/) might be more preferable. Nevertheless, the pythonic language of snakemake enables newcomers to learn and develop their own pipeline faster.\n\n### Test Data (only for testing the workflow with the example dataset)\n\n* Current test data are built from known metabolite producer strains or standard samples that have been analyzed with a Thermo Orbitrap IDX instrument. The presence of the metabolites and their fragmentation patterns has been manually confirmed using TOPPView.\n\n### Citations\n\nKontou, E.E., Walter, A., Alka, O. et al. UmetaFlow: an untargeted metabolomics workflow for high-throughput data processing and analysis. J Cheminform 15, 52 (2023). https://doi.org/10.1186/s13321-023-00724-w\n\nPfeuffer J, Sachsenberg T, Alka O, et al. OpenMS \u2013 A platform for reproducible analysis of mass spectrometry data. J Biotechnol. 2017;261:142-148. doi:10.1016/j.jbiotec.2017.05.016\n\nD\u00fchrkop K, Fleischauer M, Ludwig M, et al. SIRIUS 4: a rapid tool for turning tandem mass spectra into metabolite structure information. Nat Methods. 2019;16(4):299-302. doi:10.1038/s41592-019-0344-8\n\nD\u00fchrkop K, Shen H, Meusel M, Rousu J, B\u00f6cker S. Searching molecular structure databases with tandem mass spectra using CSI:FingerID. Proc Natl Acad Sci. 2015;112(41):12580-12585. doi:10.1073/pnas.1509788112\n\nNothias LF, Petras D, Schmid R, et al. Feature-based molecular networking in the GNPS analysis environment. Nat Methods. 2020;17(9):905-908. doi:10.1038/s41592-020-0933-6\n\nSchmid R, Petras D, Nothias LF, et al. Ion identity molecular networking for mass spectrometry-based metabolomics in the GNPS environment. Nat Commun. 2021;12(1):3832. doi:10.1038/s41467-021-23953-9\n\nM\u00f6lder F, Jablonski KP, Letcher B, et al. Sustainable data analysis with Snakemake. Published online January 18, 2021. doi:10.12688/f1000research.29032.1\n\nde Jonge, N.F., Louwen, J.J.R., Chekmeneva, E. et al. MS2Query: reliable and scalable MS2 mass spectra-based analogue search. Nat Commun 14, 1752 (2023). doi:10.1038/s41467-023-37446-4\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "<span style=\"color: green\"> **For MacOS(!) only** </span>",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage",
          "Step 2: Install all dependencies"
        ],
        "type": "Text_excerpt",
        "value": "# UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems\n\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22657.14.0-brightgreen.svg)](https://snakemake.bitbucket.io)\n[![PEP compatible](https://pepkit.github.io/img/PEP-compatible-green.svg)](https://pep.databio.org)\n\nThis is the Snakemake implementation of the [pyOpenMS workflow](https://github.com/biosustain/pyOpenMS_UmetaFlow.git) tailored by [Eftychia Eva Kontou](https://github.com/eeko-kon) and [Axel Walter](https://github.com/axelwalter).\n\n## Workflow overview\n\nThe pipeline consists of seven interconnected steps:\n\n1) File conversion: Simply add your Thermo raw files under the directory data/raw/ and they will be converted to centroid mzML files. If you have Agilent, Bruker, or other vendor files, skip that step (write \"FALSE\" for rule fileconversion in the config.yaml file - see more under \"Configure workflow\"), convert them independently using [proteowizard](https://proteowizard.sourceforge.io/) and add them under the data/mzML/ directory.\n\n2) Pre-processing: converting raw data to a feature table with a series of algorithms through feature detection, alignment and grouping. This step includes an optional removal of blank/QC samples if defined by the user. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n3) Re-quantification (optional): Re-quantify all features with missing values across samples resulted from the pre-processing step for more reliable statistical analysis and data exploration. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n4) Structural and formula predictions (SIRIUS and CSI:FingeID) and annotation of the feature matrix with those predictions (MSI level 3).\n\n5) GNPSexport: generate all the files necessary to create a [FBMN](https://ccms-ucsd.github.io/GNPSDocumentation/featurebasedmolecularnetworking-with-openms/) or [IIMN](https://ccms-ucsd.github.io/GNPSDocumentation/fbmn-iin/#iimn-networks-with-collapsed-ion-identity-edges) job at GNPS. \n\n6) Spectral matching with in-house or a publicly available library (MGF/MSP/mzML format) and annotation of the feature matrix with matches that have a score above 60 (MSI level 2).\n\n7) After FBMN or IIMN: Integrate Sirius and CSI predictions to the network (GraphML) and MSMS spectral library annotations to the feature matrix- MSI level 2 (optional).\n\n8) MS2Query: add another annotation step with a machine learning tool, MS2Query, that searches for exact spectral matches, as well as analogues, using Spec2Vec and MS2Deepscore.\n\nSee [README](workflow/rules/README.md) file for details.\n### Overview\n![dag](/images/UmetaFlow_graph.svg)\n\n## Usage\n\n### Step 1: Clone the workflow\n\n[Clone](https://help.github.com/en/articles/cloning-a-repository) this repository to your local system, into the place where you want to perform the data analysis.\n   \n    git clone https://github.com/biosustain/snakemake_UmetaFlow.git\n\nMake sure to have the right access / SSH Key. If **not**, follow the steps:\n\nStep (i): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n\nStep (ii): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\n\n\n### Step 2: Install all dependencies\n\n> **Mamba** and **Snakemake** dependencies:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Install [mambaforge](https://github.com/conda-forge/miniforge#mambaforge) for any system. This step is optional if the user already has conda installed, then replace mamba with conda for the following commands.\n>>\n>>Then install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html) through mamba with:\n>>\n>>      conda create -c conda-forge -c bioconda -n snakemake snakemake python=3.10.8\n>>      \n>**SIRIUS**, **ThermoRawFileParser** executables, and **MS2Query** models:\n>>Download the latest SIRIUS executable compatible with your operating system (linux or macOS), the ThermoRawFileParser (file converter executable for Thermo .RAW files) and MS2Query models. Use the following script to complete this step:\n>>\n>>      cd snakemake_UmetaFlow\n>>      SCRIPT_VERSION=\"0.1.5\"\n>>      wget -O setup_scripts.zip https://github.com/NBChub/umetaflow_tutorial/archive/refs/tags/$SCRIPT_VERSION.zip\n>>      unzip setup_scripts.zip && mv umetaflow_tutorial-$SCRIPT_VERSION/* setup_scripts/\n>>\n>>The important arguments here are the **ion mode** of your data (\"positive\" or \"negative\") which will fetch the respective modules for MS2Query and the **operating system** (\"osx64\" for macOS and \"linux64\" for linux) which will fetch the latest release of the sirius executable for your operating system (defaults: positive mode, osx64). Run the script with or without arguments.\n>>\n>>      bash setup_scripts/setup.sh -o \"osx64\" -m \"positive\"\n>>\n> Install **OpenMS 3.0.0**:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Grab OpenMS 3.0.0 [here](https://github.com/OpenMS/OpenMS/releases/tag/Release3.0.0).\n>>\n>>#### <span style=\"color: green\"> **For Linux(!) only** </span>\n>>Then add the binaries to your path (Linux):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n>>      source ~/.bashrc\n>>#### <span style=\"color: green\"> **For MacOS(!) only** </span>\n>>Then add the binaries to your path (MacOS) by opening one of these files in a text editor:\n>>\n>>      /etc/profile\n>>      ~/.bash_profile\n>>      ~/.bash_login (if .bash_profile does not exist)\n>>      ~/.profile (if .bash_login does not exist)\n>>and adding the path to the binaries at the very end (path-dependent):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 0.9999999999939746,
      "result": {
        "original_header": "Environments",
        "type": "Text_excerpt",
        "value": "* Conda environments are defined as .yaml file in `workflow/envs`\n* Note that not all dependencies are compatible/available as conda libraries. Once installed, the virtual environment are stored in `.snakemake/conda` with unique hashes. The ALE and pilon are example where environment needs to be modified / dependencies need to be installed.\n* It might be better to utilise containers / dockers and cloud execution for \"hard to install\" dependencies\n* Custom dependencies and databases are stored in the `resources/` folder.\n* Snakemake dependencies with conda packages is one of the drawbacks and why [Nextflow](https://www.nextflow.io/) might be more preferable. Nevertheless, the pythonic language of snakemake enables newcomers to learn and develop their own pipeline faster.\n \n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/biosustain/snakemake_UmetaFlow/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "Apache License 2.0",
        "spdx_id": "Apache-2.0",
        "type": "License",
        "url": "https://api.github.com/licenses/apache-2.0",
        "value": "https://api.github.com/licenses/apache-2.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "snakemake_UmetaFlow"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "Organization",
        "value": "biosustain"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 106078,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Jupyter Notebook",
        "size": 6205,
        "type": "Programming_language",
        "value": "Jupyter Notebook"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "related_documentation": [
    {
      "confidence": 1,
      "result": {
        "format": "readthedocs",
        "type": "Url",
        "value": "https://snakemake.readthedocs.io/"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "regular_expression"
    }
  ],
  "releases": [
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "eeko-kon",
          "type": "User"
        },
        "date_created": "2024-02-15T21:00:56Z",
        "date_published": "2024-02-15T21:13:49Z",
        "description": "Simpler installation \r\nyaml file library updates",
        "html_url": "https://github.com/biosustain/snakemake_UmetaFlow/releases/tag/0.1.1",
        "name": "v0.1.1",
        "release_id": 142235162,
        "tag": "0.1.1",
        "tarball_url": "https://api.github.com/repos/biosustain/snakemake_UmetaFlow/tarball/0.1.1",
        "type": "Release",
        "url": "https://api.github.com/repos/biosustain/snakemake_UmetaFlow/releases/142235162",
        "value": "https://api.github.com/repos/biosustain/snakemake_UmetaFlow/releases/142235162",
        "zipball_url": "https://api.github.com/repos/biosustain/snakemake_UmetaFlow/zipball/0.1.1"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "author": {
          "name": "eeko-kon",
          "type": "User"
        },
        "date_created": "2023-07-10T07:39:42Z",
        "date_published": "2023-07-12T11:42:16Z",
        "description": "First UmetaFlow complete release.\r\n\r\nWorks with OpenMS 3.0, sirius version >5.6.3, ms2query<=1.2.0",
        "html_url": "https://github.com/biosustain/snakemake_UmetaFlow/releases/tag/0.1.0",
        "name": "0.1.0",
        "release_id": 111936123,
        "tag": "0.1.0",
        "tarball_url": "https://api.github.com/repos/biosustain/snakemake_UmetaFlow/tarball/0.1.0",
        "type": "Release",
        "url": "https://api.github.com/repos/biosustain/snakemake_UmetaFlow/releases/111936123",
        "value": "https://api.github.com/repos/biosustain/snakemake_UmetaFlow/releases/111936123",
        "zipball_url": "https://api.github.com/repos/biosustain/snakemake_UmetaFlow/zipball/0.1.0"
      },
      "technique": "GitHub_API"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 2: Install all dependencies",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "> **Mamba** and **Snakemake** dependencies:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Install [mambaforge](https://github.com/conda-forge/miniforge#mambaforge) for any system. This step is optional if the user already has conda installed, then replace mamba with conda for the following commands.\n>>\n>>Then install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html) through mamba with:\n>>\n>>      conda create -c conda-forge -c bioconda -n snakemake snakemake python=3.10.8\n>>      \n>**SIRIUS**, **ThermoRawFileParser** executables, and **MS2Query** models:\n>>Download the latest SIRIUS executable compatible with your operating system (linux or macOS), the ThermoRawFileParser (file converter executable for Thermo .RAW files) and MS2Query models. Use the following script to complete this step:\n>>\n>>      cd snakemake_UmetaFlow\n>>      SCRIPT_VERSION=\"0.1.5\"\n>>      wget -O setup_scripts.zip https://github.com/NBChub/umetaflow_tutorial/archive/refs/tags/$SCRIPT_VERSION.zip\n>>      unzip setup_scripts.zip && mv umetaflow_tutorial-$SCRIPT_VERSION/* setup_scripts/\n>>\n>>The important arguments here are the **ion mode** of your data (\"positive\" or \"negative\") which will fetch the respective modules for MS2Query and the **operating system** (\"osx64\" for macOS and \"linux64\" for linux) which will fetch the latest release of the sirius executable for your operating system (defaults: positive mode, osx64). Run the script with or without arguments.\n>>\n>>      bash setup_scripts/setup.sh -o \"osx64\" -m \"positive\"\n>>\n> Install **OpenMS 3.0.0**:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Grab OpenMS 3.0.0 [here](https://github.com/OpenMS/OpenMS/releases/tag/Release3.0.0).\n>>\n>>#### <span style=\"color: green\"> **For Linux(!) only** </span>\n>>Then add the binaries to your path (Linux):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n>>      source ~/.bashrc\n>>#### <span style=\"color: green\"> **For MacOS(!) only** </span>\n>>Then add the binaries to your path (MacOS) by opening one of these files in a text editor:\n>>\n>>      /etc/profile\n>>      ~/.bash_profile\n>>      ~/.bash_login (if .bash_profile does not exist)\n>>      ~/.profile (if .bash_login does not exist)\n>>and adding the path to the binaries at the very end (path-dependent):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n\n### Step 3: Configure workflow\nConfigure the workflow according to your metabolomics data and instrument method via editing the files in the `config/` folder. \n\n1. Adjust the `config.yaml` to: \n- Configure the workflow execution (write <span style=\"color: green\">TRUE</span>/<span style=\"color: red\">FALSE</span> if you want to run/skip a specific rules of the workflow)\n- Adjust the parameters in the configuration file for your dataset as explained in the commented section in the yaml file (e.g. positive/negative ionisation, etc.)\n\n2. Add all your files in the data/raw/ or data/mzML/ directory and generate the `dataset.tsv` table to specify the samples (filenames) that will be processed. \n\n    **Suggestion**: Use the Jupyter notebook [Create_dataset_tsv](./Create_dataset_tsv.ipynb) or simply run:\n    \n    python data_files.py\n\n- `config/dataset.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n#### If there are blanks/QC samples in the file list, then define them through the script.\n\n- `config/blanks.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n\n- `config/samples.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n\n### Step 4: Execute workflow\n\nActivate the conda environment:\n\n    mamba activate snakemake\n    \n\n#### Test the workflow with the example dataset\n    \nTest your configuration by performing a dry-run via\n\n    snakemake --use-conda -n\n\nExecute the workflow locally via\n\n    snakemake --use-conda --cores all\n\nSee the [Snakemake documentation](https://snakemake.readthedocs.io/en/stable/executable.html) for further details.\n\n### Step 5: Investigate results\n\nAll the results are in a .TSV format and can be opened simply with excel or using pandas dataframes. All the files under results/interim can be ignored and eventualy discarded.\n\n## Developer Notes\n\nAll the workflow outputs are silenced for performance enhancement through the flag `-no_progress` or  `>/dev/null` in each rule or `--quiet` for snakemake command (see Execute the workflow locally via) that one can remove. Nevertheless, the error outputs, if any, are written in the specified log files.\n\n### Config & Schemas\n\n* [Config & schemas](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html) define the input formatting and are important to generate `wildcards`. The idea of using `samples` and `units` came from [here](https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling).  \n\n### Rules\n\n* [Snakefile](workflow/Snakefile): the main entry of the pipeline which tells the final output to be generated and the rules being used\n* [common.smk](workflow/rules/common.smk): a rule that generates the variables used (sample names) & other helper scripts\n* [The main rules (*.smk)](workflow/rules/): the bash code that has been chopped into modular units, with defined input & output. Snakemake then chains this rules together to generate required jobs. This should be intuitive and makes things easier for adding / changing steps in the pipeline.\n\n### Environments\n\n* Conda environments are defined as .yaml file in `workflow/envs`\n* Note that not all dependencies are compatible/available as conda libraries. Once installed, the virtual environment are stored in `.snakemake/conda` with unique hashes. The ALE and pilon are example where environment needs to be modified / dependencies need to be installed.\n* It might be better to utilise containers / dockers and cloud execution for \"hard to install\" dependencies\n* Custom dependencies and databases are stored in the `resources/` folder.\n* Snakemake dependencies with conda packages is one of the drawbacks and why [Nextflow](https://www.nextflow.io/) might be more preferable. Nevertheless, the pythonic language of snakemake enables newcomers to learn and develop their own pipeline faster.\n\n### Test Data (only for testing the workflow with the example dataset)\n\n* Current test data are built from known metabolite producer strains or standard samples that have been analyzed with a Thermo Orbitrap IDX instrument. The presence of the metabolites and their fragmentation patterns has been manually confirmed using TOPPView.\n\n### Citations\n\nKontou, E.E., Walter, A., Alka, O. et al. UmetaFlow: an untargeted metabolomics workflow for high-throughput data processing and analysis. J Cheminform 15, 52 (2023). https://doi.org/10.1186/s13321-023-00724-w\n\nPfeuffer J, Sachsenberg T, Alka O, et al. OpenMS \u2013 A platform for reproducible analysis of mass spectrometry data. J Biotechnol. 2017;261:142-148. doi:10.1016/j.jbiotec.2017.05.016\n\nD\u00fchrkop K, Fleischauer M, Ludwig M, et al. SIRIUS 4: a rapid tool for turning tandem mass spectra into metabolite structure information. Nat Methods. 2019;16(4):299-302. doi:10.1038/s41592-019-0344-8\n\nD\u00fchrkop K, Shen H, Meusel M, Rousu J, B\u00f6cker S. Searching molecular structure databases with tandem mass spectra using CSI:FingerID. Proc Natl Acad Sci. 2015;112(41):12580-12585. doi:10.1073/pnas.1509788112\n\nNothias LF, Petras D, Schmid R, et al. Feature-based molecular networking in the GNPS analysis environment. Nat Methods. 2020;17(9):905-908. doi:10.1038/s41592-020-0933-6\n\nSchmid R, Petras D, Nothias LF, et al. Ion identity molecular networking for mass spectrometry-based metabolomics in the GNPS environment. Nat Commun. 2021;12(1):3832. doi:10.1038/s41467-021-23953-9\n\nM\u00f6lder F, Jablonski KP, Letcher B, et al. Sustainable data analysis with Snakemake. Published online January 18, 2021. doi:10.12688/f1000research.29032.1\n\nde Jonge, N.F., Louwen, J.J.R., Chekmeneva, E. et al. MS2Query: reliable and scalable MS2 mass spectra-based analogue search. Nat Commun 14, 1752 (2023). doi:10.1038/s41467-023-37446-4\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "<span style=\"color: green\"> **For both systems** </span>",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage",
          "Step 2: Install all dependencies"
        ],
        "type": "Text_excerpt",
        "value": "# UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems\n\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22657.14.0-brightgreen.svg)](https://snakemake.bitbucket.io)\n[![PEP compatible](https://pepkit.github.io/img/PEP-compatible-green.svg)](https://pep.databio.org)\n\nThis is the Snakemake implementation of the [pyOpenMS workflow](https://github.com/biosustain/pyOpenMS_UmetaFlow.git) tailored by [Eftychia Eva Kontou](https://github.com/eeko-kon) and [Axel Walter](https://github.com/axelwalter).\n\n## Workflow overview\n\nThe pipeline consists of seven interconnected steps:\n\n1) File conversion: Simply add your Thermo raw files under the directory data/raw/ and they will be converted to centroid mzML files. If you have Agilent, Bruker, or other vendor files, skip that step (write \"FALSE\" for rule fileconversion in the config.yaml file - see more under \"Configure workflow\"), convert them independently using [proteowizard](https://proteowizard.sourceforge.io/) and add them under the data/mzML/ directory.\n\n2) Pre-processing: converting raw data to a feature table with a series of algorithms through feature detection, alignment and grouping. This step includes an optional removal of blank/QC samples if defined by the user. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n3) Re-quantification (optional): Re-quantify all features with missing values across samples resulted from the pre-processing step for more reliable statistical analysis and data exploration. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n4) Structural and formula predictions (SIRIUS and CSI:FingeID) and annotation of the feature matrix with those predictions (MSI level 3).\n\n5) GNPSexport: generate all the files necessary to create a [FBMN](https://ccms-ucsd.github.io/GNPSDocumentation/featurebasedmolecularnetworking-with-openms/) or [IIMN](https://ccms-ucsd.github.io/GNPSDocumentation/fbmn-iin/#iimn-networks-with-collapsed-ion-identity-edges) job at GNPS. \n\n6) Spectral matching with in-house or a publicly available library (MGF/MSP/mzML format) and annotation of the feature matrix with matches that have a score above 60 (MSI level 2).\n\n7) After FBMN or IIMN: Integrate Sirius and CSI predictions to the network (GraphML) and MSMS spectral library annotations to the feature matrix- MSI level 2 (optional).\n\n8) MS2Query: add another annotation step with a machine learning tool, MS2Query, that searches for exact spectral matches, as well as analogues, using Spec2Vec and MS2Deepscore.\n\nSee [README](workflow/rules/README.md) file for details.\n### Overview\n![dag](/images/UmetaFlow_graph.svg)\n\n## Usage\n\n### Step 1: Clone the workflow\n\n[Clone](https://help.github.com/en/articles/cloning-a-repository) this repository to your local system, into the place where you want to perform the data analysis.\n   \n    git clone https://github.com/biosustain/snakemake_UmetaFlow.git\n\nMake sure to have the right access / SSH Key. If **not**, follow the steps:\n\nStep (i): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n\nStep (ii): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\n\n\n### Step 2: Install all dependencies\n\n> **Mamba** and **Snakemake** dependencies:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Install [mambaforge](https://github.com/conda-forge/miniforge#mambaforge) for any system. This step is optional if the user already has conda installed, then replace mamba with conda for the following commands.\n>>\n>>Then install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html) through mamba with:\n>>\n>>      conda create -c conda-forge -c bioconda -n snakemake snakemake python=3.10.8\n>>      \n>**SIRIUS**, **ThermoRawFileParser** executables, and **MS2Query** models:\n>>Download the latest SIRIUS executable compatible with your operating system (linux or macOS), the ThermoRawFileParser (file converter executable for Thermo .RAW files) and MS2Query models. Use the following script to complete this step:\n>>\n>>      cd snakemake_UmetaFlow\n>>      SCRIPT_VERSION=\"0.1.5\"\n>>      wget -O setup_scripts.zip https://github.com/NBChub/umetaflow_tutorial/archive/refs/tags/$SCRIPT_VERSION.zip\n>>      unzip setup_scripts.zip && mv umetaflow_tutorial-$SCRIPT_VERSION/* setup_scripts/\n>>\n>>The important arguments here are the **ion mode** of your data (\"positive\" or \"negative\") which will fetch the respective modules for MS2Query and the **operating system** (\"osx64\" for macOS and \"linux64\" for linux) which will fetch the latest release of the sirius executable for your operating system (defaults: positive mode, osx64). Run the script with or without arguments.\n>>\n>>      bash setup_scripts/setup.sh -o \"osx64\" -m \"positive\"\n>>\n> Install **OpenMS 3.0.0**:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Grab OpenMS 3.0.0 [here](https://github.com/OpenMS/OpenMS/releases/tag/Release3.0.0).\n>>\n>>#### <span style=\"color: green\"> **For Linux(!) only** </span>\n>>Then add the binaries to your path (Linux):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n>>      source ~/.bashrc\n>>#### <span style=\"color: green\"> **For MacOS(!) only** </span>\n>>Then add the binaries to your path (MacOS) by opening one of these files in a text editor:\n>>\n>>      /etc/profile\n>>      ~/.bash_profile\n>>      ~/.bash_login (if .bash_profile does not exist)\n>>      ~/.profile (if .bash_login does not exist)\n>>and adding the path to the binaries at the very end (path-dependent):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n\n### Step 3: Configure workflow\nConfigure the workflow according to your metabolomics data and instrument method via editing the files in the `config/` folder. \n\n1. Adjust the `config.yaml` to: \n- Configure the workflow execution (write <span style=\"color: green\">TRUE</span>/<span style=\"color: red\">FALSE</span> if you want to run/skip a specific rules of the workflow)\n- Adjust the parameters in the configuration file for your dataset as explained in the commented section in the yaml file (e.g. positive/negative ionisation, etc.)\n\n2. Add all your files in the data/raw/ or data/mzML/ directory and generate the `dataset.tsv` table to specify the samples (filenames) that will be processed. \n\n    **Suggestion**: Use the Jupyter notebook [Create_dataset_tsv](./Create_dataset_tsv.ipynb) or simply run:\n    \n    python data_files.py\n\n- `config/dataset.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n#### If there are blanks/QC samples in the file list, then define them through the script.\n\n- `config/blanks.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n\n- `config/samples.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n\n### Step 4: Execute workflow\n\nActivate the conda environment:\n\n    mamba activate snakemake\n    \n\n#### Test the workflow with the example dataset\n    \nTest your configuration by performing a dry-run via\n\n    snakemake --use-conda -n\n\nExecute the workflow locally via\n\n    snakemake --use-conda --cores all\n\nSee the [Snakemake documentation](https://snakemake.readthedocs.io/en/stable/executable.html) for further details.\n\n### Step 5: Investigate results\n\nAll the results are in a .TSV format and can be opened simply with excel or using pandas dataframes. All the files under results/interim can be ignored and eventualy discarded.\n\n## Developer Notes\n\nAll the workflow outputs are silenced for performance enhancement through the flag `-no_progress` or  `>/dev/null` in each rule or `--quiet` for snakemake command (see Execute the workflow locally via) that one can remove. Nevertheless, the error outputs, if any, are written in the specified log files.\n\n### Config & Schemas\n\n* [Config & schemas](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html) define the input formatting and are important to generate `wildcards`. The idea of using `samples` and `units` came from [here](https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling).  \n\n### Rules\n\n* [Snakefile](workflow/Snakefile): the main entry of the pipeline which tells the final output to be generated and the rules being used\n* [common.smk](workflow/rules/common.smk): a rule that generates the variables used (sample names) & other helper scripts\n* [The main rules (*.smk)](workflow/rules/): the bash code that has been chopped into modular units, with defined input & output. Snakemake then chains this rules together to generate required jobs. This should be intuitive and makes things easier for adding / changing steps in the pipeline.\n\n### Environments\n\n* Conda environments are defined as .yaml file in `workflow/envs`\n* Note that not all dependencies are compatible/available as conda libraries. Once installed, the virtual environment are stored in `.snakemake/conda` with unique hashes. The ALE and pilon are example where environment needs to be modified / dependencies need to be installed.\n* It might be better to utilise containers / dockers and cloud execution for \"hard to install\" dependencies\n* Custom dependencies and databases are stored in the `resources/` folder.\n* Snakemake dependencies with conda packages is one of the drawbacks and why [Nextflow](https://www.nextflow.io/) might be more preferable. Nevertheless, the pythonic language of snakemake enables newcomers to learn and develop their own pipeline faster.\n\n### Test Data (only for testing the workflow with the example dataset)\n\n* Current test data are built from known metabolite producer strains or standard samples that have been analyzed with a Thermo Orbitrap IDX instrument. The presence of the metabolites and their fragmentation patterns has been manually confirmed using TOPPView.\n\n### Citations\n\nKontou, E.E., Walter, A., Alka, O. et al. UmetaFlow: an untargeted metabolomics workflow for high-throughput data processing and analysis. J Cheminform 15, 52 (2023). https://doi.org/10.1186/s13321-023-00724-w\n\nPfeuffer J, Sachsenberg T, Alka O, et al. OpenMS \u2013 A platform for reproducible analysis of mass spectrometry data. J Biotechnol. 2017;261:142-148. doi:10.1016/j.jbiotec.2017.05.016\n\nD\u00fchrkop K, Fleischauer M, Ludwig M, et al. SIRIUS 4: a rapid tool for turning tandem mass spectra into metabolite structure information. Nat Methods. 2019;16(4):299-302. doi:10.1038/s41592-019-0344-8\n\nD\u00fchrkop K, Shen H, Meusel M, Rousu J, B\u00f6cker S. Searching molecular structure databases with tandem mass spectra using CSI:FingerID. Proc Natl Acad Sci. 2015;112(41):12580-12585. doi:10.1073/pnas.1509788112\n\nNothias LF, Petras D, Schmid R, et al. Feature-based molecular networking in the GNPS analysis environment. Nat Methods. 2020;17(9):905-908. doi:10.1038/s41592-020-0933-6\n\nSchmid R, Petras D, Nothias LF, et al. Ion identity molecular networking for mass spectrometry-based metabolomics in the GNPS environment. Nat Commun. 2021;12(1):3832. doi:10.1038/s41467-021-23953-9\n\nM\u00f6lder F, Jablonski KP, Letcher B, et al. Sustainable data analysis with Snakemake. Published online January 18, 2021. doi:10.12688/f1000research.29032.1\n\nde Jonge, N.F., Louwen, J.J.R., Chekmeneva, E. et al. MS2Query: reliable and scalable MS2 mass spectra-based analogue search. Nat Commun 14, 1752 (2023). doi:10.1038/s41467-023-37446-4\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "<span style=\"color: green\"> **For Linux(!) only** </span>",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage",
          "Step 2: Install all dependencies"
        ],
        "type": "Text_excerpt",
        "value": "# UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems\n\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22657.14.0-brightgreen.svg)](https://snakemake.bitbucket.io)\n[![PEP compatible](https://pepkit.github.io/img/PEP-compatible-green.svg)](https://pep.databio.org)\n\nThis is the Snakemake implementation of the [pyOpenMS workflow](https://github.com/biosustain/pyOpenMS_UmetaFlow.git) tailored by [Eftychia Eva Kontou](https://github.com/eeko-kon) and [Axel Walter](https://github.com/axelwalter).\n\n## Workflow overview\n\nThe pipeline consists of seven interconnected steps:\n\n1) File conversion: Simply add your Thermo raw files under the directory data/raw/ and they will be converted to centroid mzML files. If you have Agilent, Bruker, or other vendor files, skip that step (write \"FALSE\" for rule fileconversion in the config.yaml file - see more under \"Configure workflow\"), convert them independently using [proteowizard](https://proteowizard.sourceforge.io/) and add them under the data/mzML/ directory.\n\n2) Pre-processing: converting raw data to a feature table with a series of algorithms through feature detection, alignment and grouping. This step includes an optional removal of blank/QC samples if defined by the user. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n3) Re-quantification (optional): Re-quantify all features with missing values across samples resulted from the pre-processing step for more reliable statistical analysis and data exploration. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n4) Structural and formula predictions (SIRIUS and CSI:FingeID) and annotation of the feature matrix with those predictions (MSI level 3).\n\n5) GNPSexport: generate all the files necessary to create a [FBMN](https://ccms-ucsd.github.io/GNPSDocumentation/featurebasedmolecularnetworking-with-openms/) or [IIMN](https://ccms-ucsd.github.io/GNPSDocumentation/fbmn-iin/#iimn-networks-with-collapsed-ion-identity-edges) job at GNPS. \n\n6) Spectral matching with in-house or a publicly available library (MGF/MSP/mzML format) and annotation of the feature matrix with matches that have a score above 60 (MSI level 2).\n\n7) After FBMN or IIMN: Integrate Sirius and CSI predictions to the network (GraphML) and MSMS spectral library annotations to the feature matrix- MSI level 2 (optional).\n\n8) MS2Query: add another annotation step with a machine learning tool, MS2Query, that searches for exact spectral matches, as well as analogues, using Spec2Vec and MS2Deepscore.\n\nSee [README](workflow/rules/README.md) file for details.\n### Overview\n![dag](/images/UmetaFlow_graph.svg)\n\n## Usage\n\n### Step 1: Clone the workflow\n\n[Clone](https://help.github.com/en/articles/cloning-a-repository) this repository to your local system, into the place where you want to perform the data analysis.\n   \n    git clone https://github.com/biosustain/snakemake_UmetaFlow.git\n\nMake sure to have the right access / SSH Key. If **not**, follow the steps:\n\nStep (i): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n\nStep (ii): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\n\n\n### Step 2: Install all dependencies\n\n> **Mamba** and **Snakemake** dependencies:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Install [mambaforge](https://github.com/conda-forge/miniforge#mambaforge) for any system. This step is optional if the user already has conda installed, then replace mamba with conda for the following commands.\n>>\n>>Then install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html) through mamba with:\n>>\n>>      conda create -c conda-forge -c bioconda -n snakemake snakemake python=3.10.8\n>>      \n>**SIRIUS**, **ThermoRawFileParser** executables, and **MS2Query** models:\n>>Download the latest SIRIUS executable compatible with your operating system (linux or macOS), the ThermoRawFileParser (file converter executable for Thermo .RAW files) and MS2Query models. Use the following script to complete this step:\n>>\n>>      cd snakemake_UmetaFlow\n>>      SCRIPT_VERSION=\"0.1.5\"\n>>      wget -O setup_scripts.zip https://github.com/NBChub/umetaflow_tutorial/archive/refs/tags/$SCRIPT_VERSION.zip\n>>      unzip setup_scripts.zip && mv umetaflow_tutorial-$SCRIPT_VERSION/* setup_scripts/\n>>\n>>The important arguments here are the **ion mode** of your data (\"positive\" or \"negative\") which will fetch the respective modules for MS2Query and the **operating system** (\"osx64\" for macOS and \"linux64\" for linux) which will fetch the latest release of the sirius executable for your operating system (defaults: positive mode, osx64). Run the script with or without arguments.\n>>\n>>      bash setup_scripts/setup.sh -o \"osx64\" -m \"positive\"\n>>\n> Install **OpenMS 3.0.0**:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Grab OpenMS 3.0.0 [here](https://github.com/OpenMS/OpenMS/releases/tag/Release3.0.0).\n>>\n>>#### <span style=\"color: green\"> **For Linux(!) only** </span>\n>>Then add the binaries to your path (Linux):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n>>      source ~/.bashrc\n>>#### <span style=\"color: green\"> **For MacOS(!) only** </span>\n>>Then add the binaries to your path (MacOS) by opening one of these files in a text editor:\n>>\n>>      /etc/profile\n>>      ~/.bash_profile\n>>      ~/.bash_login (if .bash_profile does not exist)\n>>      ~/.profile (if .bash_login does not exist)\n>>and adding the path to the binaries at the very end (path-dependent):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n\n### Step 3: Configure workflow\nConfigure the workflow according to your metabolomics data and instrument method via editing the files in the `config/` folder. \n\n1. Adjust the `config.yaml` to: \n- Configure the workflow execution (write <span style=\"color: green\">TRUE</span>/<span style=\"color: red\">FALSE</span> if you want to run/skip a specific rules of the workflow)\n- Adjust the parameters in the configuration file for your dataset as explained in the commented section in the yaml file (e.g. positive/negative ionisation, etc.)\n\n2. Add all your files in the data/raw/ or data/mzML/ directory and generate the `dataset.tsv` table to specify the samples (filenames) that will be processed. \n\n    **Suggestion**: Use the Jupyter notebook [Create_dataset_tsv](./Create_dataset_tsv.ipynb) or simply run:\n    \n    python data_files.py\n\n- `config/dataset.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n#### If there are blanks/QC samples in the file list, then define them through the script.\n\n- `config/blanks.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n\n- `config/samples.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n\n### Step 4: Execute workflow\n\nActivate the conda environment:\n\n    mamba activate snakemake\n    \n\n#### Test the workflow with the example dataset\n    \nTest your configuration by performing a dry-run via\n\n    snakemake --use-conda -n\n\nExecute the workflow locally via\n\n    snakemake --use-conda --cores all\n\nSee the [Snakemake documentation](https://snakemake.readthedocs.io/en/stable/executable.html) for further details.\n\n### Step 5: Investigate results\n\nAll the results are in a .TSV format and can be opened simply with excel or using pandas dataframes. All the files under results/interim can be ignored and eventualy discarded.\n\n## Developer Notes\n\nAll the workflow outputs are silenced for performance enhancement through the flag `-no_progress` or  `>/dev/null` in each rule or `--quiet` for snakemake command (see Execute the workflow locally via) that one can remove. Nevertheless, the error outputs, if any, are written in the specified log files.\n\n### Config & Schemas\n\n* [Config & schemas](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html) define the input formatting and are important to generate `wildcards`. The idea of using `samples` and `units` came from [here](https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling).  \n\n### Rules\n\n* [Snakefile](workflow/Snakefile): the main entry of the pipeline which tells the final output to be generated and the rules being used\n* [common.smk](workflow/rules/common.smk): a rule that generates the variables used (sample names) & other helper scripts\n* [The main rules (*.smk)](workflow/rules/): the bash code that has been chopped into modular units, with defined input & output. Snakemake then chains this rules together to generate required jobs. This should be intuitive and makes things easier for adding / changing steps in the pipeline.\n\n### Environments\n\n* Conda environments are defined as .yaml file in `workflow/envs`\n* Note that not all dependencies are compatible/available as conda libraries. Once installed, the virtual environment are stored in `.snakemake/conda` with unique hashes. The ALE and pilon are example where environment needs to be modified / dependencies need to be installed.\n* It might be better to utilise containers / dockers and cloud execution for \"hard to install\" dependencies\n* Custom dependencies and databases are stored in the `resources/` folder.\n* Snakemake dependencies with conda packages is one of the drawbacks and why [Nextflow](https://www.nextflow.io/) might be more preferable. Nevertheless, the pythonic language of snakemake enables newcomers to learn and develop their own pipeline faster.\n\n### Test Data (only for testing the workflow with the example dataset)\n\n* Current test data are built from known metabolite producer strains or standard samples that have been analyzed with a Thermo Orbitrap IDX instrument. The presence of the metabolites and their fragmentation patterns has been manually confirmed using TOPPView.\n\n### Citations\n\nKontou, E.E., Walter, A., Alka, O. et al. UmetaFlow: an untargeted metabolomics workflow for high-throughput data processing and analysis. J Cheminform 15, 52 (2023). https://doi.org/10.1186/s13321-023-00724-w\n\nPfeuffer J, Sachsenberg T, Alka O, et al. OpenMS \u2013 A platform for reproducible analysis of mass spectrometry data. J Biotechnol. 2017;261:142-148. doi:10.1016/j.jbiotec.2017.05.016\n\nD\u00fchrkop K, Fleischauer M, Ludwig M, et al. SIRIUS 4: a rapid tool for turning tandem mass spectra into metabolite structure information. Nat Methods. 2019;16(4):299-302. doi:10.1038/s41592-019-0344-8\n\nD\u00fchrkop K, Shen H, Meusel M, Rousu J, B\u00f6cker S. Searching molecular structure databases with tandem mass spectra using CSI:FingerID. Proc Natl Acad Sci. 2015;112(41):12580-12585. doi:10.1073/pnas.1509788112\n\nNothias LF, Petras D, Schmid R, et al. Feature-based molecular networking in the GNPS analysis environment. Nat Methods. 2020;17(9):905-908. doi:10.1038/s41592-020-0933-6\n\nSchmid R, Petras D, Nothias LF, et al. Ion identity molecular networking for mass spectrometry-based metabolomics in the GNPS environment. Nat Commun. 2021;12(1):3832. doi:10.1038/s41467-021-23953-9\n\nM\u00f6lder F, Jablonski KP, Letcher B, et al. Sustainable data analysis with Snakemake. Published online January 18, 2021. doi:10.12688/f1000research.29032.1\n\nde Jonge, N.F., Louwen, J.J.R., Chekmeneva, E. et al. MS2Query: reliable and scalable MS2 mass spectra-based analogue search. Nat Commun 14, 1752 (2023). doi:10.1038/s41467-023-37446-4\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "<span style=\"color: green\"> **For MacOS(!) only** </span>",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage",
          "Step 2: Install all dependencies"
        ],
        "type": "Text_excerpt",
        "value": "# UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems\n\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22657.14.0-brightgreen.svg)](https://snakemake.bitbucket.io)\n[![PEP compatible](https://pepkit.github.io/img/PEP-compatible-green.svg)](https://pep.databio.org)\n\nThis is the Snakemake implementation of the [pyOpenMS workflow](https://github.com/biosustain/pyOpenMS_UmetaFlow.git) tailored by [Eftychia Eva Kontou](https://github.com/eeko-kon) and [Axel Walter](https://github.com/axelwalter).\n\n## Workflow overview\n\nThe pipeline consists of seven interconnected steps:\n\n1) File conversion: Simply add your Thermo raw files under the directory data/raw/ and they will be converted to centroid mzML files. If you have Agilent, Bruker, or other vendor files, skip that step (write \"FALSE\" for rule fileconversion in the config.yaml file - see more under \"Configure workflow\"), convert them independently using [proteowizard](https://proteowizard.sourceforge.io/) and add them under the data/mzML/ directory.\n\n2) Pre-processing: converting raw data to a feature table with a series of algorithms through feature detection, alignment and grouping. This step includes an optional removal of blank/QC samples if defined by the user. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n3) Re-quantification (optional): Re-quantify all features with missing values across samples resulted from the pre-processing step for more reliable statistical analysis and data exploration. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n4) Structural and formula predictions (SIRIUS and CSI:FingeID) and annotation of the feature matrix with those predictions (MSI level 3).\n\n5) GNPSexport: generate all the files necessary to create a [FBMN](https://ccms-ucsd.github.io/GNPSDocumentation/featurebasedmolecularnetworking-with-openms/) or [IIMN](https://ccms-ucsd.github.io/GNPSDocumentation/fbmn-iin/#iimn-networks-with-collapsed-ion-identity-edges) job at GNPS. \n\n6) Spectral matching with in-house or a publicly available library (MGF/MSP/mzML format) and annotation of the feature matrix with matches that have a score above 60 (MSI level 2).\n\n7) After FBMN or IIMN: Integrate Sirius and CSI predictions to the network (GraphML) and MSMS spectral library annotations to the feature matrix- MSI level 2 (optional).\n\n8) MS2Query: add another annotation step with a machine learning tool, MS2Query, that searches for exact spectral matches, as well as analogues, using Spec2Vec and MS2Deepscore.\n\nSee [README](workflow/rules/README.md) file for details.\n### Overview\n![dag](/images/UmetaFlow_graph.svg)\n\n## Usage\n\n### Step 1: Clone the workflow\n\n[Clone](https://help.github.com/en/articles/cloning-a-repository) this repository to your local system, into the place where you want to perform the data analysis.\n   \n    git clone https://github.com/biosustain/snakemake_UmetaFlow.git\n\nMake sure to have the right access / SSH Key. If **not**, follow the steps:\n\nStep (i): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n\nStep (ii): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\n\n\n### Step 2: Install all dependencies\n\n> **Mamba** and **Snakemake** dependencies:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Install [mambaforge](https://github.com/conda-forge/miniforge#mambaforge) for any system. This step is optional if the user already has conda installed, then replace mamba with conda for the following commands.\n>>\n>>Then install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html) through mamba with:\n>>\n>>      conda create -c conda-forge -c bioconda -n snakemake snakemake python=3.10.8\n>>      \n>**SIRIUS**, **ThermoRawFileParser** executables, and **MS2Query** models:\n>>Download the latest SIRIUS executable compatible with your operating system (linux or macOS), the ThermoRawFileParser (file converter executable for Thermo .RAW files) and MS2Query models. Use the following script to complete this step:\n>>\n>>      cd snakemake_UmetaFlow\n>>      SCRIPT_VERSION=\"0.1.5\"\n>>      wget -O setup_scripts.zip https://github.com/NBChub/umetaflow_tutorial/archive/refs/tags/$SCRIPT_VERSION.zip\n>>      unzip setup_scripts.zip && mv umetaflow_tutorial-$SCRIPT_VERSION/* setup_scripts/\n>>\n>>The important arguments here are the **ion mode** of your data (\"positive\" or \"negative\") which will fetch the respective modules for MS2Query and the **operating system** (\"osx64\" for macOS and \"linux64\" for linux) which will fetch the latest release of the sirius executable for your operating system (defaults: positive mode, osx64). Run the script with or without arguments.\n>>\n>>      bash setup_scripts/setup.sh -o \"osx64\" -m \"positive\"\n>>\n> Install **OpenMS 3.0.0**:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Grab OpenMS 3.0.0 [here](https://github.com/OpenMS/OpenMS/releases/tag/Release3.0.0).\n>>\n>>#### <span style=\"color: green\"> **For Linux(!) only** </span>\n>>Then add the binaries to your path (Linux):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n>>      source ~/.bashrc\n>>#### <span style=\"color: green\"> **For MacOS(!) only** </span>\n>>Then add the binaries to your path (MacOS) by opening one of these files in a text editor:\n>>\n>>      /etc/profile\n>>      ~/.bash_profile\n>>      ~/.bash_login (if .bash_profile does not exist)\n>>      ~/.profile (if .bash_login does not exist)\n>>and adding the path to the binaries at the very end (path-dependent):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "run": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 4: Execute workflow",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "Activate the conda environment:\n\n    mamba activate snakemake\n    \n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "download",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "has_build_file"
  ],
  "somef_provenance": {
    "date": "2024-10-04 01:13:59",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 24
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 1: Clone the workflow",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "[Clone](https://help.github.com/en/articles/cloning-a-repository) this repository to your local system, into the place where you want to perform the data analysis.\n   \n    git clone https://github.com/biosustain/snakemake_UmetaFlow.git\n\nMake sure to have the right access / SSH Key. If **not**, follow the steps:\n\nStep (i): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n\nStep (ii): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\n\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "<span style=\"color: green\"> **For both systems** </span>",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage",
          "Step 2: Install all dependencies"
        ],
        "type": "Text_excerpt",
        "value": "# UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems\n\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22657.14.0-brightgreen.svg)](https://snakemake.bitbucket.io)\n[![PEP compatible](https://pepkit.github.io/img/PEP-compatible-green.svg)](https://pep.databio.org)\n\nThis is the Snakemake implementation of the [pyOpenMS workflow](https://github.com/biosustain/pyOpenMS_UmetaFlow.git) tailored by [Eftychia Eva Kontou](https://github.com/eeko-kon) and [Axel Walter](https://github.com/axelwalter).\n\n## Workflow overview\n\nThe pipeline consists of seven interconnected steps:\n\n1) File conversion: Simply add your Thermo raw files under the directory data/raw/ and they will be converted to centroid mzML files. If you have Agilent, Bruker, or other vendor files, skip that step (write \"FALSE\" for rule fileconversion in the config.yaml file - see more under \"Configure workflow\"), convert them independently using [proteowizard](https://proteowizard.sourceforge.io/) and add them under the data/mzML/ directory.\n\n2) Pre-processing: converting raw data to a feature table with a series of algorithms through feature detection, alignment and grouping. This step includes an optional removal of blank/QC samples if defined by the user. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n3) Re-quantification (optional): Re-quantify all features with missing values across samples resulted from the pre-processing step for more reliable statistical analysis and data exploration. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n4) Structural and formula predictions (SIRIUS and CSI:FingeID) and annotation of the feature matrix with those predictions (MSI level 3).\n\n5) GNPSexport: generate all the files necessary to create a [FBMN](https://ccms-ucsd.github.io/GNPSDocumentation/featurebasedmolecularnetworking-with-openms/) or [IIMN](https://ccms-ucsd.github.io/GNPSDocumentation/fbmn-iin/#iimn-networks-with-collapsed-ion-identity-edges) job at GNPS. \n\n6) Spectral matching with in-house or a publicly available library (MGF/MSP/mzML format) and annotation of the feature matrix with matches that have a score above 60 (MSI level 2).\n\n7) After FBMN or IIMN: Integrate Sirius and CSI predictions to the network (GraphML) and MSMS spectral library annotations to the feature matrix- MSI level 2 (optional).\n\n8) MS2Query: add another annotation step with a machine learning tool, MS2Query, that searches for exact spectral matches, as well as analogues, using Spec2Vec and MS2Deepscore.\n\nSee [README](workflow/rules/README.md) file for details.\n### Overview\n![dag](/images/UmetaFlow_graph.svg)\n\n## Usage\n\n### Step 1: Clone the workflow\n\n[Clone](https://help.github.com/en/articles/cloning-a-repository) this repository to your local system, into the place where you want to perform the data analysis.\n   \n    git clone https://github.com/biosustain/snakemake_UmetaFlow.git\n\nMake sure to have the right access / SSH Key. If **not**, follow the steps:\n\nStep (i): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n\nStep (ii): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\n\n\n### Step 2: Install all dependencies\n\n> **Mamba** and **Snakemake** dependencies:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Install [mambaforge](https://github.com/conda-forge/miniforge#mambaforge) for any system. This step is optional if the user already has conda installed, then replace mamba with conda for the following commands.\n>>\n>>Then install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html) through mamba with:\n>>\n>>      conda create -c conda-forge -c bioconda -n snakemake snakemake python=3.10.8\n>>      \n>**SIRIUS**, **ThermoRawFileParser** executables, and **MS2Query** models:\n>>Download the latest SIRIUS executable compatible with your operating system (linux or macOS), the ThermoRawFileParser (file converter executable for Thermo .RAW files) and MS2Query models. Use the following script to complete this step:\n>>\n>>      cd snakemake_UmetaFlow\n>>      SCRIPT_VERSION=\"0.1.5\"\n>>      wget -O setup_scripts.zip https://github.com/NBChub/umetaflow_tutorial/archive/refs/tags/$SCRIPT_VERSION.zip\n>>      unzip setup_scripts.zip && mv umetaflow_tutorial-$SCRIPT_VERSION/* setup_scripts/\n>>\n>>The important arguments here are the **ion mode** of your data (\"positive\" or \"negative\") which will fetch the respective modules for MS2Query and the **operating system** (\"osx64\" for macOS and \"linux64\" for linux) which will fetch the latest release of the sirius executable for your operating system (defaults: positive mode, osx64). Run the script with or without arguments.\n>>\n>>      bash setup_scripts/setup.sh -o \"osx64\" -m \"positive\"\n>>\n> Install **OpenMS 3.0.0**:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Grab OpenMS 3.0.0 [here](https://github.com/OpenMS/OpenMS/releases/tag/Release3.0.0).\n>>\n>>#### <span style=\"color: green\"> **For Linux(!) only** </span>\n>>Then add the binaries to your path (Linux):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n>>      source ~/.bashrc\n>>#### <span style=\"color: green\"> **For MacOS(!) only** </span>\n>>Then add the binaries to your path (MacOS) by opening one of these files in a text editor:\n>>\n>>      /etc/profile\n>>      ~/.bash_profile\n>>      ~/.bash_login (if .bash_profile does not exist)\n>>      ~/.profile (if .bash_login does not exist)\n>>and adding the path to the binaries at the very end (path-dependent):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n\n### Step 3: Configure workflow\nConfigure the workflow according to your metabolomics data and instrument method via editing the files in the `config/` folder. \n\n1. Adjust the `config.yaml` to: \n- Configure the workflow execution (write <span style=\"color: green\">TRUE</span>/<span style=\"color: red\">FALSE</span> if you want to run/skip a specific rules of the workflow)\n- Adjust the parameters in the configuration file for your dataset as explained in the commented section in the yaml file (e.g. positive/negative ionisation, etc.)\n\n2. Add all your files in the data/raw/ or data/mzML/ directory and generate the `dataset.tsv` table to specify the samples (filenames) that will be processed. \n\n    **Suggestion**: Use the Jupyter notebook [Create_dataset_tsv](./Create_dataset_tsv.ipynb) or simply run:\n    \n    python data_files.py\n\n- `config/dataset.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n#### If there are blanks/QC samples in the file list, then define them through the script.\n\n- `config/blanks.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n\n- `config/samples.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n\n### Step 4: Execute workflow\n\nActivate the conda environment:\n\n    mamba activate snakemake\n    \n\n#### Test the workflow with the example dataset\n    \nTest your configuration by performing a dry-run via\n\n    snakemake --use-conda -n\n\nExecute the workflow locally via\n\n    snakemake --use-conda --cores all\n\nSee the [Snakemake documentation](https://snakemake.readthedocs.io/en/stable/executable.html) for further details.\n\n### Step 5: Investigate results\n\nAll the results are in a .TSV format and can be opened simply with excel or using pandas dataframes. All the files under results/interim can be ignored and eventualy discarded.\n\n## Developer Notes\n\nAll the workflow outputs are silenced for performance enhancement through the flag `-no_progress` or  `>/dev/null` in each rule or `--quiet` for snakemake command (see Execute the workflow locally via) that one can remove. Nevertheless, the error outputs, if any, are written in the specified log files.\n\n### Config & Schemas\n\n* [Config & schemas](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html) define the input formatting and are important to generate `wildcards`. The idea of using `samples` and `units` came from [here](https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling).  \n\n### Rules\n\n* [Snakefile](workflow/Snakefile): the main entry of the pipeline which tells the final output to be generated and the rules being used\n* [common.smk](workflow/rules/common.smk): a rule that generates the variables used (sample names) & other helper scripts\n* [The main rules (*.smk)](workflow/rules/): the bash code that has been chopped into modular units, with defined input & output. Snakemake then chains this rules together to generate required jobs. This should be intuitive and makes things easier for adding / changing steps in the pipeline.\n\n### Environments\n\n* Conda environments are defined as .yaml file in `workflow/envs`\n* Note that not all dependencies are compatible/available as conda libraries. Once installed, the virtual environment are stored in `.snakemake/conda` with unique hashes. The ALE and pilon are example where environment needs to be modified / dependencies need to be installed.\n* It might be better to utilise containers / dockers and cloud execution for \"hard to install\" dependencies\n* Custom dependencies and databases are stored in the `resources/` folder.\n* Snakemake dependencies with conda packages is one of the drawbacks and why [Nextflow](https://www.nextflow.io/) might be more preferable. Nevertheless, the pythonic language of snakemake enables newcomers to learn and develop their own pipeline faster.\n\n### Test Data (only for testing the workflow with the example dataset)\n\n* Current test data are built from known metabolite producer strains or standard samples that have been analyzed with a Thermo Orbitrap IDX instrument. The presence of the metabolites and their fragmentation patterns has been manually confirmed using TOPPView.\n\n### Citations\n\nKontou, E.E., Walter, A., Alka, O. et al. UmetaFlow: an untargeted metabolomics workflow for high-throughput data processing and analysis. J Cheminform 15, 52 (2023). https://doi.org/10.1186/s13321-023-00724-w\n\nPfeuffer J, Sachsenberg T, Alka O, et al. OpenMS \u2013 A platform for reproducible analysis of mass spectrometry data. J Biotechnol. 2017;261:142-148. doi:10.1016/j.jbiotec.2017.05.016\n\nD\u00fchrkop K, Fleischauer M, Ludwig M, et al. SIRIUS 4: a rapid tool for turning tandem mass spectra into metabolite structure information. Nat Methods. 2019;16(4):299-302. doi:10.1038/s41592-019-0344-8\n\nD\u00fchrkop K, Shen H, Meusel M, Rousu J, B\u00f6cker S. Searching molecular structure databases with tandem mass spectra using CSI:FingerID. Proc Natl Acad Sci. 2015;112(41):12580-12585. doi:10.1073/pnas.1509788112\n\nNothias LF, Petras D, Schmid R, et al. Feature-based molecular networking in the GNPS analysis environment. Nat Methods. 2020;17(9):905-908. doi:10.1038/s41592-020-0933-6\n\nSchmid R, Petras D, Nothias LF, et al. Ion identity molecular networking for mass spectrometry-based metabolomics in the GNPS environment. Nat Commun. 2021;12(1):3832. doi:10.1038/s41467-021-23953-9\n\nM\u00f6lder F, Jablonski KP, Letcher B, et al. Sustainable data analysis with Snakemake. Published online January 18, 2021. doi:10.12688/f1000research.29032.1\n\nde Jonge, N.F., Louwen, J.J.R., Chekmeneva, E. et al. MS2Query: reliable and scalable MS2 mass spectra-based analogue search. Nat Commun 14, 1752 (2023). doi:10.1038/s41467-023-37446-4\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "<span style=\"color: green\"> **For Linux(!) only** </span>",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage",
          "Step 2: Install all dependencies"
        ],
        "type": "Text_excerpt",
        "value": "# UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems\n\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22657.14.0-brightgreen.svg)](https://snakemake.bitbucket.io)\n[![PEP compatible](https://pepkit.github.io/img/PEP-compatible-green.svg)](https://pep.databio.org)\n\nThis is the Snakemake implementation of the [pyOpenMS workflow](https://github.com/biosustain/pyOpenMS_UmetaFlow.git) tailored by [Eftychia Eva Kontou](https://github.com/eeko-kon) and [Axel Walter](https://github.com/axelwalter).\n\n## Workflow overview\n\nThe pipeline consists of seven interconnected steps:\n\n1) File conversion: Simply add your Thermo raw files under the directory data/raw/ and they will be converted to centroid mzML files. If you have Agilent, Bruker, or other vendor files, skip that step (write \"FALSE\" for rule fileconversion in the config.yaml file - see more under \"Configure workflow\"), convert them independently using [proteowizard](https://proteowizard.sourceforge.io/) and add them under the data/mzML/ directory.\n\n2) Pre-processing: converting raw data to a feature table with a series of algorithms through feature detection, alignment and grouping. This step includes an optional removal of blank/QC samples if defined by the user. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n3) Re-quantification (optional): Re-quantify all features with missing values across samples resulted from the pre-processing step for more reliable statistical analysis and data exploration. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n4) Structural and formula predictions (SIRIUS and CSI:FingeID) and annotation of the feature matrix with those predictions (MSI level 3).\n\n5) GNPSexport: generate all the files necessary to create a [FBMN](https://ccms-ucsd.github.io/GNPSDocumentation/featurebasedmolecularnetworking-with-openms/) or [IIMN](https://ccms-ucsd.github.io/GNPSDocumentation/fbmn-iin/#iimn-networks-with-collapsed-ion-identity-edges) job at GNPS. \n\n6) Spectral matching with in-house or a publicly available library (MGF/MSP/mzML format) and annotation of the feature matrix with matches that have a score above 60 (MSI level 2).\n\n7) After FBMN or IIMN: Integrate Sirius and CSI predictions to the network (GraphML) and MSMS spectral library annotations to the feature matrix- MSI level 2 (optional).\n\n8) MS2Query: add another annotation step with a machine learning tool, MS2Query, that searches for exact spectral matches, as well as analogues, using Spec2Vec and MS2Deepscore.\n\nSee [README](workflow/rules/README.md) file for details.\n### Overview\n![dag](/images/UmetaFlow_graph.svg)\n\n## Usage\n\n### Step 1: Clone the workflow\n\n[Clone](https://help.github.com/en/articles/cloning-a-repository) this repository to your local system, into the place where you want to perform the data analysis.\n   \n    git clone https://github.com/biosustain/snakemake_UmetaFlow.git\n\nMake sure to have the right access / SSH Key. If **not**, follow the steps:\n\nStep (i): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n\nStep (ii): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\n\n\n### Step 2: Install all dependencies\n\n> **Mamba** and **Snakemake** dependencies:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Install [mambaforge](https://github.com/conda-forge/miniforge#mambaforge) for any system. This step is optional if the user already has conda installed, then replace mamba with conda for the following commands.\n>>\n>>Then install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html) through mamba with:\n>>\n>>      conda create -c conda-forge -c bioconda -n snakemake snakemake python=3.10.8\n>>      \n>**SIRIUS**, **ThermoRawFileParser** executables, and **MS2Query** models:\n>>Download the latest SIRIUS executable compatible with your operating system (linux or macOS), the ThermoRawFileParser (file converter executable for Thermo .RAW files) and MS2Query models. Use the following script to complete this step:\n>>\n>>      cd snakemake_UmetaFlow\n>>      SCRIPT_VERSION=\"0.1.5\"\n>>      wget -O setup_scripts.zip https://github.com/NBChub/umetaflow_tutorial/archive/refs/tags/$SCRIPT_VERSION.zip\n>>      unzip setup_scripts.zip && mv umetaflow_tutorial-$SCRIPT_VERSION/* setup_scripts/\n>>\n>>The important arguments here are the **ion mode** of your data (\"positive\" or \"negative\") which will fetch the respective modules for MS2Query and the **operating system** (\"osx64\" for macOS and \"linux64\" for linux) which will fetch the latest release of the sirius executable for your operating system (defaults: positive mode, osx64). Run the script with or without arguments.\n>>\n>>      bash setup_scripts/setup.sh -o \"osx64\" -m \"positive\"\n>>\n> Install **OpenMS 3.0.0**:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Grab OpenMS 3.0.0 [here](https://github.com/OpenMS/OpenMS/releases/tag/Release3.0.0).\n>>\n>>#### <span style=\"color: green\"> **For Linux(!) only** </span>\n>>Then add the binaries to your path (Linux):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n>>      source ~/.bashrc\n>>#### <span style=\"color: green\"> **For MacOS(!) only** </span>\n>>Then add the binaries to your path (MacOS) by opening one of these files in a text editor:\n>>\n>>      /etc/profile\n>>      ~/.bash_profile\n>>      ~/.bash_login (if .bash_profile does not exist)\n>>      ~/.profile (if .bash_login does not exist)\n>>and adding the path to the binaries at the very end (path-dependent):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n\n### Step 3: Configure workflow\nConfigure the workflow according to your metabolomics data and instrument method via editing the files in the `config/` folder. \n\n1. Adjust the `config.yaml` to: \n- Configure the workflow execution (write <span style=\"color: green\">TRUE</span>/<span style=\"color: red\">FALSE</span> if you want to run/skip a specific rules of the workflow)\n- Adjust the parameters in the configuration file for your dataset as explained in the commented section in the yaml file (e.g. positive/negative ionisation, etc.)\n\n2. Add all your files in the data/raw/ or data/mzML/ directory and generate the `dataset.tsv` table to specify the samples (filenames) that will be processed. \n\n    **Suggestion**: Use the Jupyter notebook [Create_dataset_tsv](./Create_dataset_tsv.ipynb) or simply run:\n    \n    python data_files.py\n\n- `config/dataset.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n#### If there are blanks/QC samples in the file list, then define them through the script.\n\n- `config/blanks.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n\n- `config/samples.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n\n### Step 4: Execute workflow\n\nActivate the conda environment:\n\n    mamba activate snakemake\n    \n\n#### Test the workflow with the example dataset\n    \nTest your configuration by performing a dry-run via\n\n    snakemake --use-conda -n\n\nExecute the workflow locally via\n\n    snakemake --use-conda --cores all\n\nSee the [Snakemake documentation](https://snakemake.readthedocs.io/en/stable/executable.html) for further details.\n\n### Step 5: Investigate results\n\nAll the results are in a .TSV format and can be opened simply with excel or using pandas dataframes. All the files under results/interim can be ignored and eventualy discarded.\n\n## Developer Notes\n\nAll the workflow outputs are silenced for performance enhancement through the flag `-no_progress` or  `>/dev/null` in each rule or `--quiet` for snakemake command (see Execute the workflow locally via) that one can remove. Nevertheless, the error outputs, if any, are written in the specified log files.\n\n### Config & Schemas\n\n* [Config & schemas](https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html) define the input formatting and are important to generate `wildcards`. The idea of using `samples` and `units` came from [here](https://github.com/snakemake-workflows/dna-seq-gatk-variant-calling).  \n\n### Rules\n\n* [Snakefile](workflow/Snakefile): the main entry of the pipeline which tells the final output to be generated and the rules being used\n* [common.smk](workflow/rules/common.smk): a rule that generates the variables used (sample names) & other helper scripts\n* [The main rules (*.smk)](workflow/rules/): the bash code that has been chopped into modular units, with defined input & output. Snakemake then chains this rules together to generate required jobs. This should be intuitive and makes things easier for adding / changing steps in the pipeline.\n\n### Environments\n\n* Conda environments are defined as .yaml file in `workflow/envs`\n* Note that not all dependencies are compatible/available as conda libraries. Once installed, the virtual environment are stored in `.snakemake/conda` with unique hashes. The ALE and pilon are example where environment needs to be modified / dependencies need to be installed.\n* It might be better to utilise containers / dockers and cloud execution for \"hard to install\" dependencies\n* Custom dependencies and databases are stored in the `resources/` folder.\n* Snakemake dependencies with conda packages is one of the drawbacks and why [Nextflow](https://www.nextflow.io/) might be more preferable. Nevertheless, the pythonic language of snakemake enables newcomers to learn and develop their own pipeline faster.\n\n### Test Data (only for testing the workflow with the example dataset)\n\n* Current test data are built from known metabolite producer strains or standard samples that have been analyzed with a Thermo Orbitrap IDX instrument. The presence of the metabolites and their fragmentation patterns has been manually confirmed using TOPPView.\n\n### Citations\n\nKontou, E.E., Walter, A., Alka, O. et al. UmetaFlow: an untargeted metabolomics workflow for high-throughput data processing and analysis. J Cheminform 15, 52 (2023). https://doi.org/10.1186/s13321-023-00724-w\n\nPfeuffer J, Sachsenberg T, Alka O, et al. OpenMS \u2013 A platform for reproducible analysis of mass spectrometry data. J Biotechnol. 2017;261:142-148. doi:10.1016/j.jbiotec.2017.05.016\n\nD\u00fchrkop K, Fleischauer M, Ludwig M, et al. SIRIUS 4: a rapid tool for turning tandem mass spectra into metabolite structure information. Nat Methods. 2019;16(4):299-302. doi:10.1038/s41592-019-0344-8\n\nD\u00fchrkop K, Shen H, Meusel M, Rousu J, B\u00f6cker S. Searching molecular structure databases with tandem mass spectra using CSI:FingerID. Proc Natl Acad Sci. 2015;112(41):12580-12585. doi:10.1073/pnas.1509788112\n\nNothias LF, Petras D, Schmid R, et al. Feature-based molecular networking in the GNPS analysis environment. Nat Methods. 2020;17(9):905-908. doi:10.1038/s41592-020-0933-6\n\nSchmid R, Petras D, Nothias LF, et al. Ion identity molecular networking for mass spectrometry-based metabolomics in the GNPS environment. Nat Commun. 2021;12(1):3832. doi:10.1038/s41467-021-23953-9\n\nM\u00f6lder F, Jablonski KP, Letcher B, et al. Sustainable data analysis with Snakemake. Published online January 18, 2021. doi:10.12688/f1000research.29032.1\n\nde Jonge, N.F., Louwen, J.J.R., Chekmeneva, E. et al. MS2Query: reliable and scalable MS2 mass spectra-based analogue search. Nat Commun 14, 1752 (2023). doi:10.1038/s41467-023-37446-4\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "<span style=\"color: green\"> **For MacOS(!) only** </span>",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage",
          "Step 2: Install all dependencies"
        ],
        "type": "Text_excerpt",
        "value": "# UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems\n\n[![Snakemake](https://img.shields.io/badge/snakemake-\u22657.14.0-brightgreen.svg)](https://snakemake.bitbucket.io)\n[![PEP compatible](https://pepkit.github.io/img/PEP-compatible-green.svg)](https://pep.databio.org)\n\nThis is the Snakemake implementation of the [pyOpenMS workflow](https://github.com/biosustain/pyOpenMS_UmetaFlow.git) tailored by [Eftychia Eva Kontou](https://github.com/eeko-kon) and [Axel Walter](https://github.com/axelwalter).\n\n## Workflow overview\n\nThe pipeline consists of seven interconnected steps:\n\n1) File conversion: Simply add your Thermo raw files under the directory data/raw/ and they will be converted to centroid mzML files. If you have Agilent, Bruker, or other vendor files, skip that step (write \"FALSE\" for rule fileconversion in the config.yaml file - see more under \"Configure workflow\"), convert them independently using [proteowizard](https://proteowizard.sourceforge.io/) and add them under the data/mzML/ directory.\n\n2) Pre-processing: converting raw data to a feature table with a series of algorithms through feature detection, alignment and grouping. This step includes an optional removal of blank/QC samples if defined by the user. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n3) Re-quantification (optional): Re-quantify all features with missing values across samples resulted from the pre-processing step for more reliable statistical analysis and data exploration. Optional \"minfrac\" step here allows for removal of consensus features with too many missing values.\n\n4) Structural and formula predictions (SIRIUS and CSI:FingeID) and annotation of the feature matrix with those predictions (MSI level 3).\n\n5) GNPSexport: generate all the files necessary to create a [FBMN](https://ccms-ucsd.github.io/GNPSDocumentation/featurebasedmolecularnetworking-with-openms/) or [IIMN](https://ccms-ucsd.github.io/GNPSDocumentation/fbmn-iin/#iimn-networks-with-collapsed-ion-identity-edges) job at GNPS. \n\n6) Spectral matching with in-house or a publicly available library (MGF/MSP/mzML format) and annotation of the feature matrix with matches that have a score above 60 (MSI level 2).\n\n7) After FBMN or IIMN: Integrate Sirius and CSI predictions to the network (GraphML) and MSMS spectral library annotations to the feature matrix- MSI level 2 (optional).\n\n8) MS2Query: add another annotation step with a machine learning tool, MS2Query, that searches for exact spectral matches, as well as analogues, using Spec2Vec and MS2Deepscore.\n\nSee [README](workflow/rules/README.md) file for details.\n### Overview\n![dag](/images/UmetaFlow_graph.svg)\n\n## Usage\n\n### Step 1: Clone the workflow\n\n[Clone](https://help.github.com/en/articles/cloning-a-repository) this repository to your local system, into the place where you want to perform the data analysis.\n   \n    git clone https://github.com/biosustain/snakemake_UmetaFlow.git\n\nMake sure to have the right access / SSH Key. If **not**, follow the steps:\n\nStep (i): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\n\nStep (ii): https://docs.github.com/en/github/authenticating-to-github/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account\n\n\n### Step 2: Install all dependencies\n\n> **Mamba** and **Snakemake** dependencies:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Install [mambaforge](https://github.com/conda-forge/miniforge#mambaforge) for any system. This step is optional if the user already has conda installed, then replace mamba with conda for the following commands.\n>>\n>>Then install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html) through mamba with:\n>>\n>>      conda create -c conda-forge -c bioconda -n snakemake snakemake python=3.10.8\n>>      \n>**SIRIUS**, **ThermoRawFileParser** executables, and **MS2Query** models:\n>>Download the latest SIRIUS executable compatible with your operating system (linux or macOS), the ThermoRawFileParser (file converter executable for Thermo .RAW files) and MS2Query models. Use the following script to complete this step:\n>>\n>>      cd snakemake_UmetaFlow\n>>      SCRIPT_VERSION=\"0.1.5\"\n>>      wget -O setup_scripts.zip https://github.com/NBChub/umetaflow_tutorial/archive/refs/tags/$SCRIPT_VERSION.zip\n>>      unzip setup_scripts.zip && mv umetaflow_tutorial-$SCRIPT_VERSION/* setup_scripts/\n>>\n>>The important arguments here are the **ion mode** of your data (\"positive\" or \"negative\") which will fetch the respective modules for MS2Query and the **operating system** (\"osx64\" for macOS and \"linux64\" for linux) which will fetch the latest release of the sirius executable for your operating system (defaults: positive mode, osx64). Run the script with or without arguments.\n>>\n>>      bash setup_scripts/setup.sh -o \"osx64\" -m \"positive\"\n>>\n> Install **OpenMS 3.0.0**:\n>>#### <span style=\"color: green\"> **For both systems** </span>\n>>Grab OpenMS 3.0.0 [here](https://github.com/OpenMS/OpenMS/releases/tag/Release3.0.0).\n>>\n>>#### <span style=\"color: green\"> **For Linux(!) only** </span>\n>>Then add the binaries to your path (Linux):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n>>      source ~/.bashrc\n>>#### <span style=\"color: green\"> **For MacOS(!) only** </span>\n>>Then add the binaries to your path (MacOS) by opening one of these files in a text editor:\n>>\n>>      /etc/profile\n>>      ~/.bash_profile\n>>      ~/.bash_login (if .bash_profile does not exist)\n>>      ~/.profile (if .bash_login does not exist)\n>>and adding the path to the binaries at the very end (path-dependent):\n>>\n>>      export PATH=$PATH:/path/to/OpenMS-3.0.0/bin/\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 3: Configure workflow",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "Configure the workflow according to your metabolomics data and instrument method via editing the files in the `config/` folder. \n\n1. Adjust the `config.yaml` to: \n- Configure the workflow execution (write <span style=\"color: green\">TRUE</span>/<span style=\"color: red\">FALSE</span> if you want to run/skip a specific rules of the workflow)\n- Adjust the parameters in the configuration file for your dataset as explained in the commented section in the yaml file (e.g. positive/negative ionisation, etc.)\n\n2. Add all your files in the data/raw/ or data/mzML/ directory and generate the `dataset.tsv` table to specify the samples (filenames) that will be processed. \n\n    **Suggestion**: Use the Jupyter notebook [Create_dataset_tsv](./Create_dataset_tsv.ipynb) or simply run:\n    \n    python data_files.py\n\n- `config/dataset.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "If there are blanks/QC samples in the file list, then define them through the script.",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage",
          "Step 3: Configure workflow"
        ],
        "type": "Text_excerpt",
        "value": "- `config/blanks.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| ISP2_blank   | blank media                  |\n\n- `config/samples.tsv` example:\n\n|  sample_name |       comment                |\n|-------------:|-----------------------------:|\n| NBC_00162    | pyracrimicin                 |\n| MDNA_WGS_14  | epemicins_A_B                |\n\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Test the workflow with the example dataset",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage",
          "Step 4: Execute workflow"
        ],
        "type": "Text_excerpt",
        "value": "    \nTest your configuration by performing a dry-run via\n\n    snakemake --use-conda -n\n\nExecute the workflow locally via\n\n    snakemake --use-conda --cores all\n\nSee the [Snakemake documentation](https://snakemake.readthedocs.io/en/stable/executable.html) for further details.\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Step 5: Investigate results",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Usage"
        ],
        "type": "Text_excerpt",
        "value": "All the results are in a .TSV format and can be opened simply with excel or using pandas dataframes. All the files under results/interim can be ignored and eventualy discarded.\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Test Data (only for testing the workflow with the example dataset)",
        "parent_header": [
          "UmetaFlow: An Untargeted Metabolomics workflow for high-throughput data processing and analysis for Linux and MacOS systems",
          "Developer Notes"
        ],
        "type": "Text_excerpt",
        "value": "* Current test data are built from known metabolite producer strains or standard samples that have been analyzed with a Thermo Orbitrap IDX instrument. The presence of the metabolites and their fragmentation patterns has been manually confirmed using TOPPView.\n"
      },
      "source": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/README.md",
      "technique": "header_analysis"
    }
  ],
  "workflows": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/workflow/rules/requantification.smk"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/workflow/rules/sirius_csi.smk"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/workflow/rules/sirius.smk"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/workflow/rules/spectralmatcher.smk"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/workflow/rules/fbmn_integration.smk"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/workflow/rules/preprocessing.smk"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/workflow/rules/analogsearch.smk"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/workflow/rules/GNPSexport.smk"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/biosustain/snakemake_UmetaFlow/main/workflow/rules/fileconversion.smk"
      },
      "technique": "file_exploration"
    }
  ]
}