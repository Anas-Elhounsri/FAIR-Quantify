{
  "application_domain": [
    {
      "confidence": 12.69,
      "result": {
        "type": "String",
        "value": "Computer Vision"
      },
      "technique": "supervised_classification"
    }
  ],
  "citation": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Citations",
        "parent_header": [
          "GSC (Genotype Sparse Compression)"
        ],
        "type": "Text_excerpt",
        "value": "- **bio.tools ID**: `gsc_genotype_sparse_compression`\n- **Research Resource Identifier (RRID)**: `SCR_025071`\n- **Doi**:`https://doi.org/10.48546/WORKFLOWHUB.WORKFLOW.887.1`\n"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "code_of_conduct": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# Code of Conduct\n\nFacebook has adopted a Code of Conduct that we expect project participants to adhere to.\nPlease read the [full text](https://code.fb.com/codeofconduct/)\nso that you can understand what actions will and will not be tolerated.\n"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/CODE_OF_CONDUCT.md",
      "technique": "file_exploration"
    }
  ],
  "code_repository": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/luo-xiaolong/GSC"
      },
      "technique": "GitHub_API"
    }
  ],
  "contributing_guidelines": [
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "# Contributing to Zstandard\nWe want to make contributing to this project as easy and transparent as\npossible.\n\n## Our Development Process\nNew versions are being developed in the \"dev\" branch,\nor in their own feature branch.\nWhen they are deemed ready for a release, they are merged into \"release\".\n\nAs a consequences, all contributions must stage first through \"dev\"\nor their own feature branch.\n\n## Pull Requests\nWe actively welcome your pull requests.\n\n1. Fork the repo and create your branch from `dev`.\n2. If you've added code that should be tested, add tests.\n3. If you've changed APIs, update the documentation.\n4. Ensure the test suite passes.\n5. Make sure your code lints.\n6. If you haven't already, complete the Contributor License Agreement (\"CLA\").\n\n## Contributor License Agreement (\"CLA\")\nIn order to accept your pull request, we need you to submit a CLA. You only need\nto do this once to work on any of Facebook's open source projects.\n\nComplete your CLA here: <https://code.facebook.com/cla>\n\n## Workflow\nZstd uses a branch-based workflow for making changes to the codebase. Typically, zstd\nwill use a new branch per sizable topic. For smaller changes, it is okay to lump multiple\nrelated changes into a branch.\n\nOur contribution process works in three main stages:\n1. Local development\n    * Update:\n        * Checkout your fork of zstd if you have not already\n        ```\n        git checkout https://github.com/<username>/zstd\n        cd zstd\n        ```\n        * Update your local dev branch\n        ```\n        git pull https://github.com/facebook/zstd dev\n        git push origin dev\n        ```\n    * Topic and development:\n        * Make a new branch on your fork about the topic you're developing for\n        ```\n        # branch names should be concise but sufficiently informative\n        git checkout -b <branch-name>\n        git push origin <branch-name>\n        ```\n        * Make commits and push\n        ```\n        # make some changes =\n        git add -u && git commit -m <message>\n        git push origin <branch-name>\n        ```\n        * Note: run local tests to ensure that your changes didn't break existing functionality\n            * Quick check\n            ```\n            make shortest\n            ```\n            * Longer check\n            ```\n            make test\n            ```\n2. Code Review and CI tests\n    * Ensure CI tests pass:\n        * Before sharing anything to the community, create a pull request in your own fork against the dev branch\n        and make sure that all GitHub Actions CI tests pass. See the Continuous Integration section below for more information.\n        * Ensure that static analysis passes on your development machine. See the Static Analysis section\n        below to see how to do this.\n    * Create a pull request:\n        * When you are ready to share you changes to the community, create a pull request from your branch\n        to facebook:dev. You can do this very easily by clicking 'Create Pull Request' on your fork's home\n        page.\n        * From there, select the branch where you made changes as your source branch and facebook:dev\n        as the destination.\n        * Examine the diff presented between the two branches to make sure there is nothing unexpected.\n    * Write a good pull request description:\n        * While there is no strict template that our contributors follow, we would like them to\n        sufficiently summarize and motivate the changes they are proposing. We recommend all pull requests,\n        at least indirectly, address the following points.\n            * Is this pull request important and why?\n            * Is it addressing an issue? If so, what issue? (provide links for convenience please)\n            * Is this a new feature? If so, why is it useful and/or necessary?\n            * Are there background references and documents that reviewers should be aware of to properly assess this change?\n        * Note: make sure to point out any design and architectural decisions that you made and the rationale behind them.\n        * Note: if you have been working with a specific user and would like them to review your work, make sure you mention them using (@<username>)\n    * Submit the pull request and iterate with feedback.\n3. Merge and Release\n    * Getting approval:\n        * You will have to iterate on your changes with feedback from other collaborators to reach a point\n        where your pull request can be safely merged.\n        * To avoid too many comments on style and convention, make sure that you have a\n        look at our style section below before creating a pull request.\n        * Eventually, someone from the zstd team will approve your pull request and not long after merge it into\n        the dev branch.\n    * Housekeeping:\n        * Most PRs are linked with one or more Github issues. If this is the case for your PR, make sure\n        the corresponding issue is mentioned. If your change 'fixes' or completely addresses the\n        issue at hand, then please indicate this by requesting that an issue be closed by commenting.\n        * Just because your changes have been merged does not mean the topic or larger issue is complete. Remember\n        that the change must make it to an official zstd release for it to be meaningful. We recommend\n        that contributors track the activity on their pull request and corresponding issue(s) page(s) until\n        their change makes it to the next release of zstd. Users will often discover bugs in your code or\n        suggest ways to refine and improve your initial changes even after the pull request is merged.\n\n## Static Analysis\nStatic analysis is a process for examining the correctness or validity of a program without actually\nexecuting it. It usually helps us find many simple bugs. Zstd uses clang's `scan-build` tool for\nstatic analysis. You can install it by following the instructions for your OS on https://clang-analyzer.llvm.org/scan-build.\n\nOnce installed, you can ensure that our static analysis tests pass on your local development machine\nby running:\n```\nmake staticAnalyze\n```\n\nIn general, you can use `scan-build` to static analyze any build script. For example, to static analyze\njust `contrib/largeNbDicts` and nothing else, you can run:\n\n```\nscan-build make -C contrib/largeNbDicts largeNbDicts\n```\n\n### Pitfalls of static analysis\n`scan-build` is part of our regular CI suite. Other static analyzers are not.\n\nIt can be useful to look at additional static analyzers once in a while (and we do), but it's not a good idea to multiply the nb of analyzers run continuously at each commit and PR. The reasons are :\n\n- Static analyzers are full of false positive. The signal to noise ratio is actually pretty low.\n- A good CI policy is \"zero-warning tolerance\". That means that all issues must be solved, including false positives. This quickly becomes a tedious workload.\n- Multiple static analyzers will feature multiple kind of false positives, sometimes applying to the same code but in different ways leading to :\n   + torteous code, trying to please multiple constraints, hurting readability and therefore maintenance. Sometimes, such complexity introduce other more subtle bugs, that are just out of scope of the analyzers.\n   + sometimes, these constraints are mutually exclusive : if one try to solve one, the other static analyzer will complain, they can't be both happy at the same time.\n- As if that was not enough, the list of false positives change with each version. It's hard enough to follow one static analyzer, but multiple ones with their own update agenda, this quickly becomes a massive velocity reducer.\n\nThis is different from running a static analyzer once in a while, looking at the output, and __cherry picking__ a few warnings that seem helpful, either because they detected a genuine risk of bug, or because it helps expressing the code in a way which is more readable or more difficult to misuse. These kind of reports can be useful, and are accepted.\n\n## Continuous Integration\nCI tests run every time a pull request (PR) is created or updated. The exact tests\nthat get run will depend on the destination branch you specify. Some tests take\nlonger to run than others. Currently, our CI is set up to run a short\nseries of tests when creating a PR to the dev branch and a longer series of tests\nwhen creating a PR to the release branch. You can look in the configuration files\nof the respective CI platform for more information on what gets run when.\n\nMost people will just want to create a PR with the destination set to their local dev\nbranch of zstd. You can then find the status of the tests on the PR's page. You can also\nre-run tests and cancel running tests from the PR page or from the respective CI's dashboard.\n\nAlmost all of zstd's CI runs on GitHub Actions (configured at `.github/workflows`), which will automatically run on PRs to your\nown fork. A small number of tests run on other services (e.g. Travis CI, Circle CI, Appveyor).\nThese require work to set up on your local fork, and (at least for Travis CI) cost money.\nTherefore, if the PR on your local fork passes GitHub Actions, feel free to submit a PR\nagainst the main repo.\n\n### Third-party CI\nA small number of tests cannot run on GitHub Actions, or have yet to be migrated.\nFor these, we use a variety of third-party services (listed below). It is not necessary to set\nthese up on your fork in order to contribute to zstd; however, we do link to instructions for those\nwho want earlier signal.\n\n| Service   | Purpose                                                                                                    | Setup Links                                                                                                                                            | Config Path            |\n|-----------|------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------|\n| Travis CI | Used for testing on non-x86 architectures such as PowerPC                                                  | https://docs.travis-ci.com/user/tutorial/#to-get-started-with-travis-ci-using-github <br> https://github.com/marketplace/travis-ci                     | `.travis.yml`          |\n| AppVeyor  | Used for some Windows testing (e.g. cygwin, mingw)                                                         | https://www.appveyor.com/blog/2018/10/02/github-apps-integration/ <br> https://github.com/marketplace/appveyor                                         | `appveyor.yml`         |\n| Cirrus CI | Used for testing on FreeBSD                                                                                | https://github.com/marketplace/cirrus-ci/                                                                                                              | `.cirrus.yml`          |\n| Circle CI | Historically was used to provide faster signal,<br/> but we may be able to migrate these to Github Actions | https://circleci.com/docs/2.0/getting-started/#setting-up-circleci <br> https://youtu.be/Js3hMUsSZ2c <br> https://circleci.com/docs/2.0/enable-checks/ | `.circleci/config.yml` |\n\nNote: the instructions linked above mostly cover how to set up a repository with CI from scratch. \nThe general idea should be the same for setting up CI on your fork of zstd, but you may have to \nfollow slightly different steps. In particular, please ignore any instructions related to setting up\nconfig files (since zstd already has configs for each of these services).\n\n## Performance\nPerformance is extremely important for zstd and we only merge pull requests whose performance\nlandscape and corresponding trade-offs have been adequately analyzed, reproduced, and presented.\nThis high bar for performance means that every PR which has the potential to\nimpact performance takes a very long time for us to properly review. That being said, we\nalways welcome contributions to improve performance (or worsen performance for the trade-off of\nsomething else). Please keep the following in mind before submitting a performance related PR:\n\n1. Zstd isn't as old as gzip but it has been around for time now and its evolution is\nvery well documented via past Github issues and pull requests. It may be the case that your\nparticular performance optimization has already been considered in the past. Please take some\ntime to search through old issues and pull requests using keywords specific to your\nwould-be PR. Of course, just because a topic has already been discussed (and perhaps rejected\non some grounds) in the past, doesn't mean it isn't worth bringing up again. But even in that case,\nit will be helpful for you to have context from that topic's history before contributing.\n2. The distinction between noise and actual performance gains can unfortunately be very subtle\nespecially when microbenchmarking extremely small wins or losses. The only remedy to getting\nsomething subtle merged is extensive benchmarking. You will be doing us a great favor if you\ntake the time to run extensive, long-duration, and potentially cross-(os, platform, process, etc)\nbenchmarks on your end before submitting a PR. Of course, you will not be able to benchmark\nyour changes on every single processor and os out there (and neither will we) but do that best\nyou can:) We've adding some things to think about when benchmarking below in the Benchmarking\nPerformance section which might be helpful for you.\n3. Optimizing performance for a certain OS, processor vendor, compiler, or network system is a perfectly\nlegitimate thing to do as long as it does not harm the overall performance health of Zstd.\nThis is a hard balance to strike but please keep in mind other aspects of Zstd when\nsubmitting changes that are clang-specific, windows-specific, etc.\n\n## Benchmarking Performance\nPerformance microbenchmarking is a tricky subject but also essential for Zstd. We value empirical\ntesting over theoretical speculation. This guide it not perfect but for most scenarios, it\nis a good place to start.\n\n### Stability\nUnfortunately, the most important aspect in being able to benchmark reliably is to have a stable\nbenchmarking machine. A virtual machine, a machine with shared resources, or your laptop\nwill typically not be stable enough to obtain reliable benchmark results. If you can get your\nhands on a desktop, this is usually a better scenario.\n\nOf course, benchmarking can be done on non-hyper-stable machines as well. You will just have to\ndo a little more work to ensure that you are in fact measuring the changes you've made not and\nnoise. Here are some things you can do to make your benchmarks more stable:\n\n1. The most simple thing you can do to drastically improve the stability of your benchmark is\nto run it multiple times and then aggregate the results of those runs. As a general rule of\nthumb, the smaller the change you are trying to measure, the more samples of benchmark runs\nyou will have to aggregate over to get reliable results. Here are some additional things to keep in\nmind when running multiple trials:\n    * How you aggregate your samples are important. You might be tempted to use the mean of your\n    results. While this is certainly going to be a more stable number than a raw single sample\n    benchmark number, you might have more luck by taking the median. The mean is not robust to\n    outliers whereas the median is. Better still, you could simply take the fastest speed your\n    benchmark achieved on each run since that is likely the fastest your process will be\n    capable of running your code. In our experience, this (aggregating by just taking the sample\n    with the fastest running time) has been the most stable approach.\n    * The more samples you have, the more stable your benchmarks should be. You can verify\n    your improved stability by looking at the size of your confidence intervals as you\n    increase your sample count. These should get smaller and smaller. Eventually hopefully\n    smaller than the performance win you are expecting.\n    * Most processors will take some time to get `hot` when running anything. The observations\n    you collect during that time period will very different from the true performance number. Having\n    a very large number of sample will help alleviate this problem slightly but you can also\n    address is directly by simply not including the first `n` iterations of your benchmark in\n    your aggregations. You can determine `n` by simply looking at the results from each iteration\n    and then hand picking a good threshold after which the variance in results seems to stabilize.\n2. You cannot really get reliable benchmarks if your host machine is simultaneously running\nanother cpu/memory-intensive application in the background. If you are running benchmarks on your\npersonal laptop for instance, you should close all applications (including your code editor and\nbrowser) before running your benchmarks. You might also have invisible background applications\nrunning. You can see what these are by looking at either Activity Monitor on Mac or Task Manager\non Windows. You will get more stable benchmark results of you end those processes as well.\n    * If you have multiple cores, you can even run your benchmark on a reserved core to prevent\n    pollution from other OS and user processes. There are a number of ways to do this depending\n    on your OS:\n        * On linux boxes, you have use https://github.com/lpechacek/cpuset.\n        * On Windows, you can \"Set Processor Affinity\" using https://www.thewindowsclub.com/processor-affinity-windows\n        * On Mac, you can try to use their dedicated affinity API https://developer.apple.com/library/archive/releasenotes/Performance/RN-AffinityAPI/#//apple_ref/doc/uid/TP40006635-CH1-DontLinkElementID_2\n3. To benchmark, you will likely end up writing a separate c/c++ program that will link libzstd.\nDynamically linking your library will introduce some added variation (not a large amount but\ndefinitely some). Statically linking libzstd will be more stable. Static libraries should\nbe enabled by default when building zstd.\n4. Use a profiler with a good high resolution timer. See the section below on profiling for\ndetails on this.\n5. Disable frequency scaling, turbo boost and address space randomization (this will vary by OS)\n6. Try to avoid storage. On some systems you can use tmpfs. Putting the program, inputs and outputs on\ntmpfs avoids touching a real storage system, which can have a pretty big variability.\n\nAlso check our LLVM's guide on benchmarking here: https://llvm.org/docs/Benchmarking.html\n\n### Zstd benchmark\nThe fastest signal you can get regarding your performance changes is via the in-build zstd cli\nbench option. You can run Zstd as you typically would for your scenario using some set of options\nand then additionally also specify the `-b#` option. Doing this will run our benchmarking pipeline\nfor that options you have just provided. If you want to look at the internals of how this\nbenchmarking script works, you can check out programs/benchzstd.c\n\nFor example: say you have made a change that you believe improves the speed of zstd level 1. The\nvery first thing you should use to asses whether you actually achieved any sort of improvement\nis `zstd -b`. You might try to do something like this. Note: you can use the `-i` option to\nspecify a running time for your benchmark in seconds (default is 3 seconds).\nUsually, the longer the running time, the more stable your results will be.\n\n```\n$ git checkout <commit-before-your-change>\n$ make && cp zstd zstd-old\n$ git checkout <commit-after-your-change>\n$ make && cp zstd zstd-new\n$ zstd-old -i5 -b1 <your-test-data>\n 1<your-test-data>         :      8990 ->      3992 (2.252), 302.6 MB/s , 626.4 MB/s\n$ zstd-new -i5 -b1 <your-test-data>\n 1<your-test-data>         :      8990 ->      3992 (2.252), 302.8 MB/s , 628.4 MB/s\n```\n\nUnless your performance win is large enough to be visible despite the intrinsic noise\non your computer, benchzstd alone will likely not be enough to validate the impact of your\nchanges. For example, the results of the example above indicate that effectively nothing\nchanged but there could be a small <3% improvement that the noise on the host machine\nobscured. So unless you see a large performance win (10-15% consistently) using just\nthis method of evaluation will not be sufficient.\n\n### Profiling\nThere are a number of great profilers out there. We're going to briefly mention how you can\nprofile your code using `instruments` on mac, `perf` on linux and `visual studio profiler`\non windows.\n\nSay you have an idea for a change that you think will provide some good performance gains\nfor level 1 compression on Zstd. Typically this means, you have identified a section of\ncode that you think can be made to run faster.\n\nThe first thing you will want to do is make sure that the piece of code is actually taking up\na notable amount of time to run. It is usually not worth optimizing something which accounts for less than\n0.0001% of the total running time. Luckily, there are tools to help with this.\nProfilers will let you see how much time your code spends inside a particular function.\nIf your target code snippet is only part of a function, it might be worth trying to\nisolate that snippet by moving it to its own function (this is usually not necessary but\nmight be).\n\nMost profilers (including the profilers discussed below) will generate a call graph of\nfunctions for you. Your goal will be to find your function of interest in this call graph\nand then inspect the time spent inside of it. You might also want to to look at the\nannotated assembly which most profilers will provide you with.\n\n#### Instruments\nWe will once again consider the scenario where you think you've identified a piece of code\nwhose performance can be improved upon. Follow these steps to profile your code using\nInstruments.\n\n1. Open Instruments\n2. Select `Time Profiler` from the list of standard templates\n3. Close all other applications except for your instruments window and your terminal\n4. Run your benchmarking script from your terminal window\n    * You will want a benchmark that runs for at least a few seconds (5 seconds will\n    usually be long enough). This way the profiler will have something to work with\n    and you will have ample time to attach your profiler to this process:)\n    * I will just use benchzstd as my bencharmking script for this example:\n```\n$ zstd -b1 -i5 <my-data> # this will run for 5 seconds\n```\n5. Once you run your benchmarking script, switch back over to instruments and attach your\nprocess to the time profiler. You can do this by:\n    * Clicking on the `All Processes` drop down in the top left of the toolbar.\n    * Selecting your process from the dropdown. In my case, it is just going to be labeled\n    `zstd`\n    * Hitting the bright red record circle button on the top left of the toolbar\n6. You profiler will now start collecting metrics from your benchmarking script. Once\nyou think you have collected enough samples (usually this is the case after 3 seconds of\nrecording), stop your profiler.\n7. Make sure that in toolbar of the bottom window, `profile` is selected.\n8. You should be able to see your call graph.\n    * If you don't see the call graph or an incomplete call graph, make sure you have compiled\n    zstd and your benchmarking script using debug flags. On mac and linux, this just means\n    you will have to supply the `-g` flag alone with your build script. You might also\n    have to provide the `-fno-omit-frame-pointer` flag\n9. Dig down the graph to find your function call and then inspect it by double clicking\nthe list item. You will be able to see the annotated source code and the assembly side by\nside.\n\n#### Perf\n\nThis wiki has a pretty detailed tutorial on getting started working with perf so we'll\nleave you to check that out of you're getting started:\n\nhttps://perf.wiki.kernel.org/index.php/Tutorial\n\nSome general notes on perf:\n* Use `perf stat -r # <bench-program>` to quickly get some relevant timing and\ncounter statistics. Perf uses a high resolution timer and this is likely one\nof the first things your team will run when assessing your PR.\n* Perf has a long list of hardware counters that can be viewed with `perf --list`.\nWhen measuring optimizations, something worth trying is to make sure the hardware\ncounters you expect to be impacted by your change are in fact being so. For example,\nif you expect the L1 cache misses to decrease with your change, you can look at the\ncounter `L1-dcache-load-misses`\n* Perf hardware counters will not work on a virtual machine.\n\n#### Visual Studio\n\nTODO\n\n## Issues\nWe use GitHub issues to track public bugs. Please ensure your description is\nclear and has sufficient instructions to be able to reproduce the issue.\n\nFacebook has a [bounty program](https://www.facebook.com/whitehat/) for the safe\ndisclosure of security bugs. In those cases, please go through the process\noutlined on that page and do not file a public issue.\n\n## Coding Style\nIt's a pretty long topic, which is difficult to summarize in a single paragraph.\nAs a rule of thumbs, try to imitate the coding style of\nsimilar lines of codes around your contribution.\nThe following is a non-exhaustive list of rules employed in zstd code base:\n\n### C90\nThis code base is following strict C90 standard,\nwith 2 extensions : 64-bit `long long` types, and variadic macros.\nThis rule is applied strictly to code within `lib/` and `programs/`.\nSub-project in `contrib/` are allowed to use other conventions.\n\n### C++ direct compatibility : symbol mangling\nAll public symbol declarations must be wrapped in `extern \u201cC\u201d { \u2026 }`,\nso that this project can be compiled as C++98 code,\nand linked into C++ applications.\n\n### Minimal Frugal\nThis design requirement is fundamental to preserve the portability of the code base.\n#### Dependencies\n- Reduce dependencies to the minimum possible level.\n  Any dependency should be considered \u201cbad\u201d by default,\n  and only tolerated because it provides a service in a better way than can be achieved locally.\n  The only external dependencies this repository tolerates are\n  standard C libraries, and in rare cases, system level headers.\n- Within `lib/`, this policy is even more drastic.\n  The only external dependencies allowed are `<assert.h>`, `<stdlib.h>`, `<string.h>`,\n  and even then, not directly.\n  In particular, no function shall ever allocate on heap directly,\n  and must use instead `ZSTD_malloc()` and equivalent.\n  Other accepted non-symbol headers are `<stddef.h>` and `<limits.h>`.\n- Within the project, there is a strict hierarchy of dependencies that must be respected.\n  `programs/` is allowed to depend on `lib/`, but only its public API.\n  Within `lib/`, `lib/common` doesn't depend on any other directory.\n  `lib/compress` and `lib/decompress` shall not depend on each other.\n  `lib/dictBuilder` can depend on `lib/common` and `lib/compress`, but not `lib/decompress`.\n#### Resources\n- Functions in `lib/` must use very little stack space,\n  several dozens of bytes max.\n  Everything larger must use the heap allocator,\n  or require a scratch buffer to be emplaced manually.\n\n### Naming\n* All public symbols are prefixed with `ZSTD_`\n  + private symbols, with a scope limited to their own unit, are free of this restriction.\n    However, since `libzstd` source code can be amalgamated,\n    each symbol name must attempt to be (and remain) unique.\n    Avoid too generic names that could become ground for future collisions.\n    This generally implies usage of some form of prefix.\n* For symbols (functions and variables), naming convention is `PREFIX_camelCase`.\n  + In some advanced cases, one can also find :\n    - `PREFIX_prefix2_camelCase`\n    - `PREFIX_camelCase_extendedQualifier`\n* Multi-words names generally consist of an action followed by object:\n  - for example : `ZSTD_createCCtx()`\n* Prefer positive actions\n  - `goBackward` rather than `notGoForward`\n* Type names (`struct`, etc.) follow similar convention,\n  except that they are allowed and even invited to start by an Uppercase letter.\n  Example : `ZSTD_CCtx`, `ZSTD_CDict`\n* Macro names are all Capital letters.\n  The same composition rules (`PREFIX_NAME_QUALIFIER`) apply.\n* File names are all lowercase letters.\n  The convention is `snake_case`.\n  File names **must** be unique across the entire code base,\n  even when they stand in clearly separated directories.\n\n### Qualifiers\n* This code base is `const` friendly, if not `const` fanatical.\n  Any variable that can be `const` (aka. read-only) **must** be `const`.\n  Any pointer which content will not be modified must be `const`.\n  This property is then controlled at compiler level.\n  `const` variables are an important signal to readers that this variable isn\u2019t modified.\n  Conversely, non-const variables are a signal to readers to watch out for modifications later on in the function.\n* If a function must be inlined, mention it explicitly,\n  using project's own portable macros, such as `FORCE_INLINE_ATTR`,\n  defined in `lib/common/compiler.h`.\n\n### Debugging\n* **Assertions** are welcome, and should be used very liberally,\n  to control any condition the code expects for its correct execution.\n  These assertion checks will be run in debug builds, and disabled in production.\n* For traces, this project provides its own debug macros,\n  in particular `DEBUGLOG(level, ...)`, defined in `lib/common/debug.h`.\n\n### Code documentation\n* Avoid code documentation that merely repeats what the code is already stating.\n  Whenever applicable, prefer employing the code as the primary way to convey explanations.\n  Example 1 : `int nbTokens = n;` instead of `int i = n; /* i is a nb of tokens *./`.\n  Example 2 : `assert(size > 0);` instead of `/* here, size should be positive */`.\n* At declaration level, the documentation explains how to use the function or variable\n  and when applicable why it's needed, of the scenarios where it can be useful.\n* At implementation level, the documentation explains the general outline of the algorithm employed,\n  and when applicable why this specific choice was preferred.\n\n### General layout\n* 4 spaces for indentation rather than tabs\n* Code documentation shall directly precede function declaration or implementation\n* Function implementations and its code documentation should be preceded and followed by an empty line\n\n\n## License\nBy contributing to Zstandard, you agree that your contributions will be licensed\nunder both the [LICENSE](LICENSE) file and the [COPYING](COPYING) file in the root directory of this source tree.\n"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/CONTRIBUTING.md",
      "technique": "file_exploration"
    }
  ],
  "date_created": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2023-07-06T12:05:57Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "date_updated": [
    {
      "confidence": 1,
      "result": {
        "type": "Date",
        "value": "2024-08-19T06:35:23Z"
      },
      "technique": "GitHub_API"
    }
  ],
  "description": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "VCF compression tool"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 0.9968485400265156,
      "result": {
        "original_header": "GSC (Genotype Sparse Compression)",
        "type": "Text_excerpt",
        "value": "Genotype Sparse Compression (GSC) is an advanced tool for lossless compression of VCF files, designed to efficiently store and manage VCF files in a compressed format. It accepts VCF/BCF files as input and utilizes advanced compression techniques to significantly reduce storage requirements while ensuring fast query capabilities. In our study, we successfully compressed the VCF files from the 1000 Genomes Project (1000Gpip3), consisting of 2504 samples and 80 million variants, from an uncompressed VCF file of 803.70GB to approximately 1GB.\n \n"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "download_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://github.com/luo-xiaolong/GSC/releases"
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 2
      },
      "technique": "GitHub_API"
    }
  ],
  "forks_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/luo-xiaolong/GSC/forks"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "luo-xiaolong/GSC"
      },
      "technique": "GitHub_API"
    }
  ],
  "full_title": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "GSC (Genotype Sparse Compression)"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "regular_expression"
    }
  ],
  "has_build_file": [
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/Dockerfile",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/.circleci/images/primary/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/.circleci/images/primary/Dockerfile",
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "format": "dockerfile",
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/contrib/docker/Dockerfile"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/contrib/docker/Dockerfile",
      "technique": "file_exploration"
    }
  ],
  "has_script_file": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/build/single_file_libs/create_single_file_decoder.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/build/single_file_libs/combine.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/build/single_file_libs/build_library_test.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/build/single_file_libs/create_single_file_library.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/build/single_file_libs/build_decoder_test.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/contrib/linux-kernel/btrfs-benchmark.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/contrib/linux-kernel/btrfs-extract-benchmark.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/contrib/linux-kernel/squashfs-benchmark.sh"
      },
      "technique": "file_exploration"
    },
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/include/zstd-1.5.2/contrib/gen_html/gen-zstd-manual.sh"
      },
      "technique": "file_exploration"
    }
  ],
  "installation": [
    {
      "confidence": 0.9999999968782163,
      "result": {
        "original_header": "Dockerfile",
        "type": "Text_excerpt",
        "value": "Dockerfile can be used to build a Docker image with all necessary dependencies and GSC compressor. The image is based on Ubuntu 18.04. To build a Docker image and run a Docker container, you need Docker Desktop (https://www.docker.com). Example commands (run it within a directory with Dockerfile):\n```bash\n#build\ndocker build -t gsc_project .\n\n#run\ndocker run -it gsc_project\n``` \n"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "supervised_classification"
    },
    {
      "confidence": 1.0,
      "result": {
        "original_header": "Building the GSC command line tool",
        "type": "Text_excerpt",
        "value": "```bash\n#Clone the repository\ngit clone https://github.com/luo-xiaolong/GSC.git\ncd GSC\n\n# Clean the previous GSC build \nmake clean\n\n# Build the application\nmake\n```\nTo clean the GSC build use:\nBASH2* \n"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "supervised_classification"
    }
  ],
  "issue_tracker": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://api.github.com/repos/luo-xiaolong/GSC/issues"
      },
      "technique": "GitHub_API"
    }
  ],
  "keywords": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": ""
      },
      "technique": "GitHub_API"
    }
  ],
  "license": [
    {
      "confidence": 1,
      "result": {
        "name": "MIT License",
        "spdx_id": "MIT",
        "type": "License",
        "url": "https://api.github.com/licenses/mit",
        "value": "https://api.github.com/licenses/mit"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "type": "File_dump",
        "value": "                                        \n                 The MIT License (MIT)\n\nCopyright (c) 2021 Zexuan Zhu<zhuzx@szu.edu.cn>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/LICENSE",
      "technique": "file_exploration"
    }
  ],
  "name": [
    {
      "confidence": 1,
      "result": {
        "type": "String",
        "value": "GSC"
      },
      "technique": "GitHub_API"
    }
  ],
  "owner": [
    {
      "confidence": 1,
      "result": {
        "type": "User",
        "value": "luo-xiaolong"
      },
      "technique": "GitHub_API"
    }
  ],
  "programming_languages": [
    {
      "confidence": 1,
      "result": {
        "name": "C",
        "size": 9807100,
        "type": "Programming_language",
        "value": "C"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "C++",
        "size": 2506690,
        "type": "Programming_language",
        "value": "C++"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "HTML",
        "size": 360670,
        "type": "Programming_language",
        "value": "HTML"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Perl",
        "size": 348938,
        "type": "Programming_language",
        "value": "Perl"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Shell",
        "size": 242024,
        "type": "Programming_language",
        "value": "Shell"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Roff",
        "size": 210970,
        "type": "Programming_language",
        "value": "Roff"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Makefile",
        "size": 181991,
        "type": "Programming_language",
        "value": "Makefile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Python",
        "size": 140055,
        "type": "Programming_language",
        "value": "Python"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "M4",
        "size": 61407,
        "type": "Programming_language",
        "value": "M4"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "CMake",
        "size": 35875,
        "type": "Programming_language",
        "value": "CMake"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Meson",
        "size": 25734,
        "type": "Programming_language",
        "value": "Meson"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Assembly",
        "size": 14340,
        "type": "Programming_language",
        "value": "Assembly"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Starlark",
        "size": 10295,
        "type": "Programming_language",
        "value": "Starlark"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Batchfile",
        "size": 6642,
        "type": "Programming_language",
        "value": "Batchfile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "CSS",
        "size": 6369,
        "type": "Programming_language",
        "value": "CSS"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Scilab",
        "size": 5605,
        "type": "Programming_language",
        "value": "Scilab"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Dockerfile",
        "size": 1703,
        "type": "Programming_language",
        "value": "Dockerfile"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Lua",
        "size": 1647,
        "type": "Programming_language",
        "value": "Lua"
      },
      "technique": "GitHub_API"
    },
    {
      "confidence": 1,
      "result": {
        "name": "Swift",
        "size": 1264,
        "type": "Programming_language",
        "value": "Swift"
      },
      "technique": "GitHub_API"
    }
  ],
  "readme_url": [
    {
      "confidence": 1,
      "result": {
        "type": "Url",
        "value": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md"
      },
      "technique": "file_exploration"
    }
  ],
  "requirements": [
    {
      "confidence": 1,
      "result": {
        "original_header": "GSC requires:",
        "parent_header": [
          "GSC (Genotype Sparse Compression)",
          "Requirements"
        ],
        "type": "Text_excerpt",
        "value": "- **Compiler Compatibility**: GSC requires a modern C++14-ready compiler, such as:\n  - g++ version 10.1.0 or higher\n\n- **Build System**: Make build system is necessary for compiling GSC.\n\n- **Operating System**: GSC supports 64-bit operating systems, including:\n  - Linux (Ubuntu 18.04)\n"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "header_analysis"
    }
  ],
  "somef_missing_categories": [
    "acknowledgement",
    "run",
    "download",
    "contact",
    "contributors",
    "documentation",
    "faq",
    "support",
    "identifier",
    "executable_example"
  ],
  "somef_provenance": {
    "date": "2024-10-04 00:22:13",
    "somef_schema_version": "1.0.0",
    "somef_version": "0.9.5"
  },
  "stargazers_count": [
    {
      "confidence": 1,
      "result": {
        "type": "Number",
        "value": 5
      },
      "technique": "GitHub_API"
    }
  ],
  "type": [
    {
      "confidence": 0.82,
      "result": {
        "type": "String",
        "value": "commandline-application"
      },
      "technique": "software_type_heuristics"
    }
  ],
  "usage": [
    {
      "confidence": 1,
      "result": {
        "original_header": "Usage",
        "parent_header": [
          "GSC (Genotype Sparse Compression)"
        ],
        "type": "Text_excerpt",
        "value": "```bash\nUsage: gsc [option] [arguments] \nAvailable options: \n        compress - compress VCF/BCF file\n        decompress     - query and decompress to VCF/BCF file\n```\n- Compress the input VCF/BCF file\n```bash\nUsage of gsc compress:\n\n        gsc compress [options] [--in [in_file]] [--out [out_file]]\n\nWhere:\n\n        [options]              Optional flags and parameters for compression.\n        -i,  --in [in_file]    Specify the input file (default: VCF or VCF.GZ). If omitted, input is taken from standard input (stdin).\n        -o,  --out [out_file]  Specify the output file. If omitted, output is sent to standard output (stdout).\n\nOptions:\n\n        -M,  --mode_lossly     Choose lossy compression mode (lossless by default).\n        -b,  --bcf             Input is a BCF file (default: VCF or VCF.GZ).\n        -p,  --ploidy [X]      Set ploidy of samples in input VCF to [X] (default: 2).\n        -t,  --threads [X]     Set number of threads to [X] (default: 1).\n        -d,  --depth [X]       Set maximum replication depth to [X] (default: 100, 0 means no matches).\n        -m,  --merge [X]       Specify files to merge, separated by commas (e.g., -m chr1.vcf,chr2.vcf), or '@' followed by a file containing a list of VCF files (e.g., -m @file_with_IDs.txt). By default, all VCF files are compressed.\n```\n- Decompress / Query\n```bash\nUsage of gsc decompress and query:\n\n        gsc decompress [options] --in [in_file] --out [out_file]\n\nWhere:\n        [options]              Optional flags and parameters for compression.\n        -i,  --in [in_file]    Specify the input file . If omitted, input is taken from standard input (stdin).\n        -o,  --out [out_file]  Specify the output file (default: VCF). If omitted, output is sent to standard output (stdout).\n\nOptions:\n\n    General Options:\n\n        -M,  --mode_lossly      Choose lossy compression mode (default: lossless).\n        -b,  --bcf              Output a BCF file (default: VCF).\n\n    Filter options (applicable in lossy compression mode only): \n\n        -r,  --range [X]        Specify range in format [chr]:[start],[end] (e.g., -r chr1:4999756,4999852).\n        -s,  --samples [X]      Samples separated by comms (e.g., -s HG03861,NA18639) OR '@' sign followed by the name of a file with sample name(s) separated by whitespaces (for exaple: -s @file_with_IDs.txt). By default all samples/individuals are decompressed. \n        --header-only           Output only the header of the VCF/BCF.\n        --no-header             Output without the VCF/BCF header (only genotypes).\n        -G,  --no-genotype      Don't output sample genotypes (only #CHROM, POS, ID, REF, ALT, QUAL, FILTER, and INFO columns).\n        -C,  --out-ac-an        Write AC/AN to the INFO field.\n        -S,  --split            Split output into multiple files (one per chromosome).\n        -I, [ID=^]              Include only sites with specified ID (e.g., -I \"ID=rs6040355\").\n        --minAC [X]             Include only sites with AC <= X.\n        --maxAC [X]             Include only sites with AC >= X.\n        --minAF [X]             Include only sites with AF >= X (X: 0 to 1).\n        --maxAF [X]             Include only sites with AF <= X (X: 0 to 1).\n        --min-qual [X]          Include only sites with QUAL >= X.\n        --max-qual [X]          Include only sites with QUAL <= X.\n```"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Example",
        "parent_header": [
          "GSC (Genotype Sparse Compression)"
        ],
        "type": "Text_excerpt",
        "value": "There is an example VCF/VCF.gz/BCF file, `toy.vcf`/`toy.vcf.gz`/`toy.bcf`, in the toy folder, which can be used to test GSC"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Lossless compression:",
        "parent_header": [
          "GSC (Genotype Sparse Compression)",
          "Example",
          "Compress"
        ],
        "type": "Text_excerpt",
        "value": "The input file format is VCF. You can compress a VCF file in lossless mode using one of the following methods:\n1. **Explicit input and output file parameters**:\n   \n   Use the `--in` option to specify the input VCF file and the `--out` option for the output compressed file.\n   ```bash\n   ./gsc compress --in toy/toy.vcf --out toy/toy_lossless.gsc\n   ```\n2. **Input file parameter and output redirection**:\n   \n   Use the `--out` option for the output compressed file and redirect the input VCF file into the command.\n   ```bash\n   ./gsc compress --out toy/toy_lossless.gsc < toy/toy.vcf\n   ```\n3. **Output file redirection and input file parameter**:\n   \n   Specify the input VCF file with the `--in` option and redirect the output to create the compressed file.\n   ```bash\n   ./gsc compress --in toy/toy.vcf > toy/toy_lossless.gsc\n   ```\n4. **Input and output redirection**:\n   \n   Use shell redirection for both input and output. This method does not use the `--in` and `--out` options.\n   ```bash\n   ./gsc compress < toy/toy.vcf > toy/toy_lossless.gsc\n   ```\nThis will create a file:\n* `toy_lossless.gsc` - The compressed archive of the entire VCF file.\n"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Lossy compression:",
        "parent_header": [
          "GSC (Genotype Sparse Compression)",
          "Example",
          "Compress"
        ],
        "type": "Text_excerpt",
        "value": "The input file format is VCF. The commands are similar to those used for lossless compression, with the addition of the `-M` parameter to enable lossy compression.\n\n   For example, to compress a VCF file in lossy mode:\n\n   ```bash\n   ./gsc compress -M --in toy/toy.vcf --out toy/toy_lossy.gsc\n   ```\n   or \n  \n   Using redirection:\n   ```bash\n   ./gsc compress -M --out toy/toy_lossy.gsc < toy/toy.vcf\n   ``` \n   This will create a file:\n   * `toy_lossy.gsc` - The compressed archive of the entire VCF file is implemented with lossy compression. It only retains the 'GT' subfield within the INFO and FORMAT fields, and excludes all other subfields..\n    "
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Lossless decompression:",
        "parent_header": [
          "GSC (Genotype Sparse Compression)",
          "Example",
          "Decompress   (The commands are similar to those used for compression)"
        ],
        "type": "Text_excerpt",
        "value": "To decompress the compressed toy_lossless.gsc into a VCF file named toy_lossless.vcf:\n```bash\n./gsc decompress --in toy/toy_lossless.gsc --out toy/toy_lossless.vcf\n```"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Lossy decompression:",
        "parent_header": [
          "GSC (Genotype Sparse Compression)",
          "Example",
          "Decompress   (The commands are similar to those used for compression)"
        ],
        "type": "Text_excerpt",
        "value": "To decompress the compressed toy_lossy.gsc into a VCF file named toy_lossy.vcf:\n```bash\n./gsc decompress -M --in toy/toy_lossy.gsc --out toy/toy_lossy.vcf\n```"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Variant-based query",
        "parent_header": [
          "GSC (Genotype Sparse Compression)",
          "Example",
          "Query"
        ],
        "type": "Text_excerpt",
        "value": "Retrieve entries for chromosome 20 with POS ranging from 1 to 1,000,000, and output to the toy/query_toy_r_20_3_1000000.vcf file.\n\n```bash\n./gsc decompress -M --range 20:1,1000000 --in toy/toy_lossy.gsc --out toy/query_toy_20_3_1000000.vcf\n```\nRetrieve entries for chromosome 20 with POS ranging from 1 to 1,000,000, and output to the terminal interface.\n```bash\n./gsc decompress -M --range 20:1,1000000 --in toy/toy_lossy.gsc\n```"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Sample-based query",
        "parent_header": [
          "GSC (Genotype Sparse Compression)",
          "Example",
          "Query"
        ],
        "type": "Text_excerpt",
        "value": "Retrieve genotype columns for samples named NA00001 and NA00002, and output to the toy/query_toy_s_NA00001_NA00002.vcf file.\n```bash\n./gsc decompress -M --samples NA00001,NA00002 --in toy/toy_lossy.gsc --out toy/query_toy_s_NA00001_NA00002.vcf\n```\nor\n\nThe names NA00001 and NA00002 are stored in the toy/samples_name_file.\n```bash\n./gsc decompress -M --samples @toy/samples_name_file --in toy/toy_lossy.gsc --out toy/query_toy_s_NA00001_NA00002.vcf\n```"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "header_analysis"
    },
    {
      "confidence": 1,
      "result": {
        "original_header": "Note",
        "parent_header": [
          "GSC (Genotype Sparse Compression)",
          "Example",
          "Query"
        ],
        "type": "Text_excerpt",
        "value": "You can also perform mixed queries based on sample names and variants.\n"
      },
      "source": "https://raw.githubusercontent.com/luo-xiaolong/GSC/master/README.md",
      "technique": "header_analysis"
    }
  ]
}